{"pages":[{"title":"关于我","text":"大家好，我是江南一点雨，管理学学士，大学自学 Java 编程，从移动端到前端到后端均有涉猎，现在专注于 Java 微服务，我是 CSDN 博客专家、华为云云享专家、《Spring Boot + Vue全栈开发实战》 作者、运营了一个公众号 江南一点雨，专注于 Spring Boot + 微服务技术分享，欢迎大家关注！关注公众号回复 Java，领取松哥为大家精心准备的 Java 干货！ 我的公众号 关于我 关于我，一个野生程序员的自我修养！ 我的站点 独立站点： http://www.javaboy.org GitHub： http://github.com/lenve CSDN： http://wangsong.blog.csdn.net 思否： http://segmentfault.com/u/lenve 博客园： http://www.cnblogs.com/lenve 掘金： http://juejin.im/user/57d679af0bd1d000585012a7","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"","text":"Docker 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } Docker 教程合集(顺序已经整理好) 1 Docker 入门及安装[Docker 系列-1] docker 如日中天，这不是单纯的炒概念，docker 确确实实解决了开发与运维的痛点，因此在企业开发中得到了非常广泛的使用，本文对于 docker 的这些基本知识点再做一些简单回顾。 1 Docker 容器基本操作[Docker 系列-2] docker 中的容器就是一个轻量级的虚拟机，是镜像运行起来的一个状态，本文就先来看看容器的基本操作。 1 Docker 容器高级操作[Docker 系列-3] 上篇文章向\b读者介绍了一个 Nginx 的例子，对于 Nginx 这样一个容器而言，当它启动成功后，我们不可避免的需要对 Nginx 进行的配置进行修改，\b那么这个修改要如何完成呢\b？且看下文。 1 Docker 镜像基本操作[Docker 系列-4] 镜像也是 docker 的核心组件之一，镜像时容器运行的基础，容器是镜像运行后的形态。前面我们介绍了容器的用法，今天来和大家聊聊镜像的问题。 1 DockerHub 与容器网络[Docker 系列-5] DockerHub 类似于 GitHub 提供的代码托管服务，Docker Hub 提供了镜像托管服务，Docker Hub 地址为 http://hub.docker.com/。 1 Docker 数据卷操作[Docker 系列-6] 在前面的案例中，如果我们需要将数据从宿主机拷贝到容器中，一般都是使用 Docker 的拷贝命令，这样性能还是稍微有点差，没有办法能够达到让这种拷贝达到本地磁盘 I/O 性能呢？有！ 1 Docker 容器连接[Docker 系列-7] 数据卷容器以及和大家聊过了，本文我们再来看看使用数据卷容器实现数据的备份与恢复，然后再来看看容器的连接操作。 1 Docker 容器编排入门[Docker 系列-8] 在实际的开发环境或者生产环境，容器往往都不是独立运行的，经常需要多个容器一起运行，此时，如果继续使用 run 命令启动容器，就会非常不便，在这种情况下，docker-compose 是一个不错的选择，使用 docker-compose 可以实现简单的容器编排,本文就来看看 docker-compose 的使用。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【江南一点雨】，江南一点雨专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【江南一点雨】，【江南一点雨】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/docker/index.html"},{"title":"","text":"前后端分离教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } 前后端分离教程合集(持续更新) 1 一个Java程序猿眼中的前后端分离以及Vue.js入门 松哥的书里边，其实有涉及到 Vue，但是并没有详细说过，原因很简单，Vue 的资料都是中文的，把 Vue.js 官网的资料从头到尾浏览一遍该懂的基本就懂了，个人感觉这个是最好的 Vue.js 学习资料 1 Spring Boot + Vue 前后端分离，两种文件上传方式总结 在Vue.js 中，如果网络请求使用 axios ，并且使用了 ElementUI 库，那么一般来说，文件上传有两种不同的实现方案： 通过 Ajax 实现文件上传 通过 ElementUI 里边的 Upload 组件实现文件上传 1 Spring Boot + Vue 前后端分离开发，前端网络请求封装与配置 前端网络访问，主流方案就是 Ajax，Vue 也不例外，在 Vue2.0 之前，网络访问较多的采用 vue-resources，Vue2.0 之后，官方不再建议使用 vue-resources ，这个项目本身也停止维护，目前建议使用的方案是 axios。 1 Spring Boot + Vue 前后端分离开发，权限管理的一点思路 在传统的前后端不分的开发中，权限管理主要通过过滤器或者拦截器来进行（权限管理框架本身也是通过过滤器来实现功能），如果用户不具备某一个角色或者某一个权限，则无法访问某一个页面。 1 前后端分离时代，Java 程序员的变与不变！ 事情的起因是这样的，有个星球的小伙伴向邀请松哥在知乎上回答一个问题，原题是： 1 学艺不精，总是掉坑！前后端分离历险记 Spring Boot + Vue 这一对技术栈目前看来可以说是非常的火热，关于 Spring Boot 松哥已经写过多篇教程，如 1 两个开源的 Spring Boot + Vue 前后端分离项目可以在线体验了 折腾了一周的域名备案昨天终于搞定了。松哥第一时间想到赶紧把微人事和 V 部落部署上去，我知道很多小伙伴已经等不及了。 1 使用 Nginx 部署前后端分离项目，解决跨域问题 前后端分离这个问题其实松哥和大家聊过很多了，上周松哥把自己的两个开源项目部署在服务器上以帮助大家可以快速在线预览（喜大普奔，两个开源的 Spring Boot + Vue 前后端分离项目可以在线体验了），然后群里就有小伙伴想让松哥来聊聊如何结合 Nginx 来部署前后端分离项目？今天我们就来聊一聊这个话题。 1 前后端分离开发中动态菜单的两种实现方案 关于前后端分离开发中的权限处理问题，松哥之前写过一篇文章和大家聊这个问题： 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【江南一点雨】，江南一点雨专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【江南一点雨】，【江南一点雨】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/fronted-backend/index.html"},{"title":"","text":"Git 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } Git 教程合集(顺序已经整理好) 1 Git 概述 一直以来想出一个 Git 的教程，去年写过一篇，后来没了下文，烂尾了。最近忙里偷闲，还是想把这个 Git 系列写一遍，这次争取写完。 1 Git 基本操作 上篇文章我们简单的介绍了 Git 的诞生和发展，然后也说了 Windows 环境下 Git 的安装和一些基本的配置，本文我们就来说一说 Git 中的一些基本概念和基本操作。 1 Git 中的各种后悔药 Git 强大的撤销、版本回退功能，让我们在开发的过程中能够随意的回到任何一个时间点的状态，本文我们就来看看 Git 中的各种后悔药! 1 Git 分支管理 Svn 中也有分支管理，但是很 low，Git 的分支管理非常强大，本文先不去说分支管理内部到底怎么做的，我们先来看看 Git 中最基本的分支管理操作。 1 Git 关联远程仓库 前面我们介绍的所有操作都是在本地仓库完成的，本文我们主要来看看如何和远程仓库进行交互，为了方便起见，这里远程仓库我们选择 GitHub。 1 Git 工作区储藏 这是一篇计划之外的文章，之所以有这篇文章，是因为有一个小伙伴在阅读Git 分支管理一文时遇到了一个问题，而这个问题又比较典型，因此我想专门来谈谈 Git 中工作区的储藏问题。 1 Git 标签管理 我们可以针对某一次的提交打上一个标签，有点类似于给某次提交取个别名，比如 1.0 版本发布时打个标签叫 v1.0,2.0 版本发布时打个标签叫 v2.0 ，因为每次版本提交的结果都是一连串的哈希码，不容易记忆，打上 v1.0,v2.0 这些具有某种含义的标签后，可以方便我们进行版本管理。 1 Git 学习资料 关于Git的用法我们已经写七篇文章，介绍了Git的不少用法，这些足以应付工作中90%的需求了，剩下的10%就需要小伙伴们在工作中自己慢慢总结了，我这里再给小伙伴们推荐一点Git学习资料，为我们的Git系列画上一个句号。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【江南一点雨】，江南一点雨专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【江南一点雨】，【江南一点雨】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/git/index.html"},{"title":"","text":"MySQL 非基础教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } MySQL 非基础教程合集(持续更新) 1 提高性能，MySQL 读写分离环境搭建(一) MySQL 读写分离在互联网项目中应该算是一个非常常见的需求了。受困于 Linux 和 MySQL 版本问题，很多人经常会搭建失败，今天松哥就给大伙举一个成功的例子 1 提高性能，MySQL 读写分离环境搭建(二) 上篇文章和大家聊了 CentOS7 安装 MySQL5.7 ，这个大家一般装在虚拟机里边，装好了，把虚拟拷贝一份，这样我们就有两个 MySQL ，就可以开始今天的主从搭建了。 1 MySQL 只能做小项目？松哥要说几句公道话！ 松哥上学那会，很多人对 MySQL 有一些偏见，偏见主要集中在以下几方面： MySQL 不支持事务（事实上 MyISAM 有表锁，但是效率比较低） MySQL 存储的数据量比较小，适合小项目，大项目还是得上 Oracle、DB2 等 这么多年过去了，松哥自己在开发中一直是以 MySQL 为主，我觉得我有必要说两句公道话了。 1 北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！ 千万量级的数据，用 MySQL 要怎么存？ 1 What？Tomcat 竟然也算中间件？ 关于 MyCat 的铺垫文章已经写了两篇了： MySQL 只能做小项目？松哥要说几句公道话！ 北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！ 今天是最后一次铺垫，后面就可以迎接大 Boss 了！ 1 分布式数据库中间件 MyCat 搞起来！ 关于 MyCat 的铺垫文章已经写了三篇了： MySQL 只能做小项目？松哥要说几句公道话！ 北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！ What？Tomcat 竟然也算中间件？ 今天终于可以迎接我们的大 Boss 出场了！ 1 数据库分库分表，都有哪些分片规则？ 上次和大伙聊了 MyCat 的安装，今天来说一个新的话题，就是数据库的分片。 1 分布式数据库如何实现主键全局自增？ 前面和大家介绍了 MyCat 中数据库不同的分片规则，从留言中看出大家对分布式数据库中间件还挺感兴趣，因此今天就再来一篇，聊一聊主键全局自增要如何实现。 1 给数据库减负的八个思路 传统的企业级应用，其实很少会有海量应用，因为企业的规模本身就摆在那里，能有多少数据？高并发？海量数据？不存在的！ 不过在互联网公司中，因为应用大多是面向广大人民群众，数据量动辄上千万上亿，那么这些海量数据要怎么存储？光靠数据库吗？肯定不是。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【江南一点雨】，江南一点雨专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【江南一点雨】，【江南一点雨】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/mysql/index.html"},{"title":"标签","text":"","link":"/tags/index.html"},{"title":"","text":"MongoDB 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } MongoDB 教程合集(已排好顺序) 1 Linux 上安装 MongoDB MongoDB 在 Windows 上的安装过程整体上来说并不难，网上的资料也比较多，这里我就不介绍了，我主要说下如何在Linux环境下安装 MongoDB。 1 MongoDB 基本操作 上篇文章我们简单介绍了 MongoDB 安装以及启动命令，本文我们来看看基本的增删改查，对 MongoDB 有一个直观的认识。 1 MongoDB 数据类型 上篇文章我们介绍了 MongoDB 的最基本的增删改查操作，也介绍了一些基础的概念，MongoDB 中每条记录称作一个文档，这个文档和我们平时用的 JSON 有点像，但也不完全一样。 1 MongoDB 文档更新操作 我们在前面的文章中提到过文档的基本的增删改查操作，MongoDB 中提供的增删改查的语法非常丰富，本文我们主要来看看更新都有哪些好玩的语法。 1 MongoDB 文档查询操作(一) 上篇文章我们主要介绍了 MongoDB 的修改操作，本文我们来看看查询操作。 1 MongoDB 文档查询操作(二) 上篇文章我们对 MongoDB 中的查询操作做了简单介绍，本文我们继续来看更丰富的查询操作。 1 MongoDB 文档查询操作（三） 关于 MongoDB 中的查询，我们已经连着介绍了两篇文章了，本文我们来介绍另外一个查询概念游标。 1 MongoDB 查看执行计划 MongoDB 中的 explain() 函数可以帮助我们查看查询相关的信息，这有助于我们快速查找到搜索瓶颈进而解决它，本文我们就来看看 explain() 的一些用法及其查询结果的含义。 1 初识 MongoDB 中的索引 索引就像图书的目录一样，可以让我们快速定位到需要的内容，关系型数据库中有索引，NoSQL 中当然也有，本文我们就先来简单介绍下 MongoDB 中的索引。 1 MongoDB 中各种类型的索引 上篇文章中我们介绍了 MongoDB 中索引的简单操作，创建、查看、删除等基本操作，不过上文我们只介绍了一种类型的索引，本文我们来看看其他类型的索引。 1 MongoDB 固定集合 一般情况下我们创建的集合是没有大小的，可以一直往里边添加文档，这种集合可以动态增长，MongoDB 中还有一种集合叫做固定集合，这种集合的大小是固定的，我可以在创建的时候设置该集合中文档的数目，假设为 100 条，当集合中的文档数目达到 100 条时，如果再向集合中插入文档，则只会保留最新的 100 个文档，之前的文档则会被删除。 1 MongoDB 管道操作符(一) 熟悉 Linux 操作系统的小伙伴们应该知道 Linux 中有管道的说法，可以用来方便的处理数据。 1 MongoDB 管道操作符(二) 上篇文章中我们已经学习了 MongoDB 中几个基本的管道操作符，本文我们再来看看其他的管道操作符。 1 MongoDB 中 MapReduce 使用 玩过 Hadoop 的小伙伴对 MapReduce 应该不陌生，MapReduce 的强大且灵活，它可以将一个大问题拆分为多个小问题，将各个小问题发送到不同的机器上去处理，所有的机器都完成计算后，再将计算结果合并为一个完整的解决方案，这就是所谓的分布式计算。本文我们就来看看 MongoDB 中 MapReduce 的使用。 1 MongoDB 副本集搭建 我们之前的案例都是在单个节点上实现的，在生产环境中这种做法是有风险的，如果服务宕机、崩溃或者硬盘坏了都会对公司业务造成损失，因此我们需要数据备份。 1 MongoDB 副本集配置 上篇文章我们搭建了 MongoDB 副本集的环境，验证了数据已经可以成功的复制，本文我们就来看看 MongoDB 副本集的其他操作。 1 MongoDB副本集其他细节 副本集环境的搭建以及一些基本的操作我们都了解了，本文我们来看看这个数据复制到底是怎么实现的。 1 初识 MongoDB 分片 分片是指将数据拆分，拆分后存放在不同的机器上的过程，以此来降低单个服务器的压力，同时也解决单个服务器硬盘空间不足的问题，让我们可以用廉价的机器实现高性能的数据架构。 1 Java 操作 MongoDB 之前我们介绍的 MongoDB 的操作都是在 shell 命令中写的，在项目开发时我们当然都是用程序去操作 MongoDB 的，本文我们来看看如何用 Java 代码操作 MongoDB。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【江南一点雨】，江南一点雨专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【江南一点雨】，【江南一点雨】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/mongodb/index.html"},{"title":"","text":"Redis 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } Redis 教程合集(顺序已经整理好) 1 Linux 上安装 Redis hello，各位小伙伴们好久不见！那么从今天开始，我想和各位小伙伴分享下 Redis 的用法，本文我们就先来看看什么是 Redis 以及如何安装 Redis。 1 Redis 中的五种数据类型简介 上篇文章我们介绍了如何在 Linux 中安装 Redis，本文我们来了解下 Redis 中的五种数据类型。 1 Redis 字符串 STRING 介绍 上篇文章我们介绍了五种数据类型中一些通用的命令，本文我们来看看 STRING 数据类型独有的操作命令。 1 Redis 字符串 STRING 中 BIT 相关命令 上篇文章我们对 STRING 数据类型中一些基本的命令进行了介绍，但是没有涉及到 BIT 相关的命令，本文我们就来看看几个和 BIT 相关的命令。 1 Redis 列表与集合 前面文章我们介绍了 STRING 的基本命令，本文我们来看看 Redis 中的列表与集合。 1 Redis 散列与有序集合 前面文章我们介绍了列表与集合中的基本命令，本文我们来看看Redis中的散列与有序集合。 1 Redis 中的发布订阅和事务 hello，小伙伴们好久不见！前面我们说了 redis 中的基本数据类型，本文我们来看看 redis 中的发布订阅和事务，因为这两个都比较简单，因此我放在一篇文章中来讲。 1 Redis 快照持久化 redis 的基础知识我们已经准备的差不多了，接下来两篇文章，我想和大家聊聊 redis 持久化这个话题。 1 Redis 之 AOF 持久化 上篇文章和小伙伴们聊了使用快照的方式实现 redis 数据的持久化，这只是持久化的一种方式，本文我们就来看看另一种持久化方式， AOF(append-only file)。 1 Redis 主从复制(一) 前面两篇文章和小伙伴们聊了 redis 中的数据备份问题，也对快照备份和 AOF 备份做了对比，本文我们来聊聊 redis 中的主从复制问题，算是数据备份的第三种解决方案。 1 Redis 主从复制(二) 上篇文章和小伙伴们一起搭建了 redis 主从复制环境，但是还不完善，本文我想再和小伙伴们聊聊主从复制环境搭建的一些细节。 1 Redis 集群搭建 主从的搭建差不多说完了，本文我们来看看集群如何搭建。 1 Jedis 使用 Redis 的知识我们已经介绍的差不多了，本文我们来看看如何使用 Java 操作 redis。 1 Spring Data Redis 使用 上文我们介绍了 Redis，在开发环境中，我们还有另外一个解决方案，那就是 Spring Data Redis 。本文我们就来看看这个东西。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【江南一点雨】，江南一点雨专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【江南一点雨】，【江南一点雨】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/redis/index.html"},{"title":"","text":"最新版 Spring Boot2 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } 最新版 Spring Boot2 教程合集(顺序已经整理好) 1 这一次，我连 web.xml 都不要了，纯 Java 搭建 SSM 环境 在 Spring Boot 项目中，正常来说是不存在 XML 配置，这是因为 Spring Boot 不推荐使用 XML ，注意，并非不支持，Spring Boot 推荐开发者使用 Java 配置来搭建框架 1 创建一个 Spring Boot 项目，你会几种方法？ 现在 Spring Boot 最新稳定版是 2.1.5 ，松哥想针对此写一个系列教程，专门讲 Spring Boot2 中相关的知识点。这个系列，就从本篇开始吧。 1 你真的理解 Spring Boot 项目中的 parent 吗？ 前面和大伙聊了 Spring Boot 项目的三种创建方式，这三种创建方式，无论是哪一种，创建成功后，pom.xml 坐标文件中都有如下一段引用： 1 是时候彻底搞清楚 Spring Boot 的配置文件 application.properties 了！ 在 Spring Boot 中，配置文件有两种不同的格式，一个是 properties ，另一个是 yaml 。 1 Spring Boot中的 yaml 配置简介 搞Spring Boot的小伙伴都知道，Spring Boot中的配置文件有两种格式，properties或者yaml，一般情况下，两者可以随意使用，选择自己顺手的就行了，那么这两者完全一样吗？ 1 Spring Boot 中的静态资源到底要放在哪里？ 当我们使用 SpringMVC 框架时，静态资源会被拦截，需要添加额外配置，之前老有小伙伴在微信上问松哥 Spring Boot 中的静态资源加载问题：“松哥，我的HTML页面好像没有样式？” 1 极简 Spring Boot 整合 Thymeleaf 页面模板 虽然现在慢慢在流行前后端分离开发，但是据松哥所了解到的，还是有一些公司在做前后端不分的开发，而在前后端不分的开发中，我们就会需要后端页面模板 1 Spring Boot 整合 Freemarker，50 多行配置是怎么省略掉的？ Spring Boot2 系列教程接近完工，最近进入修修补补阶段。Freemarker 整合貌似还没和大家聊过，因此今天把这个补充上。 1 Spring Boot 中关于自定义异常处理的套路！ 在 Spring Boot 项目中 ，异常统一处理，可以使用 Spring 中 @ControllerAdvice 来统一处理，也可以自己来定义异常处理方案。Spring Boot 中，对异常的处理有一些默认的策略 1 Spring Boot 中通过 CORS 解决跨域问题 今天和小伙伴们来聊一聊通过 CORS 解决跨域问题。 1 SpringMVC 中 @ControllerAdvice 注解的三种使用场景！ @ControllerAdvice ，很多初学者可能都没有听说过这个注解，实际上，这是一个非常有用的注解，顾名思义，这是一个增强的 Controller。 1 Spring Boot 操作 Redis，三种方案全解析！ 在 Redis 出现之前，我们的缓存框架各种各样，有了 Redis ，缓存方案基本上都统一了，关于 Redis，松哥之前有一个系列教程，尚不了解 Redis 的小伙伴可以参考这个教程： 1 Spring Boot 中，Redis 缓存还能这么用！ 经过 Spring Boot 的整合封装与自动化配置，在 Spring Boot 中整合 Redis 已经变得非常容易了，开发者只需要引入 Spring Data Redis 依赖，然后简单配下 redis 的基本信息，系统就会提供一个 RedisTemplate 供开发者使用 1 Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！ 有的人可能会觉得题目有点夸张，其实不夸张，题目没有使用任何修辞手法！认真读完本文，你就知道松哥说的是对的了！ 1 另一种缓存，Spring Boot 整合 Ehcache 用惯了 Redis ，很多人已经忘记了还有另一个缓存方案 Ehcache ，是的，在 Redis 一统江湖的时代，Ehcache 渐渐有点没落了，不过，我们还是有必要了解下 Ehcache ，在有的场景下，我们还是会用到 Ehcache。 1 徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！ 我们使用 Spring Boot，基本上都是沉醉在它 Stater 的方便之中。Starter 为我们带来了众多的自动化配置，有了这些自动化配置，我们可以不费吹灰之力就能搭建一个生产级开发环境 1 Spring Boot 定义系统启动任务，你会几种方式？ 在 Servlet/Jsp 项目中，如果涉及到系统任务，例如在项目启动阶段要做一些数据初始化操作，这些操作有一个共同的特点，只在项目启动时进行，以后都不再执行，这里，容易想到 web 基础中的三大组件 1 Spring Boot 数据持久化之 JdbcTemplate 在 Java 领域，数据持久化有几个常见的方案，有Spring自带的 JdbcTemplate、有 MyBatis，还有 JPA，在这些方案中，最简单的就是 Spring 自带的 JdbcTemplate 了 1 Spring Boot 多数据源配置之 JdbcTemplate 多数据源配置也算是一个常见的开发需求，Spring 和 SpringBoot 中，对此都有相应的解决方案，不过一般来说，如果有多数据源的需求，我还是建议首选分布式数据库中间 MyCat 去解决相关问题 1 最简单的 SpringBoot 整合 MyBatis 教程 前面两篇文章和读者聊了 Spring Boot 中最简单的数据持久化方案 JdbcTemplate，JdbcTemplate 虽然简单，但是用的并不多，因为它没有 MyBatis 方便 1 极简 Spring Boot 整合 MyBatis 多数据源 关于多数据源的配置，前面和大伙介绍过 JdbcTemplate 多数据源配置，那个比较简单，本文来和大伙说说 MyBatis 多数据源的配置。 其实关于多数据源 1 一文读懂 Spring Data Jpa！ 有很多读者留言希望松哥能好好聊聊 Spring Data Jpa! 其实这个话题松哥以前零零散散的介绍过，在我的书里也有介绍过 1 是时候了解下 Spring Boot 整合 Jpa 啦 Spring Boot 中的数据持久化方案前面给大伙介绍了两种了，一个是 JdbcTemplate ，还有一个 MyBatis，JdbcTemplate 配置简单，使用也简单，但是功能也非常有限 1 Spring Boot 整合 Jpa 多数据源 本文是 Spring Boot 整合数据持久化方案的最后一篇，主要和大伙来聊聊 Spring Boot 整合 Jpa 多数据源问题。在 Spring Boot 整合 JbdcTemplate 多数据源、Spring Boot 整合 MyBatis 多数据源以及 Spring Boot 整合 Jpa 多数据源这三个知识点中 1 Spring Boot 中 10 行代码构建 RESTful 风格应用 RESTful ，到现在相信已经没人不知道这个东西了吧！关于 RESTful 的概念，我这里就不做过多介绍了，传统的 Struts 对 RESTful 支持不够友好 ，但是 SpringMVC 对于 RESTful 提供了很好的支持，常见的相关注解有： 1 Spring Boot 整合 Shiro ，两种方式全总结！ 在 Spring Boot 中做权限管理，一般来说，主流的方案是 Spring Security ，但是，仅仅从技术角度来说，也可以使用 Shiro。 1 干货|一个案例学会 Spring Security 中使用 JWT 在前后端分离的项目中，登录策略也有不少，不过 JWT 算是目前比较流行的一种解决方案了，本文就和大家来分享一下如何将 Spring Security 和 JWT 结合在一起使用，进而实现前后端分离时的登录解决方案。 1 SpringSecurity 中的角色继承问题 今天想和小伙伴们来聊一聊 Spring Security 中的角色继承问题。 1 SpringSecurity登录添加验证码 登录添加验证码是一个非常常见的需求，网上也有非常成熟的解决方案，其实，要是自己自定义登录实现这个并不难，但是如果需要在 Spring Security 框架中实现这个功能，还得稍费一点功夫 1 SpringSecurity登录使用JSON格式数据 在使用 SpringSecurity 中，大伙都知道默认的登录数据是通过 key/value 的形式来传递的，默认情况下不支持 JSON格式的登录数据 1 Spring Boot 中实现定时任务的两种方式 在 Spring + SpringMVC 环境中，一般来说，要实现定时任务，我们有两中方案，一种是使用 Spring 自带的定时任务处理器 @Scheduled 注解，另一种就是使用第三方框架 Quartz 1 SpringBoot 整合 Swagger2 ，再也不用维护接口文档了！ 前后端分离后，维护接口文档基本上是必不可少的工作。一个理想的状态是设计好后，接口文档发给前端和后端，大伙按照既定的规则各自开发，开发好了对接上了就可以上线了。 1 整理了八个开源的 Spring Boot 学习资源 今天松哥整理了几个优质 Spring Boot 开源项目给大家参考，希望能够帮助到正在学习 Spring Boot 的小伙伴！ 1 松哥整理了 15 道 Spring Boot 高频面试题，看完当面霸 什么是面霸？就是在面试中，神挡杀神佛挡杀佛，见招拆招，面到面试官自惭形秽自叹不如！松哥希望本文能成为你面霸路上的垫脚石！ 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【江南一点雨】，江南一点雨专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【江南一点雨】，【江南一点雨】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/springboot/index.html"}],"posts":[{"title":"2019 Java 全栈工程师进阶路线图，一定要收藏","text":"技术更新日新月异，对于初入职场的同学来说，经常会困惑该往那个方向发展，这一点松哥是深有体会的。 我刚开始学习 Java 那会，最大的问题就是不知道该学什么，以及学习的顺序，我相信这也是很多初学者经常面临的问题。​我当时经常胡子眉毛一把抓，那会学习资料倒是不缺，学校图书馆啥都有，就是无从下手，后来有高人指导之后，进步就很快了。 精研某一个方向，或者走全栈路线，都是可以的，两种路线各有优缺点，如果非要整个争个高下，我觉得没有必要。 自己喜欢的，才是最好的，我读书的时候就接过几百块钱的小活，没几个钱，要是跟人合作更没有赚头了，所以后来我一直在尝试全栈的路线，也一直在这条路上努力。 专精于某一方面，成为某一个领域的执牛耳者，也是让人钦佩了。 结合我自己的经验，我整理了一份 Java 全栈工程师进阶路线图，给大家参考。我整理出来的大部分知识点都有相关的学习资源，大家在公众号后台回复相应的口令就可以获取相关资源（学习资源口令）。 希望大家明白，如果你是在校学生，有大把时间，个人觉得这些东西可以挨个去学，如果你已经工作了，可以根据公司的业务需求有针对性的去学习，下面的列表仅仅起一个参考的作用，当你想学的时候，知道有哪些东西需要学习。 乾坤大挪移第一层第一层心法，主要都是基本语法，程序设计入门，悟性高者十天半月可成，差一点的 3 到 6 个月也说不准。如果有其他开发语言的功底相助，并且有张无忌的悟性与运气，相信第一层只在片刻之间就练成了。 第一层主要包括如下部分（已经列好顺序）： Java 基础语法 Java 面向对象 Java 常用类详解 Java 异常机制 Java 集合与数据结构 Java IO Java 多线程 Java 网络编程 Java 注解+反射 23 种设计模式 正则表达式 XML 解析/ JSON 解析 Java 10、11、12 新特性 AIO、BIO、NIO 乾坤大挪移第二层第二层主要是修炼数据库，从基本用法到查询优化、读写分离等等都需要掌握，这里以 MySQL 数据库为例： 数据库的基本概念 数据库和表的基本操作 索引与数据完整性约束 数据库中的各种复杂查询操作 MySQL 中常见函数的使用 存储过程、触发器以及事件等 数据库的备份与恢复 数据库用户管理与数据库安全性 事务和多用户 读写分离环境搭建+实践 JDBC 常见数据库连接池的配置+使用 乾坤大挪移第三层第三层主要是修炼 Web 基础，主要包括前端的基础知识，先不用深入学习前端，后端的 Jsp/Servlet，有人会说现在公司都不用 Jsp/Servlet 了，还学这些干嘛？但是万变不离其宗，哪个顶尖高手不是从扎马步开始的？这些掌握好了，框架的原理才好理解。 HTML CSS JavaScript jQuery Tomcat Servlet（基本用法，Session、Cookie 等） Jsp（原理、九大内置对象等） EL 和 JSTL 过滤器/监听器等 Ajax EasyUI ECharts BootStrap Git/Svn WebSocket 前三层练好后，做个大学的毕设应该是够用了。 乾坤大挪移第四层第四层主要是修炼各种框架以及工具： Spring SpringMVC MyBatis Maven/Gradle Freemarker/Thymeleaf Linux ActiveMQ/RabbitMQ Netty Zookeeper Dubbo Redis RBAC Shiro Elasticsearch Nginx SSO Activiti Quartz Spring Batch MongoDB Spring Cache Jpa 这一层修炼完，你已经基本上达到了阳顶天的水平了，阳顶天是明教教主，那你出去找个项目经理的位置坐坐估计差不多吧。 乾坤大挪移第五层最后还不得不说 Java 目前最火的微服务，这也是一项必备技能： Spring Boot 基本原理 Spring Boot 基础配置 Spring Boot 整理视图层技术 Spring Boot 整合 Web 开发 Spring Boot 整合持久层技术 Spring Boot 整合 NoSQL Spring Boot 构建 RESTful 服务 Spring Boot 整合各种缓存 Spring Boot 安全管理 Spring Boot 整合 WebSocket Spring Boot 整合消息服务 Spring Boot 整合 Swagger、邮件等 Spring Boot 应用监控 Spring Cloud Eureka、Consul 微服务注册与消费 Spring Cloud OpenFeign 服务容错保护 Resilience4j Spring Cloud Zuul/Gateway Spring Cloud Config Spring Cloud Bus Spring Cloud Stream Spring Cloud Sleuth/Zipkin Spring Cloud Admin Spring Cloud Alibaba 第五层修炼成功后，让老板加波薪水应该是可以的吧！ 乾坤大挪移第六层第六层主要是修炼前端，前端这几年发展的非常快，早已经不是画页面了，JavaScript 被玩的越来越溜了，所以前端不可小觑： HTML5 新特性 CSS3 ES6 JS 模块化 less NodeJS Webpack Grunt Gulp Zepto mpvue AngularJS/Vue/React（非专业前端建议修炼其中一个即可） 据说乾坤大挪移的作者也只修炼到这一层。 练习到这一层，基本上前后端通杀，做个架构师，或者独立接私活妥妥的了。 乾坤大挪移第七层本层主要修炼： 《养发护发指南》 《颈椎病康复指南》 《腰椎间盘突出康复指南》 ….. 祝大家早日习得盖世神功！","link":"/2019/0715/java-fullstack.html"},{"title":"40 篇原创干货，带你进入 Spring Boot 殿堂！","text":"两个月前，松哥总结过一次已经完成的 Spring Boot 教程，当时感受到了小伙伴们巨大的热情。 两个月过去了，松哥的 Spring Boot 教程又更新了不少，为了方便小伙伴们查找，这里再给大家做一个索引参考。 需要再次说明的是，这一系列教程不是终点，而是一个起点，松哥后期还会不断完善这个教程，也会持续更新 Spring Boot 最新版本的教程，希望能帮到大家。教程索引如下： Spring Boot2 教程合集入门 纯 Java 代码搭建 SSM 环境 创建一个 Spring Boot 项目的三种方法 理解 Spring Boot 项目中的 parent 基础配置 配置文件 application.properties yaml配置简介 Spring Boot 支持 Https 徒手撸一个 Spring Boot 中的 Starter 条件注解，Spring Boot 的基石！ 整合视图层 Spring Boot 整合 Thymeleaf Spring Boot 整合 Freemarker 整合 Web 开发 Spring Boot 中的静态资源 @ControllerAdvice 注解的三种使用场景！ Spring Boot 异常处理方案 CORS 解决跨域问题 Spring Boot 定义系统启动任务 Spring Boot 中实现定时任务 SpringBoot整合Swagger2 整合持久层技术 Spring Boot 整合 JdbcTemplate Spring Boot 整合 JdbcTemplate 多数据源 SpringBoot 整合 MyBatis Spring Boot 整合 MyBatis 多数据源 一文读懂 Spring Data Jpa！ Spring Boot 整合 Jpa 的教程欢迎大家在松哥的个人博客(http://www.javaboy.org)上查看，之前发布在公众号上的教程总是被公众号官方判断为有敏感词，但我一直没找到相关敏感词，所以文章总是发送失败。 整合 NoSQL Spring Boot 操作 Redis Nginx 极简入门教程！ Spring Boot 一个依赖搞定 session 共享 整合缓存框架 Spring Boot + Spring Cache + Redis Spring Boot + Spring Cache + Ehcache 构建 REST 服务 10 行代码构建 RESTful 风格应用 安全管理 Spring Boot 整合 Shiro 手把手带你入门 Spring Security！ Spring Security 登录添加验证码 SpringSecurity 登录使用 JSON 格式数据 Spring Security 中的角色继承问题 Spring Security 中使用 JWT! 热部署 LiveReload 使用 打包 可执行 jar 与普通 jar 企业开发 Spring Boot 整合邮件发送 Spring Boot 中的 Bug Spring Boot2.1.5 中的 Bug 其他资料 15 个 Spring Boot 高频面试题 八个开源的 Spring Boot 学习资源 案例另外，还有一件重要的事，就是松哥把微信公众号中文章的案例，都整理到 GitHub 上了，每个案例都对应了一篇解读的文章，方便大家学习。松哥以前写博客没养成好习惯，有的案例丢失了，现在在慢慢整理补上。 GitHub 仓库地址：https://github.com/lenve/javaboy-code-samples，欢迎大家 star。已有的案例如下图： 电子书为了方便大家学习，松哥同时整理了一个在线电子书，地址：http://springboot.javaboy.org，如下图： 在线电子书内容和公众号上面的一样，不过大家在 pc 端打开方便一些。 另外需要强调的是，这个总结不是结束，而是一个新的开始，Spring Boot2.1.7 8 月 6 号发布，松哥会继续追踪，继续产出最新版的教程，欢迎小伙伴们继续关注。 好了，这就是松哥说的干货，大家撸起袖子加油学吧！如果这个资料帮到你了，欢迎转发或者右下角在看哦。","link":"/2019/0826/springboot-guide.html"},{"title":"Docker 入门及安装[Docker 系列-1]","text":"docker 如日中天，这不是单纯的炒概念，docker 确确实实解决了开发与运维的痛点，因此在企业开发中得到了非常广泛的使用，本文对于 docker 的这些基本知识点再做一些简单回顾。 什么是 docker根据 wikipedia 中的介绍： Docker 是一个开放源代码软件项目，让应用程序布署在软件容器下的工作可以自动化进行，借此在 Linux 操作系统上，提供一个额外的软件抽象层，以及操作系统层虚拟化的自动管理机制。Docker 利用 Linux 核心中的资源分脱机制，例如 cgroups ，以及 Linux 核心名字空间（name space），来创建独立的软件容器（containers）。这可以在单一 Linux 实体下运作，避免启动一个虚拟机造成的额外负担。Linux 核心对名字空间的支持完全隔离了工作环境中应用程序的视野，包括进程树、网络、用户 ID 与挂载文件系统，而核心的 cgroup 提供资源隔离，包括 CPU 、存储器、block I/O 与网络。从 0.9 版本起，Dockers 在使用抽象虚拟是经由 libvirt 的 LXC 与 systemd - nspawn 提供界面的基础上，开始包括 libcontainer 库做为以自己的方式开始直接使用由 Linux 核心提供的虚拟化的设施。依据行业分析公司“451研究”：“Dockers 是有能力打包应用程序及其虚拟容器，可以在任何 Linux 服务器上运行的依赖性工具，这有助于实现灵活性和便携性，应用程序在任何地方都可以运行，无论是公有云、私有云、单机等。” 。 这里的介绍有点绕口，让我来介绍下 docker 解决了哪些痛点： 简化\b环境管理 传统的软件开发与发布环境复杂，配置繁琐，经常有读者在微信上问：我的代码开发环境可以运行，一旦部署到服务器上就运行不了了。这个问题\b很常见，也确实很烦人，但是问题总要解决，开发环境、测试环境、生产环境，每个环节都有可能出现这样那样的问题，如果能够在各个环境中实现一键部署，就会方便很多，例如一键安装 linux 、一键安装 mysql、一键安装 nginx 等，docker 彻底解决了这个问题。 虚拟化更加轻量级 说到容器，说到虚拟化，很多人总会想到虚拟机，想到 VMware、VirtualBox 等工具，不同于这些虚拟技术，docker 虚拟化更加轻量级，传统的虚拟机都是先虚拟出一个操作系统，然后在操作系统上完成各种各样的配置，这样并不能充分的利用物理机的性能，docker 则是一种操作系统级别的虚拟技术，它运行在操作系统之上的用户空间，所有的容器都共用一个系统内核甚至公共库，容器引擎提供了进程级别的隔离，让每个容器都像运行在单独的系统之上，但是又能够共享很多底层资源。因此 docker 更为轻量、快速和易于管理。 程序可移植 有了前面介绍的两个特点，程序可移植就是顺理成章的事情了。 docker 和虚拟机前面介绍了 docker 与传统虚拟机的差异，通过下表再来详细了解下这种差异： docker 虚拟机 相同点 1. 都可在不同的主机之间迁移2. 都具备 root 权限3. 都可以远程控制4. 都有备份、回滚操作 操作系统 在性能上有优势，可以轻易的运行多个操作系统 可以安装任何系统，但是性能不及容器 原理 和宿主机共享内核，所有容器运行在容器引擎之上，容器并非一个完整的操作系统，\b所有容器共享操作系统，在进程级进行隔离 每一个虚拟机都建立在虚拟的硬件之上，提供指令级的虚拟，具备一个完整的操作系统 优点 高效、集中。一个硬件\b节点可以运行数以百计的的容器，非常节省资源，QoS 会尽量满足，但不保证一定满足。内核\b由提供者升级，服务由服务提供者管理 对操作系统具有绝对权限，对系统版本和系统升级具有完全的管理权限。具有一整套的的资源：CPU、RAM 和磁盘。QoS 是有保证的，每一个虚拟机就像一个真实的物理机一样，可以实现不同的操作系统运行在同一物理节点上。 资源管理 弹性资源分配：资源可以在没有关闭容器的情况下添加，数据卷也无需重新分配大小 虚拟机需要重启，虚拟机里边的操作系统需要处理新加入的资源，如磁盘等，都需要重新分区。 远程管理 根据操作系统的不同，可以通过 shell 或者远程桌面进行 远程控制由虚拟化平台提供，可以在\b虚拟机启动之前连接 缺点 对内核没有控制权限，只有容器的提供者具备升级权限。只有一个内核运行在物理节点上，几乎不能实现不同的操作系统混合。容器提供者一般仅提供少数的几个操作系统 每一台虚拟机都具有更大的负载，耗费更多的资源，用户需要全权维护和管理。一台物理机上能够运行的虚拟机非常有限 配置 快速，基本上是一键配置 配置时间长 启动时间 秒级 分钟级 硬盘使用 MB GB 性能 接近原生态 弱于原生态 系统支持数量 \b单机支持上千个 一般不多于几十个 docker 与传统容器不同与传统容器，docker 早起基于 LXC，\b后来基于自研的 libContainer，docker 对于传统容器做了许多优化，如下： 跨平台的可移植性 面向应用 版本控制 组件复用 共享性 工具生态系统 docker 应用场景 加速本地开发 自动打包和部署应用 创建轻量、私有的PaaS环境 自动化测试和持续集成/部署 部署并扩展Web应用、数据库和后端服务器 创建安全沙盒 轻量级的桌面虚拟化 docker 核心组件docker 中有三大核心组件： 镜像 镜像是一个只读的静态模版，它保存了容器需要的环境和应用的执行代码，可以将镜像看成是容器的代码，当代码运行起来之后，就成了容器，\b镜像和容器的关系也类似于程序和进程的关系。 容器 容器是一个运行时环境，是镜像的一个运行状态，它是镜像执行的动态表现。 库 库是一个特定的用户存储镜像的目录，一个用户可以建立多个库来保存自己的镜像。 docker相关技术 隔离性 可度量性 移植性 安全性 docker 安装相对而言，Linux 上安装 Docker 是最容易的，其次是 Mac ，最后是 Windows ，Windows 因此要装的东西比较多，官方也提供了两个不同的安装包，支持不同的 Windows 的不同版本，一个是针对 Win10 的安装引导程序，还有一个是兼容性较好的 Toolbox ，但是在 Windows 上运行 Docker ，后期在虚拟目录等方面还会遇到各种问题，所以这里松哥是非常不建议大家在 Windows 中安装 Docker ，有 Mac 的上 Mac （Mac 上安装 Docker 就像安装普通软件一样），没有 Mac 的装 Linux 虚拟机，再装 Docker 即可，这里我就先以 CentOS 上安装 Docker 为例，来说说 Docker 安装。 分别执行如下安装命令： 12345678# 首先安装 Dockeryum -y install docker# 然后启动 Docker 服务service docker start# 测试安装是否成功docker -v 安装完成后，看到如下页面，表示安装成功： 总结本文主要向大家介绍了 Docker 的基本概念以及 Docker 的安装 ，下篇文章我们向大家介绍 Docker 中基本的容器操作。有问题欢迎留言讨论。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0522/docker-install.html"},{"title":"Docker 容器编排入门[Docker 系列-8]","text":"在实际的开发环境或者生产环境，容器往往都不是独立运行的，经常需要多个容器一起运行，此时，如果继续使用 run 命令启动容器，就会非常不便，在这种情况下，docker-compose 是一个不错的选择，使用 docker-compose 可以实现简单的容器编排,本文就来看看 docker-compose 的使用。 本文以 jpress 这样一个开源网站的部署为例，向读者介绍 docker-compose 的使用。jpress 是 Java 版的 wordPress ，不过我们不必关注 jpress 的实现，在这里我们只需要将之当作一个普通的应用即可，完成该项目的部署工作。 准备工作这里我们一共需要两个容器： Tomcat MySQL 然后需要 jpress 的 war 包，war 包地址：jpress 当然，这里的 jpress 并不是必须的，读者也可以结合自身情况，选择其他的 Java 项目或者自己写一个简单的 Java 项目部署都行。 编写 DockerfileTomcat 容器中，要下载相关的 war 等，因此我这里编写一个 Dockerfile 来做这个事。在一个空的文件夹下创建 Dockerfile ，内容如下： 1234FROM tomcatADD https://github.com/JpressProjects/jpress/raw/alpha/wars/jpress-web-newest.war /usr/local/tomcat/webapps/RUN cd /usr/local/tomcat/webapps/ \\ &amp;&amp; mv jpress-web-newest.war jpress.war 解释： 容器基于 Tomcat 创建。 下载 jpress 项目的 war 包到 tomcat 的 webapps 目录下。 给 jpress 项目重命名。 编写 docker-compose.yml在相同的目录下编写 docker-compose.yml ，内容如下（关于 yml 的基础知识，这里不做介绍，读者可以自行查找了解）： 123456789101112131415161718192021version: \"3.1\"services: web: build: . container_name: jpress ports: - \"8080:8080\" volumes: - /usr/local/tomcat/ depends_on: - db db: image: mysql container_name: mysql command: --default-authentication-plugin=mysql_native_password restart: always ports: - \"3306:3306\" environment: MYSQL_ROOT_PASSWORD: 123 MYSQL_DATABASE: jpress 解释： 首先声明了 web 容器，然后声明db容器。 build . 表示 web 容器项目构建上下文为 . ，即，将在当前目录下查找 Dockerfile 构建 web 容器。 container_name 表示容器的名字。 ports 是指容器的端口映射。 volumes 表示配置容器的数据卷。 depends_on 表示该容器依赖于 db 容器，在启动时，db 容器将先启动，web 容器后启动，这只是启动时机的先后问题，并不是说 web 容器会等 db 容器完全启动了才会启动。 对于 db 容器，则使用 image 来构建，没有使用 Dockerfile 。 restart 描述了容器的重启策略。 environment 则是启动容器时的环境变量，这里配置了数据库 root 用户的密码以及在启动时创建一个名为 jpress 的库，environment 的配置可以使用字典和数组两种形式。 OK，经过如上步骤，docker-compose.yml 就算配置成功了 运行运行的方式有好几种，但是建议使用 up 这个终极命令，up 命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。对于大部分应用都可以直接通过该命令来启动。默认情况下， docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试，通过 Ctrl-C 停止命令时，所有容器将会停止，而如果使用 docker-compose up -d 命令，则将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 因此，这里进入到 docker-compose.yml 所在目录下，执行如下命令： 1docker-compose up -d 执行结果如下： 执行后，通过 docker-compose ps 命令可以看到容器已经启动了。 初始化配置接下来，浏览器中输入 http://localhost:8080/jpress ，就可以看到 jpress 的配置页面，如下： 根据引导页面配置数据库的连接信息以及网站的基本信息： 注意：由于 mysql 和 web 都运行在容器中，因此在配置数据库地址时，不能写回环地址，否则就去 web 所在的容器里找数据库了。 配置完成后，运行如下命令，重启 web 容器： 1docker restart jpress 测试浏览器中分别查看博客首页以及后台管理页，如下图： 其他如果想要停止容器的运行，可以执行如下命令： 1docker-compose down 总结本文主要向大家介绍了简单的容器编排，专业的或者大型项目的容器编排需要结合 K8s 来做，我们后面有机会再向大家介绍。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0602/docker-container-arrangement.html"},{"title":"Docker 容器连接[Docker 系列-7]","text":"数据卷容器以及和大家聊过了，本文我们再来看看使用数据卷容器实现数据的备份与恢复，然后再来看看容器的连接操作。 利用数据卷容器可以实现实现数据的备份和恢复。 数据备份与恢复备份数据的备份操作很容易，执行如下命令： 1docker run --volumes-from mydata --name backupcontainer -v $(pwd):/backup/ ubuntu tar cvf /backup/backup.tar /usr/share/nginx/html/ 命令解释： 首先使用 --volumes-from 连接待备份容器。 -v 参数用来将当前目录挂载到容器的 /backup 目录下。 接下来，将容器中 /usr/share/nginx/html 目录下的内容备份到 /backup 目录下的 backup.tar 文件中，由于已经设置将当前目录映射到容器的 /backup 目录，因为备份在容器 /backup 目录下的压缩文件在当前目录下可以立马看到。 执行结果如下： 备份完成后，在当前目录下就可以看到 /backup 文件，打开压缩文件，发现就是 /usr/share/nginx/html 目录及内容。 恢复数据的恢复则稍微麻烦一些，操作步骤如下： 创建容器首先创建一个容器，这个容器就是要使用恢复的数据的容器，我这里创建一个 nginx 容器，如下： 1docker run -itd -p 80:80 -v /usr/share/nginx/html/ --name nginx3 nginx 创建一个名为 nginx3 的容器，并且挂载一个数据卷。 恢复数据恢复需要一个临时容器，如下：1docker run --volumes-from nginx3 -v $(pwd):/backup nginx tar xvf /backup/backup.tar 命令解释： 首先还是使用 --volumes-from 参数连接上备份容器，即第一步创建出来的 nginx3 。 然后将当前目录映射到容器的 /backup 目录下。 然后执行解压操作，将 backup.tar 文件解压。解压文件位置描述是一个容器内的地址，但是该地址已经映射到宿主机中的当前目录了，因此这里要解压缩的文件实际上就是宿主机当前目录下的文件。 容器连接一般来说，容器启动后，我们都是通过端口映射来使用容器提供的服务，实际上，端口映射只是使用容器服务的一种方式，除了这种方式外，还可以使用容器连接的方式来使用容器服务。 例如，有两个容器，一个容器运行一个 SpringBoot 项目，另一个容器运行着 mysql 数据库，可以通过容器连接使 SpringBoot 直接访问到 Mysql 数据库，而不必通过端口映射来访问 mysql 服务。 为了案例简单，我这里举另外一个例子： 有两个容器，一个 nginx 容器，另一个 ubuntu ，我启动 nginx 容器，但是并不分配端口映射，然后再启动 ubuntu ，通过容器连接，在 ubuntu 中访问 nginx 。 具体操作步骤如下： 首先启动一个 nginx 容器，但是不分配端口，命令如下： 1docker run -d --name nginx1 nginx 命令执行结果如下： 容器启动成功后，在宿主机中是无法访问的。 启动ubuntu 接下来，启动一个 ubuntu ，并且和 nginx 建立连接，如下： 1docker run -dit --name ubuntu --link nginx1:mylink ubuntu bash 这里使用 –link 建立连接，nginx1 是要建立连接的容器，后面的 mylink 则是连接的别名。 运行成功后，进入到 ubuntu 命令行： 1docker exec -it ubuntu bash 然后，有两种方式查看 nginx 的信息： 第一种 在 ubuntu 控制台直接输入 env ，查看环境变量信息： 可以看到 docker 为 nginx 创建了一系列环境变量。每个前缀变量是 MYLINK ，这就是刚刚给连接取得别名。开发者可以使用这些环境变量来配置应用程序连接到 nginx 。该连接是安全、私有的。 访问结果如下： 第二种 另一种方式则是查看 ubuntu 的 hosts 文件，如下： 可以看到，在 ubuntu 的 hosts 文件中已经给 nginx1 取了几个别名，可以直接使用这些别名来访问 nginx1 。 小贴士： 默认情况下，ubuntu 容器中没有安装 curl 命令，需要手动安装下，安装命令如下：apt-get updateapt-get install curl 总结本文主要向大家介绍了 Docker 容器中的数据备份与恢复操作，同时也向大家介绍了容器连接相关的操作，不知道你学会了吗？ 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0601/docker-link.html"},{"title":"Docker 容器高级操作[Docker 系列-3]","text":"上篇文章向\b读者介绍了一个 Nginx 的例子，对于 Nginx 这样一个容器而言，当它启动成功后，我们不可避免的需要对 Nginx 进行的配置进行修改，\b那么这个修改要如何完成呢\b？且看下文。 依附容器docker attach 依附容器这个主要是针对交互型容器而言的，该命令有一定的局限性，可以作为了解即可，真正工作中使用较少。 要是用 docker attach 命令，首先要\b确保容器已经启动，然后使用该命令才能\b进入到容器中。具体操作步骤如下： 创建一个容器，然后启动： 不要关闭当前窗口，再打开一个新的终端，执行 docker attach ubuntu : 此时就能进入到容器的命令行进行操作了。 如果容器已经关闭或者容器是一个后台容器，则该命令就无用武之地了。 由上面的操作大家可以看到，这个命令的局限性很大，使用场景也不多，因此大家作为一个了解即可。 容器内执行命令如果容器在后台启动，则可以使用 docker exec 在容器内执行命令。不同于 docker attach ，使用 docker exec 即使用户从终端退出，容器也不会停止运行，而使用 docker attach 时，如果用户从终端退出，则容器会停止运行。如下图： 基于这样的特性， 我们以后在操作容器内部时，基本上都是通过 docker exec 命令来实现。 查看容器信息容器创建成功后，用户可以\b通过 docker inspect 命令查看容器的详细信息，这些详细信息\b包括容器的\b id 、容器名、环境变量、运行命令、主机配置、网络配置以及数据卷配置等信息。执行部分结果如下图： 使用 format 参数可以只查看用户关心的数据，例如： 查看容器运行状态 查看容器ip地址 查看容器名、容器id 查看容器主机信息 容器的详细信息，在我们后边配置容器共享目录、容器网络时候非常有用，这个我们到后面再来详细介绍。 查看容器进程使用 docker top 命令可以查看容器中正在运行的进程，首先确保容器已经启动，然后执行 docker top 命令，如下： 查看容器日志交互型容器查看日志很方便，因为日志就直接在控制台打印出来了，但是对于后台型容器，如果要查看日志，则可以使用docker提供的 docker logs 命令来查看，\b如下： 如下图，首先启动一个不停打日志的\b容器，然后利用 docker logs 命令查看日志，但是\b默认情况下只能查看到历史日志，无法查看实时日志，使用 -f 参数后，就可以查看实时日志了。 使用 --tail 参数可以\b精确控制日志的输出行数， -t 参数则可以显示日志的输出时间。 该命令在执行的过程中，首先输出最近的三行日志，同时由于添加了 -f 参数，因此，还会有其他日志持续输出。同时，因为添加了 -t 参数，时间随同日志一起打印出来了。 docker 的一大优势就是可移植性，容器因此 docker 容器可以随意的进行导入导出操作。 容器导出既然是容器，我们当然希望 Docker 也能够像 VMWare 那样方便的在不同系统之间拷贝，不过 Docker 并不像 VMWare 导出容器那样方便（事实上，VMWare 中不存在容器导出操作，直接拷贝安装目录即可），在 Docker 中，使用 export 命令可以导出容器，具体操作如下： 创建一个容器，进行基本的配置操作 本案例中我首先创建一个 nginx 容器，然后启动，启动成功后，将本地一个 index.html 文件上传到容器中，修改 nginx 首页的显示内容。具体操作步骤如下： docker run -itd --name nginx -p 80:80 nginx vi ./blog/docker/index.html docker cp ./blog/docker/index.html nginx:/usr/share/nginx/html/ 首先运行一个名为 nginx 的容器，然后在宿主机中编辑一个 index.html 文件，编辑完成后，将该文件上传到容器中。然后在浏览器中输入 http://localhost:80 可以看到如下结果： 容器已经修改成功了。 接下来通过 export 命令将容器导出，如下： 该命令将容器导入到 docker\b 目录下。导出成功之后，我们就可以随意传播这个导出文件了，可以发送给其他小伙伴去使用了，相对于 VMWare 中庞大的文件，这个导出文件非常小。一般可能只有几百兆，当然也看具体情况。 容器导入其他小伙伴拿到这个文件，通过执行如下命令可以导入容器（如果自己重新导入，需要记得将 docker 中和 nginx 相关的容器和镜像删除）： 容器导入成功后，就可以使用 docker run 命令运行了。运行成功之后，我们发现自己定制的 index.html 页面依然存在，说明这是我们自己的那个容器。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0525/docker-container-advanced.html"},{"title":"Docker 数据卷操作[Docker 系列-6]","text":"数据卷入门在前面的案例中，如果我们需要将数据从宿主机拷贝到容器中，一般都是使用 Docker 的拷贝命令，这样性能还是稍微有点差，没有办法能够达到让这种拷贝达到本地磁盘 I/O 性能呢？有！ 数据卷可以绕过拷贝系统，在多个容器之间、容器和宿主机之间共享目录或者文件，数据卷绕过了拷贝系统，可以达到本地磁盘 I/O 性能。 本文先通过一个简单的案例向读者展示数据卷的基本用法。 以前面使用的 nginx 镜像为例，在运行该容器时，可以指定一个数据卷，命令如下： 1docker run -itd --name nginx -v /usr/share/nginx/html/ -p 80:80 bc26f1ed35cf 运行效果如下： 此时，我们创建了一个数据卷并且挂载到容器的 /usr/share/nginx/html/ 目录下，小伙伴们知道，该目录实际上是 nginx 保存 html 目录，在这里挂载数据卷，一会我们只需要修改本地的映射位置，就能实现页面的修改了。 接下来使用 docker inspect 命令查看刚刚创建的容器的具体情况，找到数据卷映射目录，如下： 可以看到，Docker默认将宿主机的 /var/lib/docker/volumes/0746bdcfc045b237a6fe2288a3af9d7b80136cacb3e965db65a212627e217d75/_data 目录作为source目录，接下来，进入到该目录中，如下： 此时发现该目录下的文件内容与容器中 /usr/share/nginx/html/ 目录下的文件内容一致，这是因为挂载一个空的数据卷到容器中的一个非空目录中，那么这个目录下的文件会被复制到数据卷中（如果挂载一个非空的数据卷到容器中的一个目录中，那么容器中的目录中会显示数据卷中的数据。如果原来容器中的目录中有数据，那么这些原始数据会被隐藏掉）。 小贴士： 由于 Mac 中的 Docker 有点特殊，上文提到的 /var/lib/xxxx 目录，如果是在 linux 环境下，则直接进入即可，如果是在 mac 中，需要首先执行如下命令，在新进入的命令行中进入到 /var/lib/xxx 目录下：screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty 接下来修改改文件中的index.html文件内容，如下： 1echo &quot;hello volumes&quot;&gt;index.html 修改完成后，再回到浏览器中，输入 http://localhost查看nginx中index.html 页面中的数据，发现已经发生改变。说明宿主机中的文件共享到容器中去了。 结合宿主机目录上文中对于数据卷的用法还不是最佳方案，一般来说，我们可能需要明确指定将宿主机中的一个目录挂载到容器中，这种指定方式如下： 1docker run -itd --name nginx -v /Users/sang/blog/docker/docker/:/usr/share/nginx/html/ -p 80:80 bc26f1ed35cf 这样便是将宿主机中的 /Users/sang/blog/docker/docker/ 目录挂载到容器的 /usr/share/nginx/html/ 目录下。接下来读者只需要在 /Users/sang/blog/docker/docker/ 目录下添加 html 文件，或者修改 html 文件，都能在 nginx 访问中立马看到效果。 这种用法对于开发测试非常方便，不用重新部署，重启容器等。 注意：宿主机地址是一个绝对路径 更多操作Dockerfile中的数据卷如果开发者使用了 Dockerfile 去构建镜像，也可以在构建镜像时声明数据卷，例如下面这样： 1234FROM nginxADD https://www.baidu.com/img/bd_logo1.png /usr/share/nginx/html/RUN echo &quot;hello docker volume!&quot;&gt;/usr/share/nginx/html/index.htmlVOLUME /usr/share/nginx/html/ 这样就配置了一个匿名数据卷，运行过程中，将数据写入到 /usr/share/nginx/html/ 目录中，就可以实现容器存储层的无状态变化。 查看所有数据卷使用如下命令可以查看所有数据卷： 1docker volume ls 如图： 查看数据卷详情根据 volume name 可以查看数据详情，如下： 1docker volume inspect 执行结果如下图： 删除数据卷可以使用 docker volume rm 命令删除一个数据卷，也可以使用 docker volume prune 批量删除数据卷，如下： 批量删除时，未能删除掉所有的数据卷，还剩一个，这是因为该数据卷还在使用中，将相关的容器停止并移除，再次删除数据卷就可以成功删除了，如图： 数据卷容器数据卷容器是一个专门用来挂载数据卷的容器，该容器主要是供其他容器引用和使用。所谓的数据卷容器，实际上就是一个普通的容器，举例如下： 创建数据卷容器 使用如下方式创建数据卷容器： 1docker run -itd -v /usr/share/nginx/html/ --name mydata ubuntu 命令执行效果如下图： 引用容器 使用如下命令引用数据卷容器： 12docker run -itd --volumes-from mydata -p 80:80 --name nginx1 nginxdocker run -itd --volumes-from mydata -p 81:80 --name nginx2 nginx 此时， nginx1 和 nginx2 都挂载了同一个数据卷到 /usr/share/nginx/html/ 目录下，三个容器中，任意一个修改了该目录下的文件，其他两个都能看到变化。 此时，使用 docker inspect 命令查看容器的详情，发现三个容器关于数据卷的描述都是一致的，如下图： 总结本文主要向大家介绍了数据卷中的容器操作，整体来说还是非常简单的，小伙伴们，你学会了吗？ 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0531/docker-data.html"},{"title":"DockerHub 与容器网络[Docker 系列-5]","text":"DockerHubDockerHub 类似于 GitHub 提供的代码托管服务，Docker Hub 提供了镜像托管服务，Docker Hub 地址为 https://hub.docker.com/。 利用 Docker Hub 读者可以搜索、创建、分享和管理镜像。Docker Hub 上的镜像分为两大类，一类是官方镜像，例如我们之前用到的 nginx、mysql 等，还有一类是普通的用户镜像，普通用户镜像由用户自己上传。对于国内用户，如果觉得 Docker Hub 访问速度过慢，可以使用国内一些公司提供的镜像，例如网易： https://c.163yun.com/hub#/m/home/ 本文使用官方的 Docker Hub 来演示，读者有兴趣可以尝试网易的镜像站。 首先读者打开 Docker Hub ，注册一个账号，这个比较简单，我就不赘述了。 账号注册成功之后，在客户端命令行可以登录我们刚刚注册的账号，如下： 看到 Login Succeeded 表示登录成功！ 登录成功之后，接下来就可以使用 push 命令上传我们自制的镜像了。注意，自制的镜像要能够上传，命名必须满足规范，即 namespace/name 格式，其中 namespace 必须是用户名，以前文我们创建的 Dockerfile 为例，这里重新构建一个本地镜像并上传到 Docker Hub ，如下： 首先调用 docker build 命令重新构建一个本地镜像，构建成功后，通过 docker images 命令可以看到本地已经有一个名为 wongsung/nginx 的镜像，接下来通过 docker push 命令将该镜像上传至服务端。上传成功后，用户登录Docker Hub ，就可以看到刚刚的镜像已经上传成功了，如下： 看到这个表示镜像已经上传成功了，接下来，别人就可以通过如下命令下载我刚刚上传的镜像： 1docker pull wongsung/nginx pull下来之后，就可以直接根据该镜像创建容器了。具体的创建过程读者可以参考我们本系列前面的文章。 自动化构建自动化构建，就是使用 Docker Hub 连接一个包含 Dockerfile 文件的 GitHub 仓库或者 BitBucket 仓库， Docker Hub 则会自动构建镜像，通过这种方式构建出来的镜像会被标记为 Automated Build ，也称之为受信构建 （Trusted Build） ，这种构建方式构建出来的镜像，其他人在使用时可以自由的查看 Dockerfile 内容，知道该镜像是怎么来的，同时，由于构建过程是自动的，所以能够确保仓库中的镜像都是最新的。具体构建步骤如下： 添加仓库 首先登录到 Docker Hub，点击右上角的 Create，然后选择 Create Automated Build ，如下图： 则新进入到的页面，选择 Link Account 按钮，然后，选择连接 GitHub ，在连接方式选择页面，我们选择第一种连接方式，如下： 选择完成后，按照引导登录 GitHub ，完成授权操作，授权完成后的页面如下： 构建镜像 授权完成后，再次点击右上角的 Create 按钮，选择 Create Automated Build ，在打开的页面中选择 GitHub ，如下两张图： 这里展示了刚刚关联的 GitHub 上的仓库，只有一个 docker ，然后点击进去，如下： 填入镜像的名字以及描述，然后点击 Create 按钮，创建结果如下： 如此之后，我们的镜像就算构建成功了，一旦 GitHub 仓库中的 Dockerfile 文件有更新， Docker Hub 上的镜像构建就会被自动触发，不用人工干预，从而保证镜像始终都是最新的。 接下来，用户可以通过如下命令获取镜像： 1docker pull wongsung/nginx2 获取到镜像之后，再运行即可。 有没有觉得很神奇！镜像更新只要更新自己的 GitHub 即可。镜像就会自动更新！事实上，我们使用的大部分 镜像都是这样生成的。 构建自己的 DockerHub前面我们使用的 Docker Hub 是由 Docker 官方提供的，我们也可以搭建自己的 Docker Hub ，搭建方式也很容器，因为 Docker 官方已经将 Docker 注册服务器做成镜像了，我们直接 pull 下来运行即可，没有没很酷！。具体步骤如下： 拉取镜像 运行如下命令拉取registry官方镜像： 1docker pull registry 运行 接下来运行如下命令将registry运行起来，如下： 1docker run -itd --name registry -p 5000:5000 2e2f252f3c88 运行成功后，我们就可以将自己的镜像提交到registry上了，如下： 这里需要注意的是，本地镜像的命名按照 registryHost:registryPort/imageName:tag 的格式命名。 容器运行在宿主机上，如果外网能够访问容器，才能够使用它提供的服务。本文就来了解下容器中的网络知识。 暴露网络端口在前面的文章中，我们已经有用过暴露网络端口相关的命令了，即 -p 参数，实际上，Docker 中涉及暴露网络端口的参数有两个，分别是 -p 和 -P 。下面分别来介绍。 -P 使用 -P，Docker 会在宿主机上随机为应用分配一个未被使用的端口，并将其映射到容器开放的端口，以 Nginx 为例，如下： 可以看到，Docker 为应用分配了一个随机端口 32768 ，使用该端口即可访问容器中的 nginx（http://lcalhost:32768）。 -p -p 参数则有几种不同的用法： hostPort:containerPort 这种用法是将宿主机端口和容器端口绑定起来，如下用法： 如上命令表示将宿主机的80端口映射到容器的80上，第一个 80 是宿主机的 80 ，第二个 80 是容器的 80 。 ip:hostPort:containerPort 这种是将指定的 ip 地址的端口和容器的端口进行映射。如下： 将 192.168.0.195 地址的80端口映射到容器的80端口上。 ip::containerPort 这种是将指定 ip 地址的随机端口映射到容器的开放端口上，如下： 总结本文主要向大家介绍了 DockerHub 和容器网络，DockerHub 是我们容器的集散中心，网络则使我们的容器有办法对外提供服务。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0529/dockerhub-newwork.html"},{"title":"Git 中的各种后悔药","text":"Git 强大的撤销、版本回退功能，让我们在开发的过程中能够随意的回到任何一个时间点的状态，本文我们就来看看 Git 中的各种后悔药! 本文是Git系列的第三篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作 本文将从如下三个方面介绍 Git 中的后悔药： 工作区的代码想撤销 add 到暂存区的代码想撤销 提交到本地仓库的代码想撤销 提交到远程仓库的后悔药我们统一都在关联远程仓库一文中讲解，敬请期待。 工作区的代码想撤销可能有一天我正在写代码，写了很久发现写错了，想恢复到一开始的状态，一个笨办法就是把刚刚写的代码一行一行的删除，不过这种方式成本太高，我们可以通过 git checkout -- &lt;file&gt; 命令来撤销工作区的代码修改。如下图： 首先我们执行了 git status 命令，发现工作区是干净的，然后执行了 cat 命令，发现文件只有两行内容，然后通过 vi 编辑器向文件中添加一行，保存并退出，退出来之后又执行了 git status 命令，此时工作区的状态已经发生变化，然后我们执行了 git checkout – git01.txt 命令，表示撤销之前的操作，让 git01.txt 恢复到之前的状态，该命令执行成功之后，我们再执行 cat 命令发现文件内容已经恢复了，此时再执行 git status ，状态也恢复了。 add 到暂存区的代码想撤销如果想要撤销，但是代码已经提交到暂存区了，不用担心，也能撤销，分两个步骤： 1.将暂存区的代码撤销到工作区2.将工作区的代码撤销(具体操作和’工作区的代码想撤销’一致) 将暂存区的代码撤销，我们可以使用 git reset HEAD 命令来实现。如下图： 这里的代码都比较简单，核心的过程就是先执行 git reset HEAD 命令，从暂存区撤销，剩下的操作参考’工作区的代码想撤销’一节。 提交到本地仓库的代码想撤销同样的，提交到本地仓库的代码一样也可以撤销，我们可以利用 git reset --hard &lt;版本号&gt; 命令来实现版本回退，该命令中的版本号有几种不同的写法： 1.可以使用 HEAD^ 来描述版本，一个 ^ 表示前一个版本，两个 ^^ 表示前两个版本，以此类推。2.也可以使用数字来代替 ^ ，比如说前 100 个版本可以写作 HEAD~100 。3.也可以直接写版本号，表示跳转到某一个版本处。我们每次提交成功后，都会生成一个哈希码作为版本号，所以这里我们也可以直接填版本号，哈希码很长，但是我们不用全部输入，只需要输入前面几个字符即可，就能识别出来。 看下面一系列的操作： 1.通过 git log 查看当前提交日志： 2.通过 git reset HEAD^^ 向前回退两个版本： 3.查看日志，发现最后一次提交的版本号是 695ce1fe ,利用 git reset –hard 695ce1fe 命令回到回退之前的状态： 4.通过 git reset –hard HEAD~1 回到上一个版本： 好了，Git 中的后悔药我们就先介绍到这里，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-delete.html"},{"title":"Git 关联远程仓库","text":"前面我们介绍的所有操作都是在本地仓库完成的，本文我们主要来看看如何和远程仓库进行交互，为了方便起见，这里远程仓库我们选择 GitHub。 本文是 Git 系列的第五篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药4.Git 分支管理 配置 SSH KEYSSH KEY 的配置不是必须的，不配置的话我们就只能使用 HTTPS 协议，这样每次提交时要输入用户名密码，略麻烦，所以还是配置一下。配置 SSH KEY 的原理很简单，采用非对称加密方式生成公钥和私钥，公钥告诉 GitHub ，私钥留在自己电脑上(私钥不可泄露)，当我们向 GitHub 上提交数据时，GitHub 会用我们留给它的公钥加密一段消息返回给我们的电脑，如果我们能够用私钥解密成功，说明是合法的用户，这样就避免我们输入用户名密码了。大致的原理就是这样，现在很多免登录的系统都采用了这种方式，比如 Hadoop 免登录配置也是这样。那我们就来看看这个 SSH KEY 要怎么生成。 1.查看本地是否已有 SSHKEY查看当前用户目录下是否有 .ssh 文件，如下： 如果查看之后有结果，则直接跳转到第四步，什么都没有就继续生成。 2.生成 SSH 指纹生成 SSH 指纹的命令很简单，如下： 1ssh-keygen -t rsa -b 4096 -C &quot;你的邮箱地址&quot; 注意邮箱地址要替换。 3.添加 ssh 到 ssh-agent 中执行如下命令即可： 1eval &quot;$(ssh-agent -s)&quot; OK，做好这一切之后，我们当前用户目录下已经有了一个名为 .ssh 的隐藏文件夹了，打开这个目录，会发现有一个名为 id_rsa.pub 的文件，这就是我们一会要使用的公钥文件。 4.将公钥告诉 GitHub登录 GitHub ，点击右上角的向下的箭头，选择 Settings ，在新打开的页面中左边侧栏选择 SSH and GPG keys ，如下： 完了之后点击最下面的 Add SSH key 按钮即可，如此之后，我们的 SSH KEY 就配置成功了。 创建远程仓库接下来我们在 GitHub 上创建一个仓库，登录成功之后，直接点击右上角绿色的 New repository 按钮，如下： 其实这里我们只需要填一个版本仓库的名字，我填了 test，填好之后，点击 Create repository 就 OK 了。 关联远程仓库创建成功之后，我们会看到仓库的地址，如下： `git@github.com:lenve/test.git` ，然后我需要将我们之前的本地仓库和这个远程仓库进行关联，使用 git remote add 命令，如下： 1$ git remote add origin git@github.com:lenve/test.git 在这条命令中，git 会自动将远程仓库的名字设置为 origin ，方便我们的后续操作。 推送到远程仓库推送到master分支假设我想将本地 master 分支上的内容推送到远程 master 分支上，方式如下： 1$ git push -u origin master -u参数可以在推送的同时，将 origin 仓库的 master 分支设置为本地仓库当前分支的 upstream（上游）。添加了这个参数，将来运行 git pull 命令从远程仓库获取内容时，本地仓库的这个分支就可以直接从 origin 的 master 分支获取内容，省去了另外添加参数的麻烦。这个参数也只用在第一次 push 时加上，以后直接运行 git push 命令即可。 推送到其他分支如果想推送到其他分支，还是这条命令，修改一下分支的名字即可，比如我也想把我的 fa 分支推送到远程仓库中，执行如下命令： 12$ git checkout fa$ git push -u origin fa 先切换到 fa 分支，然后执行 git push 命令，参数含义和之前的一样，这里我们创建的远程仓库的分支名也为 fa（当然我们可以取任何名字，但是为了不混淆，最好取一致的名字）。这两条命令执行成功之后，此时在网页中我们就可以看到已经有多个分支了，如下： 从远程仓库获取首次获取刚刚是我们向远程仓库提交数据，有提交当然就有获取，我们可以通过 git clone 命令克隆一个远程仓库到本地，方式也简单，在本地创建一个空文件夹，执行如下命令： 1$ git clone git@github.com:lenve/test.git 表示克隆文件到本地仓库。此时克隆的远程仓库的 master 分支到本地仓库，我们可以通过 git branch -a 来查看本地仓库和远程仓库的信息，-a 参数可以同时显示本地仓库和远程仓库的信息，如下： 我们看到远程仓库中已经有了 fa 分支了，如果我们想把 fa 分支也克隆下来，执行如下命令： 1$ git checkout -b fa origin/fa 表示根据远程仓库的 fa 分支创建一个本地仓库的 fa 分支，创建完成之后进行切换，也可以通过如下命令只创建不切换： 1$ git branch fa origin/fa 此时我在 fa 分支下修改 git01.txt 文件并提交，如下： 注意由于 fa 分支就是从远程仓库克隆下来的，所以这里可以不添加 -u 参数。 从远程仓库更新此时我们回到第一次最早的那个 test 本地仓库中，那个 test 仓库的 fa 分支现在和远程仓库不一致了，我们可以通过 git pull 命令来更新，如下： Ok，关联远程仓库我们先说这么多。有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-remote.html"},{"title":"Git 分支管理","text":"Svn 中也有分支管理，但是很 low，Git 的分支管理非常强大，本文先不去说分支管理内部到底怎么做的，我们先来看看 Git 中最基本的分支管理操作。 本文是 Git 系列的第四篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药 分支的必要性小伙伴们都知道，我们在完成一个项目时，不可能是“单线程”开发的，很多时候任务是并行的，举个栗子：项目 2.0 版本上线了，现在要着手开发 3.0 版本，同时 2.0 版本可能还有一些 bug 需要修复，这些 bug 修复之后我们可能还会发 2.1，2.2，2.3 这些版本，我们不可能等所有 bug 都修复完了再去开发 3.0 版本，修复 2.0 的 bug 和开发 3.0 的新功能是两个并行的任务，这个时候我们 3.0 的功能开发直接在 master 分支上进行肯定不合适，我们要保证有一个稳定，可以随时发版本的分支存在（一般情况下这个角色由 master 分支来扮演），此时我们就可以灵活的使用 Git 中的分支管理功能： 创建一个长期分支用来开发 3.0 功能，假设这个分支的名字就叫 v3，我们在 v3 上添加新功能，并不断测试，当 v3 稳定后，将 v3 合并到 master 分支上。 创建一个特性分支用来修复 2.0 的 bug ，一旦 bug 修复成功，就将该分支合并到 master 上，一旦发现新 bug ，就立马再创建分支进行修复，修复成功之后再合并。 以上两个步骤同步进行，这在 Svn 中简直是不可想象的，因为 Svn 的分支管理太 low，而 Git 能够让我们做到随心所欲的创建、合并和删除分支。 查看分支我们可以通过 git branch 命令来查看当前仓库有哪些分支，而我们处于哪一个分支中，如下： 这里显示当前仓库只有一个 master 分支，这是 git 默认创建出来的，master 前面的 * 表示我们当前处于这一个分支中。 分支创建和切换我们可以利用 git branch &lt;分支名&gt; 命令来创建一个分支，然后利用 git checkout &lt;分支名&gt; 来切换分支，如下： 如果小伙伴觉得这样太麻烦，可以通过 git checkout -b &lt;分支名&gt; 来一步到位，创建并切换分支，如下： 也可以通过 git checkout - 命令来切换回上一个分支，如下： 分支合并现在我切换到 fa 分支中，由于 fa 分支是从 master 分支中创建出来的，所以此时 fa 分支的内容和 master 分支的内容是一致的，然后我在 fa 分支中向 git01.txt 文件添加一行内容并提交，此时 fa 分支中的 git01.txt 和 master 分支中 git01.txt 的内容就不相同了，具体操作如下： 上图展示了此时 master 分支和 fa 分支的不同，现在我通过 git merge --no-ff &lt;分支名&gt; 命令将 fa 分支合并到 master 分支上。其中 –no-ff 表示强行关闭 fast-forward 方式， fast-forward 方式表示当条件允许时， git 直接把 HEAD 指针指向合并分支的头，完成合并，这种方式合并速度快，但是在整个过程中没有创建 commit，所以如果当我们删除掉这个分支时就再也找不回来了，因此在这里我们将之关闭。 想要合并分支，我们先切换到 master 分支上，然后执行 git merge --no-ff fa 命令即可完成分支合并，如下图： 合并成功后，我们看到 master 分支上的 git01.txt 上已经有了 fa 分支中的内容了。 以图表方式查看分支我们可以通过 git log --graph 命令来直观的查看分支的创建和合并等操作，如下图： 分支衍合所谓的分支衍合其实也是分支合并的一种方式，下面我们就来看看这个分支衍合到底是什么样的。现在我的 master 分支的内容和 fa 分支的内容是保持一致的，fa 是从 master 中创建出来的，如下图： 现在我向 fa 和 master 中各自做一次提交，如下图： 此时我们执行如下两条命令将两个分支合并： 12$ git checkout fa$ git rebase master rebase 命令在执行的过程中会首先把 fa 中的每个 commit 取消，并且将之保存为临时 patch ，再将 fa 分支更新为最新的 master 分支，然后再把那些临时的 patch 应用到 fa 上，此时 fa 分支将指向新创建的 commit 上，那些老的 commit 将会被丢弃，这些被丢弃的 commit 在执行 git gc 命令时会被删除。合并后的分支如下图： 上面的 git rebase master 命令在执行的过程中有可能会发生冲突，发生冲突时我们有两种方案，一种直接退回到之前的状态，另一种就是解决冲突继续提交。 退回到之前的状态我们可以通过如下命令来回到之前的状态： 1$ git rebase --abort 解决冲突不过大多数情况下我们都是要解决冲突的，解决之后继续提交。此时我们用编辑器打开冲突的文件，看到的内容可能是这样的： ======上面的是 HEAD 中的内容，下面的是要合并的内容，根据自己的需求编辑文件，编辑完成之后，通过如下两条命令继续完成合并： 12$ git add git01.txt$ git rebase --continue 如下图： 冲突解决我们前面提到了在分支衍合时出现冲突的解决方案，其实普通的合并也有可能出冲突，出现冲突很正常，解决就是了，git merge 合并分支时如果出现冲突还是先重新编辑冲突文件，编辑完成之后，再执行 git add 和 git commit 即可。 好了，分支管理我们就先说这么多，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-branch.html"},{"title":"Git 基本操作","text":"上篇文章我们简单的介绍了 Git 的诞生和发展，然后也说了 Windows 环境下 Git 的安装和一些基本的配置，本文我们就来说一说 Git 中的一些基本概念和基本操作。 本文是 Git 系列的第二篇，了解前面的文章有助于更好的理解本文： 1.Git 概述 工作区和暂存区和 Svn 有很大的不同，Git 中引入了暂存区/缓存区 (Stage/Index) 的概念，如下图： 工作区很好理解，就是我们能看到的工作目录，就是本地的文件夹。 这些本地的文件夹我们要通过 git add 命令先将他们添加到暂存区中。 git commit 命令则可以将暂存区中的文件提交到本地仓库中去。 在 Svn 中我们都是直接将文件提交到版本仓库中去，而在 Git 中，则多了一层关卡。 基本操作下面我主要介绍一下 Git 中的常见操作。 初始化仓库仓库的初始化有两种方式：一种是直接从远程仓库克隆，另一种则是直接从当前目录初始化，这里我们主要介绍当前目录初始化，远程仓库克隆我们在后面的文章中会说到。从当前目录初始化的方式很简单，直接执行如下命令: 1$ git init 执行完成后当前目录下会多出一个 .git 的隐藏文件夹，所有 git 需要的数据和资源都存放在该目录中。 查看仓库状态我们可以通过 git status 命令来查看仓库中文件的状态，比如，在我们仓库刚刚初始化完成之后，我们执行 git status 命令，执行效果如下： 执行结果首先展示了我们当前处于 master 分支下，然后又说暂时没有东西可以提交，因为当前仓库中还没有记录任何文件的任何状态。此时，我在当前目录下创建一个名为 git01.txt 的文件，然后再执行 git status 命令，如下： 此时执行结果中显示有一个未被追踪的文件就是我们刚刚添加的 git01.txt ，这个表示该文件目前并未被 git 仓库所管理，所以接下来我们要将这个文件添加到暂存区。 添加文件到暂存区git add 命令可以将一个文件添加到暂存区，我们现在已经有一个 git01.txt 文件了，接下来，执行如下命令将文件添加到暂存区中： 1$ git add git01.txt 文件添加到暂存区之后，我们再执行 git status 命令，可以看到如下结果： 文件提交到暂存区之后，我们看到此时的状态已经发生了变化。 提交到本地仓库当文件提交到暂存区之后，此时我们可以通过 git commit 命令将当前暂存区的文件提交到本地仓库，如下： 注意，执行 commit 命令时，我们需要加上提交备注，即 -m 参数，提交成功之后，我们再执行 git status 命令，结果如下： 此时一切又恢复宁静了，没有需要 add 的东西，也没有需要 commit 的东西。 如果我们要写的备注非常多，我们可以直接执行 git commit 命令，此时会自动打开一个 vi 编辑器，我们直接在编辑器中输入备注信息即可。假设我在 git01.txt 中随意添加一行内容，然后依次执行 git add、git commit 命令，此时系统会自动打开一个 vi 编辑器，如下： 如图所示，我们在vi编辑器中按照既定的格式编辑内容，编辑完成之后保存退出，此时文件就 commit 成功了。如果在备注信息编辑的过程中我们不想提交了，则直接删除备注信息，保存退出，此时提交就终止了，如下： 提交成功之后，我们可以通过如下命令修改提交备注： 1git commit --amend 运行该命令，会自动打开vi编辑器，此时我们可以重新编辑上次提交的备注信息。 查看提交日志通过 git log 命令我们可以查看以往仓库中提交的日志，比如提交的版本号、提交者、提交者邮箱、提交时间、提交备注等信息，如下： 有的时候我们要查看的命令并不用这么详细，可以在 git log 后面加上 --pretty=short ，这样显示出来的就只是简略信息了： 此时显示出来的是我们这个仓库中的所有日志信息，如果我只想查看某一个文件的提交日志，在 git log 后面加上文件名即可。如下： 如果我还想查看提交时文件的变化，加上 -p 参数即可，如下： 绿色的 + 表示新增的行，红色的 - 表示删除的行（当然这里没有删除的行）。 但是 git log 有一个局限性，就是不能查看已经删除的 commit 的日志，举个例子：下班了，我发现今天下午提交的代码全都是有问题的，于是做了一个版本回退，回退到今天早上的状态，然后关机回家，第二天来了后我发现搞错了，其实那些代码都是 OK 的，于是我又想让仓库版本前进到昨天下午的状态，却发现 git log 命令查看不到昨天下午提交的版本号。此时，我们可以使用 git reflog 命令来实现这一个请求， git reflog 命令可以显示整个本地仓库的 commit , 包括所有 branch 的 commit , 甚至包括已经撤销的 commit , 只要 HEAD 发生了变化, 就会在 reflog 里面看得到，而 git log 只显示当前分支的 commit ，并且不显示删除掉的 commit。如下图： 查看更改前后的差异使用 git diff 命令我们可以查看工作区和暂存区的区别以及工作区和最新提交的差别。我往 git01.txt 文件中再添加一行 hello world ，此时执行 git diff 命令，结果如下： 此时这里显示我们新增了一行。此时我们执行 git add 命令，将文件提交到暂存区，然后再执行 git diff ，如下： 此时没有任何信息输出，因此此时工作区的内容和暂存区的内容已经保持一致了。但是此时工作区和本地仓库中最新提交的内容还是不一致，我们可以通过 git diff HEAD 命令来查看，如下： 此时我们需要执行 git commit 命令将暂存区中的文件提交，提交成功之后，再执行 git diff HEAD 命令，则又恢复宁静了。如下： 压缩提交历史 git rebase -i 命令可以实现提交历史的压缩。比如我们在开发某一个功能时，提交了很多次，当所有功能都写完时，想将这些提交压缩为一个，就可以使用该命令，如下： 如上图，该命令执行之后，会自动打开一个vi编辑器，在vi编辑器中将最新提交的日志的 pick 改为 fixup 即可。压缩之后，最新一次的提交日志就没了，但是数据还在。 OK，Git 基本操作我们就先说这么多，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-basic.html"},{"title":"Git 学习资料","text":"关于Git的用法我们已经写七篇文章，介绍了Git的不少用法，这些足以应付工作中90%的需求了，剩下的10%就需要小伙伴们在工作中自己慢慢总结了，我这里再给小伙伴们推荐一点Git学习资料，为我们的Git系列画上一个句号。 书推荐两本个人觉得很不错的书： 《GitHub入门与实践》 《Pro Git》 《GitHub 入门与实践》秉承了日系技术书刊一贯的“手把手教学”风格，作者用亲切的语言，简明扼要的介绍，配以生动详实的示例一步步讲解GitHub和Git的使用方法。《Pro Git》作为 Git 官方推荐书籍，《Pro Git》 值得 Git 初学者和爱好者认真阅读一遍。 网站1.https://learngitbranching.js.org 链接是一个 git 学习网站，我们可以直接在上面练习 git 命令。 博客推荐本公号前面的几篇教程: 1.Git概述2.Git基本操作3.Git中的各种后悔药4.Git分支管理5.Git关联远程仓库6.Git工作区储藏兼谈分支管理中的一个小问题7.Git标签管理","link":"/2019/0612/git-resources.html"},{"title":"Git 工作区储藏","text":"这是一篇计划之外的文章，之所以有这篇文章，是因为有一个小伙伴在阅读Git 分支管理一文时遇到了一个问题，而这个问题又比较典型，因此我想专门来谈谈 Git 中工作区的储藏问题。 本文是 Git 系列的第六篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药4.Git 分支管理5.Git 关联远程仓库 问题回顾小伙伴遇到的问题是这样的： 现在有一个 master 分支，master 分支中有一个文件叫 01.txt ，该文件中只有一行数据，然后对 01.txt 执行 add 和 commit ，然后再从 master 分支中创建出一个新的分支 fa ，切换到 fa 分支上，然后向 01.txt 中再添加一行数据，添加成功之后，不做任何事情，再切换回 master 分支，此时用 cat 命令查看 01.txt 文件，发现竟然有两行数据，按理说 master 中的 01.txt 只有一行数据，而 fa 中的 01.txt 有两行数据，整个过程如下图： 要搞清楚这个问题，得先明白下面这个问题： cat 命令和 git 无关，就是用来查看文件的，我为了演示方便使用了 cat 命令，这和直接用记事本打开文件查看效果是一样的。 可能眼尖的小伙伴已经发现端倪了，我们上面这个操作少了两个步骤，那就是 add/commit ，fa 分支中的数据修改之后直接切换回了 master ，而没有 add/commit 。正常情况下（修改数据后 add/commit），如果 master 和 fa 分支中的数据不一致，我们执行了 git checkout - 进行分支的切换，这个时候工作区中的文件内容也是会跟着变化的（大家可以通过 cat 命令或者直接在记事本中打开工作区的文件来查看这种变化），但是如果我在 fa 分支中修改了文件却没有 add/commit 就切换回 master ，此时如果工作区的文件变化了，可能会导致我在 fa 分支中的修改丢失，因此，这个时候工作区的文件就没有变化，即工作区的文件内容还是 fa 分支中修改的内容。 解决这个问题，我们有两种方案，请小伙伴们往下看。 解决方案方案一第一种解决方案就是在某一个分支修改文件之后，先 add 并且 commit 之后再去切换分支，这个操作就比较简单了，我这里就不再演示了。 方案二(储藏)第二种解决方案就是储藏 (Stashing)，储藏适用在如下场景中： 当我在一个分支 fa 中修改了文件，但是还没有完全改好，此时我并不想 add/commit ，但是这个时候有一个更急迫的事情在另外一个分支 fb 上需要我去做，我必须要切换分支。 在这样一个场景中，如果我直接切换分支，会出现如下两个问题： 1.从 fa 切换到 fb 之后，工作区的代码还是 fa 的代码，不符合我的工作要求。2.假设我不在乎问题 1，在 fb 中直接修改工作区的代码，等我在 fb 中修改完后提交后再回到 fa ，会发现我之前的代码丢失了。 为了解决这个问题，Git 给我们提供了储藏 (Stashing)。 现在假设一开始 master 和 fa 分支中的文件内容都是一致的，而且两个分支的工作区都是干净的，即没有东西需要 add/commit ，此时，我在 master 中修改了文件，修改完成之后，执行 git status 命令我们看到 master 中有东西需要 add/commit ，此时我想切换到 fa 分支中去，但是并不想对 master 分支执行 add/commit ，这个时候我们可以执行如下命令，先将当前分支中的文件储藏起来： 1$ git stash OK，执行完 git stash 命令之后，再执行 git status ，我们发现此时 master 分支已经是干净的了，此时我们可以愉快的切换到 fa 分支中去了，切换到 fa 分支之后，我们发现 master 中的修改并没有干扰到 fa 分支，当我们完成了 fa 分支中的工作之后，再回到 master 分支，此时执行如下命令可以恢复刚刚储藏的数据： 1$ git stash apply 上面这个命令执行完之后，master 分支中的工作区中的文件就恢复了，此时执行 git status 就可以看到又有数据需要 add/commit 了。 我们也可将工作区储藏多次，这个时候我们可以执行如下命令来查看储藏： 1$ git stash list 执行效果如下： git stash apply 表示恢复最近一次储藏，如果我们想恢复到之前的某一次储藏，可以加上储藏的名字，如下： 1$ git stash apply stash@{1} 还有一些其他的关于储藏的命令： 1.恢复储藏并出栈1$ git stash pop 执行效果和 git stash apply 一样，不同的是，这里执行完之后，会将栈顶的储藏移除。 2.删除某一个储藏1$ git stash drop stash@{4} 最后一个参数是指储藏的名字。 Ok，储藏问题我们先说这么多。有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-stash.html"},{"title":"Git 标签管理","text":"我们可以针对某一次的提交打上一个标签，有点类似于给某次提交取个别名，比如 1.0 版本发布时打个标签叫 v1.0,2.0 版本发布时打个标签叫 v2.0 ，因为每次版本提交的结果都是一连串的哈希码，不容易记忆，打上 v1.0,v2.0 这些具有某种含义的标签后，可以方便我们进行版本管理。 本文是 Git 系列的第七篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药4.Git 分支管理5.Git 关联远程仓库6.Git 工作区储藏兼谈分支管理中的一个小问题 轻量级标签轻量级标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。 首先我们可以通过如下命令来查看当前仓库中的所有标签： 1$ git tag 打标签的方式很简单，直接通过 git tag &lt;tagname&gt; 来完成即可，如下命令： 1$ git tag v1 表示创建了一个名为 v1 的 tag ，这个 tag 默认是创建在最新一次的 commit 上的，如下： 我们可以利用 git show &lt;tagname&gt; 来查看标签对应的版本信息，如下： 我们可以通过 $ git tag -d &lt;tagname&gt; 命令删除一个标签： 1$ git tag -d v1 如下图： 如果我想给历史上的某次 commit 打一个标签呢?我们可以通过如下命令 git tag &lt;tagname&gt; &lt;commitversion&gt; ,如下： 1$ git tag v0.0 7d519 表示给 commit 的哈希码为 7d519 的那一次 commit 打上一个标签，如下图： 含附注的标签而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。 打一个含附注的标签很简单，使用 git tag -a &lt;tagname&gt; -m &lt;msg&gt; 命令,如下： 1$ git tag -a v0.0 -m &quot;文件初次建立&quot; 7d519 如下： 如果不加最后的版本号参数，表示给最新的一次 commit 打标签。 签署标签说到签署标签我们得先介绍一下 GPG ： GPG 是加密软件，可以使用 GPG 生成的公钥在网上安全的传播你的文件、代码。为什么说安全的？以 Google 所开发的 repo 为例，repo 即采用 GPG 验证的方式，每个里程碑 tag 都带有 GPG 加密验证，假如在里程碑 v1.12.3 处你想要做修改，修改完后将这个 tag 删除，然后又创建同名 tag 指向你的修改点，这必然是可以的。但是，在你再次 clone 你修改后的项目时，你会发现，你对此里程碑 tag 的改变不被认可，验证失败，导致你的修改在这里无法正常实现。这就是 GPG 验证的作用，这样就能够保证项目作者（私钥持有者）所制定的里程碑别人将无法修改。那么，就可以说，作者的代码是安全传播的。为什么会有这种需求？一个项目从开发到发布，再到后期的更新迭代，一定会存在若干的稳定版本与开发版本（存在不稳定因素）。作为项目发起者、持有者，有权定义他（们）所认可的稳定版本，这个稳定版本，将不允许其他开发者进行改动。还以 Google 的 repo 项目为例，项目所有者定义项目开发过程中的点 A 为稳定版 v1.12.3，那么用户在下载 v1.12.3 版本后，使用的肯定是 A 点所生成的项目、产品，就算其他开发者能够在本地对 v1.12.3 进行重新指定，指定到他们修改后的B点，但是最终修改后的版本给用户用的时候，会出现 GPG 签名验证不通过的问题，也就是说这样的修改是不生效的。 —-摘自&lt;带 GPG 签名的 Git tag&gt;一文 使用签署标签我们先要生成 GPG Key，生成命令如下： 1$ gpg --gen-key 能默认的就直接按回车默认，不能默认的就根据提示输入相应的值，这里的都很简单，不再赘述。完了之后，就可以通过如下命令来打标签了： 1$ git tag -s v0.0 -u &quot;laowang&quot; -m &quot;文件初次建立&quot; 7d519 就把上面的-a换成-s，然后添加-u参数，-u参数的值是我们在生成 GPG Key 的时候配置的 name 属性的值，注意-u参数不可以写错，否则标签会创建失败，如下： 如上图，-u 参数写错时，标签创建失败。 标签推送到远程仓库git push 命令并不会把tag提交到远程仓库中去，需要我们手动提交，如下： 1$ git push origin v0.0 表示将 v0.0 标签提交到远程仓库，也可以通过 $ git push origin --tags 提交所有的 tag 到远程仓库，如下： 此时别人调用 git pull 更新代码之后，就能看到我们的 tag。如下： Ok，Git 标签管理我们先说这么多。有问题欢迎留言讨论。 参考资料： 《GitHub入门与实践》 《Pro Git》","link":"/2019/0612/git-tag.html"},{"title":"Git 概述","text":"一直以来想出一个 Git 的教程，去年写过一篇，后来没了下文，烂尾了。最近忙里偷闲，还是想把这个 Git 系列写一遍，这次争取写完。 本文我主要想先简单介绍下Git，然后介绍下 Git 的安装。 毫无疑问， Git 是目前最优秀的分布式版本控制工具，木有之一，可是我见到的很多人还是不会用，我的老东家每天忍受着 SVN 带来的痛苦，却迟迟不愿切换到 Git 上，个人感觉，许多中小公司不用 Git ，不是因为 Git 不好，而是他们的项目经理不会用（逃。 OK，那么今天我们就先来简单介绍下 Git 的发展史以及 Git 的优点，然后再来看看 Git 的安装。 Git诞生记Git 诞生于 2005 年 4 月，由 Linux 的作者 Linus Torvalds 花费了两周的时间用 C 写了一个分布式版本控制系统，这就是 Git1.0 ，大牛写代码就是这么 666666 。 其实早在 Git 之前，这个世界就已经有一些非常流行的版本控制工具 （VCS） ，比如 CVS ，SVN，ClearCase 等，然而这些工具要么运行慢，要么要收费，都不是 Linus Torvalds 的菜。 当时，有一个公司叫做 BitMover ，该公司有一款产品叫做 BitKeeper ，这是一个分布式版本控制工具，但是这是收费的，不过这是一个有情怀的公司，老板 Larry 说服 Linus Torvalds 使用 BitKeeper 来管理 Linux 内核源码，于是，在 2002 到 2005 年之间，Linux 内核开发团队一直使用 BitKeeper 来管理 Linux 源码。 可是在开发的过程中， Linus Torvalds 有一个叫做安德鲁·垂鸠的小伙伴不安分了，他写了一个简单程序，可以连接 BitKeeper 的存储库，BitKeeper 著作权拥有者拉里·麦沃伊认为安德鲁·垂鸠对 BitKeeper 内部使用的协议进行逆向工程，决定收回无偿使用 BitKeeper 的授权。 合作无法继续，于是 Linus Torvalds 决定自己开发一套分布式版本控制工具，就是 Git ，至于这个分布式版本控制工具为什么叫 Git，Linus Torvalds 并没有给出一个让所有人满意的解释，倒是很多开发者一直尝试去给 Git 一个合理的解释，常见的一个解释就是 Global Information Tracker （中文译作全局信息追踪器）。 如果当年不是 BitMover 公司的逼迫，估计我们还不一定见到这么优秀的分布式版本控制工具。值得一说的是， 2016 年 5 月 11 日 BitKeeper 宣布以 Apache 2.0 许可证开源（新闻链接http://www.solidot.org/story?sid=48171），我很好奇 BitKeeper 宣布开源的时候他的老板心中是何感受？ Git的优势Git 一出世立马就成为最流行的分布式版本控制工具，2008 年 4 月，GitHub 正式上线，GitHub 是一个利用 Git 进行版本控制，专门用于存放代码与内容的共享虚拟主机服务，GitHub 上线之后，许多开源项目都移植到 GitHub 上了，不管你从事那门语言的研发，都会在 GitHub 上找到你需要的项目吧！OK，说了这么多，接下来我们也该说说 Git 这个分布式版本控制工具的优势了。Git 主要有以下几个优势： 与传统的集中式版本控制工具不同，分布式版本控制工具不需要联网就可以工作，每台电脑都是一个完整的版本仓库。 Git 可以胜任上万人的开发规模，这个大家看看 GitHub 中的开源项目就知道了，不需要我多说。 性能优异。我们前面说过 Linus Torvalds 之所以不愿意使用 CVS、SVN 等版本控制工具就是因为这些工具的性能太差。所以 Linus Torvalds 在开发 Git 时就决定要革除积弊，确保 Git 的运行效率。笔者在上家公司做开发时，深受 SVN 的毒害，但是公司还是不愿意迁移到 Git 上，我猜测是由于项目经理不会用。 保证项目的安全。我们知道，在 SVN 之前还有一个集中式版本控制工具叫做 CVS ，这个 CVS 有一个问题，就是你的文件有的时候会莫名其妙的丢失，做开发的各位筒子都知道，如果你的项目中突然有一个文件不见了，你不出一身汗才怪。所以，Git 使用 SHA1 这种通用的加密散列函数来对数据库中的对象进行命名，从而来确保文件的安全。 好用的分支。用过 Git 的人都知道 Git 中的分支用起来有多么爽，分支在我们的项目中用的非常普遍，可是 SVN 虽然也有分支，但是却不能像 Git 用的这么爽。这个做过项目的筒子都知道。我们到后文会给大家详细介绍分支的用法。 OK，Git 的优势还有很多种，这里我就不再一一列举了，有兴趣的大家自行搜索。 Git 的安装作为一个屌丝码农，我的本子还是 windows ,不过我的本本装了双系统，所以我这里就只给大家演示一下 Windows 下如何安装 Git 以及 Ubuntu 下如何安装 Git。 windows7 安装 Gitwindows 安装 Git 整体上来说有两种解决方案 安装Cygwin（下载地址http://cygwin.com/）用来模拟Linux运行环境，但是Cygwin大配置非常麻烦，容易出错，所以一般不推荐这种方式。 安装独立的Git，也就是msysGit（下载地址https://git-for-windows.github.io/），这就是一个简单的exe文件，一路next就安装成功了。安装成功后，在你的开始菜单中找到Git Bash，点击Git Bash，输入git –version查看git版本号，运行界面如下： Ubuntu 安装 Gitubuntu 安装 Git 就是一句话: 1sudo apt-get install git 早期的 Linux 版本直接运行下面的代码即可： 1sudo apt-get install git-core 安装成功之后，输入 git –version 查看 git 版本号。 基本配置不管是 Windows 安装还是 Linux 安装，安装好之后，我们都先通过如下两行命令做一个基本配置,配置的信息将展示在我们每一次提交的后面，所以不要使用不方便公开的信息，如果不配置以后每次提交的时候都会让你输入用户名和密码，配置方式如下： 12$ git config --global user.name &quot;zhangsan&quot; $ git config --global user.email &quot;111@qq.com&quot; 这个配置会保存在当前用户目录下的 .gitconfig 文件中，如下： OK，本文我们就先说到这里，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-install.html"},{"title":"Jedis 使用","text":"Redis 的知识我们已经介绍的差不多了，本文我们来看看如何使用 Java 操作 redis。 本文是Redis系列的第十三篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一)11.Redis 主从复制(二)12.Redis 集群搭建 有哪些解决方案查看 redis 官网，我们发现用 Java 操作 redis，我们有多种解决方案，如下图： 这里的解决方案有多种，我们采用 Jedis，其他的框架也都大同小异，我这里权当抛砖引玉，小伙伴也可以研究研究其他的方案，欢迎投稿。 配置客户端要能够成功连接上 redis 服务器，需要检查如下三个配置： 1.远程 Linux 防火墙已经关闭，以我这里的 CentOS7 为例，关闭防火墙命令 systemctl stop firewalld.service ，同时还可以再补一刀 systemctl disable firewalld.service 表示禁止防火墙开机启动。 2.关闭 redis 保护模式，在 redis.conf 文件中，修改 protected 为 no，如下： 1protected-mode no 3.注释掉 redis 的 ip 地址绑定，还是在 redis.conf 中，将 bind:127.0.0.1 注释掉，如下： 1# bind:127.0.0.1 确认了这三步之后，就可以远程连接 redis 了。 Java 端配置上面的配置完成后，我们可以创建一个普通的 JavaSE 工程来测试下了，Java 工程创建成功后，添加 Jedis 依赖，如下： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 然后我们可以通过如下一个简单的程序测试一下连接是否成功： 12345public static void main(String[] args) { Jedis jedis = new Jedis(\"192.168.248.128\", 6379); String ping = jedis.ping(); System.out.println(ping);} 运行之后，看到如下结果表示连接成功了： 连接成功之后，剩下的事情就比较简单了，Jedis 类中方法名称和 redis 中的命令基本是一致的，看到方法名小伙伴就知道是干什么的，因此这些我这里不再重复叙述。 频繁的创建和销毁连接会影响性能，我们可以采用连接池来部分的解决这个问题： 12345678public static void main(String[] args) { GenericObjectPoolConfig config = new GenericObjectPoolConfig(); config.setMaxTotal(100); config.setMaxIdle(20); JedisPool jedisPool = new JedisPool(config, \"192.168.248.128\", 6379); Jedis jedis = jedisPool.getResource(); System.out.println(jedis.ping());} 这样就不会频繁创建和销毁连接了，在 JavaSE 环境中可以把连接池配置成一个单例模式，如果用了 Spring 容器的话，可以把连接池交给 Spring 容器管理。 上面这种连接都是连接单节点的 Redis，如果是一个 Redis 集群，要怎么连接呢？很简单，如下： 1234567891011Set&lt;HostAndPort&gt; clusterNodes = new HashSet&lt;HostAndPort&gt;();clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7001));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7002));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7003));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7004));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7005));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7006));JedisCluster jc = new JedisCluster(clusterNodes);jc.set(\"address\", \"深圳\");String address = jc.get(\"address\");System.out.println(address); JedisCluster 中的方法与 Redis 命令也是基本一致，我就不再重复介绍了。 好了，jedis 就说这么多，有问题欢迎留言讨论。","link":"/2019/0615/redis-jedis.html"},{"title":"Linux 上安装 Redis","text":"hello，各位小伙伴们好久不见！那么从今天开始，我想和各位小伙伴分享下 Redis 的用法，本文我们就先来看看什么是 Redis 以及如何安装 Redis。 什么是 RedisRedis 是一个使用 ANSI C 编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。从 2015 年 6 月开始，Redis 的开发由 Redis Labs 赞助，而 2013 年 5 月至 2015 年 6 月期间，其开发由 Pivotal 赞助。在 2013 年 5 月之前，其开发由 VMware 赞助。根据月度排行网站 DB-Engines.com 的数据显示，Redis是 最流行的键值对存储数据库。 Redis 具有如下特点： Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用，不会造成数据丢失 Redis 支持五种不同的数据结构类型之间的映射，包括简单的 key/value 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储 Redis 支持 master-slave 模式的数据备份 Redis 具有如下功能： 内存存储和持久化：redis 支持异步将内存中的数据写到硬盘上，在持久化的同时不影响继续服务 取最新N个数据的操作，如：可以将最新的 10 条评论的 ID 放在 Redis 的 List 集合里面 数据可以设置过期时间 自带发布、订阅消息系统 定时器、计数器 Redis 安装Windows 版 Redis 的安装，整体来说还是非常简单的，网上也有很多教程，考虑到 Redis 的大部分使用场景都是在 Linux 上，因此这里我对 Windows 上的安装不做介绍，小伙伴们有兴趣可以自行搜索，下面我们主要来看下 Linux 上怎么安装 Redis 。 环境： CentOS7 redis4.0.8 1.首先下载 Redis，下载地址https://redis.io/，下载获得 redis-4.0.8.tar.gz 后将它放入我们的 Linux 目录 /opt 2./opt 目录下，对文件进行解压，解压命令: tar -zxvf redis-4.0.8.tar.gz ，如下： 3.解压完成后出现文件夹：redis-4.0.8，进入到该目录中: cd redis-4.0.8 4.在 redis-4.0.8 目录下执行 make 命令进行编译 5.如果 make 完成后继续执行 make install 进行安装 OK，至此，我们的 redis 就算安装成功了。 6.在我们启动之前，需要先做一个简单的配置：修改 redis.conf 文件，将里面的 daemonize no 改成 yes，让服务在后台启动，如下： 7.启动，通过redis-server redis.conf命令启动redis，如下： 8.测试 首先我们可以通过 redis-cli 命令进入到控制台，然后通过 ping 命令进行连通性测试，如果看到 pong ，表示连接成功了，如下： 9.关闭，通过 shutdown 命令我们可以关闭实例，如下： OK，至此，我们的 Redis 就安装成功了，整体来说还是非常简单的，有问题欢迎留言讨论。","link":"/2019/0615/linux-redis.html"},{"title":"MyBatis 中 @Param 注解的四种使用场景，最后一种经常被人忽略！","text":"有一些小伙伴觉得 MyBatis 只有方法中存在多个参数的时候，才需要添加 @Param 注解，其实这个理解是不准确的。即使 MyBatis 方法只有一个参数，也可能会用到 @Param 注解。 但是，在你总结出规律之前，你可能会觉得莫名其妙，有的时候一个参数明明不用添加 @Param 注解，有的时候，却需要添加，不添加会报错。 有的人会觉得这是 MyBatis 各个版本差异的锅，不可否认，MyBatis 发展很快，不同版本之间的差异还挺明显的，不过这个加不加 @Param 注解的问题，却并不是版本的锅！今天松哥就和大家来聊一聊这个问题，到底哪些情况下需要添加 @Param 注解。 首先，如下几个需要添加 @Param 注解的场景，相信大家都已经有共识了： 第一种：方法有多个参数，需要 @Param 注解 例如下面这样： 1234@Mapperpublic interface UserMapper { Integer insert(@Param(\"username\") String username, @Param(\"address\") String address);} 对应的 XML 文件如下： 123&lt;insert id=\"insert\" parameterType=\"org.javaboy.helloboot.bean.User\"&gt; insert into user (username,address) values (#{username},#{address});&lt;/insert&gt; 这是最常见的需要添加 @Param 注解的场景。 第二种：方法参数要取别名，需要 @Param 注解 当需要给参数取一个别名的时候，我们也需要 @Param 注解，例如方法定义如下： 1234@Mapperpublic interface UserMapper { User getUserByUsername(@Param(\"name\") String username);} 对应的 XML 定义如下： 123&lt;select id=\"getUserByUsername\" parameterType=\"org.javaboy.helloboot.bean.User\"&gt; select * from user where username=#{name};&lt;/select&gt; 老实说，这种需求不多，费事。 第三种：XML 中的 SQL 使用了 $ ，那么参数中也需要 @Param 注解 $ 会有注入漏洞的问题，但是有的时候你不得不使用 $ 符号，例如要传入列名或者表名的时候，这个时候必须要添加 @Param 注解，例如： 1234@Mapperpublic interface UserMapper { List&lt;User&gt; getAllUsers(@Param(\"order_by\")String order_by);} 对应的 XML 定义如下： 123456&lt;select id=\"getAllUsers\" resultType=\"org.javaboy.helloboot.bean.User\"&gt; select * from user &lt;if test=\"order_by!=null and order_by!=''\"&gt; order by ${order_by} desc &lt;/if&gt;&lt;/select&gt; 前面这三种，都很容易懂，相信很多小伙伴也都懂，除了这三种常见的场景之外，还有一个特殊的场景，经常被人忽略。 第四种，那就是动态 SQL ，如果在动态 SQL 中使用了参数作为变量，那么也需要 @Param 注解，即使你只有一个参数。 如果我们在动态 SQL 中用到了 参数作为判断条件，那么也是一定要加 @Param 注解的，例如如下方法： 1234@Mapperpublic interface UserMapper { List&lt;User&gt; getUserById(@Param(\"id\")Integer id);} 定义出来的 SQL 如下： 123456&lt;select id=\"getUserById\" resultType=\"org.javaboy.helloboot.bean.User\"&gt; select * from user &lt;if test=\"id!=null\"&gt; where id=#{id} &lt;/if&gt;&lt;/select&gt; 这种情况，即使只有一个参数，也需要添加 @Param 注解，而这种情况却经常被人忽略！ 好了，不知道大家有没有 GET 到呢？有问题欢迎留言讨论。","link":"/2019/0723/mybatis-@param.html"},{"title":"MyBatis中主键回填的两种实现方式","text":"主键回填其实是一个非常常见的需求，特别是在数据添加的过程中，我们经常需要添加完数据之后，需要获取刚刚添加的数据 id，无论是 Jdbc 还是各种各样的数据库框架都对此提供了相关的支持，本文我就来和和大家分享下数据库主键回填在 MyBatis 中的两种实现思路。 原生写法框架来源于我们学过的基础知识，主键回填实际上是一个在 JDBC 中就被支持的写法，有的小伙伴可能不知道这一点，因此这里我先来说说在 JDBC 中如何实现主键回填。 JDBC 中实现主键回填其实非常容易，主要是在构造 PreparedStatement 时指定需要主键回填，然后在插入成功后，查询刚刚插入数据的 id ，示例代码如下： 1234567891011121314151617public int insert(Person person) { Connection con = null; PreparedStatement ps = null; ResultSet rs = null; con = DBUtils.getConnection(); ps = con.prepareStatement(\"INSERT INTO person(username,password,money) VALUES(?,?,?)\", PreparedStatement.RETURN_GENERATED_KEYS); ps.setObject(1, person.getUsername()); ps.setObject(2, person.getPassword()); ps.setObject(3, person.getMoney()); int i = ps.executeUpdate(); rs = ps.getGeneratedKeys(); int id = -1; if (rs.next()) { id = rs.getInt(1); } return id;} 和普通的插入 SQL 不同之处主要体现在两个地方： 第一个是构造 PreparedStatement 时，多了一个参数，指定了需要主键回填。 在更新操作执行完成之后，调用 getGeneratedKeys ，然后又会获取到一个 ResultSet 对象，从这个游标集中就可以获取到刚刚插入数据的id。 这个是原生的写法，在 MyBatis 中，对此需求提供了两种不同的实现方案，下面分别来看。 框架写法一般情况下，主键有两种生成方式： 主键自增长 自定义主键（一般可以使用UUID，或者类UUID） 如果是第二种，主键一般是在Java代码中生成，然后传入数据库执行插入操作，如果是第一个主键自增长，此时，Java 可能需要知道数据添加成功后的主键。 MyBatis 的基本用法就无需多说了，这也不是本文的重点，我们还是来看看 MyBatis 中主键回填的两种不同实现方式吧！ 方式一第一种方式比较简单，也是松哥推荐的一种实现方式： 123&lt;insert id=\"insertBook\" useGeneratedKeys=\"true\" keyProperty=\"id\"&gt; insert into t_book (b_name,author) values (#{name},#{author});&lt;/insert&gt; 这种方式比较简单，就是在插入节点上添加 useGeneratedKeys 属性，同时设置接收回传主键的属性。配置完成后，我们执行一个插入操作，插入时传入一个对象，插入完成后，这个对象的 id 就会被自动赋值，值就是刚刚插入成功的id。 松哥推荐大家使用这种方式，原因很简单，这种方式实现简便省事。 方式二第二种方式则是利用MySQL自带的 last_insert_id() 函数查询刚刚插入的id，示例代码如下： 123456&lt;insert id=\"insertBook\"&gt; &lt;selectKey keyProperty=\"id\" resultType=\"java.lang.Integer\"&gt; SELECT LAST_INSERT_ID() &lt;/selectKey&gt; insert into t_book (b_name,author) values (#{name},#{author});&lt;/insert&gt; 这种方式是在 insert 节点中添加 selectKey 来实现主键回填，实际上这种方式的功能更加丰富，因为 selectKey 节点中的 SQL 我们既可以在插入之前执行，也可以在插入之后执行（通过设置节点的 Order 属性为 AFTER 或者 BEFORE 可以实现），具体什么时候执行，还是要看具体的需求，如果是做主键回填，我们当然需要在插入 SQL 执行之后执行 selectKey 节点中的 SQL。 注意第二种方式一样也要通过设置 keyProperty 来指定将查询到的数据绑定到哪个属性上。 总结好了，本文向大家介绍了 MyBatis 中主键回填的两种方式，大家有没有 get 到呢？有问题欢迎留言讨论。","link":"/2019/0424/mybatis-key-generated.html"},{"title":"MySQL 只能做小项目？松哥要说几句公道话！","text":"松哥上学那会，很多人对 MySQL 有一些偏见，偏见主要集中在以下几方面： MySQL 不支持事务（事实上 MyISAM 有表锁，但是效率比较低） MySQL 存储的数据量比较小，适合小项目，大项目还是得上 Oracle、DB2 等 这么多年过去了，松哥自己在开发中一直是以 MySQL 为主，我觉得我有必要说两句公道话了。 公道话第一个问题关于第一个不支持事务的问题，这有一定的历史原因。MySQL 从设计之初，存储引擎就是可插拔的，允许公司或者个人按照自己的需求定义自己的存储引擎（当然，普通的公司或者个人其实是没有这个实力的）。MySQL 自研的使用较广的存储引擎是 MyISAM ，MyISAM 支持表锁，不支持行锁，所以在处理高并发写操作时效率要低一些，另外 MyISAM 也不支持外键（虽然现在实际项目中外键已经用的比较少了）。 但是这个问题并非无解。这就不得不说 MySQL 中另外一个大名鼎鼎的存储引擎 InnoDB 了。 InnoDB 存储引擎是由一家位于芬兰赫尔辛基的名为 Innobase Oy 的公司开发的，InnoDB 存储引擎的历史甚至比 MySQL 还要悠久。 InnoDB 刚刚开发的时侯，就是作为一个完整的数据库来开发的，因此功能很完备。开发出来之后，创始人是想将这个数据库卖掉的，但是没有找到买家。 后来 MySQL2.0 推出后，这种可插拔的存储引擎吸引了 Innobase Oy 公司创始人 Heikki Tuuri 的注意，在和 MySQL 沟通之后，决定将 InnoDB 作为一个存储引擎引入到 MySQL 中，MySQL 虽然支持 InnoDB ，但是实际上还是主推自家的 MyISAM。 但是 InnoDB 实在太优秀了，最终在 2006 年的时侯，成功吸引到大魔王 Oracle 的注意，大手一挥，就把 InnoDB 收购了。 MySQL 主推自家的 MyISAM ，日子过得也很惨淡，最终在 2008 年被 sun 公司以 10 亿美元拿下，这个操作巩固了 sun 在开源领域的领袖的地位，可是一直以来 sun 公司的变现能力都比较弱，最终 sun 自己在 2009 年被 Oracle 收入囊中。那会松哥还在读高中，某一天吃午饭的时侯，餐厅的电视机上播放央视的午间新闻，看到了这条消息，现在还有一些印象。 Oracle 收购 sun 之后，InnoDB 和 MySQL 就都成了 Oracle 的产品了，这下整合就变得非常容易了，在后来发布的版本中，InnoDB 慢慢就成为了 MySQL 的默认存储引擎。在最新的 MySQL8 中，元数据表也使用了 InnoDB 作为存储引擎。 InnoDB 存储引擎主要有如下特点： 支持事务 支持 4 个级别的事务隔离 支持多版本读 支持行级锁 读写阻塞与事务隔离级别相关 支持缓存，既能缓存索引，也能缓存数据 整个表和主键以 Cluster 方式存储，组成一颗平衡树 … 当然也不是说 InnoDB 一定就是好的，在实际开发中，还是要根据具体的场景来选择到底是使用 InnoDB 还是 MyISAM 。 所以第一个问题不攻自破。 第二个问题第二个问题确实是一个硬伤。 你要是拿 MySQL 和 Oracle 比，肯定是要差一点点感觉。毕竟一个免费一个收费，而且收费的还很贵。但是这个问题并非无解。 相信很多小伙伴都听过国内很多大厂都使用了 MySQL 来存储数据。大厂用 MySQL ，是因为他们有能力研发出自己的存储引擎，小厂一般没有这个实力，没法去研发出自己的存储引擎，但是 Oracle 又用不起，那么怎么办呢？ 这几年兴起的分布式数据库中间件刚好可以很好的解决这个问题。Java 领域，类似的工具很多，例如 Sharding-JDBC 、MyCat 等，通过这些工具，可以很好的实现数据库分库分表，以及数据表的动态扩展、读写分离、分布式事务解决等。有了这些工具，极大的提高了 MySQL 的应用场景。 另一方面，近些年流行微服务，这不是单纯的炒概念，微服务架构将一个大的项目拆分成很多个小的微服务，各个微服务处理自己很小的一部分事情，这更符合人类分工协作的特点。在微服务架构中，我们对大表的需求、对多表联合查询的需求都会有所降低，MySQL 也更具用武之地。 因此，第二个问题也是可以解决的。 据松哥了解，互联网公司使用 MySQL 还是比较多的，传统软件公司，可能会更青睐 Oracle 等数据库。 不过话说回来，云计算，也是未来一个方向。 结语为什么要写这篇文章呢？因为松哥打算出几篇文章给大家介绍一下分布式数据库中间件 MyCat 和 Sharding-JDBC 的用法，有了这些分布式数据库中间件，就可以让你的 MySQL 真正具备可以媲美大型数据库的能力。本文算是一个引子吧。 后面松哥就先更新 MyCat 。","link":"/2019/0622/mysql.html"},{"title":"Nginx 极简入门教程！","text":"上篇文章和大家聊了 Spring Session 实现 Session 共享的问题，有的小伙伴看了后表示对 Nginx 还是很懵，因此有了这篇文章，算是一个 Nginx 扫盲入门吧！ 基本介绍Nginx 是一个高性能的 HTTP 和反向代理 web 服务器，同时也提供了 IMAP/POP3/SMTP 服务。 Nginx 是由伊戈尔·赛索耶夫为俄罗斯访问量第二的 Rambler.ru 站点开发的，第一个公开版本 0.1.0 发布于 2004 年 10 月 4 日。 Nginx 特点是占有内存少，并发能力强。 事实上 nginx 的并发能力确实在同类型的网页服务器中表现较好，一般来说，如果我们在项目中引入了 Nginx ，我们的项目架构可能是这样： 在这样的架构中 ， Nginx 所代表的角色叫做负载均衡服务器或者反向代理服务器，所有请求首先到达 Nginx 上，再由 Nginx 根据提前配置好的转发规则，将客户端发来的请求转发到某一个 Tomcat 上去。 那么这里涉及到两个概念： 负载均衡服务器 就是进行请求转发，降低某一个服务器的压力。负载均衡策略很多，也有很多层，对于一些大型网站基本上从 DNS 就开始负载均衡，负载均衡有硬件和软件之分，各自代表分别是 F5 和 Nginx （目前 Nginx 已经被 F5 收购），早些年，也可以使用 Apache 来做负载均衡，但是效率不如 Nginx ，所以现在主流方案是 Nginx 。 反向代理服务器： 另一个概念是反向代理服务器，得先说正向代理，看下面一张图： 在这个过程中，Google 并不知道真正访问它的客户端是谁，它只知道这个中间服务器在访问它。因此，这里的代理，实际上是中间服务器代理了客户端，这种代理叫做正向代理。 那么什么是反向代理呢？看下面一张图： 在这个过程中，10086 这个号码相当于是一个代理，真正提供服务的，是话务员，但是对于客户来说，他不关心到底是哪一个话务员提供的服务，他只需要记得 10086 这个号码就行了。 所有的请求打到 10086 上，再由 10086 将请求转发给某一个话务员去处理。因此，在这里，10086 就相当于是一个代理，只不过它代理的是话务员而不是客户端，这种代理称之为反向代理。 Nginx 的优势在 Java 开发中，Nginx 有着非常广泛的使用，随便举几点： 使用 Nginx 做静态资源服务器：Java 中的资源可以分为动态和静态，动态需要经过 Tomcat 解析之后，才能返回给浏览器，例如 JSP 页面、Freemarker 页面、控制器返回的 JSON 数据等，都算作动态资源，动态资源经过了 Tomcat 处理，速度必然降低。对于静态资源，例如图片、HTML、JS、CSS 等资源，这种资源可以不必经过 Tomcat 解析，当客户端请求这些资源时，之间将资源返回给客户端就行了。此时，可以使用 Nginx 搭建静态资源服务器，将静态资源直接返回给客户端。 使用 Nginx 做负载均衡服务器，无论是使用 Dubbo 还是 Spirng Cloud ，除了使用各自自带的负载均衡策略之外，也都可以使用 Nginx 做负载均衡服务器。 支持高并发、内存消耗少、成本低廉、配置简单、运行稳定等。 Nginx 安装：由于基本上都是在 Linux 上使用 Nginx，因此松哥这里主要向大家展示 CentOS 7 安装 Nginx： 首先下载 Nginx 1wget http://nginx.org/download/nginx-1.17.0.tar.gz 然后解压下载的目录，进入解压目录中，在编译安装之前，需要安装两个依赖： 12yum -y install pcre-develyum -y install openssl openssl-devel 然后开始编译安装： 123./configuremakemake install 装好之后，默认安装位置在 ： 1/usr/local/nginx/sbin/nginx 进入到该目录的 sbin 目录下，执行 nginx 即可启动 Nginx ： Nginx 启动成功之后，在浏览器中直接访问 Nginx 地址： 看到如上页面，表示 Nginx 已经安装成功了。 如果修改了 Nginx 配置，则可以通过如下命令重新加载 Nginx 配置文件： 1./nginx -s reload 总结本文算是一个简单的 Nginx 扫盲文，希望大家看完后对 Nginx 有一个基本的认知。本文先说到这里，有问题欢迎留言讨论。","link":"/2019/0605/nginx-guide.html"},{"title":"Redis 中的五种数据类型简介","text":"上篇文章我们介绍了如何在 Linux 中安装 Redis，本文我们来了解下 Redis 中的五种数据类型。 本文是 Redis 系列的第二篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis 五大数据类型介绍redis 中的数据都是以 key/value 的形式存储的，五大数据类型主要是指 value 的数据类型，包含如下五种： STRINGSTRING 是 redis 中最基本的数据类型，redis 中的 STRING 类型是二进制安全的，即它可以包含任何数据，比如一个序列化的对象甚至一个 jpg 图片，要注意的是 redis 中的字符串大小上限是 512M 。 LISTLIST 是一个简单的字符串列表，按照插入顺序进行排序，我们可以从 LIST 的头部 (LEFT) 或者尾部 (RIGHT) 插入一个元素，也可以从 LIST 的头部(LEFT)或者尾部 (RIGHT) 弹出一个元素。 HASHHASH 类似于 Java 中的 Map ，是一个键值对集合，在 redis 中可以用来存储对象。 SETSET 是 STRING 类型的无序集合，不同于 LIST ，SET 中的元素不可以重复。 ZSETZSET 和 SET 一样，也是 STRING 类型的元素的集合，不同的是 ZSET 中的每个元素都会关联一个 double 类型的分数，ZSET 中的成员都是唯一的，但是所关联的分数可以重复。 OK，通过上面的介绍，相信小伙伴们对五大数据类型都有一个大致的认识了，接下来我们就来看看这五种数据类型要怎么操作。 key 相关的命令由于五大数据类型的数据结构本身有差异，因此对应的命令也会不同，但是有一些命令不管对于哪种数据类型都是存在的，我们今天就先来看看这样一些特殊的命令。 首先通过 redis-server redis.conf 命令启动 redi s，再通过 redis-cli 命令进入到控制台中，如下： 首先我们可以通过 set 命令插入一条记录： 12127.0.0.1:6379&gt; set k1 v1OK DEL 命令看到 OK 表示插入成功。通过 DEL 命令我们可以删除一个已经存在的 key，如下： 12127.0.0.1:6379&gt; DEL k1(integer) 1 看到 (integer) 1 表示数据已经删除成功。 DUMP 命令DUMP 命令可以序列化给定的 key，并返回序列化之后的值： 12127.0.0.1:6379&gt; DUMP k1&quot;\\x00\\x02v1\\b\\x00\\xe6\\xc8\\\\\\xe1bI\\xf3c&quot; EXISTS 命令EXISTS 命令用来检测一个给定的 key 是否存在，如下： 12345127.0.0.1:6379&gt; EXISTS k1(integer) 1127.0.0.1:6379&gt; EXISTS k2(integer) 0127.0.0.1:6379&gt; 上面的运行结果表示 k1 存在而 k2 不存在。 TTL 命令TTL 命令可以查看一个给定 key 的有效时间： 1234127.0.0.1:6379&gt; TTL k1(integer) -1127.0.0.1:6379&gt; TTL k2(integer) -2 -2 表示 key 不存在或者已过期；-1 表示 key 存在并且没有设置过期时间（永久有效）。当然，我们可以通过下面的命令给 key 设置一个过期时间： EXPIRE 命令EXPIRE 命令可以给 key 设置有效期，在有效期过后，key 会被销毁。 12345127.0.0.1:6379&gt; EXPIRE k1 30(integer) 1127.0.0.1:6379&gt; TTL k1(integer) 25127.0.0.1:6379&gt; 30 表示 30 秒，TTL k1 返回 25 表示这个 key 的有效期还剩 25 秒。 PERSIST 命令PERSIST 命令表示移除一个 key 的过期时间，这样该 key 就永远不会过期： 12345678127.0.0.1:6379&gt; EXPIRE k1 60(integer) 1127.0.0.1:6379&gt; ttl k1(integer) 57127.0.0.1:6379&gt; PERSIST k1(integer) 1127.0.0.1:6379&gt; ttl k1(integer) -1 PEXPIRE 命令PEXPIRE 命令的功能和 EXPIRE 命令的功能基本一致，只不过这里设置的参数是毫秒： 12127.0.0.1:6379&gt; PEXPIRE k1 60000(integer) 1 PTTL 命令PTTL 命令和 TTL 命令基本一致，只不过 PTTL 返回的是毫秒数： 12127.0.0.1:6379&gt; PTTL k1(integer) 25421 KEYS 命令KEYS 命令可以获取满足给定模式的所有 key，比如： 1234127.0.0.1:6379&gt; KEYS *1) &quot;k3&quot;2) &quot;k2&quot;3) &quot;k1&quot; KEYS * 表示获取所有的 KEY， * 也可以是一个正则表达式。 OK,key 相关的命令我们就介绍这么多，当然还有很多其他的，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-datatype.html"},{"title":"Redis 中的发布订阅和事务","text":"hello，小伙伴们好久不见！前面我们说了 redis 中的基本数据类型，本文我们来看看 redis 中的发布订阅和事务，因为这两个都比较简单，因此我放在一篇文章中来讲。 本文是 Redis 系列的第七篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中BIT相关命令5.Redis 列表与集合6.Redis 散列与有序集合 发布订阅redis 的发布订阅系统有点类似于我们生活中的电台，电台可以在某一个频率上发送广播，而我们可以接收任何一个频率的广播，Android 中的 broadcast 也和这类似。 订阅消息的方式如下: 1234567891011127.0.0.1:6379&gt; SUBSCRIBE c1 c2 c3Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;c1&quot;3) (integer) 11) &quot;subscribe&quot;2) &quot;c2&quot;3) (integer) 21) &quot;subscribe&quot;2) &quot;c3&quot;3) (integer) 3 这个表示接收 c1，c2，c3 三个频道传来的消息，发送消息的方式如下： 12127.0.0.1:6379&gt; PUBLISH c1 &quot;hello redis!&quot;(integer) 1 当 c1 这个频道上有消息发出时，此时在消息订阅控制台可以看到如下输出： 1231) &quot;message&quot;2) &quot;c1&quot;3) &quot;hello redis!&quot; 在 redis 中，我们也可以使用模式匹配订阅，如下： 12345127.0.0.1:6379&gt; PSUBSCRIBE c*Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot;2) &quot;c*&quot;3) (integer) 1 此时可以接收到所有以 c 开头的频道发来的消息。 tipsredis 中的发布订阅系统在某些场景下还是非常好用的，但是也有一些问题需要注意：由于网络在传输过程中可能会遭遇断线等意外情况，断线后需要进行重连，然而这会导致断线期间的数据丢失。 事务既然 redis 是一种 NoSQL 数据库，那它当然也有事务的功能，不过这里的事务和我们关系型数据库中的事务有一点点差异。 redis 中事务的用法非常简单，我们通过 MULTI 命令开启一个事务，如下： 12127.0.0.1:6379&gt; MULTIOK 在 MULTI 命令执行之后，我们可以继续发送命令去执行，此时的命令不会被立马执行，而是放在一个队列中，如下： 123456127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3QUEUED 当所有的命令都输入完成后，我们可以通过 EXEC 命令发起执行，也可以通过 DISCARD 命令清空队列，如下： 1234127.0.0.1:6379&gt; EXEC1) OK2) OK3) OK 事务中的异常情况redis 中事务的异常情况总的来说分为两类： 进入队列之前就能发现的错误，比如命令输错； 执行 EXEC 之后才能发现的错误，比如给一个非数字字符加 1 ； 那么对于这两种不同的异常，redis 中有不同的处理策略。对于第一种错误，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务（这个是2.6.5之后的版本做法，之前的版本做法小伙伴可以参考官方文档）。如下： 12345678910111213141516171819127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set kv1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3 3 3QUEUED127.0.0.1:6379&gt; set k4 v4QUEUED127.0.0.1:6379&gt; EXEC1) OK2) OK3) (error) ERR syntax error4) OK127.0.0.1:6379&gt; keys *1) &quot;k4&quot;2) &quot;k2&quot;3) &quot;kv1&quot; 而对于第二种情况，redis 并没有对它们进行特别处理， 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。如下： 1234567891011127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set k1 vvQUEUED127.0.0.1:6379&gt; INCR k1QUEUED127.0.0.1:6379&gt; EXEC1) OK2) (error) ERR value is not an integer or out of range127.0.0.1:6379&gt; GET k1&quot;vv&quot; 不同于关系型数据库，redis 中的事务出错时没有回滚，对此，官方的解释如下： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 WATCH 命令事务中的 WATCH 命令可以用来监控一个 key，通过这种监控，我们可以为 redis 事务提供(CAS)行为。 如果有至少一个被 WATCH 监视的键在 EXEC 执行之前被修改了，那么整个事务都会被取消，EXEC 返回 nil-reply 来表示事务已经失败。如下： 通过 unwatch 命令，可以取消对一个 key 的监控，如下： OK,发布订阅和事务我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-pub-sub.html"},{"title":"Redis 主从复制(一)","text":"前面两篇文章和小伙伴们聊了 redis 中的数据备份问题，也对快照备份和 AOF 备份做了对比，本文我们来聊聊 redis 中的主从复制问题，算是数据备份的第三种解决方案。 本文是 Redis 系列的第十篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化 主从复制主从复制可以在一定程度上扩展 redis 性能，redis 的主从复制和关系型数据库的主从复制类似，从机能够精确的复制主机上的内容。实现了主从复制之后，一方面能够实现数据的读写分离，降低 master 的压力，另一方面也能实现数据的备份。 配置方式假设我有三个 redis 实例，地址分别如下： 123192.168.248.128:6379 192.168.248.128:6380 192.168.248.128:6381 即同一台服务器上三个实例，配置方式如下： 1.将 redis.conf 文件更名为 redis6379.conf ，方便我们区分，然后把 redis6379.conf 再复制两份，分别为 redis6380.conf 和 redis6381.conf 。如下： 2.打开 redis6379.conf ，将如下配置均加上 6379,(默认是 6379 的不用修改)，如下： 12345port 6379pidfile /var/run/redis_6379.pidlogfile &quot;6379.log&quot;dbfilename dump6379.rdbappendfilename &quot;appendonly6379.aof&quot; 3.同理，分别打开 redis6380.conf和redis6381.conf 两个配置文件，将第二步涉及到 6379 的分别改为 6380 和 6381 。4.输入如下命令，启动三个 redis 实例： 123[root@localhost redis-4.0.8]# redis-server redis6379.conf[root@localhost redis-4.0.8]# redis-server redis6380.conf[root@localhost redis-4.0.8]# redis-server redis6381.conf 5.输入如下命令，分别进入三个实例的控制台： 123[root@localhost redis-4.0.8]# redis-cli -p 6379[root@localhost redis-4.0.8]# redis-cli -p 6380[root@localhost redis-4.0.8]# redis-cli -p 6381 此时我就成功配置了三个 redis 实例了。 6.假设在这三个实例中，6379 是主机，即 master，6380 和 6381 是从机，即 slave，那么如何配置这种实例关系呢，很简单，分别在 6380 和 6381 上执行如下命令： 12127.0.0.1:6381&gt; SLAVEOF 127.0.0.1 6379OK 这一步也可以通过在两个从机的 redis.conf 中添加如下配置来解决： 1slaveof 127.0.0.1 6379 OK，主从关系搭建好后，我们可以通过如下命令可以查看每个实例当前的状态，如下: 1234567891011121314127.0.0.1:6379&gt; INFO replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=6380,state=online,offset=56,lag=1slave1:ip=127.0.0.1,port=6381,state=online,offset=56,lag=0master_replid:26ca818360d6510b717e471f3f0a6f5985b6225dmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:56second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:56 我们可以看到 6379 是一个主机，上面挂了两个从机，两个从机的地址、端口等信息都展现出来了。如果我们在 6380 上执行 INFO replication ，显示信息如下: 1234567891011121314151617181920127.0.0.1:6380&gt; INFO replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:6master_sync_in_progress:0slave_repl_offset:630slave_priority:100slave_read_only:1connected_slaves:0master_replid:26ca818360d6510b717e471f3f0a6f5985b6225dmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:630second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:630 我们可以看到 6380 是一个从机，从机的信息以及它的主机的信息都展示出来了。 7.此时，我们在主机中存储一条数据，在从机中就可以 get 到这条数据了。 主从复制注意点 如果主机已经运行了一段时间了，并且了已经存储了一些数据了，此时从机连上来，那么从机会将主机上所有的数据进行备份，而不是从连接的那个时间点开始备份。 配置了主从复制之后，主机上可读可写，但是从机只能读取不能写入（可以通过修改 redis.conf 中 slave-read-only 的值让从机也可以执行写操作）。 在整个主从结构运行过程中，如果主机不幸挂掉，重启之后，他依然是主机，主从复制操作也能够继续进行。 复制原理每一个 master 都有一个 replication ID ，这是一个较大的伪随机字符串，标记了一个给定的数据集。每个 master 也持有一个偏移量，master 将自己产生的复制流发送给 slave 时，发送多少个字节的数据，自身的偏移量就会增加多少，目的是当有新的操作修改自己的数据集时，它可以以此更新 slave 的状态。复制偏移量即使在没有一个 slave 连接到 master 时，也会自增，所以基本上每一对给定的 Replication ID, offset 都会标识一个 master 数据集的确切版本。当 slave 连接到 master 时，它们使用 PSYNC 命令来发送它们记录的旧的 master replication ID 和它们至今为止处理的偏移量。通过这种方式，master 能够仅发送 slave 所需的增量部分。但是如果 master 的缓冲区中没有足够的命令积压缓冲记录，或者如果 slave 引用了不再知道的历史记录 （replication ID） ，则会转而进行一个全量重同步：在这种情况下，slave 会得到一个完整的数据集副本，从头开始(参考 redis 官网)。 简单来说，就是以下几个步骤： slave 启动成功连接到 master 后会发送一个 sync 命令。 Master 接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令。 在后台进程执行完毕之后，master 将传送整个数据文件到 slave ,以完成一次完全同步。 全量复制：而 slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave ,完成同步。 但是只要是重新连接 master ,一次完全同步（全量复制)将被自动执行。 OK,redis 主从复制我们先介绍这么多，更多资料小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-master-slave-1.html"},{"title":"Redis 主从复制(二)","text":"上篇文章和小伙伴们一起搭建了 redis 主从复制环境，但是还不完善，本文我想再和小伙伴们聊聊主从复制环境搭建的一些细节。 本文是 Redis 系列的第十一篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一) 本文接上文，所用三个 redis 实例和上文一致，这里就不再赘述三个实例搭建方式。 一场接力赛在上篇文章中，我们搭建的主从复制模式是下面这样的： 实际上，一主二仆的主从复制，我们可以搭建成下面这种结构： 搭建方式很简单，在前文基础上，我们只需要修改 6381 的 master 即可，在 6381 实例上执行如下命令，让 6381 从 6380 实例上复制数据，如下： 12127.0.0.1:6381&gt; SLAVEOF 127.0.0.1 6380OK 此时，我们再看 6379 的 slave ，如下： 12345678910111213127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=6380,state=online,offset=0,lag=1master_replid:4a38bbfa37586c29139b4ca1e04e8a9c88793651master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:0 只有一个 slave，就 6380 ，我们再看 6380 的信息，如下： 123456789101112131415161718192021127.0.0.1:6380&gt; info replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:1master_sync_in_progress:0slave_repl_offset:70slave_priority:100slave_read_only:1connected_slaves:1slave0:ip=127.0.0.1,port=6381,state=online,offset=70,lag=0master_replid:4a38bbfa37586c29139b4ca1e04e8a9c88793651master_replid2:0000000000000000000000000000000000000000master_repl_offset:70second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:70 6380 此时的角色是一个从机，它的主机是 6379，但是 6380 自己也有一个从机，那就是 6381 .此时我们的主从结构如下图： 哨兵模式结合上篇文章，我们一共介绍了两种主从模式了，但是这两种，不管是哪一种，都会存在这样一个问题，那就是当主机宕机时，就会发生群龙无首的情况，如果在主机宕机时，能够从从机中选出一个来充当主机，那么就不用我们每次去手动重启主机了，这就涉及到一个新的话题，那就是哨兵模式。 所谓的哨兵模式，其实并不复杂，我们还是在我们前面的基础上来搭建哨兵模式。假设现在我的 master 是 6379 ，两个从机分别是 6380 和 6381 ，两个从机都是从 6379 上复制数据。先按照上文的步骤，我们配置好一主二仆，然后在 redis 目录下打开 sentinel.conf 文件，做如下配置： 1sentinel monitor mymaster 127.0.0.1 6379 1 其中 mymaster 是给要监控的主机取的名字，随意取，后面是主机地址，最后面的 2 表示有多少个 sentinel 认为主机挂掉了，就进行切换（我这里只有一个，因此设置为1）。好了，配置完成后，输入如下命令启动哨兵： 1redis-sentinel sentinel.conf 然后启动我们的一主二仆架构，启动成功后，关闭 master，观察哨兵窗口输出的日志，如下： 小伙伴们可以看到，6379 挂掉之后，redis 内部重新举行了选举，6380 重新上位。此时，如果 6379 重启，也不再是扛把子了，只能屈身做一个 slave 了。 注意问题由于所有的写操作都是先在 Master 上操作，然后同步更新到 Slave 上，所以从 Master 同步到 Slave 机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave 机器数量的增加也会使这个问题更加严重。因此我们还需要集群来进一步提升 redis 性能，这个问题我们将在后面说到。 OK,redis 主从复制问题我们就介绍这么多，更多资料小伙伴们可以参考官方文档http://www.redis.net.cn/tutorial/3501.html。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-master-slave-2.html"},{"title":"Redis 之 AOF 持久化","text":"上篇文章和小伙伴们聊了使用快照的方式实现 redis 数据的持久化，这只是持久化的一种方式，本文我们就来看看另一种持久化方式， AOF(append-only file)。 本文是 Redis 系列的第九篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化 AOF 持久化与快照持久化不同，AOF 持久化是将被执行的命令写到 aof 文件末尾，在恢复时只需要从头到尾执行一遍写命令即可恢复数据，AOF 在 redis 中默认也是没有开启的，需要我们手动开启，开启方式如下： 打开 redis.conf 配置文件，修改 appendonly 属性值为 yes ，如下： 1appendonly yes 另外几个和 AOF 相关的属性如下： 1234567appendfilename &quot;appendonly.aof&quot;# appendfsync alwaysappendfsync everysec# appendfsync nono-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 这几个属性的含义分别如下： 1.appendfilename 表示生成的 AOF 备份文件的文件名。2.appendfsync 表示备份的时机，always 表示每执行一个命令就备份一次，everysec 表示每秒备份一次，no 表示将备份时机交给操作系统。3.no-appendfsync-on-rewrite 表示在对 aof 文件进行压缩时，是否执行同步操作。4.最后两行配置表示 AOF 文件的压缩时机，这个我们一会再细说。 同时为了避免快照备份的影响，我们将快照备份关闭，关闭方式如下： 1234save &quot;&quot;# save 900 1# save 300 10# save 60 10000 此时，当我们在 redis 中进行数据操作时，就会自动生成 AOF 的配置文件 appendonly.aof ，如下： 注意此时没有 dump.rdb 文件，这时我们将 redis 关闭并重启，会发现之前的数据都还在，这就是 AOF 备份的结果。 AOF 备份的几个关键点1.通过上面的介绍，小伙伴们了解到 appendfsync 的取值一共有三种，我们在项目中首选 everysec，always 选项会严重降低 redis 性能。2.使用 everysec ，最坏的情况下我们可能丢失1秒的数据。 AOF 文件的重写与压缩AOF 备份有很多明显的优势，当然也有劣势，那就是文件大小。随着系统的运行，AOF 的文件会越来越大，甚至把整个电脑的硬盘填满，AOF 文件的重写与压缩机制可以在一定程度上缓解这个问题。当 AOF 的备份文件过大时，我们可以向 redis 发送一条 bgrewriteaof 命令进行文件重写，如下： 123127.0.0.1:6379&gt; BGREWRITEAOFBackground append only file rewriting started(0.71s) bgrewriteaof 的执行原理和我们上文说的 bgsave 的原理一致，这里我就不再赘述，因此 bgsave 执行过程中存在的问题在这里也一样存在。 bgrewriteaof 也可以自动执行，自动执行时间则依赖于 auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size 配置，auto-aof-rewrite-percentage 100 表示当目前 aof 文件大小超过上一次重写时的 aof 文件大小的百分之多少时会再次进行重写，如果之前没有重写，则以启动时的 aof 文件大小为依据，同时还要求 AOF 文件的大小至少要大于 64M(auto-aof-rewrite-min-size 64mb)。 最佳实践 如果 redis 只做缓存服务器，那么可以不使用任何持久化方式。 同时开启两种持久化方式，在这种情况下,当 redis 重启的时候会优先载入 AOF 文件来恢复原始的数据, 因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整；RDB 的数据不完整时，同时使用两者时服务器重启也只会找 AOF 文件。那要不要只使用 AOF 呢？ 作者建议不要，因为 RDB 更适合用于备份数据库( AOF 在不断变化不好备份)， 快速重启，而且不会有 AOF 可能潜在的 bug ，留着作为一个万一的手段。 因为 RDB 文件只用作后备用途，建议只在 slave 上持久化 RDB 文件，而且只要 15 分钟备份一次就够了，只保留 save 900 1 这条规则。 如果 Enalbe AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只 load 自己的 AOF 文件就可以了。代价一是带来了持续的 IO，二是 AOF rewrite 的最后将 rewrite 过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少 AOF rewrite 的频率，AOF 重写的基础大小默认值 64M 太小了，可以设到 5G 以上。默认超过原大小 100% 大小时重写可以改到适当的数值。 如果不 Enable AOF ，仅靠 Master-Slave Replication 实现高可用性也可以。能省掉一大笔 IO 也减少了 rewrite 时带来的系统波动。代价是如果 Master/Slave 同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个 Master/Slave 中的 RDB 文件，载入较新的那个。 OK,redis 数据持久化我们就介绍这么多，更多资料，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-aof.html"},{"title":"Redis 列表与集合","text":"前面文章我们介绍了 STRING 的基本命令，本文我们来看看 Redis 中的列表与集合。 本文是 Redis 系列的第五篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令 列表列表是 Redis 中另外一种数据类型。下面我们来看看列表中一些基本的操作命令。 LPUSH将一个或多个值 value 插入到列表 key 的表头，如果有多个 value 值，那么各个 value 值按从左到右的顺序依次插入到表头，如下： 12127.0.0.1:6379&gt; LPUSH k1 v1 v2 v3(integer) 3 LRANGE返回列表 key 中指定区间内的元素，区间以偏移量 start 和 stop 指定，下标 (index) 参数 start 和 stop 都以 0 为底，即 0 表示列表的第一个元素，1 表示列表的第二个元素，以此类推。我们也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。如下： 1234127.0.0.1:6379&gt; LRANGE k1 0 -11) &quot;v3&quot;2) &quot;v2&quot;3) &quot;v1&quot; RPUSHRPUSH 与 LPUSH 的功能基本一致，不同的是 RPUSH 的中的 value 值是按照从右到左的顺序依次插入，如下： 12345678127.0.0.1:6379&gt; RPUSH k2 1 2 3 4 5(integer) 5127.0.0.1:6379&gt; LRANGE k2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;5) &quot;5&quot; RPOPRPOP 命令可以移除并返回列表 key 的尾元素。如下： 1234567127.0.0.1:6379&gt; RPOP k2&quot;5&quot;127.0.0.1:6379&gt; LRANGE k2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot; LPOPLPOP 和 RPOP 类似，不同的是 LPOP 移除并返回列表 key 的头元素，如下： 123456127.0.0.1:6379&gt; LPOP k2&quot;1&quot;127.0.0.1:6379&gt; LRANGE k2 0 -11) &quot;2&quot;2) &quot;3&quot;3) &quot;4&quot; LINDEXLINDEX 命令可以返回列表 key 中，下标为 index 的元素，正数下标 0 表示第一个元素，也可以使用负数下标，-1 表示倒数第一个元素，如下： 1234127.0.0.1:6379&gt; LINDEX k2 0&quot;2&quot;127.0.0.1:6379&gt; LINDEX k2 -1&quot;4&quot; LTRIMLTRIM 命令可以对一个列表进行修剪，即让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。下标与之前介绍的写法都一致，这里不赘述。如下： 123456789127.0.0.1:6379&gt; LRANGE k1 0 -11) &quot;v3&quot;2) &quot;v2&quot;3) &quot;v1&quot;127.0.0.1:6379&gt; LTRIM k1 0 1OK127.0.0.1:6379&gt; LRANGE k1 0 -11) &quot;v3&quot;2) &quot;v2&quot; BLPOPBLPOP 是阻塞式列表的弹出原语。它是命令 LPOP 的阻塞版本，当给定列表内没有任何元素可供弹出的时候，连接将被 BLPOP 命令阻塞。当给定多个 key 参数时，按参数 key 的先后顺序依次检查各个列表，弹出第一个非空列表的头元素。同时，在使用该命令时也需要指定阻塞的时长，时长单位为秒，在该时长内如果没有元素可供弹出，则阻塞结束。返回的结果是 key 和 value 的组合，如下： 123456127.0.0.1:6379&gt; BLPOP k1 101) &quot;k1&quot;2) &quot;v2&quot;127.0.0.1:6379&gt; BLPOP k1 10(nil)(10.03s) 最后，BRPOP、BPOPLPUSH、BRPOPLPUSH 都是相应命令的阻塞版本，这里就不赘述了。 集合接下来我们来看看集合中一些常见的操作命令： SADDSADD 命令可以添加一个或多个指定的 member 元素到集合的 key 中，指定的一个或者多个元素 member 如果已经在集合 key 中存在则忽略，如果集合 key 不存在，则新建集合 key ,并添加 member 元素到集合 key 中。如下： 12127.0.0.1:6379&gt; SADD k1 v1 v2 v3 v4(integer) 4 SREMSREM 命令可以在 key 集合中移除指定的元素，如果指定的元素不是 key 集合中的元素则忽略。如果 key 集合不存在则被视为一个空的集合，该命令返回 0 。如下： 1234127.0.0.1:6379&gt; SREM k1 v2(integer) 1127.0.0.1:6379&gt; SREM k1 v10(integer) 0 SISMEMBERSISMEMBER 命令可以返回成员 member 是否是存储的集合 key 的成员。如下： 12127.0.0.1:6379&gt; SISMEMBER k1 v3(integer) 1 SCARDSCARD 命令可以返回集合存储的 key 的基数(集合元素的数量)，如下： 12127.0.0.1:6379&gt; SCARD k1(integer) 3 SMEMBERSSMEMBERS 命令可以返回 key 集合所有的元素，如下： 1234127.0.0.1:6379&gt; SMEMBERS k11) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot; SRANDMEMBERSRANDMEMBER 仅需我们提供 key 参数,它就会随机返回 key 集合中的一个元素，从 Redis2.6 开始,该命令也可以接受一个可选的 count 参数,如果 count 是整数且小于元素的个数，则返回 count 个随机元素,如果 count 是整数且大于集合中元素的个数时,则返回集合中的所有元素,当 count 是负数,则会返回一个包含 count 的绝对值的个数元素的数组，如果 count 的绝对值大于元素的个数,则返回的结果集里会出现一个元素出现多次的情况。如下： 1234567891011121314151617127.0.0.1:6379&gt; SRANDMEMBER k1&quot;v4&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 21) &quot;v4&quot;2) &quot;v1&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 51) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 -11) &quot;v4&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 -51) &quot;v3&quot;2) &quot;v1&quot;3) &quot;v1&quot;4) &quot;v3&quot;5) &quot;v3&quot; SPOPSPOP 命令的用法和 SRANDMEMBER 类似，不同的是，SPOP 每次选择一个随机的元素之后，该元素会出栈，而 SRANDMEMBER 则不会出栈，只是将该元素展示出来。 SMOVESMOVE 命令可以将 member 从 source 集合移动到 destination 集合中，如下： 1234567127.0.0.1:6379&gt; SMOVE k1 k2 v1(integer) 1127.0.0.1:6379&gt; SMEMBERS k11) &quot;v4&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; SMEMBERS k21) &quot;v1&quot; SDIFFSDIFF 可以用来返回一个集合与给定集合的差集的元素，如下： 123127.0.0.1:6379&gt; SDIFF k1 k21) &quot;v4&quot;2) &quot;v3&quot; k1 中的元素是 v3、v4，k2 中的元素是 v1，差集就是 v3、v4. SDIFFSTORESDIFFSTORE 命令与 SDIFF 命令基本一致，不同的是 SDIFFSTORE 命令会将结果保存在一个集合中，如下： 12345127.0.0.1:6379&gt; SDIFFSTORE key k1 k2(integer) 2127.0.0.1:6379&gt; SMEMBERS key1) &quot;v4&quot;2) &quot;v3&quot; SINTERSINTER 命令可以用来计算指定 key 之间元素的交集，如下： 12345678127.0.0.1:6379&gt; SMEMBERS k11) &quot;v4&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; SMEMBERS k21) &quot;v1&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; SINTER k1 k21) &quot;v3&quot; SINTERSTORESINTERSTORE 命令和 SINTER 命令类似，不同的是它会将结果保存到一个新的集合中，如下： 1234127.0.0.1:6379&gt; SINTERSTORE k3 k1 k2(integer) 1127.0.0.1:6379&gt; SMEMBERS k31) &quot;v3&quot; SUNIONSUNION 可以用来计算两个集合的并集，如下： 1234127.0.0.1:6379&gt; SUNION k1 k21) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot; SUNIONSTORESUNIONSTORE 和 SUNION 命令类似，不同的是它会将结果保存到一个新的集合中，如下： 123456127.0.0.1:6379&gt; SUNIONSTORE k4 k1 k2(integer) 3127.0.0.1:6379&gt; SMEMBERS k41) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot; OK,列表和集合的命令我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-list-set.html"},{"title":"Redis 字符串 STRING 中 BIT 相关命令","text":"上篇文章我们对 STRING 数据类型中一些基本的命令进行了介绍，但是没有涉及到 BIT 相关的命令，本文我们就来看看几个和 BIT 相关的命令。 本文是 Redis 系列的第四篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍 BIT 相关的命令是指 BITCOUNT/BITFIELD/BITOP/BITPOS/SETBIT/GETBIT 几个命令，灵活使用这几个命令，可以给我们的项目带来很多惊喜。 准备知识在学习这几个命令之前，我们得先了解下 redis 中字符串的存储方式，redis 中的字符串都是以二进制的方式进行存储的，比如说我执行如下命令： 12127.0.0.1:6379&gt; SET k1 aOK a 对应的 ASCII 码是 97 ，转换为二进制数据是 01100001 ，我们 BIT 相关命令都是对这个二进制数据进行操作。请继续往下看。 GETBITGETBIT 命令可以返回 key 对应的 value 在 offset 处的 bit 值，以上文提到的 k1 为例， a 对应的二进制数据是 01100001 ，所以当 offset 为 0 时，对应的 bit 值为 0 ； offset 为 1 时，对应的 bit 值为 1 ； offset 为 2 时，对应的 bit 值为 1 ；offset 为 3 时，对应的 bit 值为 0，依此类推….，如下： 12345678910111213141516127.0.0.1:6379&gt; GETBIT k1 0(integer) 0127.0.0.1:6379&gt; GETBIT k1 1(integer) 1127.0.0.1:6379&gt; GETBIT k1 2(integer) 1127.0.0.1:6379&gt; GETBIT k1 3(integer) 0127.0.0.1:6379&gt; GETBIT k1 4(integer) 0127.0.0.1:6379&gt; GETBIT k1 5(integer) 0127.0.0.1:6379&gt; GETBIT k1 6(integer) 0127.0.0.1:6379&gt; GETBIT k1 7(integer) 1 SETBITSETBIT 可以用来修改二进制数据，比如 a 对应的 ASCII 码为 97，c 对应的 ASCII 码为 99，97 转为二进制是 01100001 ，99 转为二进制是 01100011 ，两个的差异在于第六位一个是 0 一个是 1 ，通过 SETBIT 命令，我们可以将 k1 的第六位的 0 改为 1 （第六位是从 0 开始算），如下： 1234127.0.0.1:6379&gt; SETBIT k1 6 1(integer) 0127.0.0.1:6379&gt; GET k1&quot;c&quot; 此时，k1 中存储的字符也就变为了 c。SETBIT 在执行时所返回的数字，表示该位上原本的 bit 值。 BITCOUNTBITCOUNT 可以用来统计这个二进制数据中 1 的个数，如下： 12127.0.0.1:6379&gt; BITCOUNT k1(integer) 4 关于 BITCOUNT，redis 官网上有一个非常有意思的案例：用户上线次数统计。节选部分原文如下： 举个例子，如果今天是网站上线的第 100 天，而用户 peter 在今天阅览过网站，那么执行命令 SETBIT peter 100 1 ；如果明天 peter 也继续阅览网站，那么执行命令 SETBIT peter 101 1 ，以此类推。当要计算 peter 总共以来的上线次数时，就使用 BITCOUNT 命令：执行 BITCOUNT peter ，得出的结果就是 peter 上线的总天数。 这种统计方式最大的好处就是节省空间并且运算速度快。每天占用一个 bit，一年也就 365 个 bit，10 年也就 10*365 个 bit ，也就是 456 个字节，对于这么大的数据，bit 的操作速度非常快。 BITOPBITOP 可以对一个或者多个二进制位串执行并 (AND)、或 (OR)、异或 (XOR) 以及非 (NOT) 运算，如下：a 对应的 ASCII 码转为二进制是 01100001 ，c 对应的二进制位串是 01100011 。对这两个二进制位串分别执行 AND\\OR\\XOR 的结果如下： 12345678910111213141516127.0.0.1:6379&gt; set k1 aOK127.0.0.1:6379&gt; set k2 cOK127.0.0.1:6379&gt; BITOP and k3 k1 k2(integer) 1127.0.0.1:6379&gt; get k3&quot;a&quot;127.0.0.1:6379&gt; BITOP or k3 k1 k2(integer) 1127.0.0.1:6379&gt; get k3&quot;c&quot;127.0.0.1:6379&gt; BITOP xor k3 k1 k2(integer) 1127.0.0.1:6379&gt; get k3&quot;\\x02&quot; 另外，BITOP 也可以执行 NOT 运算，但是注意参数个数，如下： 12127.0.0.1:6379&gt; BITOP not k3 k4(integer) 1 这里会对 k4 的二进制位串取反，将取反结果交给 k3 。 BITPOSBITPOS 用来获取二进制位串中第一个 1 或者 0 的位置，如下： 123456127.0.0.1:6379&gt; set k1 aOK127.0.0.1:6379&gt; BITPOS k1 1(integer) 1127.0.0.1:6379&gt; BITPOS k1 0(integer) 0 也可以在后面设置一个范围，不过后面的范围是字节的范围，而不是二进制位串的范围。 OK,STRING 中 BIT 相关的命令我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-string-bit.html"},{"title":"Redis 字符串 STRING 介绍","text":"上篇文章我们介绍了五种数据类型中一些通用的命令，本文我们来看看 STRING 数据类型独有的操作命令。 本文是 Redis 系列的第三篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介 STRINGAPPEND使用 APPEND 命令时，如果 key 已经存在，则会直接在 value 后追加值，如果 key 不存在，则会先创建一个 value 为空字符串的 key ，然后再追加： 12345678127.0.0.1:6379&gt; APPEND k1 hello(integer) 5127.0.0.1:6379&gt; GET k1&quot;hello&quot;127.0.0.1:6379&gt; APPEND k1 world(integer) 10127.0.0.1:6379&gt; GET k1&quot;helloworld&quot; DECRDECR 命令可以实现对 value 的减 1 操作，如果 key 不存在，则 key 对应的初始值会被置为 0 ，如果 key 的 value 不为数字，则会报错，如下： 12345678910127.0.0.1:6379&gt; SET k3 19OK127.0.0.1:6379&gt; DECR k3(integer) 18127.0.0.1:6379&gt; GET k3&quot;18&quot;127.0.0.1:6379&gt; SET k4 aaOK127.0.0.1:6379&gt; DECR k4(error) ERR value is not an integer or out of range DECRBYDECRBY 和 DECR 类似，不同的是 DECRBY 可以指定步长，如下： 123456127.0.0.1:6379&gt; GET k3&quot;8&quot;127.0.0.1:6379&gt; DECRBY k3 4(integer) 4127.0.0.1:6379&gt; GET k3&quot;4&quot; GETGET 命令用来获取对应 key 的 value，如果 key 不存在则返回 nil ，如下： 12127.0.0.1:6379&gt; GET k5(nil) GETRANGEGETRANGE 用来返回 key 所对应的 value 的子串，子串由 start 和 end 决定，从左往右计算，如果下标是负数，则从右往左计算，其中 -1 表示最后一个字符， -2 是倒数第二个…，如下： 123456127.0.0.1:6379&gt; SET k1 helloworldOK127.0.0.1:6379&gt; GETRANGE k1 0 2&quot;hel&quot;127.0.0.1:6379&gt; GETRANGE k1 -3 -1&quot;rld&quot; GETSETGETSET 命令可以用来获取 key 所对应的 value ，并对 key 进行重置，如下： 12345678127.0.0.1:6379&gt; SET k1 v1OK127.0.0.1:6379&gt; GET k1&quot;v1&quot;127.0.0.1:6379&gt; GETSET k1 vv&quot;v1&quot;127.0.0.1:6379&gt; GET k1&quot;vv&quot; INCRINCR 操作可以对指定 key 的 value 执行加 1 操作，如果指定的 key 不存在，那么在加 1 操作之前，会先将 key 的 value 设置为 0 ，如果 key 的 value 不是数字，则会报错。如下： 12127.0.0.1:6379&gt; INCR k2(integer) 1 INCRBYINCRBY 和 INCR 功能类似，不同的是可以指定增长的步长，如下： 12127.0.0.1:6379&gt; INCRBY k2 99(integer) 100 INCRBYFLOATINCRBYFLOAT 命令可以用来增长浮点数，如下： 1234127.0.0.1:6379&gt; SET k1 0.5OK127.0.0.1:6379&gt; INCRBYFLOAT k1 0.33&quot;0.83&quot; MGET与MSETMGET 与 MSET 分别用来批量设置值和批量获取值，如下： 123456127.0.0.1:6379&gt; MSET k1 v1 k2 v2 k3 v3OK127.0.0.1:6379&gt; MGET k1 k2 k31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot; SETEXSETEX 用来给 key 设置 value ，同时设置过期时间，等效于先给 key 设置 value ，再给 key 设置过期时间，如下： 123456127.0.0.1:6379&gt; SETEX k1 30 v1OK127.0.0.1:6379&gt; TTL k1(integer) 26127.0.0.1:6379&gt; GET k1&quot;v1&quot; PSETEXPSETEX 的作用和 SETEX 类似，不同的是，这里设置过期时间的单位是毫秒，如下： 1234127.0.0.1:6379&gt; PSETEX k1 60000 v1OK127.0.0.1:6379&gt; PTTL k1(integer) 55412 SETNXSETNX 是 SET if Not eXists 的简写，SET 命令在执行时，如果 key 已经存在，则新值会覆盖掉旧值，而对于 SETNX 命令，如果 key 已经存在，则不做任何操作，如果 key 不存在，则效果等同于 SET 命令。如下： 123456127.0.0.1:6379&gt; SETNX k1 v1(integer) 1127.0.0.1:6379&gt; SETNX k1 vv(integer) 0127.0.0.1:6379&gt; GET k1&quot;v1&quot; MSETNXMSETNX 兼具了 SETNX 和 MSET 的特性，但是 MSETNX 在执行时，如果有一个 key 存在，则所有的都不会执行，如下： 12127.0.0.1:6379&gt; MSETNX k1 v1 k2 v2(integer) 0 因为 k1 已经存在，所以 k2 也没执行成功。 SETRANGESETRANGE 用来覆盖一个已经存在的 key 的 value ，如下： 12345678127.0.0.1:6379&gt; set k1 helloworldOK127.0.0.1:6379&gt; get k1&quot;helloworld&quot;127.0.0.1:6379&gt; SETRANGE k1 5 redis(integer) 10127.0.0.1:6379&gt; get k1&quot;helloredis&quot; 但是如果已经存在的 key 的 value 长度小于 offset ，则不足的地方用 0 补齐，如下： 123456127.0.0.1:6379&gt; set k1 helloredisOK127.0.0.1:6379&gt; SETRANGE k1 20 --java(integer) 26127.0.0.1:6379&gt; GET k1&quot;helloredis\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00--java&quot; STRLENSTRLEN 用来计算 key 的 value 的长度，如下： 12127.0.0.1:6379&gt; STRLEN k1(integer) 26 OK,STRING 相关的命令我们就介绍这么多，当然还有很多其他的，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-string.html"},{"title":"Redis 快照持久化","text":"redis 的基础知识我们已经准备的差不多了，接下来两篇文章，我想和大家聊聊 redis 持久化这个话题。 本文是 Redis 系列的第八篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务 redis 持久化整体上来说，redis 持久化有两种方式，快照持久化和 AOF ，在项目中我们可以根据实际情况选择合适的持久化方式，也可以不用持久化，这关键看我们的 redis 在项目中扮演了什么样的角色。那么我将分别用两篇文章来介绍这两种不同的持久化方式，本文我们先来看看第一种方式。 快照持久化快照持久化，顾名思义，就是通过拍摄快照的方式实现数据的持久化，redis 可以在某个时间点上对内存中的数据创建一个副本文件，副本文件中的数据在 redis 重启时会被自动加载，我们也可以将副本文件拷贝到其他地方一样可以使用。 如何配置快照持久化redis中的快照持久化默认是开启的，redis.conf中相关配置主要有如下几项： 1234567save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesdbfilename dump.rdbdir ./ 前面三个 save 相关的选项表示备份的频率，分别表示 900 秒内至少一个键被更改则进行快照，300 秒内至少 10 个键被更改则进行快照，60 秒内至少 10000 个键被更改则进行快照， stop-writes-on-bgsave-error 表示在快照创建出错后，是否继续执行写命令， rdbcompression 则表示是否对快照文件进行压缩， dbfilename 表示生成的快照文件的名字，dir 则表示生成的快照文件的位置，在 redis 中，快照持久化默认就是开启的。我们可以通过如下步骤验证快照持久化的效果： 1.进入 redis 安装目录，如果有 dump.rdb 文件，先将之删除。如下： 2.启动 redis ，随便向 redis 中存储几个数据，然后关闭redis并退出，如下： 12345678[root@localhost redis-4.0.8]# redis-server redis.conf[root@localhost redis-4.0.8]# redis-cli127.0.0.1:6379&gt; set k1 v1OK127.0.0.1:6379&gt; set k2 v2OK127.0.0.1:6379&gt; SHUTDOWNnot connected&gt; exit 3.退出来后，我们发现刚刚删掉的 dump.rdb 文件又回来了，这就是生成的备份文件。4.此时再次启动 redis 并进入，发现刚刚存储的数据都还在，这是因为 redis 在启动时加载了 dump.rdb 中的数据。好了，关闭 redis 并退出。5.将 redis 目录下的 dump.rdb 文件删除。6.再次启动 redis 并进入到控制台，所有的数据都不存在了。 快照持久化操作流程通过上面的介绍，小伙伴们对快照持久化都有一个大致的认识了，那么这个东西到底是怎么运行的？持久化的时机是什么？我们来仔细扒一扒。 1.在 redis 运行过程中，我们可以向 redis 发送一条 save 命令来创建一个快照，save 是一个阻塞命令，redis 在接收到 save 命令之后，开始执行备份操作之后，在备份操作执行完毕之前，将不再处理其他请求，其他请求将被挂起，因此这个命令我们用的不多。save 命令执行如下： 12127.0.0.1:6379&gt; SAVEOK 2.在 redis 运行过程中，我们也可以发送一条 bgsave 命令来创建一个快照，不同于 save 命令，bgsave 命令会 fork 一个子进程，然后这个子进程负责执行将快照写入硬盘，而父进程则继续处理客户端发来的请求，这样就不会导致客户端命令阻塞了。如下： 12127.0.0.1:6379&gt; BGSAVEBackground saving started 3.如果我们在 redis.conf 中配置了如下选项： 123save 900 1save 300 10save 60 10000 那么当条件满足时，比如 900 秒内有一个 key 被操作了，那么 redis 就会自动触发 bgsava 命令进行备份。我们可以根据实际需求在 redis.conf 中配置多个这种触发规则。 4.还有一种情况也会触发 save 命令，那就是我们执行 shutdown 命令时，当我们用 shutdown 命令关闭 redis 时，此时也会执行一个 save 命令进行备份操作，并在备份操作完成后将服务器关闭。 5.还有一种特殊情况也会触发 bgsave 命令，就是在主从备份的时候。当从机连接上主机后，会发送一条 sync 命令来开始一次复制操作，此时主机会开始一次 bgsave 操作，并在 bgsave 操作结束后向从机发送快照数据实现数据同步。 快照持久化的缺点快照持久化有一些缺点，比如 save 命令会发生阻塞，bgsave 虽然不会发生阻塞，但是 fork 一个子进程又要耗费资源，在一些极端情况下，fork 子进程的时间甚至超过数据备份的时间。定期的持久化也会让我们存在数据丢失的风险，最坏的情况我们可能丢失掉最近一次备份到当下的数据，具体丢失多久的数据，要看我们项目的承受能力，我们可以根据项目的承受能力配饰 save 参数。 OK,快照持久化我们就介绍这么多，更多资料，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-rdb.html"},{"title":"Spring Boot + Vue 前后端分离开发，前端网络请求封装与配置","text":"前端网络访问，主流方案就是 Ajax，Vue 也不例外，在 Vue2.0 之前，网络访问较多的采用 vue-resources，Vue2.0 之后，官方不再建议使用 vue-resources ，这个项目本身也停止维护，目前建议使用的方案是 axios。今天松哥就带大家来看看 axios 的使用。 axios 引入axios 使用步骤很简单，首先在前端项目中，引入 axios： 1npm install axios -S 装好之后，按理说可以直接使用了，但是，一般在生产环境中，我们都需要对网络请求进行封装。 因为网络请求可能会出错，这些错误有的是代码错误导致的，也有的是业务错误，不管是哪一种错误，都需要开发者去处理，而我们不可能在每一次发送请求时都去枚举各种错误情况。 因此我们需要对前端请求进行封装，封装完成后，将前端错误统一处理，这样，开发者只需要在每一次发送请求的地方处理请求成功的情况即可。 请求封装在 axios 中，我们可以使用 axios 自带的拦截器来实现对错误的统一处理。 在 axios 中，有请求拦截器，也有响应拦截器。 请求拦截器中可以统一添加公共的请求参数，例如单点登录中前端统一添加 token 参数。 响应拦截器则可以实现对错误的统一处理。 另外一个需要注意的地方则是错误的展示需要使用一种通用的方式，而不可以和页面绑定（例如，登录失败，在用户名/密码输入框后面展示错误信息，不支持这种错误显示方式），这里推荐使用 ElementUI 中的 Massage 来展示错误信息，这是一个页面无关的组件。 封装后的 axios 如下： 12345678910111213141516171819202122232425262728293031import axios from 'axios'import {Message} from 'element-ui'axios.interceptors.request.use(config =&gt; { return config;}, err =&gt; { Message.error({message: '请求超时!'});})axios.interceptors.response.use(data =&gt; { if (data.status &amp;&amp; data.status == 200 &amp;&amp; data.data.status == 500) { Message.error({message: data.data.msg}); return; } if (data.data.msg) { Message.success({message: data.data.msg}); } return data.data;}, err =&gt; { if (err.response.status == 504 || err.response.status == 404) { Message.error({message: '服务器被吃了⊙﹏⊙∥'}); } else if (err.response.status == 403) { Message.error({message: '权限不足,请联系管理员!'}); } else if (err.response.status == 401) { Message.error({message: err.response.data.msg}); } else { if (err.response.data.msg) { Message.error({message: err.response.data.msg}); }else{ Message.error({message: '未知错误!'}); } }}) 代码解释： 首先导入 axios 和 Massage 组件 接下来定义一个请求拦截器 最后定义一个响应拦截器，这个拦截器有两个参数，第一个参数 data 表示服务端处理成功的响应，第二个 err 表示服务端处理失败的响应。对照着 jQuery 中的 Ajax ，第一个相当于 success 回调，第二个相当于 error 回调。 响应的 data 表示服务端返回的数据，数据格式是 {data:{status:200,msg&quot;&quot;,obj:{}},status:200} 其中，data 中的对象就是服务端返回的具体的 JSON ，外面的 status 表示 HTTP 响应码，里边的 status 是自定义的 RespBean 中返回的数据 首先判断 HTTP 响应码为 200 ，并且服务端返回的 status 为 500 ，表示业务逻辑错误，此时直接通过 Message 将错误信息展示出来，然后 return 即可。 如果服务端返回的字段中包含 msg ，则将 msg 显示出来，这个 msg 一般是成功的提示。 最后返回 data.data ，即将服务端返回的数据 return ，这个数据最终会来到请求调用的地方。 当 HTTP 响应码大于等于 400 时，进入 err 中。 方法封装请求封装完成后，还需要对方法进行封装，方便调用： 123456789101112131415161718192021222324252627282930313233let base = '';export const postRequest = (url, params) =&gt; { return axios({ method: 'post', url: `${base}${url}`, data: params, headers: { 'Content-Type': 'application/json' } });}export const putRequest = (url, params) =&gt; { return axios({ method: 'put', url: `${base}${url}`, data: params, headers: { 'Content-Type': 'application/json' } });}export const deleteRequest = (url) =&gt; { return axios({ method: 'delete', url: `${base}${url}` });}export const getRequest = (url) =&gt; { return axios({ method: 'get', url: `${base}${url}` });} 由于在前后端分离项目中，大多数情况下，后端接口都采用 RESTful 风格来设计，所以前端主要封装 GET\\POST\\PUT\\DELETE 方法，然后所有的请求参数都是用 JSON。 这里一开始定义了一个 base 变量，这是请求的前缀，方便后期维护（如果需要统一修改请求前缀）。 制作 Vue 插件封装好的方法已经可以直接使用了，但是比较麻烦，每次使用时，都需要在相关的 vue 文件中引入方法，像下面这样： 1import {postRequest} from \"../utils/api\"; 但是这种操作方式太麻烦，所以我们可以考虑将方法进一步封装成 Vue 的插件，这样在每一个 vue 文件中，不需要引入方法就能够直接调用方法了。 参考 Vue 官方文档 https://cn.vuejs.org/v2/guide/plugins.html，如下： 官方给出了 5 种插件制作方式，我们这里采用第 4 种方案。具体操作就是在 main.js 中引入所有的封装好的方法，然后挂载到 Vue.prototype 上即可，如下： 12345678import {postRequest} from &quot;./utils/api&quot;;import {putRequest} from &quot;./utils/api&quot;;import {deleteRequest} from &quot;./utils/api&quot;;import {getRequest} from &quot;./utils/api&quot;;Vue.prototype.getRequest = getRequest;Vue.prototype.deleteRequest = deleteRequest;Vue.prototype.putRequest = putRequest;Vue.prototype.postRequest = postRequest; 封装完成后，以后在 vue 文件中，直接通过 this 就可以获取到网络请求方法的引用了，如下： 12345this.postRequest(\"/doLogin\", this.user).then(msg=&gt;{ if (msg) { //登录成功，页面跳转 }}) 注意 ，then 中的 msg 就是响应拦截器中返回的 msg ，这个 msg 如果没有值，表示请求失败（失败已经在拦截器中进行处理了），如果有值，表示请求成功！ 配置请求转发在前后端分离中，前端和后端在不同的端口或者地址上运行，如果前端直接向后端发送请求，这个请求是跨域的。 但是在项目部署时，前端打包编译后拷贝到 Java 项目中，和 Java 项目一起运行，此时不存在跨域问题。 所以这里我们的解决思路不是解决跨域问题，而是通过配置 NodeJS 的请求转发，来实现网络请求顺利发送。 请求转发在 vue 项目的 config/index.js 文件中配置： 添加了请求转发配置之后，一定要重启前端项目才会生效。 此时启动前端项目，就可以顺利发送网络请求了。 总结本文主要和大伙分享了在前后端分离的情况下，如何对前端网络请求进行封装，并且如何配置请求转发，这是前后端分离中的基础课，小伙伴们有问题欢迎留言讨论。松哥将自己封装的网络请求库已经放在 GitHub 上，欢迎大家参考 https://github.com/lenve/javaboy-code-samples。","link":"/2019/0521/springboot-vue-axios.html"},{"title":"Spring Boot + Vue 前后端分离开发，权限管理的一点思路","text":"在传统的前后端不分的开发中，权限管理主要通过过滤器或者拦截器来进行（权限管理框架本身也是通过过滤器来实现功能），如果用户不具备某一个角色或者某一个权限，则无法访问某一个页面。 但是在前后端分离中，页面的跳转统统交给前端去做，后端只提供数据，这种时候，权限管理不能再按照之前的思路来。 首先要明确一点，前端是展示给用户看的，所有的菜单显示或者隐藏目的不是为了实现权限管理，而是为了给用户一个良好的体验，不能依靠前端隐藏控件来实现权限管理，即数据安全不能依靠前端。 这点就像普通的表单提交一样，前端做数据校验是为了提高效率，提高用户体验，后端才是真正的确保数据完整性。 所以，真正的数据安全管理是在后端实现的，后端在接口设计的过程中，就要确保每一个接口都是在满足某种权限的基础上才能访问，也就是说，不怕将后端数据接口地址暴露出来，即使暴露出来，只要你没有相应的角色，也是访问不了的。 前端为了良好的用户体验，需要将用户不能访问的接口或者菜单隐藏起来。 有人说，如果用户直接在地址拦输入某一个页面的路径，怎么办？此时，如果没有做任何额外的处理的话，用户确实可以通过直接输入某一个路径进入到系统中的某一个页面中，但是，不用担心数据泄露问题，因为没有相关的角色，就无法访问相关的接口。 但是，如果用户非这样操作，进入到一个空白的页面，用户体验不好，此时，我们可以使用 Vue 中的前置路由导航守卫，来监听页面跳转，如果用户想要去一个未获授权的页面，则直接在前置路由导航守卫中将之拦截下来，重定向到登录页，或者直接就停留在当前页，不让用户跳转，也可以顺手再给用户一点点未获授权的提示信息。 总而言之一句话，前端的所有操作，都是为了提高用户体验，不是为了数据安全，真正的权限校验要在后端来做，后端如果是 SSM 架构，建议使用 Shiro ，如果是 Spring Boot + 微服务，建议使用 Spring Security 。","link":"/2019/0523/springboot-vue-permission.html"},{"title":"Spring Boot + Vue 前后端分离，两种文件上传方式总结","text":"在Vue.js 中，如果网络请求使用 axios ，并且使用了 ElementUI 库，那么一般来说，文件上传有两种不同的实现方案： 通过 Ajax 实现文件上传 通过 ElementUI 里边的 Upload 组件实现文件上传 两种方案，各有优缺点，我们分别来看。 准备工作首先我们需要一点点准备工作，就是在后端提供一个文件上传接口，这是一个普通的 Spring Boot 项目，如下： 12345678910111213141516SimpleDateFormat sdf = new SimpleDateFormat(\"/yyyy/MM/dd/\");@PostMapping(\"/import\")public RespBean importData(MultipartFile file, HttpServletRequest req) throws IOException { String format = sdf.format(new Date()); String realPath = req.getServletContext().getRealPath(\"/upload\") + format; File folder = new File(realPath); if (!folder.exists()) { folder.mkdirs(); } String oldName = file.getOriginalFilename(); String newName = UUID.randomUUID().toString() + oldName.substring(oldName.lastIndexOf(\".\")); file.transferTo(new File(folder,newName)); String url = req.getScheme() + \"://\" + req.getServerName() + \":\" + req.getServerPort() + \"/upload\" + format + newName; System.out.println(url); return RespBean.ok(\"上传成功!\");} 这里的文件上传比较简单，上传的文件按照日期进行归类，使用 UUID 给文件重命名。 这里为了简化代码，我省略掉了异常捕获，上传结果直接返回成功，后端代码大伙可根据自己的实际情况自行修改。 Ajax 上传在 Vue 中，通过 Ajax 实现文件上传，方案和传统 Ajax 实现文件上传基本上是一致的，唯一不同的是查找元素的方式。 12&lt;input type=\"file\" ref=\"myfile\"&gt;&lt;el-button @click=\"importData\" type=\"success\" size=\"mini\" icon=\"el-icon-upload2\"&gt;导入数据&lt;/el-button&gt; 在这里，首先提供一个文件导入 input 组件，再来一个导入按钮，在导入按钮的事件中来完成导入的逻辑。 123456789101112importData() { let myfile = this.$refs.myfile; let files = myfile.files; let file = files[0]; var formData = new FormData(); formData.append(\"file\", file); this.uploadFileRequest(\"/system/basic/jl/import\",formData).then(resp=&gt;{ if (resp) { console.log(resp); } })} 关于这段上传核心逻辑，解释如下： 首先利用 Vue 中的 $refs 查找到存放文件的元素。 type 为 file 的 input 元素内部有一个 files 数组，里边存放了所有选择的 file，由于文件上传时，文件可以多选，因此这里拿到的 files 对象是一个数组。 从 files 对象中，获取自己要上传的文件，由于这里是单选，所以其实就是数组中的第一项。 构造一个 FormData ，用来存放上传的数据,FormData 不可以像 Java 中的 StringBuffer 使用链式配置。 构造好 FromData 后，就可以直接上传数据了，FormData 就是要上传的数据。 文件上传注意两点，1. 请求方法为 post，2. 设置 Content-Type 为 multipart/form-data 。 这种文件上传方式，实际上就是传统的 Ajax 上传文件，和大家常见的 jQuery 中写法不同的是，这里元素查找的方式不一样（实际上元素查找也可以按照JavaScript 中原本的写法来实现），其他写法一模一样。这种方式是一个通用的方式，和使用哪一种前端框架无关。最后再和大家来看下封装的上传方法： 12345678910export const uploadFileRequest = (url, params) =&gt; { return axios({ method: 'post', url: `${base}${url}`, data: params, headers: { 'Content-Type': 'multipart/form-data' } });} 经过这几步的配置后，前端就算上传完成了，可以进行文件上传了。 使用 Upload 组件如果使用 Upload ，则需要引入 ElementUI，所以一般建议，如果使用了 ElementUI 做 UI 控件的话，则可以考虑使用 Upload 组件来实现文件上传，如果没有使用 ElementUI 的话，则不建议使用 Upload 组件，至于其他的 UI 控件，各自都有自己的文件上传组件，具体使用可以参考各自文档。 123456789&lt;el-upload style=\"display: inline\" :show-file-list=\"false\" :on-success=\"onSuccess\" :on-error=\"onError\" :before-upload=\"beforeUpload\" action=\"/system/basic/jl/import\"&gt; &lt;el-button size=\"mini\" type=\"success\" :disabled=\"!enabledUploadBtn\" :icon=\"uploadBtnIcon\"&gt;{{btnText}}&lt;/el-button&gt;&lt;/el-upload&gt; show-file-list 表示是否展示上传文件列表，默认为true，这里设置为不展示。 before-upload 表示上传之前的回调，可以在该方法中，做一些准备工作，例如展示一个进度条给用户 。 on-success 和 on-error 分别表示上传成功和失败时候的回调，可以在这两个方法中，给用户一个相应的提示，如果有进度条，还需要在这两个方法中关闭进度条。 action 指文件上传地址。 上传按钮的点击状态和图标都设置为变量 ，在文件上传过程中，修改上传按钮的点击状态为不可点击，同时修改图标为一个正在加载的图标 loading。 上传的文本也设为变量，默认上传 button 的文本是 数据导入 ，当开始上传后，将找个 button 上的文本修改为 正在导入。 相应的回调如下： 123456789101112131415onSuccess(response, file, fileList) { this.enabledUploadBtn = true; this.uploadBtnIcon = 'el-icon-upload2'; this.btnText = '数据导入';},onError(err, file, fileList) { this.enabledUploadBtn = true; this.uploadBtnIcon = 'el-icon-upload2'; this.btnText = '数据导入';},beforeUpload(file) { this.enabledUploadBtn = false; this.uploadBtnIcon = 'el-icon-loading'; this.btnText = '正在导入';} 在文件开始上传时，修改上传按钮为不可点击，同时修改上传按钮的图标和文本。 文件上传成功或者失败时，修改上传按钮的状态为可以点击，同时恢复上传按钮的图标和文本。 上传效果图如下： 总结两种上传方式各有优缺点： 第一种方式最大的优势是通用，一招鲜吃遍天，到哪里都能用，但是对于上传过程的监控，进度条的展示等等逻辑都需要自己来实现。 第二种方式不够通用，因为它是 ElementUI 中的组件，得引入 ElementUI 才能使用，不过这种方式很明显有需多比较方便的回调，可以实现非常方便的处理常见的各种上传问题。 常规的上传需求第二种方式可以满足，但是如果要对上传的方法进行定制，则还是建议使用第一种上传方案。","link":"/2019/0428/springboot-vue-upload.html"},{"title":"Spring Boot 中的同一个 Bug，竟然把我坑了两次！","text":"真是郁闷，不过这事又一次提醒我解决问题还是要根治，不能囫囵吞枣，否则相同的问题可能会以不同的形式出现，每次都得花时间去搞。刨根问底，一步到位，再遇到类似问题就可以分分钟解决了。 如果大家没看过松哥之前写的 Spring Boot 整合 Spring Session，可以先回顾下： Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！ 第一次踩坑事情是这样的，大概在今年 6 月初的时候，我在项目中使用到了 Session 共享，当时采用的方案就是 Redis+Spring Session。本来这是一个很简单的问题，我在以前的项目中也用过多次这种方案，早已轻车熟路，但是那次有点不对劲，项目启动时候报了如下错误： 一模一样的代码，但是运行就是会出错，我感觉莫名其妙。因为在 Spring Boot 中整合 Spring Session 是一个非常简单的操作，就几行 Redis 的配置而已，我在确认了代码没问题之后，很快想到了可能是版本问题，因为当时 Spring Boot2.1.5 刚刚发布，我喜欢用最新版。于是我尝试将 Spring Boot 的版本切换到 2.1.4 ，切换回去之后，果然就 OK了，再次启动项目又不会报错了。于是基本确定这是 Spring Boot 的版本升级带来的问题。 但是当时我并没有深究，我以为就是官方出于安全考虑，让你在使用 Redis 时强制加上 Spring Security（因为根据错误提示，很容想到加上 Spring Security 依赖），加上 Spring Security 依赖之后，果然就没有问题了，我也没有多想，这件事就这样过了。​ 第二次踩坑前两天我在给星球上的小伙伴录制 Spring Boot 视频的时候，采用了 Spring Boot 最新版 2.1.7，也是 Spring Session，但是在创建项目的时候，忘记添加 Spring Security 依赖了（第一次踩坑之后，我每次用 Spring Session 都会自觉的加上 Spring Security 依赖），运行的时候竟然没报错！我就郁闷了。 于是我去试了 Spring Boot2.1.4、Spring Boot2.1.6 发现都没有问题，在使用 Spring Session 的时候都不需要添加 Spring Security 依赖，只有 Spring Boot2.1.5 才有这个问题。于是我大概明白了，这可能是一个 Bug，而不是版本升级的新功能。 这一次，那我就打算追究一下问题的根源。 源头要追究问题的源头，我们当然得从 Spring Session 的自动化配置类开始。 在 Spring Boot2.1.5 的 org.springframework.boot.autoconfigure.session.SessionAutoConfiguration 类中，我看到如下源码： 123456789101112@Bean@Conditional(DefaultCookieSerializerCondition.class)public DefaultCookieSerializer cookieSerializer(ServerProperties serverProperties, ObjectProvider&lt;SpringSessionRememberMeServices&gt; springSessionRememberMeServices) { //..... map.from(cookie::getMaxAge).to((maxAge) -&gt; cookieSerializer .setCookieMaxAge((int) maxAge.getSeconds())); springSessionRememberMeServices.ifAvailable(( rememberMeServices) -&gt; cookieSerializer.setRememberMeRequestAttribute( SpringSessionRememberMeServices.REMEMBER_ME_LOGIN_ATTR)); return cookieSerializer;} 从这一段源码中我们可以看到，这里使用到了 SpringSessionRememberMeServices ，而这个类中则用到 Spring Security 中相关的类。因此，如果不引入 Spring Security 就会报错。 我们再来看看 Spring Boot2.1.6 中 org.springframework.boot.autoconfigure.session.SessionAutoConfiguration 类的源码，如下： 12345678910@Bean@Conditional(DefaultCookieSerializerCondition.class)public DefaultCookieSerializer cookieSerializer(ServerProperties serverProperties) { //... map.from(cookie::getMaxAge).to((maxAge) -&gt; cookieSerializer.setCookieMaxAge((int) maxAge.getSeconds())); if (ClassUtils.isPresent(REMEMBER_ME_SERVICES_CLASS, getClass().getClassLoader())) { new RememberMeServicesCookieSerializerCustomizer().apply(cookieSerializer); } return cookieSerializer;} 可以看到，在 Spring Boot2.1.6 中，这个问题已经得到修复。这里就没有 2.1.5 那么冲动了，上来了先用 ClassUtils.isPresent 方法判断了下 REMEMBER_ME_SERVICES_CLASS(org.springframework.security.web.authentication.RememberMeServices) 是否存在，存在的话，才有后面的操作。 至此，这个问题就总算弄懂了。 结语大家平时遇到问题，如果项目不是很赶的话，可以留意多想想，多追究一下原因，说不定你会有很多意外的收获。我这次就是一个活生生的例子，一开始没多想，后来又发现不对劲，前前后后一折腾，反而又多浪费了一些时间。","link":"/2019/0814/springboot-bug.html"},{"title":"Spring Boot 中的静态资源到底要放在哪里？","text":"当我们使用 SpringMVC 框架时，静态资源会被拦截，需要添加额外配置，之前老有小伙伴在微信上问松哥Spring Boot 中的静态资源加载问题：“松哥，我的HTML页面好像没有样式？”，今天我就通过一篇文章，来和大伙仔细聊一聊这个问题。 SSM 中的配置要讲 Spring Boot 中的问题，我们得先回到 SSM 环境搭建中，一般来说，我们可以通过 &lt;mvc:resources /&gt; 节点来配置不拦截静态资源，如下： 123&lt;mvc:resources mapping=\"/js/**\" location=\"/js/\"/&gt;&lt;mvc:resources mapping=\"/css/**\" location=\"/css/\"/&gt;&lt;mvc:resources mapping=\"/html/**\" location=\"/html/\"/&gt; 由于这是一种Ant风格的路径匹配符，/** 表示可以匹配任意层级的路径，因此上面的代码也可以像下面这样简写： 1&lt;mvc:resources mapping=\"/**\" location=\"/\"/&gt; 这种配置是在 XML 中的配置，大家知道，SpringMVC 的配置除了在XML中配置，也可以在 Java 代码中配置，如果在Java代码中配置的话，我们只需要自定义一个类，继承自WebMvcConfigurationSupport即可： 12345678@Configuration@ComponentScan(basePackages = \"org.sang.javassm\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/**\").addResourceLocations(\"/\"); }} 重写 WebMvcConfigurationSupport 类中的addResourceHandlers方法，在该方法中配置静态资源位置即可，这里的含义和上面 xml 配置的含义一致，因此无需多说。这是我们传统的解决方案，在Spring Boot 中，其实配置方式和这个一脉相承，只是有一些自动化的配置了。 Spring Boot 中的配置在 Spring Boot 中，如果我们是从 https://start.spring.io 这个网站上创建的项目，或者使用 IntelliJ IDEA 中的 Spring Boot 初始化工具创建的项目，默认都会存在 resources/static 目录，很多小伙伴也知道静态资源只要放到这个目录下，就可以直接访问，除了这里还有没有其他可以放静态资源的位置呢？为什么放在这里就能直接访问了呢？这就是本文要讨论的问题了。 整体规划首先，在 Spring Boot 中，默认情况下，一共有5个位置可以放静态资源，五个路径分别是如下5个： classpath:/META-INF/resources/ classpath:/resources/ classpath:/static/ classpath:/public/ / 前四个目录好理解，分别对应了resources目录下不同的目录，第5个 / 是啥意思呢？我们知道，在 Spring Boot 项目中，默认是没有 webapp 这个目录的，当然我们也可以自己添加（例如在需要使用JSP的时候），这里第5个 / 其实就是表示 webapp 目录中的静态资源也不被拦截。如果同一个文件分别出现在五个目录下，那么优先级也是按照上面列出的顺序。 不过，虽然有5个存储目录，除了第5个用的比较少之外，其他四个，系统默认创建了 classpath:/static/ ， 正常情况下，我们只需要将我们的静态资源放到这个目录下即可，也不需要额外去创建其他静态资源目录，例如我在 classpath:/static/ 目录下放了一张名为1.png 的图片，那么我的访问路径是： 1http://localhost:8080/1.png 这里大家注意，请求地址中并不需要 static，如果加上了static反而多此一举会报404错误。很多人会觉得奇怪，为什么不需要添加 static呢？资源明明放在 static 目录下。其实这个效果很好实现，例如在SSM配置中，我们的静态资源拦截配置如果是下面这样： 1&lt;mvc:resources mapping=\"/**\" location=\"/static/\"/&gt; 如果我们是这样配置的话，请求地址如果是 http://localhost:8080/1.png 实际上系统会去 /static/1.png 目录下查找相关的文件。 所以我们理所当然的猜测，在 Spring Boot 中可能也是类似的配置。 源码解读胡适之先生说：“大胆猜想，小心求证”，我们这里就通过源码解读来看看 Spring Boot 中的静态资源到底是怎么配置的。 首先我们在 WebMvcAutoConfiguration 类中看到了 SpringMVC 自动化配置的相关的内容，找到了静态资源拦截的配置，如下： 可以看到这里静态资源的定义和我们前面提到的Java配置SSM中的配置非常相似，其中，this.mvcProperties.getStaticPathPattern() 方法对应的值是 “/**”，this.resourceProperties.getStaticLocations()方法返回了四个位置，分别是：”classpath:/META-INF/resources/“, “classpath:/resources/“,”classpath:/static/“, “classpath:/public/“，然后在getResourceLocations方法中，又添加了“/”，因此这里返回值一共有5个。其中，/表示webapp目录，即webapp中的静态文件也可以直接访问。静态资源的匹配路径按照定义路径优先级依次降低。因此这里的配置和我们前面提到的如出一辙。这样大伙就知道了为什么Spring Boot 中支持5个静态资源位置，同时也明白了为什么静态资源请求路径中不需要/static，因为在路径映射中已经自动的添加上了/static了。 自定义配置当然，这个是系统默认配置，如果我们并不想将资源放在系统默认的这五个位置上，也可以自定义静态资源位置和映射，自定义的方式也有两种，可以通过 application.properties 来定义，也可以在 Java 代码中来定义，下面分别来看。 application.properties在配置文件中定义的方式比较简单，如下： 12spring.resources.static-locations=classpath:/spring.mvc.static-path-pattern=/** 第一行配置表示定义资源位置，第二行配置表示定义请求 URL 规则。以上文的配置为例，如果我们这样定义了，表示可以将静态资源放在 resources目录下的任意地方，我们访问的时候当然也需要写完整的路径，例如在resources/static目录下有一张名为1.png 的图片，那么访问路径就是 http://localhost:8080/static/1.png ,注意此时的static不能省略。 Java 代码定义当然，在Spring Boot中我们也可以通过 Java代码来自定义，方式和 Java 配置的 SSM 比较类似，如下： 1234567@Configurationpublic class WebMVCConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/**\").addResourceLocations(\"classpath:/aaa/\"); }} 这里代码基本和前面一致，比较简单，不再赘述。 总结这里需要提醒大家的是，松哥见到有很多人用了 Thymeleaf 之后，会将静态资源也放在 resources/templates 目录下，注意，templates 目录并不是静态资源目录，它是一个放页面模板的位置（你看到的 Thymeleaf 模板虽然后缀为 .html，其实并不是静态资源）。好了，通过上面的讲解，相信大家对 Spring Boot 中静态资源的位置有一个深刻了解了，应该不会再在项目中出错了吧！","link":"/2019/0408/springboot-static-resources.html"},{"title":"Spring Boot 中自定义 SpringMVC 配置，到底继承谁？","text":"用过 Spring Boot 的小伙伴都知道，我们只需要在项目中引入 spring-boot-starter-web 依赖，SpringMVC 的一整套东西就会自动给我们配置好，但是，真实的项目环境比较复杂，系统自带的配置不一定满足我们的需求，往往我们还需要结合实际情况自定义配置。 自定义配置就有讲究了，由于 Spring Boot 的版本变迁，加上这一块本身就有几个不同写法，很多小伙伴在这里容易搞混，今天松哥就来和大家说一说这个问题。 概览首先我们需要明确，跟自定义 SpringMVC 相关的类和注解主要有如下四个： WebMvcConfigurerAdapter WebMvcConfigurer WebMvcConfigurationSupport @EnableWebMvc 这四个中，除了第四个是注解，另外三个两个类一个接口，里边的方法看起来好像都类似，但是实际使用效果却大不相同，因此很多小伙伴容易搞混，今天松哥就来和大家聊一聊这个问题。 WebMvcConfigurerAdapter我们先来看 WebMvcConfigurerAdapter，这个是在 Spring Boot 1.x 中我们自定义 SpringMVC 时继承的一个抽象类，这个抽象类本身是实现了 WebMvcConfigurer 接口，然后抽象类里边都是空方法，我们来看一下这个类的声明： 123public abstract class WebMvcConfigurerAdapter implements WebMvcConfigurer { //各种 SpringMVC 配置的方法} 再来看看这个类的注释： 1234567/** * An implementation of {@link WebMvcConfigurer} with empty methods allowing * subclasses to override only the methods they're interested in. * @deprecated as of 5.0 {@link WebMvcConfigurer} has default methods (made * possible by a Java 8 baseline) and can be implemented directly without the * need for this adapter */ 这段注释关于这个类说的很明白了。同时我们也看到，从 Spring5 开始，由于我们要使用 Java8，而 Java8 中的接口允许存在 default 方法，因此官方建议我们直接实现 WebMvcConfigurer 接口，而不是继承 WebMvcConfigurerAdapter 。 也就是说，在 Spring Boot 1.x 的时代，如果我们需要自定义 SpringMVC 配置，直接继承 WebMvcConfigurerAdapter 类即可。 WebMvcConfigurer根据上一小节的解释，小伙伴们已经明白了，WebMvcConfigurer 是我们在 Spring Boot 2.x 中实现自定义配置的方案。 WebMvcConfigurer 是一个接口，接口中的方法和 WebMvcConfigurerAdapter 中定义的空方法其实一样，所以用法上来说，基本上没有差别，从 Spring Boot 1.x 切换到 Spring Boot 2.x ，只需要把继承类改成实现接口即可。 松哥在之前的案例中(40 篇原创干货，带你进入 Spring Boot 殿堂！)，凡是涉及到自定义 SpringMVC 配置的地方，也都是通过实现 WebMvcConfigurer 接口来完成的。 WebMvcConfigurationSupport前面两个都好理解，还有一个 WebMvcConfigurationSupport ，这个又是干什么用的呢？ 松哥之前有一篇文章中用过这个类，不知道小伙伴们有没有留意，就是下面这篇： 纯 Java 代码搭建 SSM 环境 这篇文章我放弃了 Spring 和 SpringMVC 的 xml 配置文件，转而用 Java 代替这两个 xml 配置。那么在这里我自定义 SpringMVC 配置的时候，就是通过继承 WebMvcConfigurationSupport 类来实现的。在 WebMvcConfigurationSupport 类中，提供了用 Java 配置 SpringMVC 所需要的所有方法。我们来看一下这个方法的摘要： 有一点眼熟，可能有小伙伴发现了，这里的方法其实和前面两个类中的方法基本是一样的。 在这里首先大家需要明确的是，WebMvcConfigurationSupport 类本身是没有问题的，我们自定义 SpringMVC 的配置是可以通过继承 WebMvcConfigurationSupport 来实现的。但是继承 WebMvcConfigurationSupport 这种操作我们一般只在 Java 配置的 SSM 项目中使用，Spring Boot 中基本上不会这么写，为什么呢？ 小伙伴们知道，Spring Boot 中，SpringMVC 相关的自动化配置是在 WebMvcAutoConfiguration 配置类中实现的，那么我们来看看这个配置类的生效条件： 123456789@Configuration@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class })@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter({ DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class })public class WebMvcAutoConfiguration {} 我们从这个类的注解中可以看到，它的生效条件有一条，就是当不存在 WebMvcConfigurationSupport 的实例时，这个自动化配置才会生生效。因此，如果我们在 Spring Boot 中自定义 SpringMVC 配置时选择了继承 WebMvcConfigurationSupport，就会导致 Spring Boot 中 SpringMVC 的自动化配置失效。 Spring Boot 给我们提供了很多自动化配置，很多时候当我们修改这些配置的时候，并不是要全盘否定 Spring Boot 提供的自动化配置，我们可能只是针对某一个配置做出修改，其他的配置还是按照 Spring Boot 默认的自动化配置来，而继承 WebMvcConfigurationSupport 来实现对 SpringMVC 的配置会导致所有的 SpringMVC 自动化配置失效，因此，一般情况下我们不选择这种方案。 在 Java 搭建的 SSM 项目中(纯 Java 代码搭建 SSM 环境)，因为本身就没什么自动化配置，所以我们使用了继承 WebMvcConfigurationSupport。 @EnableWebMvc最后还有一个 @EnableWebMvc 注解，这个注解很好理解，它的作用就是启用 WebMvcConfigurationSupport。我们来看看这个注解的定义： 123/** * Adding this annotation to an {@code @Configuration} class imports the Spring MVC * configuration from {@link WebMvcConfigurationSupport}, e.g.: 可以看到，加了这个注解，就会自动导入 WebMvcConfigurationSupport，所以在 Spring Boot 中，我们也不建议使用 @EnableWebMvc 注解，因为它一样会导致 Spring Boot 中的 SpringMVC 自动化配置失效。 总结不知道上面的解释小伙伴有没有看懂？我再简单总结一下： Spring Boot 1.x 中，自定义 SpringMVC 配置可以通过继承 WebMvcConfigurerAdapter 来实现。 Spring Boot 2.x 中，自定义 SpringMVC 配置可以通过实现 WebMvcConfigurer 接口来完成。 如果在 Spring Boot 中使用继承 WebMvcConfigurationSupport 来实现自定义 SpringMVC 配置，或者在 Spring Boot 中使用了 @EnableWebMvc 注解，都会导致 Spring Boot 中默认的 SpringMVC 自动化配置失效。 在纯 Java 配置的 SSM 环境中，如果我们要自定义 SpringMVC 配置，有两种办法，第一种就是直接继承自 WebMvcConfigurationSupport 来完成 SpringMVC 配置，还有一种方案就是实现 WebMvcConfigurer 接口来完成自定义 SpringMVC 配置，如果使用第二种方式，则需要给 SpringMVC 的配置类上额外添加 @EnableWebMvc 注解，表示启用 WebMvcConfigurationSupport，这样配置才会生效。换句话说，在纯 Java 配置的 SSM 中，如果你需要自定义 SpringMVC 配置，你离不开 WebMvcConfigurationSupport ，所以在这种情况下建议通过继承 WebMvcConfigurationSupport 来实现自动化配置。 不知道小伙伴们有没有看懂呢？有问题欢迎留言讨论。","link":"/2019/0816/spring-boot-springmvc.html"},{"title":"Spring Boot 中通过 CORS 解决跨域问题","text":"今天和小伙伴们来聊一聊通过 CORS 解决跨域问题。 同源策略很多人对跨域有一种误解，以为这是前端的事，和后端没关系，其实不是这样的，说到跨域，就不得不说说浏览器的同源策略。 同源策略是由 Netscape 提出的一个著名的安全策略，它是浏览器最核心也最基本的安全功能，现在所有支持 JavaScript 的浏览器都会使用这个策略。所谓同源是指协议、域名以及端口要相同。同源策略是基于安全方面的考虑提出来的，这个策略本身没问题，但是我们在实际开发中，由于各种原因又经常有跨域的需求，传统的跨域方案是 JSONP，JSONP 虽然能解决跨域但是有一个很大的局限性，那就是只支持 GET 请求，不支持其他类型的请求，而今天我们说的 CORS（跨域源资源共享）（CORS，Cross-origin resource sharing）是一个 W3C 标准，它是一份浏览器技术的规范，提供了 Web 服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，这是 JSONP 模式的现代版。 在 Spring 框架中，对于 CORS 也提供了相应的解决方案，今天我们就来看看 SpringBoot 中如何实现 CORS 。 实践接下来我们就来看看 Spring Boot 中如何实现这个东西。 首先创建两个普通的 Spring Boot 项目，这个就不用我多说，第一个命名为 provider 提供服务，第二个命名为 consumer 消费服务，第一个配置端口为 8080 ，第二个配置配置为 8081 ，然后在 provider 上提供两个 hello 接口，一个 get ，一个 post ，如下： 1234567891011@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @PostMapping(\"/hello\") public String hello2() { return \"post hello\"; }} 在 consumer 的 resources/static 目录下创建一个 html 文件，发送一个简单的 ajax 请求，如下： 12345678910111213141516&lt;div id=\"app\"&gt;&lt;/div&gt;&lt;input type=\"button\" onclick=\"btnClick()\" value=\"get_button\"&gt;&lt;input type=\"button\" onclick=\"btnClick2()\" value=\"post_button\"&gt;&lt;script&gt; function btnClick() { $.get('http://localhost:8080/hello', function (msg) { $(\"#app\").html(msg); }); } function btnClick2() { $.post('http://localhost:8080/hello', function (msg) { $(\"#app\").html(msg); }); }&lt;/script&gt; 然后分别启动两个项目，发送请求按钮，观察浏览器控制台如下： 1Access to XMLHttpRequest at &apos;http://localhost:8080/hello&apos; from origin &apos;http://localhost:8081&apos; has been blocked by CORS policy: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. 可以看到，由于同源策略的限制，请求无法发送成功。 使用 CORS 可以在前端代码不做任何修改的情况下，实现跨域，那么接下来看看在 provider 中如何配置。首先可以通过 @CrossOrigin 注解配置某一个方法接受某一个域的请求，如下： 1234567891011121314@RestControllerpublic class HelloController { @CrossOrigin(value = \"http://localhost:8081\") @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @CrossOrigin(value = \"http://localhost:8081\") @PostMapping(\"/hello\") public String hello2() { return \"post hello\"; }} 这个注解表示这两个接口接受来自 http://localhost:8081 地址的请求，配置完成后，重启 provider ，再次发送请求，浏览器控制台就不会报错了，consumer 也能拿到数据了。 此时观察浏览器请求网络控制台，可以看到响应头中多了如下信息： 这个表示服务端愿意接收来自 http://localhost:8081 的请求，拿到这个信息后，浏览器就不会再去限制本次请求的跨域了。 provider 上，每一个方法上都去加注解未免太麻烦了，在 Spring Boot 中，还可以通过全局配置一次性解决这个问题，全局配置只需要在配置类中重写 addCorsMappings 方法即可，如下： 12345678910@Configurationpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(\"/**\") .allowedOrigins(\"http://localhost:8081\") .allowedMethods(\"*\") .allowedHeaders(\"*\"); }} /** 表示本应用的所有方法都会去处理跨域请求， allowedMethods 表示允许通过的请求数，allowedHeaders 则表示允许的请求头。经过这样的配置之后，就不必在每个方法上单独配置跨域了。 存在的问题了解了整个 CORS 的工作过程之后，我们通过 Ajax 发送跨域请求，虽然用户体验提高了，但是也有潜在的威胁存在，常见的就是 CSRF（Cross-site request forgery）跨站请求伪造。跨站请求伪造也被称为 one-click attack 或者 session riding，通常缩写为 CSRF 或者 XSRF ，是一种挟制用户在当前已登录的 Web 应用程序上执行非本意的操作的攻击方法，举个例子： 假如一家银行用以运行转账操作的 URL 地址如下： http://icbc.com/aa?bb=cc ，那么，一个恶意攻击者可以在另一个网站上放置如下代码： &lt;img src=&quot;http://icbc.com/aa?bb=cc&quot;&gt; ，如果用户访问了恶意站点，而她之前刚访问过银行不久，登录信息尚未过期，那么她就会遭受损失。 基于此，浏览器在实际操作中，会对请求进行分类，分为简单请求，预先请求，带凭证的请求等，预先请求会首先发送一个 options 探测请求，和浏览器进行协商是否接受请求。默认情况下跨域请求是不需要凭证的，但是服务端可以配置要求客户端提供凭证，这样就可以有效避免 csrf 攻击。 好了，这个问题就说这么多。 本文案例我已上传到 GitHub，欢迎大家 star:https://github.com/lenve/javaboy-code-samples","link":"/2019/0613/springboot-cors.html"},{"title":"Spring Boot 修改静态资源一定要重启项目才会生效吗？未必！","text":"回顾热部署Spring Boot 中的热部署相信大家都用过吧，只需要添加 spring-boot-devtools 依赖就可以轻松实现热部署。Spring Boot 中热部署最最关键的原理就是两个不同的 classloader： base classloader restart classloader 其中 base classloader 用来加载那些不会变化的类，例如各种第三方依赖，而 restart classloader 则用来加载那些会发生变化的类，例如你自己写的代码。Spring Boot 中热部署的原理就是当代码发生变化时，base classloader 不变，而 restart classloader 则会被废弃，被另一个新的 restart classloader 代替。在整个过程中，因为只重新加载了变化的类，所以启动速度要被重启快。 但是有另外一个问题，就是静态资源文件！使用 devtools ，默认情况下当静态资源发生变化时，并不会触发项目重启。虽然我们可以通过配置解决这一问题，但是没有必要！因为静态资源文件发生变化后不需要编译，按理说保存后刷新下就可以访问到了。 那么如何才能实现静态资源变化后，不编译就能自动刷新呢？ LiveReload 可以帮助我们实现这一功能！ LiveReloaddevtools 中默认嵌入了 LiveReload 服务器，利用 LiveReload 可以实现静态文件的热部署，LiveReload 可以在资源发生变化时自动触发浏览器更新，LiveReload 支持 Chrome、Firefox 以及 Safari 。以 Chrome 为例，在 Chrome 应用商店搜索 LiveReload ，结果如下图： 将第一个搜索结果添加到 Chrome 中，添加成功后，在 Chrome 右上角有一个 LiveReload 图标 在浏览器中打开项目的页面，然后点击浏览器右上角的 LiveReload 按钮，打开 LiveReload 连接。 注意： LiveReload 是和浏览器选项卡绑定在一起的，在哪个选项卡中打开了 LiveReload，就在哪个选项卡中访问页面，这样才有效果。 打开 LiveReload 之后，我们启动一个加了 devtools 依赖的 Spring Boot 项目： 123456&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 此时随便在 resources/static 目录下添加一个静态 html 页面，然后启动 Spring Boot 项目，在打开了 LiveReload 的选项卡中访问 html 页面。 访问成功后，我们再去手动修改 html 页面代码，修改成功后，回到浏览器，不用做任何操作，就会发现浏览器自动刷新了，页面已经更新了。 整个过程中，我的 Spring Boot 项目并没有重启。 如果开发者安装并且启动了 LiveReload 插件，同时也添加了 devtools 依赖，但是却并不想当静态页面发生变化时浏览器自动刷新，那么可以在 application.properties 中添加如下代码进行配置： 1spring.devtools.livereload.enabled=false 最佳实践建议开发者使用 LiveReload 策略而不是项目重启策略来实现静态资源的动态加载，因为项目重启所耗费时间一般来说要超过使用LiveReload 所耗费的时间。 Firefox 也可以安装 LiveReload 插件，装好之后和 Chrome 用法基本一致，这里不再赘述。","link":"/2019/0808/springboot-livereload.html"},{"title":"Spring Boot 加入 Https 功能有那么难吗？","text":"https 现在已经越来越普及了，特别是做一些小程序或者公众号开发的时候，https 基本上都是刚需了。 不过一个 https 证书还是挺费钱的，个人开发者可以在各个云服务提供商那里申请一个免费的证书。我印象中有效期一年，可以申请 20 个。 今天要和大家聊的是在 Spring Boot 项目中，如何开启 https 配置，为我们的接口保驾护航。 https 简介我们先来看看什么是 https，根据 wikipedia 上的介绍： 超文本传输安全协议(HyperText Transfer Protocol Secure)，缩写：HTTPS；常称为 HTTP over TLS、HTTP over SSL 或 HTTP Secure）是一种通过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。这个协议由网景公司(Netscape)在 1994 年首次提出，随后扩展到互联网上。 历史上，HTTPS 连接经常用于网络上的交易支付和企业信息系统中敏感信息的传输。在 2000 年代末至 2010 年代初，HTTPS 开始广泛使用，以确保各类型的网页真实，保护账户和保持用户通信，身份和网络浏览的私密性。 另外，还有一种安全超文本传输协议（S-HTTP），也是 HTTP 安全传输的一种实现，但是 HTTPS 的广泛应用而成为事实上的 HTTP 安全传输实现，S-HTTP并没有得到广泛支持。 准备工作首先我们需要有一个 https 证书，我们可以从各个云服务厂商处申请一个免费的，不过自己做实验没有必要这么麻烦，我们可以直接借助 Java 自带的 JDK 管理工具 keytool 来生成一个免费的 https 证书。 进入到 %JAVVA_HOME%\\bin 目录下，执行如下命令生成一个数字证书： 1keytool -genkey -alias tomcathttps -keyalg RSA -keysize 2048 -keystore D:\\javaboy.p12 -validity 365 命令含义如下： genkey 表示要创建一个新的密钥。 alias 表示 keystore 的别名。 keyalg 表示使用的加密算法是 RSA ，一种非对称加密算法。 keysize 表示密钥的长度。 keystore 表示生成的密钥存放位置。 validity 表示密钥的有效时间，单位为天。 具体生成过程如下图： 命令执行完成后 ，我们在 D 盘目录下会看到一个名为 javaboy.p12 的文件。如下图： 有了这个文件之后，我们的准备工作就算是 OK 了。 引入 https接下来我们需要在项目中引入 https。 将上面生成的 javaboy.p12 拷贝到 Spring Boot 项目的 resources 目录下。然后在 application.properties 中添加如下配置： 123server.ssl.key-store=classpath:javaboy.p12server.ssl.key-alias=tomcathttpsserver.ssl.key-store-password=111111 其中： key-store表示密钥文件名。 key-alias表示密钥别名。 key-store-password就是在cmd命令执行过程中输入的密码。 配置完成后，就可以启动 Spring Boot 项目了，此时如果我们直接使用 Http 协议来访问接口，就会看到如下错误： 改用 https 来访问 ，结果如下： 这是因为我们自己生成的 https 证书不被浏览器认可，不过没关系，我们直接点击继续访问就可以了（实际项目中只需要更换一个被浏览器认可的 https 证书即可）。 请求转发考虑到 Spring Boot 不支持同时启动 HTTP 和 HTTPS ，为了解决这个问题，我们这里可以配置一个请求转发，当用户发起 HTTP 调用时，自动转发到 HTTPS 上。 具体配置如下： 12345678910111213141516171819202122232425262728@Configurationpublic class TomcatConfig { @Bean TomcatServletWebServerFactory tomcatServletWebServerFactory() { TomcatServletWebServerFactory factory = new TomcatServletWebServerFactory(){ @Override protected void postProcessContext(Context context) { SecurityConstraint constraint = new SecurityConstraint(); constraint.setUserConstraint(\"CONFIDENTIAL\"); SecurityCollection collection = new SecurityCollection(); collection.addPattern(\"/*\"); constraint.addCollection(collection); context.addConstraint(constraint); } }; factory.addAdditionalTomcatConnectors(createTomcatConnector()); return factory; } private Connector createTomcatConnector() { Connector connector = new Connector(\"org.apache.coyote.http11.Http11NioProtocol\"); connector.setScheme(\"http\"); connector.setPort(8081); connector.setSecure(false); connector.setRedirectPort(8080); return connector; }} 在这里，我们配置了 Http 的请求端口为 8081，所有来自 8081 的请求，将被自动重定向到 8080 这个 https 的端口上。 如此之后，我们再去访问 http 请求，就会自动重定向到 https。 结语Spring Boot 中加入 https 其实很方便。如果你使用了 nginx 或者 tomcat 的话，https 也可以发非常方便的配置，从各个云服务厂商处申请到 https 证书之后，官方都会有一个详细的配置教程，一般照着做，就不会错了。","link":"/2019/0813/springboot-https.html"},{"title":"Spring Boot 定义系统启动任务，你会几种方式？","text":"在 Servlet/Jsp 项目中，如果涉及到系统任务，例如在项目启动阶段要做一些数据初始化操作，这些操作有一个共同的特点，只在项目启动时进行，以后都不再执行，这里，容易想到web基础中的三大组件（ Servlet、Filter、Listener ）之一 Listener ，这种情况下，一般定义一个 ServletContextListener，然后就可以监听到项目启动和销毁，进而做出相应的数据初始化和销毁操作，例如下面这样： 12345678910public class MyListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent sce) { //在这里做数据初始化操作 } @Override public void contextDestroyed(ServletContextEvent sce) { //在这里做数据备份操作 }} 当然，这是基础 web 项目的解决方案，如果使用了 Spring Boot，那么我们可以使用更为简便的方式。Spring Boot 中针对系统启动任务提供了两种解决方案，分别是 CommandLineRunner 和 ApplicationRunner，分别来看。 CommandLineRunner使用 CommandLineRunner 时，首先自定义 MyCommandLineRunner1 并且实现 CommandLineRunner 接口： 1234567@Component@Order(100)public class MyCommandLineRunner1 implements CommandLineRunner { @Override public void run(String... args) throws Exception { }} 关于这段代码，我做如下解释： 首先通过 @Compoent 注解将 MyCommandLineRunner1 注册为Spring容器中的一个 Bean。 添加 @Order注解，表示这个启动任务的执行优先级，因为在一个项目中，启动任务可能有多个，所以需要有一个排序。@Order 注解中，数字越小，优先级越大，默认情况下，优先级的值为 Integer.MAX_VALUE，表示优先级最低。 在 run 方法中，写启动任务的核心逻辑，当项目启动时，run方法会被自动执行。 run 方法的参数，来自于项目的启动参数，即项目入口类中，main方法的参数会被传到这里。 此时启动项目，run方法就会被执行，至于参数，可以通过两种方式来传递，如果是在 IDEA 中，可以通过如下方式来配置参数： 另一种方式，则是将项目打包，在命令行中启动项目，然后启动时在命令行传入参数，如下： 1java -jar devtools-0.0.1-SNAPSHOT.jar 三国演义 西游记 注意，这里参数传递时没有key，直接写value即可，执行结果如下： ApplicationRunnerApplicationRunner 和 CommandLineRunner 功能一致，用法也基本一致，唯一的区别主要体现在对参数的处理上，ApplicationRunner 可以接收更多类型的参数（ApplicationRunner 除了可以接收 CommandLineRunner 的参数之外，还可以接收 key/value形式的参数）。 使用 ApplicationRunner ，自定义类实现 ApplicationRunner 接口即可，组件注册以及组件优先级的配置都和 CommandLineRunner 一致，如下： 123456789101112131415@Component@Order(98)public class MyApplicationRunner1 implements ApplicationRunner { @Override public void run(ApplicationArguments args) throws Exception { List&lt;String&gt; nonOptionArgs = args.getNonOptionArgs(); System.out.println(\"MyApplicationRunner1&gt;&gt;&gt;\"+nonOptionArgs); Set&lt;String&gt; optionNames = args.getOptionNames(); for (String key : optionNames) { System.out.println(\"MyApplicationRunner1&gt;&gt;&gt;\"+key + \":\" + args.getOptionValues(key)); } String[] sourceArgs = args.getSourceArgs(); System.out.println(\"MyApplicationRunner1&gt;&gt;&gt;\"+Arrays.toString(sourceArgs)); }} 当项目启动时，这里的 run 方法就会被自动执行，关于 run 方法的参数 ApplicationArguments ，我说如下几点： args.getNonOptionArgs();可以用来获取命令行中的无key参数（和CommandLineRunner一样）。 args.getOptionNames();可以用来获取所有key/value形式的参数的key。 args.getOptionValues(key));可以根据key获取key/value 形式的参数的value。 args.getSourceArgs(); 则表示获取命令行中的所有参数。 ApplicationRunner 定义完成后，传启动参数也是两种方式，参数类型也有两种，第一种和 CommandLineRunner 一致，第二种则是 –key=value 的形式，在 IDEA 中定义方式如下： 或者使用 如下启动命令： 1java -jar devtools-0.0.1-SNAPSHOT.jar 三国演义 西游记 --age=99 运行结果如下： 总结整体来说 ，这两种的用法的差异不大 ，主要体现在对参数的处理上，小伙伴可以根据项目中的实际情况选择合适的解决方案。","link":"/2019/0415/springboot-commandlinerunner.html"},{"title":"Spring Boot 打包成的可执行 jar ，为什么不能被其他项目依赖？","text":"前两天被人问到这样一个问题: “松哥，为什么我的 Spring Boot 项目打包成的 jar ，被其他项目依赖之后，总是报找不到类的错误？” 大伙有这样的疑问，就是因为还没搞清楚可执行 jar 和普通 jar 到底有什么区别？今天松哥就和大家来聊一聊这个问题。 多了一个插件Spring Boot 中默认打包成的 jar 叫做 可执行 jar，这种 jar 不同于普通的 jar，普通的 jar 不可以通过 java -jar xxx.jar 命令执行，普通的 jar 主要是被其他应用依赖，Spring Boot 打成的 jar 可以执行，但是不可以被其他的应用所依赖，即使强制依赖，也无法获取里边的类。但是可执行 jar 并不是 Spring Boot 独有的，Java 工程本身就可以打包成可执行 jar 。 有的小伙伴可能就有疑问了，既然同样是执行 mvn package 命令进行项目打包，为什么 Spring Boot 项目就打成了可执行 jar ，而普通项目则打包成了不可执行 jar 呢？ 这我们就不得不提 Spring Boot 项目中一个默认的插件配置 spring-boot-maven-plugin ，这个打包插件存在 5 个方面的功能，从插件命令就可以看出： 五个功能分别是： build-info：生成项目的构建信息文件 build-info.properties repackage：这个是默认 goal，在 mvn package 执行之后，这个命令再次打包生成可执行的 jar，同时将 mvn package 生成的 jar 重命名为 *.origin run：这个可以用来运行 Spring Boot 应用 start：这个在 mvn integration-test 阶段，进行 Spring Boot 应用生命周期的管理 stop：这个在 mvn integration-test 阶段，进行 Spring Boot 应用生命周期的管理 这里功能，默认情况下使用就是 repackage 功能，其他功能要使用，则需要开发者显式配置。 打包repackage 功能的 作用，就是在打包的时候，多做一点额外的事情： 首先 mvn package 命令 对项目进行打包，打成一个 jar，这个 jar 就是一个普通的 jar，可以被其他项目依赖，但是不可以被执行 repackage 命令，对第一步 打包成的 jar 进行再次打包，将之打成一个 可执行 jar ，通过将第一步打成的 jar 重命名为 *.original 文件 举个例子： 对任意一个 Spring Boot 项目进行打包，可以执行 mvn package 命令，也可以直接在 IDEA 中点击 package ，如下 ： 打包成功之后， target 中的文件如下： 这里有两个文件，第一个 restful-0.0.1-SNAPSHOT.jar 表示打包成的可执行 jar ，第二个 restful-0.0.1-SNAPSHOT.jar.original 则是在打包过程中 ，被重命名的 jar，这是一个不可执行 jar，但是可以被其他项目依赖的 jar。通过对这两个文件的解压，我们可以看出这两者之间的差异。 两种 jar 的比较可执行 jar 解压之后，目录如下： 可以看到，可执行 jar 中，我们自己的代码是存在 于 BOOT-INF/classes/ 目录下，另外，还有一个 META-INF 的目录，该目录下有一个 MANIFEST.MF 文件，打开该文件，内容如下： 12345678910Manifest-Version: 1.0Implementation-Title: restfulImplementation-Version: 0.0.1-SNAPSHOTStart-Class: org.javaboy.restful.RestfulApplicationSpring-Boot-Classes: BOOT-INF/classes/Spring-Boot-Lib: BOOT-INF/lib/Build-Jdk-Spec: 1.8Spring-Boot-Version: 2.1.6.RELEASECreated-By: Maven Archiver 3.4.0Main-Class: org.springframework.boot.loader.JarLauncher 可以看到，这里定义了一个 Start-Class，这就是可执行 jar 的入口类，Spring-Boot-Classes 表示我们自己代码编译后的位置，Spring-Boot-Lib 则表示项目依赖的 jar 的位置。 换句话说，如果自己要打一个可执行 jar 包的话，除了添加相关依赖之外，还需要配置 META-INF/MANIFEST.MF 文件。 这是可执行 jar 的结构，那么不可执行 jar 的结构呢？ 我们首先将默认的后缀 .original 除去，然后给文件重命名，重命名完成，进行解压： 解压后可以看到，不可执行 jar 根目录就相当于我们的 classpath，解压之后，直接就能看到我们的代码，它也有 META-INF/MANIFEST.MF 文件，但是文件中没有定义启动类等。 12345Manifest-Version: 1.0Implementation-Title: restfulImplementation-Version: 0.0.1-SNAPSHOTBuild-Jdk-Spec: 1.8Created-By: Maven Archiver 3.4.0 注意 这个不可以执行 jar 也没有将项目的依赖打包进来。 从这里我们就可以看出，两个 jar ，虽然都是 jar 包，但是内部结构是完全不同的，因此一个可以直接执行，另一个则可以被其他项目依赖。 一次打包两个 jar一般来说，Spring Boot 直接打包成可执行 jar 就可以了，不建议将 Spring Boot 作为普通的 jar 被其他的项目所依赖。如果有这种需求，建议将被依赖的部分，单独抽出来做一个普通的 Maven 项目，然后在 Spring Boot 中引用这个 Maven 项目。 如果非要将 Spring Boot 打包成一个普通 jar 被其他项目依赖，技术上来说，也是可以的，给 spring-boot-maven-plugin 插件添加如下配置： 1234567891011&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 配置的 classifier 表示可执行 jar 的名字，配置了这个之后，在插件执行 repackage 命令时，就不会给 mvn package 所打成的 jar 重命名了，所以，打包后的 jar 如下： 第一个 jar 表示可以被其他项目依赖的 jar ，第二个 jar 则表示一个可执行 jar。 好了，关于 Spring Boot 中 jar 的问题，我们就说这么多，有问题欢迎留言讨论。","link":"/2019/0709/springboot-jar.html"},{"title":"Spring Boot 操作 Redis，三种方案全解析！","text":"在 Redis 出现之前，我们的缓存框架各种各样，有了 Redis ，缓存方案基本上都统一了，关于 Redis，松哥之前有一个系列教程，尚不了解 Redis 的小伙伴可以参考这个教程： Redis 教程合集 使用 Java 操作 Redis 的方案很多，Jedis 是目前较为流行的一种方案，除了 Jedis ，还有很多其他解决方案，如下： 除了这些方案之外，还有一个使用也相当多的方案，就是 Spring Data Redis。 在传统的 SSM 中，需要开发者自己来配置 Spring Data Redis ，这个配置比较繁琐，主要配置 3 个东西：连接池、连接器信息以及 key 和 value 的序列化方案。 在 Spring Boot 中，默认集成的 Redis 就是 Spring Data Redis，默认底层的连接池使用了 lettuce ，开发者可以自行修改为自己的熟悉的，例如 Jedis。 Spring Data Redis 针对 Redis 提供了非常方便的操作模板 RedisTemplate 。这是 Spring Data 擅长的事情，那么接下来我们就来看看 Spring Boot 中 Spring Data Redis 的具体用法。 方案一：Spring Data Redis创建工程创建工程，引入 Redis 依赖： 创建成功后，还需要手动引入 commos-pool2 的依赖，因此最终完整的 pom.xml 依赖如下： 1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 这里主要就是引入了 Spring Data Redis + 连接池。 配置 Redis 信息接下来配置 Redis 的信息，信息包含两方面，一方面是 Redis 的基本信息，另一方面则是连接池信息: 123456789spring.redis.database=0spring.redis.password=123spring.redis.port=6379spring.redis.host=192.168.66.128spring.redis.lettuce.pool.min-idle=5spring.redis.lettuce.pool.max-idle=10spring.redis.lettuce.pool.max-active=8spring.redis.lettuce.pool.max-wait=1msspring.redis.lettuce.shutdown-timeout=100ms 自动配置当开发者在项目中引入了 Spring Data Redis ，并且配置了 Redis 的基本信息，此时，自动化配置就会生效。 我们从 Spring Boot 中 Redis 的自动化配置类中就可以看出端倪： 12345678910111213141516171819202122@Configuration@ConditionalOnClass(RedisOperations.class)@EnableConfigurationProperties(RedisProperties.class)@Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class })public class RedisAutoConfiguration { @Bean @ConditionalOnMissingBean(name = \"redisTemplate\") public RedisTemplate&lt;Object, Object&gt; redisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean public StringRedisTemplate stringRedisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; }} 这个自动化配置类很好理解： 首先标记这个是一个配置类，同时该配置在 RedisOperations 存在的情况下才会生效(即项目中引入了 Spring Data Redis) 然后导入在 application.properties 中配置的属性 然后再导入连接池信息（如果存在的话） 最后，提供了两个 Bean ，RedisTemplate 和 StringRedisTemplate ，其中 StringRedisTemplate 是 RedisTemplate 的子类，两个的方法基本一致，不同之处主要体现在操作的数据类型不同，RedisTemplate 中的两个泛型都是 Object ，意味者存储的 key 和 value 都可以是一个对象，而 StringRedisTemplate 的 两个泛型都是 String ，意味者 StringRedisTemplate 的 key 和 value 都只能是字符串。如果开发者没有提供相关的 Bean ，这两个配置就会生效，否则不会生效。 使用接下来，可以直接在 Service 中注入 StringRedisTemplate 或者 RedisTemplate 来使用： 1234567891011@Servicepublic class HelloService { @Autowired RedisTemplate redisTemplate; public void hello() { ValueOperations ops = redisTemplate.opsForValue(); ops.set(\"k1\", \"v1\"); Object k1 = ops.get(\"k1\"); System.out.println(k1); }} Redis 中的数据操作，大体上来说，可以分为两种： 针对 key 的操作，相关的方法就在 RedisTemplate 中 针对具体数据类型的操作，相关的方法需要首先获取对应的数据类型，获取相应数据类型的操作方法是 opsForXXX 调用该方法就可以将数据存储到 Redis 中去了，如下： k1 前面的字符是由于使用了 RedisTemplate 导致的，RedisTemplate 对 key 进行序列化之后的结果。 RedisTemplate 中，key 默认的序列化方案是 JdkSerializationRedisSerializer 。 而在 StringRedisTemplate 中，key 默认的序列化方案是 StringRedisSerializer ，因此，如果使用 StringRedisTemplate ，默认情况下 key 前面不会有前缀。 不过开发者也可以自行修改 RedisTemplate 中的序列化方案，如下: 123456789101112@Servicepublic class HelloService { @Autowired RedisTemplate redisTemplate; public void hello() { redisTemplate.setKeySerializer(new StringRedisSerializer()); ValueOperations ops = redisTemplate.opsForValue(); ops.set(\"k1\", \"v1\"); Object k1 = ops.get(\"k1\"); System.out.println(k1); }} 当然也可以直接使用 StringRedisTemplate： 1234567891011@Servicepublic class HelloService { @Autowired StringRedisTemplate stringRedisTemplate; public void hello2() { ValueOperations ops = stringRedisTemplate.opsForValue(); ops.set(\"k2\", \"v2\"); Object k1 = ops.get(\"k2\"); System.out.println(k1); }} 另外需要注意 ，Spring Boot 的自动化配置，只能配置单机的 Redis ，如果是 Redis 集群，则所有的东西都需要自己手动配置，关于如何操作 Redis 集群，松哥以后再来和大家分享。 方案二：Spring Cache通过 Spring Cache 的形式来操作 Redis，Spring Cache 统一了缓存江湖的门面，这种方案，松哥之前有过一篇专门的文章介绍，小伙伴可以移步这里：Spring Boot中，Redis缓存还能这么用！。 方案三：回归原始时代第三种方案，就是直接使用 Jedis 或者 其他的客户端工具来操作 Redis ，这种方案在 Spring Boot 中也是支持的，虽然操作麻烦，但是支持，这种操作松哥之前也有介绍的文章，因此这里就不再赘述了，可以参考 Jedis 使用。 总结Spring Boot 中，Redis 的操作，这里松哥给大家总结了三种方案，实际上前两个使用广泛一些，直接使用 Jedis 还是比较少，基本上 Spring Boot 中没见过有人直接这么搞。 好了，本文就说到这里，有问题欢迎留言讨论。","link":"/2019/0603/springboot-redis.html"},{"title":"Spring Boot中的yaml配置简介","text":"搞Spring Boot的小伙伴都知道，Spring Boot中的配置文件有两种格式，properties或者yaml，一般情况下，两者可以随意使用，选择自己顺手的就行了，那么这两者完全一样吗？肯定不是啦！本文就来和大伙重点介绍下yaml配置，最后再来看看yaml和properties配置有何区别。 狡兔三窟首先application.yaml在Spring Boot中可以写在四个不同的位置，分别是如下位置： 项目根目录下的config目录中 项目根目录下 classpath下的config目录中 classpath目录下 四个位置中的application.yaml文件的优先级按照上面列出的顺序依次降低。即如果有同一个属性在四个文件中都出现了，以优先级高的为准。 那么application.yaml是不是必须叫application.yaml这个名字呢？当然不是必须的。开发者可以自己定义yaml名字，自己定义的话，需要在项目启动时指定配置文件的名字，像下面这样： 当然这是在IntelliJ IDEA中直接配置的，如果项目已经打成jar包了，则在项目启动时加入如下参数： 1java -jar myproject.jar --spring.config.name=app 这样配置之后，在项目启动时，就会按照上面所说的四个位置按顺序去查找一个名为app.yaml的文件。当然这四个位置也不是一成不变的，也可以自己定义，有两种方式，一个是使用spring.config.location属性，另一个则是使用spring.config.additional-location这个属性，在第一个属性中，表示自己重新定义配置文件的位置，项目启动时就按照定义的位置去查找配置文件，这种定义方式会覆盖掉默认的四个位置，也可以使用第二种方式，第二种方式则表示在四个位置的基础上，再添加几个位置，新添加的位置的优先级大于原本的位置。 配置方式如下： 这里要注意，配置文件位置时，值一定要以/结尾。 数组注入yaml也支持数组注入，例如 1234my: servers: - dev.example.com - another.example.com 这段数据可以绑定到一个带Bean的数组中： 12345678910@ConfigurationProperties(prefix=\"my\")@Componentpublic class Config { private List&lt;String&gt; servers = new ArrayList&lt;String&gt;(); public List&lt;String&gt; getServers() { return this.servers; }} 项目启动后，配置中的数组会自动存储到servers集合中。当然，yaml不仅可以存储这种简单数据，也可以在集合中存储对象。例如下面这种： 123456redis: redisConfigs: - host: 192.168.66.128 port: 6379 - host: 192.168.66.129 port: 6380 这个可以被注入到如下类中： 123456@Component@ConfigurationProperties(prefix = \"redis\")public class RedisCluster { private List&lt;SingleRedisConfig&gt; redisConfigs; //省略getter/setter} 优缺点不同于properties文件的无序，yaml配置是有序的，这一点在有些配置中是非常有用的，例如在Spring Cloud Zuul的配置中，当我们配置代理规则时，顺序就显得尤为重要了。当然yaml配置也不是万能的，例如，yaml配置目前不支持@PropertySource注解。","link":"/2019/0416/springboot-yaml.html"},{"title":"Spring Boot中通过CORS解决跨域问题","text":"今天和小伙伴们来聊一聊通过CORS解决跨域问题。 同源策略很多人对跨域有一种误解，以为这是前端的事，和后端没关系，其实不是这样的，说到跨域，就不得不说说浏览器的同源策略。同源策略是由Netscape提出的一个著名的安全策略，它是浏览器最核心也最基本的安全功能，现在所有支持JavaScript的浏览器都会使用这个策略。所谓同源是指协议、域名以及端口要相同。同源策略是基于安全方面的考虑提出来的，这个策略本身没问题，但是我们在实际开发中，由于各种原因又经常有跨域的需求，传统的跨域方案是JSONP，JSONP虽然能解决跨域但是有一个很大的局限性，那就是只支持GET请求，不支持其他类型的请求，而今天我们说的CORS（跨域源资源共享）（CORS，Cross-origin resource sharing）是一个W3C标准，它是一份浏览器技术的规范，提供了Web服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，这是JSONP模式的现代版。在Spring框架中，对于CORS也提供了相应的解决方案，今天我们就来看看SpringBoot中如何实现CORS。 实践接下来我们就来看看Spring Boot中如何实现这个东西。 首先创建两个普通的SpringBoot项目，这个就不用我多说，第一个命名为provider提供服务，第二个命名为consumer消费服务，第一个配置端口为8080，第二个配置配置为8081，然后在provider上提供两个hello接口，一个get，一个post，如下： 1234567891011@RestControllerpublic class HelloController { @GetMapping(&quot;/hello&quot;) public String hello() { return &quot;hello&quot;; } @PostMapping(&quot;/hello&quot;) public String hello2() { return &quot;post hello&quot;; }} 在consumer的resources/static目录下创建一个html文件，发送一个简单的ajax请求，如下： 12345678910111213141516&lt;div id=&quot;app&quot;&gt;&lt;/div&gt;&lt;input type=&quot;button&quot; onclick=&quot;btnClick()&quot; value=&quot;get_button&quot;&gt;&lt;input type=&quot;button&quot; onclick=&quot;btnClick2()&quot; value=&quot;post_button&quot;&gt;&lt;script&gt; function btnClick() { $.get(&apos;http://localhost:8080/hello&apos;, function (msg) { $(&quot;#app&quot;).html(msg); }); } function btnClick2() { $.post(&apos;http://localhost:8080/hello&apos;, function (msg) { $(&quot;#app&quot;).html(msg); }); }&lt;/script&gt; 然后分别启动两个项目，发送请求按钮，观察浏览器控制台如下： 1Access to XMLHttpRequest at &apos;http://localhost:8080/hello&apos; from origin &apos;http://localhost:8081&apos; has been blocked by CORS policy: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. 可以看到，由于同源策略的限制，请求无法发送成功。 使用CORS可以在前端代码不做任何修改的情况下，实现跨域，那么接下来看看在provider中如何配置。首先可以通过@CrossOrigin注解配置某一个方法接受某一个域的请求，如下： 1234567891011121314@RestControllerpublic class HelloController { @CrossOrigin(value = &quot;http://localhost:8081&quot;) @GetMapping(&quot;/hello&quot;) public String hello() { return &quot;hello&quot;; } @CrossOrigin(value = &quot;http://localhost:8081&quot;) @PostMapping(&quot;/hello&quot;) public String hello2() { return &quot;post hello&quot;; }} 这个注解表示这两个接口接受来自http://localhost:8081地址的请求，配置完成后，重启provider，再次发送请求，浏览器控制台就不会报错了，consumer也能拿到数据了。 此时观察浏览器请求网络控制台，可以看到响应头中多了如下信息： 这个表示服务端愿意接收来自http://localhost:8081的请求，拿到这个信息后，浏览器就不会再去限制本次请求的跨域了。 provider上，每一个方法上都去加注解未免太麻烦了，在Spring Boot中，还可以通过全局配置一次性解决这个问题，全局配置只需要在配置类中重写addCorsMappings方法即可，如下： 12345678910@Configurationpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;http://localhost:8081&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;); }} /**表示本应用的所有方法都会去处理跨域请求，allowedMethods表示允许通过的请求数，allowedHeaders则表示允许的请求头。经过这样的配置之后，就不必在每个方法上单独配置跨域了。 存在的问题了解了整个CORS的工作过程之后，我们通过Ajax发送跨域请求，虽然用户体验提高了，但是也有潜在的威胁存在，常见的就是CSRF（Cross-site request forgery）跨站请求伪造。跨站请求伪造也被称为one-click attack 或者 session riding，通常缩写为CSRF或者XSRF，是一种挟制用户在当前已登录的Web应用程序上执行非本意的操作的攻击方法，举个例子： 假如一家银行用以运行转账操作的URL地址如下：http://icbc.com/aa?bb=cc，那么，一个恶意攻击者可以在另一个网站上放置如下代码：&lt;img src=&quot;http://icbc.com/aa?bb=cc&quot;&gt;，如果用户访问了恶意站点，而她之前刚访问过银行不久，登录信息尚未过期，那么她就会遭受损失。 基于此，浏览器在实际操作中，会对请求进行分类，分为简单请求，预先请求，带凭证的请求等，预先请求会首先发送一个options探测请求，和浏览器进行协商是否接受请求。默认情况下跨域请求是不需要凭证的，但是服务端可以配置要求客户端提供凭证，这样就可以有效避免csrf攻击。 好了，这个问题就说这么多，关于springboot中cors，还有一个小小的视频教程，加入我的知识星球免费观看。","link":"/2019/0412/springboot-cors.html"},{"title":"Spring Boot中，Redis缓存还能这么用！","text":"经过Spring Boot的整合封装与自动化配置，在Spring Boot中整合Redis已经变得非常容易了，开发者只需要引入Spring Data Redis依赖，然后简单配下redis的基本信息，系统就会提供一个RedisTemplate供开发者使用，但是今天松哥想和大伙聊的不是这种用法，而是结合Cache的用法。Spring3.1中开始引入了令人激动的Cache，在Spring Boot中，可以非常方便的使用Redis来作为Cache的实现，进而实现数据的缓存。 工程创建首先创建一个Spring Boot工程，注意创建的时候需要引入三个依赖，web、cache以及redis，如下图： 对应的依赖内容如下： 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 基本配置工程创建好之后，首先需要简单配置一下Redis，Redis的基本信息，另外，这里要用到Cache，因此还需要稍微配置一下Cache，如下： 1234spring.redis.port=6380spring.redis.host=192.168.66.128spring.cache.cache-names=c1 简单起见，这里我只是配置了Redis的端口和地址，然后给缓存取了一个名字，这个名字在后文会用到。 另外，还需要在配置类上添加如下代码，表示开启缓存： 123456789@SpringBootApplication@EnableCachingpublic class RediscacheApplication { public static void main(String[] args) { SpringApplication.run(RediscacheApplication.class, args); }} 完成了这些配置之后，Spring Boot就会自动帮我们在后台配置一个RedisCacheManager，相关的配置是在org.springframework.boot.autoconfigure.cache.RedisCacheConfiguration类中完成的。部分源码如下： 123456789101112131415161718192021@Configuration@ConditionalOnClass(RedisConnectionFactory.class)@AutoConfigureAfter(RedisAutoConfiguration.class)@ConditionalOnBean(RedisConnectionFactory.class)@ConditionalOnMissingBean(CacheManager.class)@Conditional(CacheCondition.class)class RedisCacheConfiguration { @Bean public RedisCacheManager cacheManager(RedisConnectionFactory redisConnectionFactory, ResourceLoader resourceLoader) { RedisCacheManagerBuilder builder = RedisCacheManager .builder(redisConnectionFactory) .cacheDefaults(determineConfiguration(resourceLoader.getClassLoader())); List&lt;String&gt; cacheNames = this.cacheProperties.getCacheNames(); if (!cacheNames.isEmpty()) { builder.initialCacheNames(new LinkedHashSet&lt;&gt;(cacheNames)); } return this.customizerInvoker.customize(builder.build()); }} 看类上的注解，发现在万事俱备的情况下，系统会自动提供一个RedisCacheManager的Bean，这个RedisCacheManager间接实现了Spring中的Cache接口，有了这个Bean，我们就可以直接使用Spring中的缓存注解和接口了，而缓存数据则会被自动存储到Redis上。在单机的Redis中，这个Bean系统会自动提供，如果是Redis集群，这个Bean需要开发者来提供（后面的文章会讲到）。 缓存使用这里主要向小伙伴们介绍缓存中几个核心的注解使用。 @CacheConfig这个注解在类上使用，用来描述该类中所有方法使用的缓存名称，当然也可以不使用该注解，直接在具体的缓存注解上配置名称，示例代码如下： 1234@Service@CacheConfig(cacheNames = \"c1\")public class UserService {} @Cacheable这个注解一般加在查询方法上，表示将一个方法的返回值缓存起来，默认情况下，缓存的key就是方法的参数，缓存的value就是方法的返回值。示例代码如下： 12345@Cacheable(key = \"#id\")public User getUserById(Integer id,String username) { System.out.println(\"getUserById\"); return getUserFromDBById(id);} 当有多个参数时，默认就使用多个参数来做key，如果只需要其中某一个参数做key，则可以在@Cacheable注解中，通过key属性来指定key，如上代码就表示只使用id作为缓存的key，如果对key有复杂的要求，可以自定义keyGenerator。当然，Spring Cache中提供了root对象，可以在不定义keyGenerator的情况下实现一些复杂的效果： @CachePut这个注解一般加在更新方法上，当数据库中的数据更新后，缓存中的数据也要跟着更新，使用该注解，可以将方法的返回值自动更新到已经存在的key上，示例代码如下： 1234@CachePut(key = \"#user.id\")public User updateUserById(User user) { return user;} @CacheEvict这个注解一般加在删除方法上，当数据库中的数据删除后，相关的缓存数据也要自动清除，该注解在使用的时候也可以配置按照某种条件删除（condition属性）或者或者配置清除所有缓存（allEntries属性），示例代码如下： 1234@CacheEvict()public void deleteUserById(Integer id) { //在这里执行删除操作， 删除是去数据库中删除} 总结在Spring Boot中，使用Redis缓存，既可以使用RedisTemplate自己来实现，也可以使用使用这种方式，这种方式是Spring Cache提供的统一接口，实现既可以是Redis，也可以是Ehcache或者其他支持这种规范的缓存框架。从这个角度来说，Spring Cache和Redis、Ehcache的关系就像JDBC与各种数据库驱动的关系。 好了，关于这个问题就说到这里，有问题欢迎留言讨论。","link":"/2019/0416/springboot-redis.html"},{"title":"Spring Boot多数据源配置之JdbcTemplate","text":"多数据源配置也算是一个常见的开发需求，Spring和SpringBoot中，对此都有相应的解决方案，不过一般来说，如果有多数据源的需求，我还是建议首选分布式数据库中间件MyCat去解决相关问题，之前有小伙伴在我的知识星球上提问，他的数据根据条件的不同，可能保存在四十多个不同的数据库中，怎么办？这种场景下使用多数据源其实就有些费事了，我给的建议是使用MyCat，然后分表策略使用sharding-by-intfile。当然如果一些简单的需求，还是可以使用多数据源的，Spring Boot中，JdbcTemplate、MyBatis以及Jpa都可以配置多数据源，本文就先和大伙聊一聊JdbcTemplate中多数据源的配置（关于JdbcTemplate的用法，如果还有小伙伴不了解，可以参考我的上篇文章）。 创建工程首先是创建工程，和前文一样，创建工程时，也是选择Web、Jdbc以及MySQL驱动，如下图： 创建成功之后，一定接下来手动添加Druid依赖，由于这里一会需要开发者自己配置DataSoruce，所以这里必须要使用druid-spring-boot-starter依赖，而不是传统的那个druid依赖，因为druid-spring-boot-starter依赖提供了DruidDataSourceBuilder类，这个可以用来构建一个DataSource实例，而传统的Druid则没有该类。完整的依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt; 配置数据源.接下来，在application.properties中配置数据源，不同于上文，这里的数据源需要配置两个，如下： spring.datasource.one.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=utf-8 spring.datasource.one.username=root spring.datasource.one.password=root spring.datasource.one.type=com.alibaba.druid.pool.DruidDataSource spring.datasource.two.url=jdbc:mysql:///test02?useUnicode=true&amp;characterEncoding=utf-8 spring.datasource.two.username=root spring.datasource.two.password=root spring.datasource.two.type=com.alibaba.druid.pool.DruidDataSource 这里通过one和two对数据源进行了区分，但是加了one和two之后，这里的配置就没法被SpringBoot自动加载了（因为前面的key变了），需要我们自己去加载DataSource了，此时，需要自己配置一个DataSourceConfig，用来提供两个DataSource Bean，如下： @Configuration public class DataSourceConfig { @Bean @ConfigurationProperties(prefix = \"spring.datasource.one\") DataSource dsOne() { return DruidDataSourceBuilder.create().build(); } @Bean @ConfigurationProperties(prefix = \"spring.datasource.two\") DataSource dsTwo() { return DruidDataSourceBuilder.create().build(); } } 这里提供了两个Bean，其中@ConfigurationProperties是Spring Boot提供的类型安全的属性绑定，以第一个Bean为例，@ConfigurationProperties(prefix = &quot;spring.datasource.one&quot;)表示使用spring.datasource.one前缀的数据库配置去创建一个DataSource，这样配置之后，我们就有了两个不同的DataSource，接下来再用这两个不同的DataSource去创建两个不同的JdbcTemplate。 配置JdbcTemplate实例创建JdbcTemplateConfig类，用来提供两个不同的JdbcTemplate实例，如下： @Configuration public class JdbcTemplateConfig { @Bean JdbcTemplate jdbcTemplateOne(@Qualifier(\"dsOne\") DataSource dsOne) { return new JdbcTemplate(dsOne); } @Bean JdbcTemplate jdbcTemplateTwo(@Qualifier(\"dsTwo\") DataSource dsTwo) { return new JdbcTemplate(dsTwo); } } 每一个JdbcTemplate的创建都需要一个DataSource，由于Spring容器中现在存在两个DataSource，默认使用类型查找，会报错，因此加上@Qualifier注解，表示按照名称查找。这里创建了两个JdbcTemplate实例，分别对应了两个DataSource。 接下来直接去使用这个JdbcTemplate就可以了。 测试使用关于JdbcTemplate的详细用法大伙可以参考我的上篇文章，这里我主要演示数据源的差异，在Controller中注入两个不同的JdbcTemplate，这两个JdbcTemplate分别对应了不同的数据源，如下： @RestController public class HelloController { @Autowired @Qualifier(\"jdbcTemplateOne\") JdbcTemplate jdbcTemplateOne; @Resource(name = \"jdbcTemplateTwo\") JdbcTemplate jdbcTemplateTwo; @GetMapping(\"/user\") public List&lt;User&gt; getAllUser() { List&lt;User&gt; list = jdbcTemplateOne.query(\"select * from t_user\", new BeanPropertyRowMapper&lt;&gt;(User.class)); return list; } @GetMapping(\"/user2\") public List&lt;User&gt; getAllUser2() { List&lt;User&gt; list = jdbcTemplateTwo.query(\"select * from t_user\", new BeanPropertyRowMapper&lt;&gt;(User.class)); return list; } } 和DataSource一样，Spring容器中的JdbcTemplate也是有两个，因此不能通过byType的方式注入进来，这里给大伙提供了两种注入思路，一种是使用@Resource注解，直接通过byName的方式注入进来，另外一种就是@Autowired注解加上@Qualifier注解，两者联合起来，实际上也是byName。将JdbcTemplate注入进来之后，jdbcTemplateOne和jdbcTemplateTwo此时就代表操作不同的数据源，使用不同的JdbcTemplate操作不同的数据源，实现了多数据源配置。 好了，这个问题就先说到这里，关于这个多数据源配置，还有一个小小的视频教程，加入我的星球免费观看： 关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0406/springboot-jdbctemplate.html"},{"title":"Spring Data Redis 使用","text":"上文我们介绍了 Redis，在开发环境中，我们还有另外一个解决方案，那就是 Spring Data Redis 。本文我们就来看看这个东西。 本文是 Redis 系列的第十四篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一)11.Redis 主从复制(二)12.Redis 集群搭建13.Jedis 使用 Spring Data Redis 介绍Spring Data Redis 是 Spring 官方推出，可以算是 Spring 框架集成 Redis 操作的一个子框架，封装了 Redis 的很多命令，可以很方便的使用 Spring 操作 Redis 数据库，Spring 对很多工具都提供了类似的集成，如 Spring Data MongDB、Spring Data JPA 等, Spring Data Redis 只是其中一种。 环境搭建要使用 SDR，首先需要搭建 Spring+SpringMVC 环境，由于这个不是本文的重点，因此这一步我直接略过，Spring+SpringMVC 环境搭建成功后，接下来我们要整合 SDR，首先需要添加如下依赖： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt;&lt;/dependency&gt; 然后创建在 resources 目录下创建 redis.properties 文件作为 redis 的配置文件，如下： 123456redis.host=192.168.248.128redis.port=6379redis.maxIdle=300redis.maxTotal=600redis.maxWait=1000redis.testOnBorrow=true 在 spring 的配置文件中，添加如下 bean： 1234567891011121314151617181920212223242526&lt;!--引入redis.properties文件--&gt;&lt;context:property-placeholder location=\"classpath:redis.properties\"/&gt;&lt;!--配置连接池信息--&gt;&lt;bean class=\"redis.clients.jedis.JedisPoolConfig\" id=\"poolConfig\"&gt; &lt;property name=\"maxIdle\" value=\"${redis.maxIdle}\"/&gt; &lt;property name=\"maxTotal\" value=\"${redis.maxTotal}\"/&gt; &lt;property name=\"maxWaitMillis\" value=\"${redis.maxWait}\"/&gt; &lt;property name=\"testOnBorrow\" value=\"${redis.testOnBorrow}\"/&gt;&lt;/bean&gt;&lt;!--配置基本连接信息--&gt;&lt;bean class=\"org.springframework.data.redis.connection.jedis.JedisConnectionFactory\" id=\"connectionFactory\"&gt; &lt;property name=\"hostName\" value=\"${redis.host}\"/&gt; &lt;property name=\"port\" value=\"${redis.port}\"/&gt; &lt;property name=\"poolConfig\" ref=\"poolConfig\"/&gt;&lt;/bean&gt;&lt;!--配置RedisTemplate--&gt;&lt;bean class=\"org.springframework.data.redis.core.RedisTemplate\" id=\"redisTemplate\"&gt; &lt;property name=\"connectionFactory\" ref=\"connectionFactory\"/&gt; &lt;!--key和value要进行序列化，否则存储对象时会出错--&gt; &lt;property name=\"keySerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt; &lt;property name=\"valueSerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.JdkSerializationRedisSerializer\"/&gt; &lt;/property&gt;&lt;/bean&gt; 好了，在 Spring 中配置了 redisTemplate 之后，接下来我们就可以在 Dao 层注入 redisTemplate 进而使用了。 接下来我们首先创建实体类 User ，注意 User 一定要可序列化： 123456public class User implements Serializable{ private String username; private String password; private String id; //get/set省略} 然后在 Dao 层实现数据的添加和获取，如下： 1234567891011121314151617181920212223@Repositorypublic class HelloDao { @Autowired RedisTemplate redisTemplate; public void set(String key, String value) { ValueOperations ops = redisTemplate.opsForValue(); ops.set(key, value); } public String get(String key) { ValueOperations ops = redisTemplate.opsForValue(); return ops.get(key).toString(); } public void setuser(User user) { ValueOperations ops = redisTemplate.opsForValue(); ops.set(user.getId(), user); } public User getuser(String id) { ValueOperations&lt;String, User&gt; ops = redisTemplate.opsForValue(); User user = ops.get(id); System.out.println(user); return user; }} SDR 官方文档中对 Redistemplate 的介绍，通过 Redistemplate 可以调用 ValueOperations 和 ListOperations 等等方法，分别是对 Redis 命令的高级封装。但是 ValueOperations 等等这些命令最终是要转化成为 RedisCallback 来执行的。也就是说通过使用 RedisCallback 可以实现更强的功能。 最后，给大家展示下我的 Service 和 Controller ，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Servicepublic class HelloService { @Autowired HelloDao helloDao; public void set(String key, String value) { helloDao.set(key,value); } public String get(String key) { return helloDao.get(key); } public void setuser(User user) { helloDao.setuser(user); } public String getuser(String id) { String s = helloDao.getuser(id).toString(); return s; }}Controller：@Controllerpublic class HelloController { @Autowired HelloService helloService; @RequestMapping(\"/set\") @ResponseBody public void set(String key, String value) { helloService.set(key, value); } @RequestMapping(\"/get\") @ResponseBody public String get(String key) { return helloService.get(key); } @RequestMapping(\"/setuser\") @ResponseBody public void setUser() { User user = new User(); user.setId(\"1\"); user.setUsername(\"深圳\"); user.setPassword(\"sang\"); helloService.setuser(user); } @RequestMapping(value = \"/getuser\",produces = \"text/html;charset=UTF-8\") @ResponseBody public String getUser() { return helloService.getuser(\"1\"); }} 测试过程就不再展示了，小伙伴们可以用 POSTMAN 等工具自行测试。 好了，Spring Data Redis 我们就说到这里，有问题欢迎留言讨论。","link":"/2019/0615/springdata-redis.html"},{"title":"Spring Security 前后端分离登录，非法请求直接返回 JSON","text":"hello 各位小伙伴，国庆节终于过完啦，松哥也回来啦，今天开始咱们继续发干货！ 关于 Spring Security，松哥之前发过多篇文章和大家聊聊这个安全框架的使用： 手把手带你入门 Spring Security！ Spring Security 登录添加验证码 SpringSecurity 登录使用 JSON 格式数据 Spring Security 中的角色继承问题 Spring Security 中使用 JWT! Spring Security 结合 OAuth2 不过，今天要和小伙伴们聊一聊 Spring Security 中的另外一个问题，那就是在 Spring Security 中未获认证的请求默认会重定向到登录页，但是在前后端分离的登录中，这个默认行为则显得非常不合适，今天我们主要来看看如何实现未获认证的请求直接返回 JSON ，而不是重定向到登录页面。 前置知识这里关于 Spring Security 的基本用法我就不再赘述了，如果小伙伴们不了解，可以参考上面的 6 篇文章。 大家知道，在自定义 Spring Security 配置的时候，有这样几个属性： 123456789101112@Overrideprotected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .anyRequest().authenticated() .formLogin() .loginProcessingUrl(\"/doLogin\") .loginPage(\"/login\") //其他配置 .permitAll() .and() .csrf().disable();} 这里有两个比较重要的属性： loginProcessingUrl：这个表示配置处理登录请求的接口地址，例如你是表单登录，那么 form 表单中 action 的值就是这里填的值。 loginPage：这个表示登录页的地址，例如当你访问一个需要登录后才能访问的资源时，系统就会自动给你通过重定向跳转到这个页面上来。 这种配置在前后端不分的登录中是没有问题的，在前后端分离的登录中，这种配置就有问题了。我举个简单的例子，例如我想访问 /hello 接口，但是这个接口需要登录之后才能访问，我现在没有登录就直接去访问这个接口了，那么系统会给我返回 302，让我去登录页面，在前后端分离中，我的后端一般是没有登录页面的，就是一个提示 JSON，例如下面这样： 1234@GetMapping(\"/login\")public RespBean login() { return RespBean.error(\"尚未登录，请登录!\");} 完整代码大家可以参考我的微人事项目。 也就是说，当我没有登录直接去访问 /hello 这个接口的时候，我会看到上面这段 JSON 字符串。在前后端分离开发中，这个看起来没问题（后端不再做页面跳转，无论发生什么都是返回 JSON）。但是问题就出在这里，系统默认的跳转是一个重定向，就是说当你访问 /hello 的时候，服务端会给浏览器返回 302，同时响应头中有一个 Location 字段，它的值为 http://localhost:8081/login ，也就是告诉浏览器你去访问 http://localhost:8081/login 地址吧。浏览器收到指令之后，就会直接去访问 http://localhost:8081/login 地址，如果此时是开发环境并且请求还是 Ajax 请求，就会发生跨域。因为前后端分离开发中，前端我们一般在 NodeJS 上启动，然后前端的所有请求通过 NodeJS 做请求转发，现在服务端直接把请求地址告诉浏览器了，浏览器就会直接去访问 http://localhost:8081/login 了，而不会做请求转发了，因此就发生了跨域问题。 解决方案很明显，上面的问题我们不能用跨域的思路来解决，虽然这种方式看起来也能解决问题，但不是最佳方案。 如果我们的 Spring Security 在用户未获认证的时候去请求一个需要认证后才能请求的数据，此时不给用户重定向，而是直接就返回一个 JSON，告诉用户这个请求需要认证之后才能发起，就不会有上面的事情了。 这里就涉及到 Spring Security 中的一个接口 AuthenticationEntryPoint ，该接口有一个实现类：LoginUrlAuthenticationEntryPoint ，该类中有一个方法 commence，如下： 1234567891011121314151617181920212223242526/** * Performs the redirect (or forward) to the login form URL. */public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) { String redirectUrl = null; if (useForward) { if (forceHttps &amp;&amp; \"http\".equals(request.getScheme())) { redirectUrl = buildHttpsRedirectUrlForRequest(request); } if (redirectUrl == null) { String loginForm = determineUrlToUseForThisRequest(request, response, authException); if (logger.isDebugEnabled()) { logger.debug(\"Server side forward to: \" + loginForm); } RequestDispatcher dispatcher = request.getRequestDispatcher(loginForm); dispatcher.forward(request, response); return; } } else { redirectUrl = buildRedirectUrlToLoginPage(request, response, authException); } redirectStrategy.sendRedirect(request, response, redirectUrl);} 首先我们从这个方法的注释中就可以看出，这个方法是用来决定到底是要重定向还是要 forward，通过 Debug 追踪，我们发现默认情况下 useForward 的值为 false，所以请求走进了重定向。 那么我们解决问题的思路很简单，直接重写这个方法，在方法中返回 JSON 即可，不再做重定向操作，具体配置如下： 1234567891011121314151617181920212223242526@Overrideprotected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .anyRequest().authenticated() .formLogin() .loginProcessingUrl(\"/doLogin\") .loginPage(\"/login\") //其他配置 .permitAll() .and() .csrf().disable().exceptionHandling() .authenticationEntryPoint(new AuthenticationEntryPoint() { @Override public void commence(HttpServletRequest req, HttpServletResponse resp, AuthenticationException authException) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); RespBean respBean = RespBean.error(\"访问失败!\"); if (authException instanceof InsufficientAuthenticationException) { respBean.setMsg(\"请求失败，请联系管理员!\"); } out.write(new ObjectMapper().writeValueAsString(respBean)); out.flush(); out.close(); } });} 在 Spring Security 的配置中加上自定义的 AuthenticationEntryPoint 处理方法，该方法中直接返回相应的 JSON 提示即可。这样，如果用户再去直接访问一个需要认证之后才可以访问的请求，就不会发生重定向操作了，服务端会直接给浏览器一个 JSON 提示，浏览器收到 JSON 之后，该干嘛干嘛。 结语好了，一个小小的重定向问题和小伙伴们分享下，不知道大家有没有看懂呢？这也是我最近在重构微人事的时候遇到的问题。预计 11 月份，微人事的 Spring Boot 版本会升级到目前最新版，请小伙伴们留意哦。","link":"/2019/1015/springsecurity.html"},{"title":"SpringBoot整合Swagger2，再也不用维护接口文档了！","text":"前后端分离后，维护接口文档基本上是必不可少的工作。一个理想的状态是设计好后，接口文档发给前端和后端，大伙按照既定的规则各自开发，开发好了对接上了就可以上线了。当然这是一种非常理想的状态，实际开发中却很少遇到这样的情况，接口总是在不断的变化之中，有变化就要去维护，做过的小伙伴都知道这件事有多么头大！还好，有一些工具可以减轻我们的工作量，Swagger2就是其中之一，至于其他类似功能但是却收费的软件，这里就不做过多介绍了。本文主要和大伙来聊下在Spring Boot中如何整合Swagger2。 工程创建当然，首先是创建一个Spring Boot项目，加入web依赖，创建成功后，加入两个Swagger2相关的依赖，完整的依赖如下： 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; Swagger2配置Swagger2的配置也是比较容易的，在项目创建成功之后，只需要开发者自己提供一个Docket的Bean即可，如下： 1234567891011121314151617181920@Configuration@EnableSwagger2public class SwaggerConfig { @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .pathMapping(\"/\") .select() .apis(RequestHandlerSelectors.basePackage(\"com.nvn.controller\")) .paths(PathSelectors.any()) .build().apiInfo(new ApiInfoBuilder() .title(\"SpringBoot整合Swagger\") .description(\"SpringBoot整合Swagger，详细信息......\") .version(\"9.0\") .contact(new Contact(\"啊啊啊啊\",\"blog.csdn.net\",\"aaa@gmail.com\")) .license(\"The Apache License\") .licenseUrl(\"http://www.baidu.com\") .build()); }} 这里提供一个配置类，首先通过@EnableSwagger2注解启用Swagger2，然后配置一个Docket Bean，这个Bean中，配置映射路径和要扫描的接口的位置，在apiInfo中，主要配置一下Swagger2文档网站的信息，例如网站的title，网站的描述，联系人的信息，使用的协议等等。 如此，Swagger2就算配置成功了，非常方便。 此时启动项目，输入http://localhost:8080/swagger-ui.html，能够看到如下页面，说明已经配置成功了： 创建接口接下来就是创建接口了，Swagger2相关的注解其实并不多，而且很容易懂，下面我来分别向小伙伴们举例说明： 123456789101112131415161718192021222324252627282930@RestController@Api(tags = \"用户管理相关接口\")@RequestMapping(\"/user\")public class UserController { @PostMapping(\"/\") @ApiOperation(\"添加用户的接口\") @ApiImplicitParams({ @ApiImplicitParam(name = \"username\", value = \"用户名\", defaultValue = \"李四\"), @ApiImplicitParam(name = \"address\", value = \"用户地址\", defaultValue = \"深圳\", required = true) } ) public RespBean addUser(String username, @RequestParam(required = true) String address) { return new RespBean(); } @GetMapping(\"/\") @ApiOperation(\"根据id查询用户的接口\") @ApiImplicitParam(name = \"id\", value = \"用户id\", defaultValue = \"99\", required = true) public User getUserById(@PathVariable Integer id) { User user = new User(); user.setId(id); return user; } @PutMapping(\"/{id}\") @ApiOperation(\"根据id更新用户的接口\") public User updateUserById(@RequestBody User user) { return user; }} 这里边涉及到多个API，我来向小伙伴们分别说明： @Api注解可以用来标记当前Controller的功能。 @ApiOperation注解用来标记一个方法的作用。 @ApiImplicitParam注解用来描述一个参数，可以配置参数的中文含义，也可以给参数设置默认值，这样在接口测试的时候可以避免手动输入。 如果有多个参数，则需要使用多个@ApiImplicitParam注解来描述，多个@ApiImplicitParam注解需要放在一个@ApiImplicitParams注解中。 需要注意的是，@ApiImplicitParam注解中虽然可以指定参数是必填的，但是却不能代替@RequestParam(required = true)，前者的必填只是在Swagger2框架内必填，抛弃了Swagger2，这个限制就没用了，所以假如开发者需要指定一个参数必填，@RequestParam(required = true)注解还是不能省略。 如果参数是一个对象（例如上文的更新接口），对于参数的描述也可以放在实体类中。例如下面一段代码： 12345678910@ApiModelpublic class User { @ApiModelProperty(value = \"用户id\") private Integer id; @ApiModelProperty(value = \"用户名\") private String username; @ApiModelProperty(value = \"用户地址\") private String address; //getter/setter} 好了，经过如上配置之后，接下来，刷新刚刚打开的页面，可以看到如下效果： 可以看到，所有的接口这里都列出来了，包括接口请求方式，接口地址以及接口的名字等，点开一个接口，可以看到如下信息： 可以看到，接口的参数，参数要求，参数默认值等等统统都展示出来了，参数类型下的query表示参数以key/value的形式传递，点击右上角的Try it out，就可以进行接口测试： 点击Execute按钮，表示发送请求进行测试。测试结果会展示在下面的Response中。 小伙伴们注意，参数类型下面的query表示参数以key/value的形式传递，这里的值也可能是body，body表示参数以请求体的方式传递，例如上文的更新接口，如下： 当然还有一种可能就是这里的参数为path，表示参数放在路径中传递，例如根据id查询用户的接口： 当然，除了这些之外，还有一些响应值的注解，都比较简单，小伙伴可以自己摸索下。 在Security中的配置如果我们的Spring Boot项目中集成了Spring Security，那么如果不做额外配置，Swagger2文档可能会被拦截，此时只需要在Spring Security的配置类中重写configure方法，添加如下过滤即可： 1234567@Overridepublic void configure(WebSecurity web) throws Exception { web.ignoring() .antMatchers(\"/swagger-ui.html\") .antMatchers(\"/v2/**\") .antMatchers(\"/swagger-resources/**\");} 如此之后，Swagger2文件就不需要认证就能访问了。不知道小伙伴们有没有看懂呢？有问题欢迎留言讨论。","link":"/2019/0416/springboot-swagger.html"},{"title":"SpringMVC 中 @ControllerAdvice 注解的三种使用场景！","text":"@ControllerAdvice ，很多初学者可能都没有听说过这个注解，实际上，这是一个非常有用的注解，顾名思义，这是一个增强的 Controller。使用这个 Controller ，可以实现三个方面的功能： 全局异常处理 全局数据绑定 全局数据预处理 灵活使用这三个功能，可以帮助我们简化很多工作，需要注意的是，这是 SpringMVC 提供的功能，在 Spring Boot 中可以直接使用，下面分别来看。 全局异常处理使用 @ControllerAdvice 实现全局异常处理，只需要定义类，添加该注解即可定义方式如下： 12345678910@ControllerAdvicepublic class MyGlobalExceptionHandler { @ExceptionHandler(Exception.class) public ModelAndView customException(Exception e) { ModelAndView mv = new ModelAndView(); mv.addObject(\"message\", e.getMessage()); mv.setViewName(\"myerror\"); return mv; }} 在该类中，可以定义多个方法，不同的方法处理不同的异常，例如专门处理空指针的方法、专门处理数组越界的方法…，也可以直接向上面代码一样，在一个方法中处理所有的异常信息。 @ExceptionHandler 注解用来指明异常的处理类型，即如果这里指定为 NullpointerException，则数组越界异常就不会进到这个方法中来。 全局数据绑定全局数据绑定功能可以用来做一些初始化的数据操作，我们可以将一些公共的数据定义在添加了 @ControllerAdvice 注解的类中，这样，在每一个 Controller 的接口中，就都能够访问导致这些数据。 使用步骤，首先定义全局数据，如下： 12345678910@ControllerAdvicepublic class MyGlobalExceptionHandler { @ModelAttribute(name = \"md\") public Map&lt;String,Object&gt; mydata() { HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"age\", 99); map.put(\"gender\", \"男\"); return map; }} 使用 @ModelAttribute 注解标记该方法的返回数据是一个全局数据，默认情况下，这个全局数据的 key 就是返回的变量名，value 就是方法返回值，当然开发者可以通过 @ModelAttribute 注解的 name 属性去重新指定 key。 定义完成后，在任何一个Controller 的接口中，都可以获取到这里定义的数据： 12345678910@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello(Model model) { Map&lt;String, Object&gt; map = model.asMap(); System.out.println(map); int i = 1 / 0; return \"hello controller advice\"; }} 全局数据预处理考虑我有两个实体类，Book 和 Author，分别定义如下： 12345678910public class Book { private String name; private Long price; //getter/setter}public class Author { private String name; private Integer age; //getter/setter} 此时，如果我定义一个数据添加接口，如下： 12345@PostMapping(\"/book\")public void addBook(Book book, Author author) { System.out.println(book); System.out.println(author);} 这个时候，添加操作就会有问题，因为两个实体类都有一个 name 属性，从前端传递时 ，无法区分。此时，通过 @ControllerAdvice 的全局数据预处理可以解决这个问题 解决步骤如下: 1.给接口中的变量取别名 12345@PostMapping(\"/book\")public void addBook(@ModelAttribute(\"b\") Book book, @ModelAttribute(\"a\") Author author) { System.out.println(book); System.out.println(author);} 2.进行请求数据预处理在 @ControllerAdvice 标记的类中添加如下代码: 12345678@InitBinder(\"b\")public void b(WebDataBinder binder) { binder.setFieldDefaultPrefix(\"b.\");}@InitBinder(\"a\")public void a(WebDataBinder binder) { binder.setFieldDefaultPrefix(\"a.\");} @InitBinder(“b”) 注解表示该方法用来处理和Book和相关的参数,在方法中,给参数添加一个 b 前缀,即请求参数要有b前缀. 3.发送请求 请求发送时,通过给不同对象的参数添加不同的前缀,可以实现参数的区分. 总结这就是松哥给大伙介绍的 @ControllerAdvice 的几个简单用法，这些点既可以在传统的 SSM 项目中使用，也可以在 Spring Boot + Spring Cloud 微服务中使用，欢迎大家有问题一起讨论。","link":"/2019/0422/springmvc-controlleradvice.html"},{"title":"SpringMVC 方法四种类型返回值总结，你用过几种？","text":"SpringMVC 现在算是 Java 领域的一个基础性框架了，很多人天天用，可是对于 SpringMVC 方法的返回值，你又是否完全清楚呢？今天松哥就来和大家聊一聊 SpringMVC 中四种不同类型的返回值，看看有没有 get 到你的知识盲点？ 1. ModelAndView以前前后端不分的情况下，ModelAndView 应该是最最常见的返回值类型了，现在前后端分离后，后端都是以返回 JSON 数据为主了。后端返回 ModelAndView 这个比较容易理解，开发者可以在 ModelAndView 对象中指定视图名称，然后也可以绑定数据，像下面这样： 12345678910111213141516171819@RequestMapping(\"/book\")public ModelAndView getAllBook() { ModelAndView mv = new ModelAndView(); List&lt;Book&gt; books = new ArrayList&lt;&gt;(); Book b1 = new Book(); b1.setId(1); b1.setName(\"三国演义\"); b1.setAuthor(\"罗贯中\"); books.add(b1); Book b2 = new Book(); b2.setId(2); b2.setName(\"红楼梦\"); b2.setAuthor(\"曹雪芹\"); books.add(b2); //指定数据模型 mv.addObject(\"bs\", books); mv.setViewName(\"book\");//指定视图名 return mv;} 返回 ModelAndView ，最常见的两个操作就是指定数据模型+指定视图名 。 2. Void返回值为 void 时，可能是你真的没有值要返回，也可能是你有其他办法，松哥将之归为如下四类，大伙来看下。 2.1 没有值如果确实没有返回值，那就返回 void ，但是一定要注意，此时，方法上需要添加 @ResponseBody 注解，像下面这样： 12345@RequestMapping(\"/test2\")@ResponseBodypublic void test2(){ //你的代码} 2.2 重定向由于 SpringMVC 中的方法默认都具备 HttpServletResponse 参数，因此可以重拾 Servlet/Jsp 中的技能，可以实现重定向，像下面这样手动设置响应头： 123456@RequestMapping(\"/test1\")@ResponseBodypublic void test1(HttpServletResponse resp){ resp.setStatus(302); resp.addHeader(\"Location\",\"/aa/index\");} 也可以像下面这样直接调用重定向的方法： 12345@RequestMapping(\"/test1\")@ResponseBodypublic void test1(HttpServletResponse resp){ resp.sendRedirect(\"/aa/index\");} 当然，重定向无论你怎么写，都是 Servlet/Jsp 中的知识点，上面两种写法都相当于是重回远古时代。 2.3 服务端跳转既然可以重定向，当然也可以服务端跳转，像下面这样： 1234@GetMapping(\"/test5\")public void test5(HttpServletRequest req,HttpServletResponse resp) { req.getRequestDispatcher(\"/WEB-INF/jsp/index.jsp\").forward(req,resp);} 2.4 返回字符串当然也可以利用 HttpServletResponse 返回其他字符串数据，包括但不局限于 JSON，像下面这样： 123456789101112131415161718192021@RequestMapping(\"/test2\")@ResponseBodypublic void test2(HttpServletResponse resp) throws IOException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); List&lt;Book&gt; books = new ArrayList&lt;&gt;(); Book b1 = new Book(); b1.setId(1); b1.setName(\"三国演义\"); b1.setAuthor(\"罗贯中\"); books.add(b1); Book b2 = new Book(); b2.setId(2); b2.setName(\"红楼梦\"); b2.setAuthor(\"曹雪芹\"); books.add(b2); String s = new Gson().toJson(books); out.write(s); out.flush(); out.close();} 这是返回值为 void 时候的情况，方法返回值为 void ，不一定就真的不返回了，可能还有其他的方式给前端数据。 3. String当 SpringMVC 方法的返回值为 String 类型时，也有几种不同情况。 3.1 逻辑视图名返回 String 最常见的是逻辑视图名，这种时候一般利用默认的参数 Model 来传递数据，像下面这样 ： 12345@RequestMapping(\"/hello\")public String aaa(Model model) { model.addAttribute(\"username\", \"张三\"); return \"hello\";} 此时返回的 hello 就是逻辑视图名，需要携带的数据放在 model 中。 3.2 重定向也可以重定向，事实上，如果在 SpringMVC 中有重定向的需求，一般采用这种方式： 1234@RequestMapping(\"/test4\")public String test4() { return \"redirect:/aa/index\";} 3.3 forward 转发也可以 forward 转发，事实上，如果在 SpringMVC 中有 forward 转发的需求，一般采用这种方式： 1234@RequestMapping(\"/test3\")public String test3() { return \"forward:/WEB-INF/jsp/order.jsp\";} 3.4 真的是 String当然，也有一种情况，就是你真的想返回一个 String ，此时，只要在方法上加上 @ResponseBody 注解即可，或者 Controller 上本身添加的是组合注解 @RestController，像下面这样： 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello provider!\"; }} 也可以像下面这样 ： 12345678@Controllerpublic class HelloController { @GetMapping(\"/hello\") @ResponseBody public String hello() { return \"hello provider!\"; }} 这是返回值为 String 的几种情况。 4. JSON返回 JSON 算是最最常见的了，现在前后端分离的趋势下，大部分后端只需要返回 JSON 即可，那么常见的 List 集合、Map，实体类等都可以返回，这些数据由 HttpMessageConverter 自动转为 JSON ，如果大家用了 Jackson 或者 Gson ，不需要额外配置就可以自动返回 JSON 了，因为框架帮我们提供了对应的 HttpMessageConverter ，如果大家使用了 Alibaba 的 Fastjson 的话，则需要自己手动提供一个相应的 HttpMessageConverter 的实例，方法的返回值像下面这样： 123456789101112131415161718192021222324@GetMapping(\"/user\")@ResponseBodypublic User getUser() { User user = new User(); List&lt;String&gt; favorites = new ArrayList&lt;&gt;(); favorites.add(\"足球\"); favorites.add(\"篮球\"); user.setFavorites(favorites); user.setUsername(\"zhagnsan\"); user.setPassword(\"123\"); return user;}@GetMapping(\"/users\")@ResponseBodypublic List&lt;User&gt; getALlUser() { List&lt;User&gt; users = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { User e = new User(); e.setUsername(\"zhangsan:\" + i); e.setPassword(\"pwd:\" + i); users.add(e); } return users;} 总结好了，这是松哥为大伙总结的 SpringMVC 方法四种不同类型的返回值，难倒是不难！有问题欢迎大伙留言讨论。","link":"/2019/0506/springmvc-return.html"},{"title":"Spring Security 中的角色继承问题","text":"今天想和小伙伴们来聊一聊 Spring Security 中的角色继承问题。 角色继承实际上是一个很常见的需求，因为大部分公司治理可能都是金字塔形的，上司可能具备下属的部分甚至所有权限，这一现实场景，反映到我们的代码中，就是角色继承了。 Spring Security 中为开发者提供了相关的角色继承解决方案，但是这一解决方案在最近的 Spring Security 版本变迁中，使用方法有所变化。今天除了和小伙伴们分享角色继承外，也来顺便说说这种变化，避免小伙伴们踩坑，同时购买了我的书的小伙伴也需要留意，书是基于 Spring Boot2.0.4 这个版本写的，这个话题和最新版 Spring Boot 的还是有一点差别。 版本分割线上文说过，SpringSecurity 在角色继承上有两种不同的写法，在 Spring Boot2.0.8（对应 Spring Security 也是 5.0.11）上面是一种写法，从 Spring Boot2.1.0（对应 Spring Security5.1.1）又是另外一种写法，本文将从这两种角度出发，向读者介绍两种不同的角色继承写法。 以前的写法这里说的以前写法，就是指 SpringBoot2.0.8（含）之前的写法，在之前的写法中，角色继承只需要开发者提供一个 RoleHierarchy 接口的实例即可，例如下面这样： 1234567@BeanRoleHierarchy roleHierarchy() { RoleHierarchyImpl roleHierarchy = new RoleHierarchyImpl(); String hierarchy = \"ROLE_dba &gt; ROLE_admin ROLE_admin &gt; ROLE_user\"; roleHierarchy.setHierarchy(hierarchy); return roleHierarchy;} 在这里我们提供了一个 RoleHierarchy 接口的实例，使用字符串来描述了角色之间的继承关系， ROLE_dba 具备 ROLE_admin 的所有权限，而 ROLE_admin 则具备 ROLE_user 的所有权限，继承与继承之间用一个空格隔开。提供了这个 Bean 之后，以后所有具备 ROLE_user 角色才能访问的资源， ROLE_dba 和 ROLE_admin 也都能访问，具备 ROLE_amdin 角色才能访问的资源， ROLE_dba 也能访问。 现在的写法但是上面这种写法仅限于 Spring Boot2.0.8（含）之前的版本，在之后的版本中，这种写法则不被支持，新版的写法是下面这样： 1234567@BeanRoleHierarchy roleHierarchy() { RoleHierarchyImpl roleHierarchy = new RoleHierarchyImpl(); String hierarchy = \"ROLE_dba &gt; ROLE_admin \\n ROLE_admin &gt; ROLE_user\"; roleHierarchy.setHierarchy(hierarchy); return roleHierarchy;} 变化主要就是分隔符，将原来用空格隔开的地方，现在用换行符了。这里表达式的含义依然和上面一样，不再赘述。 上面两种不同写法都是配置角色的继承关系，配置完成后，接下来指定角色和资源的对应关系即可，如下： 123456789101112131415@Overrideprotected void configure(HttpSecurity http) throws Exception { http.authorizeRequests().antMatchers(\"/admin/**\") .hasRole(\"admin\") .antMatchers(\"/db/**\") .hasRole(\"dba\") .antMatchers(\"/user/**\") .hasRole(\"user\") .and() .formLogin() .loginProcessingUrl(\"/doLogin\") .permitAll() .and() .csrf().disable();} 这个表示 /db/** 格式的路径需要具备 dba 角色才能访问， /admin/** 格式的路径则需要具备 admin 角色才能访问， /user/** 格式的路径，则需要具备 user 角色才能访问，此时提供相关接口，会发现，dba 除了访问 /db/** ，也能访问 /admin/** 和 /user/** ，admin 角色除了访问 /admin/** ，也能访问 /user/** ，user 角色则只能访问 /user/** 。 源码分析这样两种不同的写法，其实也对应了两种不同的解析策略，角色继承关系的解析在 RoleHierarchyImpl 类的 buildRolesReachableInOneStepMap 方法中，Spring Boot2.0.8（含）之前该方法的源码如下： 12345678910111213141516171819202122232425private void buildRolesReachableInOneStepMap() { Pattern pattern = Pattern.compile(\"(\\\\s*([^\\\\s&gt;]+)\\\\s*&gt;\\\\s*([^\\\\s&gt;]+))\"); Matcher roleHierarchyMatcher = pattern .matcher(this.roleHierarchyStringRepresentation); this.rolesReachableInOneStepMap = new HashMap&lt;GrantedAuthority, Set&lt;GrantedAuthority&gt;&gt;(); while (roleHierarchyMatcher.find()) { GrantedAuthority higherRole = new SimpleGrantedAuthority( roleHierarchyMatcher.group(2)); GrantedAuthority lowerRole = new SimpleGrantedAuthority( roleHierarchyMatcher.group(3)); Set&lt;GrantedAuthority&gt; rolesReachableInOneStepSet; if (!this.rolesReachableInOneStepMap.containsKey(higherRole)) { rolesReachableInOneStepSet = new HashSet&lt;&gt;(); this.rolesReachableInOneStepMap.put(higherRole, rolesReachableInOneStepSet); } else { rolesReachableInOneStepSet = this.rolesReachableInOneStepMap .get(higherRole); } addReachableRoles(rolesReachableInOneStepSet, lowerRole); logger.debug(\"buildRolesReachableInOneStepMap() - From role \" + higherRole + \" one can reach role \" + lowerRole + \" in one step.\"); }} 从这段源码中我们可以看到，角色的继承关系是通过正则表达式进行解析，通过空格进行切分，然后构建相应的 map 出来。 Spring Boot2.1.0（含）之后该方法的源码如下： 12345678910111213141516171819202122232425262728private void buildRolesReachableInOneStepMap() { this.rolesReachableInOneStepMap = new HashMap&lt;GrantedAuthority, Set&lt;GrantedAuthority&gt;&gt;(); try (BufferedReader bufferedReader = new BufferedReader( new StringReader(this.roleHierarchyStringRepresentation))) { for (String readLine; (readLine = bufferedReader.readLine()) != null;) { String[] roles = readLine.split(\" &gt; \"); for (int i = 1; i &lt; roles.length; i++) { GrantedAuthority higherRole = new SimpleGrantedAuthority( roles[i - 1].replaceAll(\"^\\\\s+|\\\\s+$\", \"\")); GrantedAuthority lowerRole = new SimpleGrantedAuthority(roles[i].replaceAll(\"^\\\\s+|\\\\s+$ Set&lt;GrantedAuthority&gt; rolesReachableInOneStepSet; if (!this.rolesReachableInOneStepMap.containsKey(higherRole)) { rolesReachableInOneStepSet = new HashSet&lt;GrantedAuthority&gt;(); this.rolesReachableInOneStepMap.put(higherRole, rolesReachableInOneStepSet); } else { rolesReachableInOneStepSet = this.rolesReachableInOneStepMap.get(higherRole); } addReachableRoles(rolesReachableInOneStepSet, lowerRole); if (logger.isDebugEnabled()) { logger.debug(\"buildRolesReachableInOneStepMap() - From role \" + higherRole + \" one can reach role \" + lowerRole + \" in one step.\"); } } } } catch (IOException e) { throw new IllegalStateException(e); }} 从这里我们可以看到，这里并没有一上来就是用正则表达式，而是先将角色继承字符串转为一个 BufferedReader ，然后一行一行的读出来，再进行解析，最后再构建相应的 map。从这里我们可以看出为什么前后版本对此有不同的写法。 那么小伙伴在开发过程中，还是需要留意这一个差异。好了，角色继承我们就先说到这里，本文并没有讲 Spring Security 一些基本用法，想了解 Spring Security 更多用法，敬请留意后续文章。","link":"/2019/0613/springsecurity-role.html"},{"title":"Spring Security 登录添加验证码","text":"登录添加验证码是一个非常常见的需求，网上也有非常成熟的解决方案，其实，要是自己自定义登录实现这个并不难，但是如果需要在 Spring Security 框架中实现这个功能，还得稍费一点功夫，本文就和小伙伴来分享下在 Spring Security 框架中如何添加验证码。 关于 Spring Security 基本配置，这里就不再多说，小伙伴有不懂的可以参考我的书《SpringBoot+Vue全栈开发实战》，本文主要来看如何加入验证码功能。 准备验证码要有验证码，首先得先准备好验证码，本文采用 Java 自画的验证码，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 生成验证码的工具类 */public class VerifyCode { private int width = 100;// 生成验证码图片的宽度 private int height = 50;// 生成验证码图片的高度 private String[] fontNames = { \"宋体\", \"楷体\", \"隶书\", \"微软雅黑\" }; private Color bgColor = new Color(255, 255, 255);// 定义验证码图片的背景颜色为白色 private Random random = new Random(); private String codes = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"; private String text;// 记录随机字符串 /** * 获取一个随意颜色 * * @return */ private Color randomColor() { int red = random.nextInt(150); int green = random.nextInt(150); int blue = random.nextInt(150); return new Color(red, green, blue); } /** * 获取一个随机字体 * * @return */ private Font randomFont() { String name = fontNames[random.nextInt(fontNames.length)]; int style = random.nextInt(4); int size = random.nextInt(5) + 24; return new Font(name, style, size); } /** * 获取一个随机字符 * * @return */ private char randomChar() { return codes.charAt(random.nextInt(codes.length())); } /** * 创建一个空白的BufferedImage对象 * * @return */ private BufferedImage createImage() { BufferedImage image = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g2 = (Graphics2D) image.getGraphics(); g2.setColor(bgColor);// 设置验证码图片的背景颜色 g2.fillRect(0, 0, width, height); return image; } public BufferedImage getImage() { BufferedImage image = createImage(); Graphics2D g2 = (Graphics2D) image.getGraphics(); StringBuffer sb = new StringBuffer(); for (int i = 0; i &lt; 4; i++) { String s = randomChar() + \"\"; sb.append(s); g2.setColor(randomColor()); g2.setFont(randomFont()); float x = i * width * 1.0f / 4; g2.drawString(s, x, height - 15); } this.text = sb.toString(); drawLine(image); return image; } /** * 绘制干扰线 * * @param image */ private void drawLine(BufferedImage image) { Graphics2D g2 = (Graphics2D) image.getGraphics(); int num = 5; for (int i = 0; i &lt; num; i++) { int x1 = random.nextInt(width); int y1 = random.nextInt(height); int x2 = random.nextInt(width); int y2 = random.nextInt(height); g2.setColor(randomColor()); g2.setStroke(new BasicStroke(1.5f)); g2.drawLine(x1, y1, x2, y2); } } public String getText() { return text; } public static void output(BufferedImage image, OutputStream out) throws IOException { ImageIO.write(image, \"JPEG\", out); }} 这个工具类很常见，网上也有很多，就是画一个简单的验证码，通过流将验证码写到前端页面，提供验证码的 Controller 如下： 123456789101112@RestControllerpublic class VerifyCodeController { @GetMapping(\"/vercode\") public void code(HttpServletRequest req, HttpServletResponse resp) throws IOException { VerifyCode vc = new VerifyCode(); BufferedImage image = vc.getImage(); String text = vc.getText(); HttpSession session = req.getSession(); session.setAttribute(\"index_code\", text); VerifyCode.output(image, resp.getOutputStream()); }} 这里创建了一个 VerifyCode 对象，将生成的验证码字符保存到 session 中，然后通过流将图片写到前端，img标签如下： 1&lt;img src=\"/vercode\" alt=\"\"&gt; 展示效果如下： 自定义过滤器在登陆页展示验证码这个就不需要我多说了，接下来我们来看看如何自定义验证码处理器： 12345678910111213141516171819202122@Componentpublic class VerifyCodeFilter extends GenericFilterBean { private String defaultFilterProcessUrl = \"/doLogin\"; @Override public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; if (\"POST\".equalsIgnoreCase(request.getMethod()) &amp;&amp; defaultFilterProcessUrl.equals(request.getServletPath())) { // 验证码验证 String requestCaptcha = request.getParameter(\"code\"); String genCaptcha = (String) request.getSession().getAttribute(\"index_code\"); if (StringUtils.isEmpty(requestCaptcha)) throw new AuthenticationServiceException(\"验证码不能为空!\"); if (!genCaptcha.toLowerCase().equals(requestCaptcha.toLowerCase())) { throw new AuthenticationServiceException(\"验证码错误!\"); } } chain.doFilter(request, response); }} 自定义过滤器继承自 GenericFilterBean ，并实现其中的 doFilter 方法，在 doFilter 方法中，当请求方法是 POST ，并且请求地址是 /doLogin 时，获取参数中的 code 字段值，该字段保存了用户从前端页面传来的验证码，然后获取 session 中保存的验证码，如果用户没有传来验证码，则抛出验证码不能为空异常，如果用户传入了验证码，则判断验证码是否正确，如果不正确则抛出异常，否则执行 chain.doFilter(request, response); 使请求继续向下走。 配置最后在 Spring Security 的配置中，配置过滤器，如下： 12345678910111213141516171819@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Autowired VerifyCodeFilter verifyCodeFilter; ... ... @Override protected void configure(HttpSecurity http) throws Exception { http.addFilterBefore(verifyCodeFilter, UsernamePasswordAuthenticationFilter.class); http.authorizeRequests() .antMatchers(\"/admin/**\").hasRole(\"admin\") ... ... .permitAll() .and() .csrf().disable(); }} 这里只贴出了部分核心代码，即 http.addFilterBefore(verifyCodeFilter, UsernamePasswordAuthenticationFilter.class); ，如此之后，整个配置就算完成了。接下来在登录中，就需要传入验证码了，如果不传或者传错，都会抛出异常，例如不传的话，抛出如下异常： 好了，本文就先说到这里，有问题欢迎留言讨论。","link":"/2019/0613/springsecurity-verifycode.html"},{"title":"What？Tomcat 竟然也算中间件？","text":"关于 MyCat 的铺垫文章已经写了两篇了： MySQL 只能做小项目？松哥要说几句公道话！ 北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！ 今天是最后一次铺垫，后面就可以迎接大 Boss 了！ 本来今天就该讲 MyCat 了，但是我发现还有一个概念值得和大家聊一下，那就是 Java 中间件！ 因为 MyCat 是一个分布式数据库中间件，要理解 MyCat ，那你就得先知道到底什么是中间件！ 松哥去年在一次外训中专门讲过中间件，本来想直接和大家分享一下讲稿，但是没找到，所以又动手敲了下。 中间件简介说起中间件，很多人首先想到的就是消息中间件，那么除了消息中间件呢？其实我们日常开发中，接触到的中间件太多了，我们来看维基百科上的一段介绍： 中间件（英语：Middleware），又译中间件、中介层，是提供系统软件和应用软件之间连接的软件，以便于软件各部件之间的沟通。在现代信息技术应用框架如 Web 服务、面向服务的体系结构等项目中应用比较广泛。如数据库、Apache 的 Tomcat ，IBM 公司的 WebSphere ,BEA 公司的 WebLogic 应用服务器，东方通公司的 Tong 系列中间件，以及 Kingdee 公司的等都属于中间件。 看到这个，你可能会大吃一惊，原来我们不知不觉不知不觉中已经用过这么多中间件了！甚至连 Tomcat 也是一个中间件！ 中间件，顾名思义，就是连接在两个软件之间的东西，是软件之间的一个粘合剂，一个胶水一样的东西。它位于操作系统和我们的应用程序之间，可以让开发者方便地处理通信、输入和输出，使开发者能够专注于自己的业务逻辑开发。 这么一说，好像 Tomcat 确实还有点像中间件！位于我们的操作系统和应用程序之间！ 中间件分类中间件有很多，早在 1998 年 IDC 公司就将中间件分成了 6 大类，国内 2005 年之前出版的中间件相关的书上，很多都是按照这 6 大类来分的，分别是： 终端仿真/屏幕转换 数据访问中间件（UDA） 远程过程调用中间件（RPC） 消息中间件（MOM） 交易中间件（TPM） 对象中间件 这里边除了消息中间件和交易中间件大家可能听说过之外，其他的中间件估计都很少听说，这是因为时代在变化，有的中间件慢慢被淘汰了（例如 终端仿真/屏幕转换 中间件），有的则慢慢合并到其他框架中去了（例如 远程过程调用中间件）。 数据库中间件那么什么是数据库中间件呢？ 前面文章我们提到，如果数据量比较大的话，我们需要对数据进行分库分表，分完之后，原本存在一个数据库中的数据，现在就存在多个数据库中了，那么我们的项目结构可能就是下面这个样子了： 我们要在 Java 代码中配置复杂的多数据源，配置读写分离，数据查询的时候还要进行数据的预处理，例如从多个 DB 上加载到的数据要先进行排序、过滤等等操作，这样我们的 Java 代码就参杂了很多业务无关的方法，而且这些参杂进来的代码，大多数都还是重复的。 为了使开发人员，将更多精力放到业务上，我们引入数据库中间件，像下面这样： 这张图非常形象的说明了什么是中间件！一个介于两个应用程序之间的东西。引入 MyCat 中间件之后，我们的应用程序将只需要连接 MyCat 就行了，再由 MyCat 去操作各种不同的 DB，各个分布式数据库的排序、结果集合并、数据过滤等操作都在 MyCat 中完成，这样我们的 Java 应用又可以专注于业务的开发了，那些繁琐的重复的操作，又交给 MyCat 去完成。 如果没有数据库中间件，那么我们的 Java 应用程序将直接面对分片集群，数据源切换、事务处理、数据聚合等等众多问题，这样原本该是专注于业务的 Java 应用程序，将会花大量的工作来处理分片后的问题，而且大部分的代码又都是重复的！ 有了数据库中间件，应用只需要集中与业务处理，大量的通用的数据聚合，事务，数据源切换都由中间件来处理，中间件的性能与处理能力将直接决定应用的读写性能，所以在项目中选择一款好的数据库中间件至关重要。 结语好了，本文就简单介绍下中间件的基本概念，下文就可以正式开始聊分布式数据库中间件了！ 如果小伙伴觉得松哥自己写的技术文章还过得去的话，欢迎积极转发，松哥每隔 15 天会根据微信后台数据，选一个分享转发最多的一位小伙伴，送一本松哥自己的签名书或者 Java 领域的经典书籍，欢迎大家积极参与，下一次开奖时间在下周。 参考资料： MyCat 官方文档 曾宪杰.大型网站系统与Java中间件实践[M].北京：电子工业出版社，2014.","link":"/2019/0627/middleware.html"},{"title":"MongoDB 固定集合","text":"一般情况下我们创建的集合是没有大小的，可以一直往里边添加文档，这种集合可以动态增长，MongoDB 中还有一种集合叫做固定集合，这种集合的大小是固定的，我可以在创建的时候设置该集合中文档的数目，假设为 100 条，当集合中的文档数目达到 100 条时，如果再向集合中插入文档，则只会保留最新的 100 个文档，之前的文档则会被删除。 一般像日志信息我们就可以使用固定集合，其他一些需要定期删除的数据也可以使用固定集合，本文我们就来看看这个固定集合的使用。 创建固定集合的创建方式也比较简单，如下： 1db.createCollection(&quot;my_collect&quot;,{capped:true,size:10000,max:100}) capped:true 参数表示该集合为一个固定大小集合，size 表示集合的大小，单位为 kb，max 则表示集合中文档的最大数量。我们这里相当于给了固定集合两个限制条件，只要有任意一个限制条件满足，集合都会开始将更古老的数据删除。固定集合一旦创建成功就不能再修改，想修改只能删除重来。此时我们可以尝试向集合中添加 120 条简单的数据，然后我们会发现最早的 20 条数据消失了。 除了直接创建一个固定集合外，我们也可以通过 convertToCapped 操作将一个普通集合转为一个固定集合，如下： 1db.runCommand({convertToCapped:&quot;sang_collect&quot;,size:10}) 自然排序问题自然排序就是按照文档在磁盘中的顺序来进行排列，在普通的集合中自然排序并没有多大的意义，因为文档的位置总是在变化，而固定集合中的文档是按照文档被插入的顺序保存的，自然顺序也就是文档的插入顺序，因此我们可以利用自然排序对文档从旧到新排序，如下： 1db.sang_collect.find().sort({$natural:1}) 也可以从新到旧排序： 1db.sang_collect.find().sort({$natural:-1}) 固定集合中的其他操作和普通集合基本一致，这里就不再赘述。 好了，MongoDB 中的固定集合我们就说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0911/mongodb-collections.html"},{"title":"MongoDB 管道操作符(一)","text":"熟悉 Linux 操作系统的小伙伴们应该知道 Linux 中有管道的说法，可以用来方便的处理数据。 MongoDB2.2 版本也引入了新的数据聚合框架，一个文档可以经过多个节点组成的管道，每个节点都有自己特殊的功能，比如文档分组、文档过滤等，每一个节点都会接受一连串的文档，对这些文档做一些类型转换，然后将转换后的文档传递给下一个节点，最后一个节点则会将结果返回给客户端。本文我们就先来看几个基本的管道操作符。 $match$match 可以用来对文档进行筛选，筛选完成之后我们就可以在筛选获得到的文档子集上来做数据聚合操作了，我们之前介绍的查询的操作符在 $match 中都可以使用，比如获取集合中所有 author 为”杜甫”的文档，如下： 1db.sang_collect.aggregate({$match:{author:&quot;杜甫&quot;}}) 我们在实际使用时最好将 $match 放在管道的前面，这样可以减少后面管道的工作量，同时，我们在投射和分组之前执行 $match 还可以用索引。 $project基本用法$project 可以用来 提取想要的字段 ，如下： 1db.sang_collect.aggregate({$project:{title:1,_id:0}}) 1 表示要该字段，0 表示不要该字段，也可以对返回的字段进行 重命名 ，比如将 title 改为 articleTitle，如下： 1db.sang_collect.aggregate({$project:{&quot;articleTitle&quot;:&quot;$title&quot;}}) 不过这里有一个问题需要注意，如果原字段上有索引，重命名之后的字段上就没有索引了，因此最好在重命名之前使用索引。 数学表达式数学表达式可以用来对一组数值进行加减乘除取模，比如我的数据结构如下： 123456789101112{ &quot;_id&quot; : ObjectId(&quot;59f841f5b998d8acc7d08863&quot;), &quot;orderAddressL&quot; : &quot;ShenZhen&quot;, &quot;prodMoney&quot; : 45.0, &quot;freight&quot; : 13.0, &quot;discounts&quot; : 3.0, &quot;orderDate&quot; : ISODate(&quot;2017-10-31T09:27:17.342Z&quot;), &quot;prods&quot; : [ &quot;可乐&quot;, &quot;奶茶&quot; ]} 订单的总费用为商品费用加上运费，查询如下： 1db.sang_collect.aggregate({$project:{totalMoney:{$add:[&quot;$prodMoney&quot;,&quot;$freight&quot;]}}}) 实际付款的费用是总费用减去折扣，如下： 1db.sang_collect.aggregate({$project:{totalPay:{$subtract:[{$add:[&quot;$prodMoney&quot;,&quot;$freight&quot;]},&quot;$discounts&quot;]}}}) 再来三个无厘头运算，比如计算 prodMoney 和 freight 和 discounts 的乘积： 1db.sang_collect.aggregate({$project:{test1:{$multiply:[&quot;$prodMoney&quot;,&quot;$freight&quot;,&quot;$discounts&quot;]}}}) 再比如求 $prodMoney 和 $freight 的商，如下： 1db.sang_collect.aggregate({$project:{test1:{$divide:[&quot;$prodMoney&quot;,&quot;$freight&quot;]}}}) 再比如用 $freight 对 $prodMoney 取模，如下： 1db.sang_collect.aggregate({$project:{test1:{$mod:[&quot;$prodMoney&quot;,&quot;$freight&quot;]}}}) 加法和乘法都可以接收多个参数，其余的都接收两个参数。 日期表达式日期表达式可以从一个日期类型中提取出年、月、日、星期、时、分、秒等信息，如下： 1db.sang_collect.aggregate({$project:{&quot;年份&quot;:{$year:&quot;$orderDate&quot;},&quot;月份&quot;:{$month:&quot;$orderDate&quot;},&quot;一年中第几周&quot;:{$week:&quot;$orderDate&quot;},&quot;日期&quot;:{$dayOfMonth:&quot;$orderDate&quot;},&quot;星期&quot;:{$dayOfWeek:&quot;$orderDate&quot;},&quot;一年中第几天&quot;:{$dayOfYear:&quot;$orderDate&quot;},&quot;时&quot;:{$hour:&quot;$orderDate&quot;},&quot;分&quot;:{$minute:&quot;$orderDate&quot;},&quot;秒&quot;:{$second:&quot;$orderDate&quot;},&quot;毫秒&quot;:{$millisecond:&quot;$orderDate&quot;},&quot;自定义格式化时间&quot;:{$dateToString:{format:&quot;%Y年%m月%d %H:%M:%S&quot;,date:&quot;$orderDate&quot;}}}}) 执行结果如下： 1234567891011121314{ \"_id\" : ObjectId(\"59f841f5b998d8acc7d08861\"), \"年份\" : 2017, \"月份\" : 10, \"一年中第几周\" : 44, \"日期\" : 31, \"星期\" : 3, \"一年中第几天\" : 304, \"时\" : 9, \"分\" : 27, \"秒\" : 17, \"毫秒\" : 342, \"自定义格式化时间\" : \"2017年10月31 09:27:17\"} $dayOfWeek 返回的是星期，1 表示星期天，7 表示星期六，$week 表示本周是本年的第几周，从 0 开始计。$dateToString 是 MongoDB3.0+ 中的功能。格式化的字符还有以下几种： 字符 含义 取值范围 %Y Year (4 digits, zero padded) 0000-9999 %m Month (2 digits, zero padded) 01-12 %d Day of Month (2 digits, zero padded) 01-31 %H Hour (2 digits, zero padded, 24-hour clock) 00-23 %M Minute (2 digits, zero padded) 00-59 %S Second (2 digits, zero padded) 00-60 %L Millisecond (3 digits, zero padded) 000-999 %j Day of year (3 digits, zero padded) 001-366 %w Day of week (1-Sunday, 7-Saturday) 1-7 %U Week of year (2 digits, zero padded) 00-53 字符串表达式字符串表达式中有字符串的截取、拼接、转大写、转小写等操作，比如我截取 orderAddressL 前两个字符返回，如下： 1db.sang_collect.aggregate({$project:{addr:{$substr:[&quot;$orderAddressL&quot;,0,2]}}}) 再比如我将 orderAddressL 和 orderDate 拼接后返回： 1db.sang_collect.aggregate({$project:{addr:{$concat:[&quot;$orderAddressL&quot;,{$dateToString:{format:&quot;--%Y年%m月%d&quot;,date:&quot;$orderDate&quot;}}]}}}) 结果如下： 1234{ \"_id\" : ObjectId(\"59f841f5b998d8acc7d08861\"), \"addr\" : \"NanJing--2017年10月31\"} 再比如我将 orderAddressL 全部转为小写返回： 1db.sang_collect.aggregate({$project:{addr:{$toLower:&quot;$orderAddressL&quot;}}}) 再比如我将 orderAddressL 全部转为大写返回： 1db.sang_collect.aggregate({$project:{addr:{$toUpper:&quot;$orderAddressL&quot;}}}) 逻辑表达式想要比较两个数字的大小，可以使用 $cmp 操作符，如下： 1db.sang_collect.aggregate({$project:{test:{$cmp:[&quot;$freight&quot;,&quot;$discounts&quot;]}}}) 如果第一个参数大于第二个参数返回正数，第一个参数小于第二个则返回负数，也可以利用 $strcasecmp 来比较字符串（中文无效）： 1db.sang_collect.aggregate({$project:{test:{$strcasecmp:[{$dateToString:{format:&quot;..%Y年%m月%d&quot;,date:&quot;$orderDate&quot;}},&quot;$orderAddressL&quot;]}}}) 至于我们之前介绍的 $eq/$ne/$gt/$gte/$lt/$lte 等操作符在这里一样是适用的。另外还有 and、$or、$not 等表达式可用，以 $and 为例，如下： 1db.sang_collect.aggregate({$project:{test:{$and:[{&quot;$eq&quot;:[&quot;$freight&quot;,&quot;$prodMoney&quot;]},{&quot;$eq&quot;:[&quot;$freight&quot;,&quot;$discounts&quot;]}]}}}) $and 中的每个参数都为 true 时返回 true，$or 则表示参数中有一个为 true 就返回 true，$not 则会对它的参数的值取反，如下： 1db.sang_collect.aggregate({$project:{test:{$not:{&quot;$eq&quot;:[&quot;$freight&quot;,&quot;$prodMoney&quot;]}}}}) 另外还有两个流程控制语句，如下： 1db.sang_collect.aggregate({$project:{test:{$cond:[false,&quot;trueExpr&quot;,&quot;falseExpr&quot;]}}}) $cond 第一个参数如果为 true，则返回 trueExpr，否则返回 falseExpr. 1db.sang_collect.aggregate({$project:{test:{$ifNull:[null,&quot;replacementExpr&quot;]}}}) $ifNull 第一个参数如果为 null，则返回 replacementExpr，否则就返回第一个参数。 好了，MongoDB 中的管道操作符我们就先说到这里，下篇文章继续，小伙伴们有问题欢迎留言讨论。参考资料： 《MongoDB权威指南第2版》 mongodb聚合利用日期分组","link":"/2019/0912/mongodb-pipelines.html"},{"title":"MongoDB 管道操作符(二)","text":"上篇文章中我们已经学习了 MongoDB 中几个基本的管道操作符，本文我们再来看看其他的管道操作符。 $group基本操作$group 可以用来对文档进行分组，比如我想将订单按照城市进行分组，并统计出每个城市的订单数量： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,count:{$sum:1}}}) 我们将要分组的字段传递给 $group 函数的 _id 字段，然后每当查到一个，就给 count 加 1，这样就可以统计出每个城市的订单数量。 算术操作符通过算术操作符我们可以对分组后的文档进行求和或者求平均数。比如我想计算每个城市订单运费总和，如下： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,totalFreight:{$sum:&quot;$freight&quot;}}}) 先按地址分组，再求和。这里贴出部分查询结果，如下： 12345678{ \"_id\" : \"HaiKou\", \"totalFreight\" : 20.0}{ \"_id\" : \"HangZhou\", \"totalFreight\" : 10.0} 也可以计算每个城市运费的平均数，如下： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,avgFreight:{$avg:&quot;$freight&quot;}}}) 先按地址分组，然后再计算平均数。 极值操作符极值操作符用来获取分组后数据集的边缘值，比如获取每个城市最贵的运费，如下： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,maxFreight:{$max:&quot;$freight&quot;}}}) 查询每个城市最便宜的运费： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,minFreight:{$min:&quot;$freight&quot;}}}) 按城市分组之后，获取该城市第一个运费单： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,firstFreight:{$first:&quot;$freight&quot;}}}) 获取分组后的最后一个运费单： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,lastFreight:{$last:&quot;$freight&quot;}}}) 数据操作符$addToSet 可以将分组后的某一个字段放到一个数组中，但是重复的元素将只出现一次，而且元素加入到数组中的顺序是无规律的，比如将分组后的每个城市的运费放到一个数组中，如下： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,freights:{$addToSet:&quot;$freight&quot;}}}) 重复的 freight 将不会被添加进来。 $push 则对重复的数据不做限制，都可以添加进来，如下： 1db.sang_collect.aggregate({$group:{_id:&quot;$orderAddressL&quot;,freights:{$push:&quot;$freight&quot;}}}) $unwind$unwind 用来实现对文档的拆分,可以将文档中的值拆分为单独的文档，比如我的数据如下： 1234567891011121314{ \"_id\" : ObjectId(\"59f93c8b8523cfae4cf4ba86\"), \"name\" : \"鲁迅\", \"books\" : [ { \"name\" : \"呐喊\", \"publisher\" : \"花城出版社\" }, { \"name\" : \"彷徨\", \"publisher\" : \"南海出版出\" } ]} 使用 $unwind 命令将其拆分为独立文档，如下： 1db.sang_books.aggregate({$unwind:&quot;$books&quot;}) 拆分结果如下： 12345678910111213141516{ \"_id\" : ObjectId(\"59f93c8b8523cfae4cf4ba86\"), \"name\" : \"鲁迅\", \"books\" : { \"name\" : \"呐喊\", \"publisher\" : \"花城出版社\" }}{ \"_id\" : ObjectId(\"59f93c8b8523cfae4cf4ba86\"), \"name\" : \"鲁迅\", \"books\" : { \"name\" : \"彷徨\", \"publisher\" : \"南海出版出\" }} 其他操作符$sort 操作可以对文档进行排序，如下： 1db.sang_collect.aggregate({$sort:{orderAddressL:1}}) 用法和我们之前介绍普通搜索中的一致，可以按照存在的字段排序，也可以按照重命名的字段排序，如下： 1db.sang_collect.aggregate({$project:{oa:&quot;$orderAddressL&quot;}},{$sort:{oa:-1}}) 1 表示升序、-1 表示降序。 $limit 返回结果中的前 n 个文档，如下表示返回结果中的前三个文档： 1db.sang_collect.aggregate({$project:{oa:&quot;$orderAddressL&quot;}},{$limit:3}) $skip 表示跳过前 n 个文档，比如跳过前 5 个文档，如下： 1db.sang_collect.aggregate({$project:{oa:&quot;$orderAddressL&quot;}},{$skip:5}) $skip 的效率低，要慎用。 总结在管道开始执行的阶段尽可能过滤掉足够多的数据，这样做有两个好处： 只有从集合中直接查询时才会使用索引，尽早执行过滤可以让索引发挥作用； 该过滤的数据过滤掉之后，也可以降低后面管道的执行压力。另外，MongoDB 不允许一个聚合操作占用过多的内存，如果有一个聚合操作占用了超过 20% 的内存，则会直接报错。 好了，MongoDB 中的管道操作符我们就先说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0913/mongodb-pipelines.html"},{"title":"MongoDB 副本集搭建","text":"我们之前的案例都是在单个节点上实现的，在生产环境中这种做法是有风险的，如果服务宕机、崩溃或者硬盘坏了都会对公司业务造成损失，因此我们需要数据备份。 在 MongoDB 中我们可以通过副本集来实现这一需求，MongoDB 副本集 (Replica Set) 是有自动故障恢复功能的主从集群，有一个 Primary 节点和一个或多个 Secondary 节点组成，如果 Primary 崩溃了，会自动从 Secondary 中选择一个将其升级为新的主服务器，本文我们先来看看副本集环境的搭建。 单台服务器模拟我们在实际的生产环境中肯定是多台服务器部署，但是在自己学习过程中，我们可以在一台服务器上来模拟这个环境，这样可以简化我们的操作，让小伙伴们快速上手。下一小节我会和大家分享如何在真实的生产环境中创建副本集。 好了，开始吧。 首先我们在 Linux 根目录下创建 /data/db 目录作为我们的数据保存目录，然后执行如下命令启动一个 mongo shell： 1mongo --nodb –nodb 表示启动时不连接任何数据库，然后通过如下命令创建一个副本集： 1replicaSet=new ReplSetTest({nodes:3}) 在创建的日志中，我们可以看到三个实例的端口号，我这里分别是 20000、20001、20002，此时我们的副本集创建好了，但是并未启动，接下来执行如下命令启动三个 mongodb 实例： 1replicaSet.startSet() 再执行如下命令配置复制功能： 1replicaSet.initiate() 这样环境基本就配好了，此时当前的 shell 不要关闭，我们重新打开一个 Linux 命令窗口，执行如下命令： 1mongo 192.168.248.128:20000/sang_1 表示连接端口为 20000 的那个实例中的 sang_1 数据库，连接成功后，我们可以执行如下命令查看当前实例的身份，如下： 1db.isMaster() 返回的数据很多，其中有一条是 &quot;ismaster&quot; : true，表示这是一个主节点，此时我们再分别打开两个 Linux 窗口，分别执行如下两条命令，进入另外两个节点： 12mongo 192.168.248.128:20001/sang_1mongo 192.168.248.128:20002/sang_1 连接成功之后，依然可以通过 db.isMaster() 命令来查看备份节点的身份，我们发现此时 &quot;ismaster&quot; : false，表示这是一个备份节点，此时我们可以先做个简单的测试了，此时我在主节点(端口为 20000)那个节点上写一个文档，写完之后，我们看看其他副本集成员上是否有我刚才的写的文档的副本，执行命令顺序如下： 主节点写入数据： 1db.collect1.insert({x:&quot;hahaha&quot;}) 任意一个副本节点，先执行如下命令表示可以从备份节点读取数据： 1db.setSlaveOk() 然后再在备份节点中执行如下命令读取数据： 1db.collect1.find() 此时，我们发现数据已经备份成功了。 如果此时我们尝试向备份节点中直接写入文档，会发现写入失败，这里需要注意备份节点中的数据都是备份来的，不可以直接写入，想写入，除非等它的身份转为主节点才可以。 此时，我们尝试通过如下命令关闭主节点： 12use admindb.shutdownServer() 然后查看两个备份节点的 db.isMaster(),发现有一个备份节点自动上位成为了主节点。 最后如果想关闭副本集，可以回到第一个shell命令行中，输入如下命令： 1replicaSet.stopSet() 多台服务器模拟OK，以上操作是我们单台服务器模拟搭建副本集，方便我们做实验，在生产环境中，我们可能有多个服务器，多台服务器又要如何搭建副本集呢？各位看官继续向下看。 首先准备好三台装好了MongoDB的服务器，地址分别如下： 123192.168.248.128192.168.248.135192.168.248.136 修改每台服务器的配置文件 mongodb.conf，添加 replSet=rs，表示副本集的名称，修改后的配置文件内容如下： 12345dbpath=/opt/mongodb/dblogpath=/opt/mongodb/logs/mongodb.logport=27017fork=truereplSet=rs 修改完成之后，分别启动三台服务器上的 MongoDB，启动成功之后，连接上任意一台的 shell，连接成功之后，先定义配置文件，如下： 1config={_id:&quot;rs&quot;,members:[{_id:0,host:&quot;192.168.248.128:27017&quot;},{_id:1,host:&quot;192.168.248.135:27017&quot;},{_id:2,host:&quot;192.168.248.136:27017&quot;}]} id 后面跟着的是副本集的名称，也就是我们在 mongodb.conf 中定义的名称，后面三个是副本集的成员，定义好之后，再执行如下命令初始化副本集： 1rs.initiate(config) 初始化成功之后，我们就可以通过 rs.status() 来查看副本集的状态，也可以看到每个服务器的角色，部分日志内容如下： 123456789101112131415161718192021222324252627{\"members\" : [{ \"_id\" : 0, \"name\" : \"192.168.248.128:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\",},{ \"_id\" : 1, \"name\" : \"192.168.248.135:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"syncingTo\" : \"192.168.248.128:27017\"},{ \"_id\" : 2, \"name\" : \"192.168.248.136:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"syncingTo\" : \"192.168.248.128:27017\",}]} 我们可以看到每台服务器的角色，有 primary，也有 secondary，secondary 上还注明了从哪个服务器上同步数据。所有这些工作做好之后，我们就可以按照上文介绍的方式来测一下这里的副本集了，测试工作我就不再重复介绍了。 好了，MongoDB 中副本集的搭建我们就先说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0915/mongodb-replicaset-install.html"},{"title":"MongoDB 副本集配置","text":"上篇文章我们搭建了 MongoDB 副本集的环境，验证了数据已经可以成功的复制，本文我们就来看看 MongoDB 副本集的其他操作。 环境准备三台服务器，地址分别是： 123192.168.248.128192.168.248.135192.168.248.136 按照上文介绍的步骤搭建副本集环境，这里不再赘述。 副本集成员添加删除在副本集环境搭建好之后，我们可以利用如下命令删除一个副本集成员： 1rs.remove(&apos;192.168.248.128:27017&apos;) 上面的命令执行完成后，我们可以通过 rs.status() 命令来查看是否删除成功，也可以通过如下命令来为副本集添加一个成员： 1rs.add(&apos;192.168.248.128:27017&apos;) 当然，副本集也是可以更新的，使用 reconfig 命令即可，如下： 首先定义 config，如下： 1config={_id:&quot;rs&quot;,members:[{_id:3,host:&quot;192.168.248.128&quot;},{_id:1,host:&quot;192.168.248.135&quot;}]} 然后执行更新操作： 1rs.reconfig(config) 我们也可以利用 config=rs.config() 获取原始的 config 文件，然后进行修改，修改之后再执行 rs.reconfig(config)，如下： 123config=rs.config()config.members[0].host=&quot;192.168.248.136&quot;rs.reconfig(config) 选举仲裁者在上文中给小伙伴们演示了主节点挂掉后的情况，和其他的(如 Redis )数据库主从复制不同，MongoDB 中主节点挂掉之后会自动从备份节点中选出一个新的主节点出来，这是一个选举的过程，投票选举，但是如果备份节点数为偶数的话，可能会出现两台服务器票数相等的情况，为了避免这种问题的出现，我们一般有两种解决方案： 数据节点为奇数个，这样就会避免上面描述的问题出现。 使用选举仲裁者，这是一种特殊的成员，仲裁者不保存数据，也不为客户端提供服务，只是在选举投票出现僵持时出来投个票，一个副本集中最多只能有一个仲裁者。 选举仲裁者占用的系统资源很小，因此对部署的服务器性能没多大要求，向副本集中添加仲裁者的方式如下： 1rs.addArb(&apos;192.168.248.128:27017&apos;) 也可以利用我们之前说的 reconfig 来操作： 123config=rs.config()config.members[2]={_id:2,host:&apos;192.168.248.128&apos;,arbiterOnly:true}rs.reconfig(config) 添加完成之后，我们可以通过 rs.status() 命令来查看是否添加成功，如果看到如下内容，表示添加成功： 123456789101112{\"_id\" : 2,\"name\" : \"192.168.248.128:27017\",\"health\" : 1,\"state\" : 7,\"stateStr\" : \"ARBITER\",\"uptime\" : 2,\"lastHeartbeat\" : ISODate(\"2017-11-03T08:56:12.406Z\"),\"lastHeartbeatRecv\" : ISODate(\"2017-11-03T08:56:08.417Z\"),\"pingMs\" : NumberLong(1),\"configVersion\" : 8} 仲裁者的移除和普通节点的移除是一样的，这里不再赘述。 优先级问题优先级用来描述一个备份节点成为主节点的优先性问题，优先级的取值范围为 [0-100]，默认为 1，数字越大优先级越高，越有可能成为主节点，0 表示该节点永远不能成为主节点。 我们可以在添加节点时指定优先级，如下: 1rs.add({_id:0,host:&apos;192.168.248.128:27017&apos;,priority:2}) 也可以为已有的节点设置优先级： 123config=rs.config()config.members[0].priority=99rs.reconfig(config) 好了，MongoDB 中副本集的配置我们就先说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0916/mongodb-replicaset-settings.html"},{"title":"MongoDB副本集其他细节","text":"副本集环境的搭建以及一些基本的操作我们都了解了，本文我们来看看这个数据复制到底是怎么实现的。 数据同步方式MongoDB 中的复制功能主要是使用操作日志 oplog.rs 来实现的，oplog.rs 包含了主节点的每一次写操作，oplog.rs 是主节点中 local 数据库的一个固定集合，我们可以通过如下命令查看到： 12use localshow tables 如下： 备份节点通过查询这个集合就知道要复制哪些数据，同时，每一个备份节点也都维护着自己的 oplog.rs，自己的 oplog.rs 则用来记录每一次从主节点复制数据的操作，如此，每一个备份节点都可以再作为数据源提供给其他成员使用，如果某一个备份节点在使用的过程中挂掉了，那么当它重启之后，会自动从 oplog.rs 的最后一个操作开始同步。 上文我们也已经说过 oplog.rs 是一个固定集合，我们可以通过 db.getCollection('oplog.rs').stats() 这个命令来查看这个固定集合的属性，包括集合大小等，执行部分结果如下： 123456789101112{ \"ns\" : \"local.oplog.rs\", \"size\" : 18170305, \"count\" : 177443, \"avgObjSize\" : 102, \"storageSize\" : 5902336, \"capped\" : true, \"max\" : -1, \"maxSize\" : 1038090240, \"sleepCount\" : 0, \"sleepMS\" : 0,} 既然是固定集合，它里边能够保存的数据大小就是有限的。通常，oplog.rs 使用空间的增长速度与系统处理处理写请求的速率近乎相同，比如主节点每分钟处理了 1KB 的写入请求，那么 oplog.rs 也可能会在一分钟内写入 1KB 条操作日志，但是如果主节点执行了批量删除的命令，比如下面这种： 1db.c1.deleteMany({x:{$type:1}}) 此时每一个受影响的文档都会产生一条 oplog 中的日志，这个时候 oplog.rs 中的日志会快速增加。 成员状态到目前为止我们了解到的成员状态有两种，一个是 PRIMARY ，还有一个是 SECONDDARY ，成员状态的获取需要靠心跳来维护，副本集中的每一个成员每隔两秒就会向其他成员发送一个心跳请求，用来检查成员的状态，成员的状态主要有如下几种： STARTUP副本集中的成员刚刚启动时处于这个状态下，此时，MongoDB 会去加载成员的副本集配置，配置加载成功之后，就进入到 STARTUP2 的状态。 STARTUP2整个初始化同步过程都处于这个状态。 RECOVERING这个状态是由 STARTUP2 状态来的，此时成员运转正常，但是此时还不能处理读取请求。 ARBITER这是仲裁者所处的状态。 DOWN当一个原本运行正常的成员无法访问到时，该成员就处于 DOWN 的状态。 UNKNOWN如果一个成员无法到达其他任何成员，该成员就处于 UNKNOWN 状态，比如我们利用 rs.add() 方法添加一个不存在的成员，这个成员的状态就是 UNKNOWN。 REMOVED成员被从副本集中移除时就变成这个状态。 ROLLBACK如果成员正在进行数据回滚，它就处于 ROLLBACK 状态，回滚结束后会转换为 RECOVERING 状态。 FATAL当一个成员发生了不可挽回的错误时，且不再尝试恢复正常的话，就处于这个状态。 主节点转备份节点通过如下命令可以让主节点转为备份节点： 1rs.stepDown() 主节点转为备份节点之后会有新的主节点被选举出来，可以通过 rs.status() 来查看新的主节点。 rs.status()方法前面我们已经多次使用过 rs.status() 方法， rs.status() 方法会列出每个备份节点的含义，我们来看看这些参数的含义，先来列出一个 rs.status() 方法的返回值样例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344{&quot;members&quot; : [ { &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.248.135:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 241, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1509881297, 1), &quot;t&quot; : NumberLong(16) }, &quot;optimeDurable&quot; : { &quot;ts&quot; : Timestamp(1509881297, 1), &quot;t&quot; : NumberLong(16) }, &quot;optimeDate&quot; : ISODate(&quot;2017-11-05T11:28:17Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2017-11-05T11:28:17Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-11-05T11:28:18.073Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-11-05T11:28:18.769Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;syncingTo&quot; : &quot;192.168.248.136:27017&quot;, &quot;configVersion&quot; : 15 }, { &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.248.136:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 250, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1509881297, 1), &quot;t&quot; : NumberLong(16) }, &quot;optimeDate&quot; : ISODate(&quot;2017-11-05T11:28:17Z&quot;), &quot;electionTime&quot; : Timestamp(1509881276, 1), &quot;electionDate&quot; : ISODate(&quot;2017-11-05T11:27:56Z&quot;), &quot;configVersion&quot; : 15, &quot;self&quot; : true }]} stateStr用来描述当前节点的状态。 uptime表示从成员可达到现在所经历的时间。 optimeDate表示每个成员的oplog中最后一个操作发生的时间。 lastHeartbeat表示当前服务器最后一次收到其他成员心跳的时间。 pingMs表示心跳从当前服务器到达某个成员所花费的平均时间。 syncingTo表示同步的数据源。 health表示该服务器是否可达，1表示可达，0表示不可达。 复制链问题数据复制时可以从主节点直接复制，也可以从备份节点开始复制，从备份节点复制可以形成复制链，如果想禁止复制链，即所有的数据都从主节点复制，可以通过 chainingAllowed 属性来设置，具体步骤如下： 123config=rs.config()config.settings.chainingAllowed=falsers.reconfig(config) 好了，MongoDB 中副本集的其他细节我们就先说到这里，小伙伴们有问题欢迎留言讨论。参考资料： 《MongoDB权威指南第2版》","link":"/2019/0917/mongodb-replicaset-details.html"},{"title":"Java 操作 MongoDB","text":"之前我们介绍的 MongoDB 的操作都是在 shell 命令中写的，在项目开发时我们当然都是用程序去操作 MongoDB 的，本文我们来看看如何用 Java 代码操作 MongoDB。 准备工作首先我们需要驱动，MongoDB 的 Java 驱动我们可以直接在 Maven 中央仓库去下载，也可以创建 Maven 工程添加如下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver&lt;/artifactId&gt; &lt;version&gt;3.5.0&lt;/version&gt;&lt;/dependency&gt; 建议通过 Maven 来添加依赖，如果自己下载 jar，需要下载如下三个 jar： org.mongodb:bson:jar:3.5.0 org.mongodb:mongodb-driver-core:jar:3.5.0 org.mongodb:mongodb-driver:jar:3.5.0 另外，在使用 Java 操作 MongoDB 之前，记得启动 MongoDB 哦~ 获取集合所有准备工作完成之后，我们首先需要一个 MongoClient，如下： 1MongoClient client = new MongoClient(\"192.168.248.136\", 27017); 然后通过如下方式获取一个数据库，如果要获取的数据库本身就存在，直接获取到，不存在 MongoDB 会自动创建： 1MongoDatabase sang = client.getDatabase(\"sang\"); 然后通过如下方式获取一个名为c1的集合，这个集合存在的话就直接获取到，不存在的话 MongoDB 会自动创建出来，如下： 1MongoCollection&lt;Document&gt; c = sang.getCollection(\"c1\"); 有了集合之后，我们就可以向集合中插入数据了。 增和在 shell 中的操作一样，我们可以一条一条的添加数据，也可以批量添加，添加单条数据操作如下： 123Document d1 = new Document();d1.append(\"name\", \"三国演义\").append(\"author\", \"罗贯中\");c.insertOne(d1); 添加多条数据的操作如下： 12345678List&lt;Document&gt; collections = new ArrayList&lt;Document&gt;();Document d1 = new Document();d1.append(\"name\", \"三国演义\").append(\"author\", \"罗贯中\");collections.add(d1);Document d2 = new Document();d2.append(\"name\", \"红楼梦\").append(\"author\", \"曹雪芹\");collections.add(d2);c.insertMany(collections); 改可以修改查到的第一条数据，操作如下： 1c.updateOne(Filters.eq(\"author\", \"罗贯中\"), new Document(\"$set\", new Document(\"name\", \"三国演义123\"))); 上例中小伙伴们也看到了修改器要如何使用，不管是 $set 还是 $inc，用法都一致，我这里不再一个一个演示。也可以修改查到的所有数据，如下： 1c.updateMany(Filters.eq(\"author\", \"罗贯中\"), new Document(\"$set\", new Document(\"name\", \"三国演义456\"))); 删可以删除查到的一条数据，如下： 1c.deleteOne(Filters.eq(\"author\", \"罗贯中\")); 也可以删除查到的所有数据： 1c.deleteMany(Filters.eq(\"author\", \"罗贯中\")); Filters 里边还有其他的查询条件，都是见名知意，不赘述。 查可以直接查询所有文档： 12345FindIterable&lt;Document&gt; documents = c.find();MongoCursor&lt;Document&gt; iterator = documents.iterator();while (iterator.hasNext()) { System.out.println(iterator.next());} 也可以按照条件查询： 12345FindIterable&lt;Document&gt; documents = c.find(Filters.eq(\"author\", \"罗贯中\"));MongoCursor&lt;Document&gt; iterator = documents.iterator();while (iterator.hasNext()) { System.out.println(iterator.next());} 其他的方法基本都是见名知意，这里不再赘述。 验证问题上面我们演示的获取一个集合是不需要登录 MongoDB 数据库的，如果需要登录，我们获取集合的方式改为下面这种： 1234567ServerAddress serverAddress = new ServerAddress(\"192.168.248.128\", 27017);List&lt;MongoCredential&gt; credentialsList = new ArrayList&lt;MongoCredential&gt;();MongoCredential mc = MongoCredential.createScramSha1Credential(\"readuser\",\"sang\",\"123\".toCharArray());credentialsList.add(mc);MongoClient client = new MongoClient(serverAddress,credentialsList);MongoDatabase sang = client.getDatabase(\"sang\");c = sang.getCollection(\"c1\"); MongoCredential 是一个凭证，第一个参数为用户名，第二个参数是要在哪个数据库中验证，第三个参数是密码的 char 数组，然后将登录地址封装成一个 ServerAddress，最后将两个参数都传入 MongoClient 中实现登录功能。 其他配置在连接数据库的时候也可以设置连接超时等信息，在MongoClientOptions中设置即可，设置方式如下： 12345678910111213ServerAddress serverAddress = new ServerAddress(\"192.168.248.128\", 27017);List&lt;MongoCredential&gt; credentialsList = new ArrayList&lt;MongoCredential&gt;();MongoCredential mc = MongoCredential.createScramSha1Credential(\"rwuser\",\"sang\",\"123\".toCharArray());credentialsList.add(mc);MongoClientOptions options = MongoClientOptions.builder() //设置连接超时时间为10s .connectTimeout(1000*10) //设置最长等待时间为10s .maxWaitTime(1000*10) .build();MongoClient client = new MongoClient(serverAddress,credentialsList,options);MongoDatabase sang = client.getDatabase(\"sang\");c = sang.getCollection(\"c1\"); 好了，Java 操作 MongoDB 我们就先说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0919/mongodb-in-java.html"},{"title":"Linux 上安装 MongoDB","text":"MongoDB 在 Windows 上的安装过程整体上来说并不难，网上的资料也比较多，这里我就不介绍了，我主要说下如何在Linux环境下安装 MongoDB。 环境： CentOS 7MongoDB 3.4.9 下载 MongoDB首先去 MongoDB 官网下载 MongoDB ，地址 https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.4.9.tgz 。将下载的文件放到 /opt 目录下。 解压解压下载到的 tgz 文件,并给文件夹重命名为 mongodb，然后创建 db、logs 目录分别用来存放数据和日志。如下： 配置进入到 bin 目录下，编辑 mongodb.conf 文件，内容如下： 12345dbpath=/opt/mongodb/dblogpath=/opt/mongodb/logs/mongodb.logport=27017fork=truenohttpinterface=true 执行结果如下： 测试做完这一切之后，我们就可以启动 MongoDB 了，还是在 bin 目录下，执行 ./mongod -f mongodb.conf 命令表示启动 MongoDB ，然后执行 mongo 命令表示表示进入到 MongDB 的控制台，进入到控制台之后，我们输入 db.version() 命令，如果能显示出当前 MongoDB 的版本号，说明安装成功了。如下： 默认情况下，连接地址是 127.0.0.1:27017 ，连接的数据库是 test 数据库，我们也可以手动指定连接地址和连接的数据库： 1mongo 127.0.0.1:27017/admin 此时连接成功之后，输入 db 命令，我们可以看到当前连接的数据库是 admin。 配置开机启动我们也可以配置开机启动，编辑 /etc/rc.d/rc.local 文件，如下： 配置完成之后自行关机重启测试。 配置环境变量每次都要进入到安装目录中去输入命令，麻烦，我们直接配置环境变量即可，编辑当前用户目录下的 .bash_profile 文件，如下： 关闭 MongoDB 服务使用 db.shutdownServer(); 命令可以关闭到 MongoDB 服务，但是这个命令的执行要在 admin 数据库下，所以先切换到 admin，再关闭服务，完整运行过程如下： 安全管理上面我们所做的所有的操作都没有涉及到用户，我们在用 Oracle、MySQL 或者 MSSQL 时都有用户名密码需要登录才可以操作，MongoDB 中当然也有，但是需要我们手动添加。在添加之前，我们先来说说 MongoDB 中用户管理的几个特点： MongoDB 中的账号是在某一个库里边进行设置的，我们在哪一个库里边进行设置，就要在哪一个库里边进行验证。 创建用户时，我们需要指定用户名、用户密码和用户角色，用户角色表示了该用户的权限。 OK，假设我给 admin 数据库创建一个用户，方式如下： 12use admindb.createUser({user:&quot;root&quot;,pwd:&quot;123&quot;,roles:[{role:&quot;userAdminAnyDatabase&quot;,db:&quot;admin&quot;}]}) user 表示用户名，pwd 表示密码，role 表示角色，db 表示这个用户应用在哪个数据库上。用户的角色，有如下几种(参考资料)： 角色名 备注 Read 允许用户读取指定数据库 readWrite 允许用户读写指定数据库 dbAdmin 允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profile userAdmin 允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户 clusterAdmin 只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。 readAnyDatabase 只在admin数据库中可用，赋予用户所有数据库的读权限 readWriteAnyDatabase 只在admin数据库中可用，赋予用户所有数据库的读写权限 userAdminAnyDatabase 只在admin数据库中可用，赋予用户所有数据库的userAdmin权限 dbAdminAnyDatabase 只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。 root 只在admin数据库中可用。超级账号，超级权限 用户创建成功之后，我们关闭掉当前MongoDB服务实例，然后重新启动新的实例，启动方式如下： 1mongod -f /opt/mongodb/bin/mongodb.conf --auth 启动成功之后，如果我们直接执行如下命令，会提示没有权限： 1show dbs 执行结果如下： 123&quot;errmsg&quot; : &quot;not authorized on admin to execute command { listDatabases: 1.0 }&quot;,&quot;code&quot; : 13,&quot;codeName&quot; : &quot;Unauthorized&quot; 此时我们需要先进入到 admin 数据库中，然后授权，操作如下： 12use admindb.auth(&quot;root&quot;,&quot;123&quot;) auth 方法执行结果返回 1 表示认证成功。然后再去执行 show dbs 就可以看到预期结果了。此时我再在 sang 库下创建一个只读用户，如下： 12use sangdb.createUser({user:&quot;readuser&quot;,pwd:&quot;123&quot;,roles:[{role:&quot;read&quot;,db:&quot;sang&quot;}]}) 创建成功之后，再按照上面的流程进入到 sang 库中，使用 readuser 用户进行认证，认证成功之后一切我们就可以在 sang 库中执行查询操作了，步骤如下： 12use sangdb.auth(&quot;readuser&quot;,&quot;123&quot;) 做完这两步之后再执行查询操作就没有任何问题了，但是此时如果执行插入操作会提示没有权限，那我们可以创建一个有读写功能的用户执行相应的操作，这里就不再赘述。 好了，MongoDB 的安装我们就说这么多，有问题欢迎留言讨论。","link":"/2019/0901/linux-install-mongodb.html"},{"title":"MongoDB 基本操作","text":"上篇文章我们简单介绍了 MongoDB 安装以及启动命令，本文我们来看看基本的增删改查，对 MongoDB 有一个直观的认识。 客户端安装配置上篇文章我们提到可以在 MongoDB 启动成功之后通过 mongo 命令进入 MongoDB 客户端，然后在客户端输入操作命令执行增删改查等操作。当然，我们也可以通过一些客户端工具来连接 MongoDB，比如 Robo 3T。 首先我们下载 Robo 3T(下载地址 https://robomongo.org/download )，下载成功之后解压，找到 .exe 可执行文件双击启动，启动后新建一个连接，输入 ip 地址(注意连接之前要确保 Linux 上的 MongoDB 已经启动)，如下： 连接成功之后，我们就可以看到数据库的信息了，如下： shell 简介暂时我们所有的操作都先放在 test 数据库中进行(默认情况下，test 数据库为空，这里不显示空的数据库，此时执行可以选中 CentOS 菜单，右键单击点击 Open Shell，默认打开 test 数据库)，选中 test ，右键单击，选择 Open Shell，如下： 这里的 shell 是一个功能完整的 JavaScript 解释器，可以运行 JavaScript 程序，这个很好玩，如下我定义一个函数然后调用： 函数定义和调用的代码写好之后，按左上角的三角符号表示运行，也可以按 F5 或者 Ctrl+Enter 组合键。我们也可以调用 JavaScript 的标准函数库，如下： 再比如调用Date函数，如下： 如果我们没有使用 Robo 3T 工具，而是直接在命令行通过 mongo 命令来启动 shell，如下： 此时，shell 会连接到 MongoDB 服务器的 test 数据库，并将数据库连接赋值给全局变量 db，我们将通过 db 这个变量实现很多功能，我们也可以查看 db 当前指向哪个数据库，直接使用 db 命令，如下： 再比如我们可以通过 use 命令来切换数据库（上文中也有用到过），如下： 增在添加之前我们先来说说数据库的创建，上文我们提到了 use 命令，表示切换到某一个数据库中去，如果我们想切换到一个并不存在的数据库中去，系统就会自动的帮我们创建这个数据库。但是一个空的数据库系统并不会显示出来，往这个数据库中插入一条记录，我们就可以看到数据库存在了，如下： 在 MongoDB 中，我们插入的每一条记录都是一个 json 字符串，这个 json 字符串我们称作文档，多个文档可以组成一个集合，这个文档就类似于我们关系型数据库中的一行数据，而集合就类似于关系型数据库中的一张表，集合也不用专门去创建，直接输入向哪个集合中插入数据即可，此时集合就会被自动的创建出来了。 当然我们也可以批量的添加文档，如下(批量添加一样也可以使用insert方法来完成)： 1db.sang_collect.insertMany([{x:1},{x:2},{x:3}]) 如果在插入某一个文档时出错，则其后面的文档就会插入失败，而在其之前已经插入的文档则不受影响，如下： 1db.sang_collect.insertMany([{_id:99,x:99},{_id:99,x:98},{_id:97,x:97}]) 由于第二个文档的 _id 字段与前面的重复，所以第二第三个文档插入失败，第一个文档则插入成功。 查数据添加成功之后我们再来看看查询，利用 db.sang.find() 方法我们可以查看所有文档(所有记录)，如果只查看一个文档(一条记录)，可以通过 db.sang.findOne() 命令，在查看之前我先用一个 for 循环多插入几条数据，如下： 然后分别调用find和findOne方法查看，如下： 查出来的数据除了我们插入的 x 之外，还有一个 _id 字段，这是系统自动为我们添加的字段，我们也可以自己传入 _id,但是 _id 字段不能重复，如下： find 和 findOne 中也可以传入查询参数，这个我们后面再详细说。 改update 操作可以用来更新数据，它接收两个参数，第一个参数表示更新条件，第二个参数表示要更新的数据，比如我将所有 x:1 的数据改为 x:999,如下： 删remove 操作可以用来删除数据，如下： shell 其他操作我们也可以将要执行的脚本放在一个 js 文件中，在使用 shell 脚本时指定要执行的 js 文件，如下： 1mongo ~/myjs.js shell 会依次执行 js 中的脚本，并在执行完成后退出。如下： 我的 js 脚本中是两行插入语句，此时我们重新进入到 shell 中，就可以看到刚刚的数据已经插入成功了。 如果有每次启动都要加载的 js 文件，我们可以将其内容放在 .mongorc.js 文件中，该文件放在当前用户目录下，如下： 这样，每次启动都会打印一个 &quot;你好，欢迎使用MongoDB&quot;. 好了，MongoDB 的基本操作我们先说这么多，让各位小伙伴对 MongoDB 有一个基本的认识，后面的文章我们会详细的介绍 MongoDB 中增删改查的各种情况，敬请关注。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0902/mongodb-basic-operations.html"},{"title":"MongoDB 数据类型","text":"上篇文章我们介绍了 MongoDB 的最基本的增删改查操作，也介绍了一些基础的概念，MongoDB 中每条记录称作一个文档，这个文档和我们平时用的 JSON 有点像，但也不完全一样。 JSON 是一种轻量级的数据交换格式。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言，JSON 易于阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率，但是 JSON 也有它的局限性，比如它只有 null、布尔、数字、字符串、数组和对象这几种数据类型，没有日期类型，只有一种数字类型，无法区分浮点数和整数，也没法表示正则表达式或者函数。由于这些局限性，BSON 闪亮登场啦，BSON 是一种类 JSON 的二进制形式的存储格式，简称 Binary JSON，它和 JSON 一样，支持内嵌的文档对象和数组对象，但是 BSON 有 JSON 没有的一些数据类型，如 Date 和 BinData 类型，MongoDB 使用 BSON 做为文档数据存储和网络传输格式。本文我们就来说说 MongoDB 中都支持哪些数据类型，其实也是来看看 BSON 有哪些好玩的地方。 数字shell 默认使用 64 位浮点型数值，如下： 12db.sang_collec.insert({x:3.1415926})db.sang_collec.insert({x:3}) 对于整型值，我们可以使用 NumberInt 或者 NumberLong 表示，如下： 12db.sang_collec.insert({x:NumberInt(10)})db.sang_collec.insert({x:NumberLong(12)}) 字符串字符串也可以直接存储，如下： 1db.sang_collec.insert({x:&quot;hello MongoDB!&quot;}) 正则表达式正则表达式主要用在查询里边，查询时我们可以使用正则表达式，语法和 JavaScript 中正则表达式的语法相同，比如查询所有 key 为 x ，value 以 hello 开始的文档且不区分大小写： 1db.sang_collec.find({x:/^(hello)(.[a-zA-Z0-9])+/i}) 数组数组一样也是被支持的，如下： 1db.sang_collec.insert({x:[1,2,3,4,new Date()]}) 数组中的数据类型可以是多种多样的。 日期MongoDB 支持 Date 类型的数据，可以直接 new 一个 Date 对象，如下： 1db.sang_collec.insert({x:new Date()}) 内嵌文档一个文档也可以作为另一个文档的 value，这个其实很好理解，如下： 1db.sang_collect.insert({name:&quot;三国演义&quot;,author:{name:&quot;罗贯中&quot;,age:99}}); 书有一个属性是作者，作者又有 name，年龄等属性。 ObjectId我们在前面提到过，我们每次插入一条数据系统都会自动帮我们插入一个 _id 键，这个键的值不可以重复，它可以是任何类型的，我们也可以手动的插入，默认情况下它的数据类型是 ObjectId，由于 MongoDB 在设计之初就是用作分布式数据库，所以使用 ObjectId 可以避免不同数据库中 _id 的重复（如果使用自增的方式在分布式系统中就会出现重复的 _id 的值），这个特点有点类似于 Git 中的版本号和 Svn 中版本号的区别。 ObjectId 使用 12 字节的存储空间，每个字节可以存储两个十六进制数字，所以一共可以存储 24 个十六进制数字组成的字符串，在这 24 个字符串中，前 8 位表示时间戳，接下来 6 位是一个机器码，接下来 4 位表示进程 id，最后 6 位表示计数器。 二进制MongoDB 中也可以存储二进制数据，不过这种情况并不多，二进制数据的存储不能在 shell 中操作，我们在后面的代码中会介绍这种存储方式。 代码文档中也可以包括 JavaScript 代码，如下： 1db.sang_collect.insert({x:function f1(a,b){return a+b;}}); 好了，MongoDB 的数据类型我们就先介绍这么多，这里只是做一个大致的了解，后文我们还会再详细的说到这些东西的详细使用方式。小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0903/mongodb-data-types.html"},{"title":"MongoDB 文档查询操作(一)","text":"上篇文章我们主要介绍了 MongoDB 的修改操作，本文我们来看看查询操作。 find 方法再探find 方法是很重要的一个查询方法，我们在前面也已经使用过多次了，一般情况下我们调用的是： 1find() 没有传入任何参数，这个等价于： 1find({}) 都表示没有查询条件，查询所有的数据。如果有查询条件，我们传入查询条件即可，查询条件也是一个文档，如下表示查询 x 为 1 的文档： 1db.sang_collect.find({x:1}) 如果查询条件文档中有多个字段，多个字段之间的关系是 AND，如下表示查询 x 为 1 并且 y 为 99 的文档： 1db.sang_collect.find({x:1,y:99}) 默认情况下，每次查询都会返回文档中所有的 key/value 对，我们也可以自定义返回的字段，如下表示只返回 x 字段，其他字段都不返回： 1db.sang_collect.find({},{x:1}) 参数 1 表示返回某一个字段，0 表示不返回某一个字段，当我们设置只返回 x 的时候， _id 默认还是返回的，如果不想返回 _id ，我们可以设置 _id 为 0，如下： 1db.sang_collect.find({},{x:1,_id:0}) 此时返回的数据中就不包括 _id 字段了。 各种查询条件比较运算符这里的比较运算符都比较好理解，如下表： 符号 含义 12345678910111213141516171819202122232425262728293031323334353637|```$lte```|```&lt;=```||```$gt```|```&gt;```||```$gte```|```&gt;=```||```$ne```|```!=```|比如我想查询所有成绩在 [90,100] 之间的学生：原始数据如下：```json/* 2 */{ &quot;_id&quot; : ObjectId(&quot;59f0b17249fc5c9c2412a666&quot;), &quot;name&quot; : &quot;zs&quot;, &quot;score&quot; : 100.0}/* 3 */{ &quot;_id&quot; : ObjectId(&quot;59f0b17249fc5c9c2412a667&quot;), &quot;name&quot; : &quot;ls&quot;, &quot;score&quot; : 90.0}/* 4 */{ &quot;_id&quot; : ObjectId(&quot;59f0b17249fc5c9c2412a668&quot;), &quot;name&quot; : &quot;ww&quot;, &quot;score&quot; : 70.0}/* 5 */{ &quot;_id&quot; : ObjectId(&quot;59f0b17249fc5c9c2412a669&quot;), &quot;name&quot; : &quot;zl&quot;, &quot;score&quot; : 80.0} 查询操作如下： 1db.sang_collect.find({score:{$lte:100,$gte:90}}) 查询结果如下： 12345678910111213/* 1 */{ \"_id\" : ObjectId(\"59f0b17249fc5c9c2412a666\"), \"name\" : \"zs\", \"score\" : 100.0}/* 2 */{ \"_id\" : ObjectId(\"59f0b17249fc5c9c2412a667\"), \"name\" : \"ls\", \"score\" : 90.0} 如果要查询分数不为 90 的学生，操作如下： 1db.sang_collect.find({score:{$ne:90}}) $in和$nin$in 有点类似于 SQL 中的 in 关键字，表示查询某一个字段在某一个范围中的所有文档，比如我想查询x为1或者2的所有文档，如下： 1db.sang_collect.find({x:{$in:[1,2]}}) $nin 的作用和 $in 恰好相反，表示查询某一个字段不在某一个范围内的所有文档，比如我想查询x不为1或者2(不为1且不为2)的所有文档，如下： 1db.sang_collect.find({x:{$nin:[1,2]}}) $or$or 有点类似于 SQL 中的 or 关键字，表示多个查询条件之间是或的关系，比如我想查询 x 为 1 或者 y 为 99 的文档，如下： 1db.sang_collect.find({$or:[{x:1},{y:99}]}) $type$type 可以用来根据数据类型查找数据，比如我想要查找 x 类型为数字的文档，如下： 1db.sang_collect.find({x:{$type:1}}) 1 表示数字，其他数据类型对应的数字参见下表。 类型 对应数字 别名 说明 Double1 1 double String 2 string Object 3 object Array 4 array Binary data 5 binData Undefined 6 undefined 弃用 ObjectId 7 objectId Boolean 8 bool Date 9 date Null 10 null Regular Expression 11 regex DBPointer 12 dbPointer JavaScript 13 javascript Symbol 14 symbol JavaScript(with scope) 15 javascriptWithScope 32-bit integer 16 int Timestamp 17 timestamp 64-bit integer 18 long Min key -1 minKey Max key 127 maxKey $not$not 用来执行取反操作，比如我想要查询所有 x 的类型不为数字的文档，如下： 1db.sang_collect.find({x:{$not:{$type:1}}}) $and$and 类似于 SQL 中的 and，比如我想查询 y 大于 98 并且小于 100 的数据，如下： 1db.sang_collect.find({$and:[{y:{$gt:98}},{y:{$lt:100}}]}) 上面的操作我们也可以使用下面简化的写法： 1db.sang_collect.find({y:{$lt:100,$gt:98}}) 好了，MongoDB 中的查询操作还是非常丰富的，本文我们先说到这里，下篇文章我们继续介绍，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0905/mongodb-documents-query.html"},{"title":"MongoDB 文档查询操作(二)","text":"上篇文章我们对 MongoDB 中的查询操作做了简单介绍，本文我们继续来看更丰富的查询操作。 nullnull 的查询稍微有点不同，假如我想查询 z 为 null 的数据，如下： 1db.sang_collect.find({z:null}) 这样不仅会查出 z 为 null 的文档，也会查出所有没有 z 字段的文档，如果只想查询 z 为 null 的字段，那就再多加一个条件，判断一下z这个字段存在不，如下： 1db.sang_collect.find({z:{$in:[null],$exists:true}}) 正则表达式查询使用正则表达式查询我们在前面也已经介绍过了，这里的正则表达式语法和 JavaScript 中的正则表达式语法一致，比如查询所有 key 为 x，value 以 hello 开始的文档且不区分大小写： 1db.sang_collec.find({x:/^(hello)(.[a-zA-Z0-9])+/i}) 数组查询假设我有一个数据集如下： 12345678{ \"_id\" : ObjectId(\"59f1ad41e26b36b25bc605ae\"), \"books\" : [ \"三国演义\", \"红楼梦\", \"水浒传\" ]} 查询 books 中含有三国演义的文档，如下： 1db.sang_collect.find({books:&quot;三国演义&quot;}) 如果要查询既有三国演义又有红楼梦的文档，可以使用 $all，如下： 1db.sang_collect.find({books:{$all:[&quot;三国演义&quot;,&quot;红楼梦&quot;]}}) 当然我们也可以使用精确匹配，比如查询 books 为 &quot;三国演义&quot;,&quot;红楼梦&quot;, &quot;水浒传&quot; 的数据，如下： 1db.sang_collect.find({books:[&quot;三国演义&quot;,&quot;红楼梦&quot;, &quot;水浒传&quot;]}) 不过这种就会一对一的精确匹配。 也可以按照下标匹配，比如我想查询数组中下标为 2 的项的为 &quot;水浒传&quot; 的文档，如下： 1db.sang_collect.find({&quot;books.2&quot;:&quot;水浒传&quot;}) 也可以按照数组长度来查询，比如我想查询数组长度为 3 的文档： 1db.sang_collect.find({books:{$size:3}}) 如果想查询数组中的前两条数据，可以使用 $slice，如下： 1db.sang_collect.find({},{books:{$slice:2}}) 注意这里要写在 find 的第二个参数的位置。2 表示数组中前两个元素，-2 表示从后往前数两个元素。也可以截取数组中间的元素，比如查询数组的第二个到第四个元素： 1db.sang_collect.find({},{books:{$slice:[1,3]}}) 数组中的与的问题也值得说一下，假设我有如下数据： 1234567{ \"_id\" : ObjectId(\"59f208bc7b00f982986c669c\"), \"x\" : [ 5.0, 25.0 ]} 我想将数组中 value 取值在 (10,20) 之间的文档获取到，如下操作： 1db.sang_collect.find({x:{$lt:20,$gt:10}}) 此时上面这个文档虽然不满足条件却依然被查找出来了，因为 5&lt;20 ，而 25&gt;10，要解决这个问题，我们可以使用 $elemMatch，如下： 1234567891011121314151617db.sang_collect.find({x:{$elemMatch:{$lt:20,$gt:10}}})``` $elemMatch 要求 MongoDB 同时使用查询条件中的两个语句与一个数组元素进行比较。## 嵌套文档查询嵌套文档有两种查询方式，比如我的数据如下：```json{ &quot;_id&quot; : ObjectId(&quot;59f20c9b7b00f982986c669f&quot;), &quot;x&quot; : 1.0, &quot;y&quot; : { &quot;z&quot; : 2.0, &quot;k&quot; : 3.0 }} 想要查询上面这个文档，我的查询语句如下： 1db.sang_collect.find({y:{z:2,k:3}}) 但是这种写法要求严格匹配，顺序都不能变，假如写成了 db.sang_collect.find({y:{k:3,z:2}})，就匹配不到了，因此这种方法不够灵活，我们一般推荐的是下面这种写法： db.sang_collect.find({&quot;y.z&quot;:2,&quot;y.k&quot;:3}) 这种写法可以任意颠倒顺序。 好了，MongoDB 中的查询操作还是非常丰富的，本文我们先说到这里，下篇文章我们介绍游标，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0906/mongodb-documents-query.html"},{"title":"MongoDB 文档查询操作（三）","text":"关于 MongoDB 中的查询，我们已经连着介绍了两篇文章了，本文我们来介绍另外一个查询概念游标。 基本操作游标这个概念在很多地方都有，Java 中 JDBC 里的 ResultSet ，Android 中的 Cursor 等等都是，MongoDB 中也有类似的概念。当我们调用 find 方法时，就可以返回一个游标，如下： 1var cursor = db.sang_collect.find(); 游标中有 hasNext() 方法，也有 next() 方法，这两个方法结合可以用来遍历结果，如下： 123while(cursor.hasNext()){ print(cursor.next())} next() 方法可以获取查询到的每一个文档，如下： 123456789101112{ \"_id\" : ObjectId(\"59f299579babb96c21ddc9e8\"), \"x\" : 0.0, \"y\" : 1000.0}/* 2 */{ \"_id\" : ObjectId(\"59f299579babb96c21ddc9e9\"), \"x\" : 1.0, \"y\" : 999.0} 如果我只想获取文档中的某一个字段，可以按如下方式： 123while(cursor.hasNext()){ print(cursor.next().y)} cursor 也实现了 JavaScript 中的迭代器接口，所以我们也可以直接调用 forEach 方法来遍历： 123cursor.forEach(function(x){ print(x) }) 当我们调用 find 方法获取 cursor 时，shell 并不会立即查询数据库，而是在真正使用数据时才会去加载，这有点类似于数据库框架中的懒加载，shell 在每次查询的时候会获取前 100 条结果或者前 4MB 数据(两者之间取最小)，然后我们调用 hasNext 和 next 时 shell 就不用再去连接数据库了，直接一条一条的返回查询到的数据，这 100 条或者 4MB 数据全部被返回之后，shell 才会再次发起请求向 MongoDB 要数据。 limitlimit 是 cursor 中的方法，用来限制返回结果的数量，比如我只想获取查询的前三条结果，方式如下： 1var cursor = db.sang_collect.find().limit(3) skipskip 也是 cursor 中的方法，用来表示跳过的记录数，比如我想获取第2到第5条记录，如下： 1var cursor = db.sang_collect.find().skip(2).limit(4) 跳过前两条( 0 和 1 )然后获取后面 4 条数据，skip 和 limit 结合有点类似于 MySQL 中的 limit，可以用来做分页，不过这种分页方式效率过低。 sortsort 用来实现排序功能，比如按 x 排序，如下： 1var cursor = db.sang_collect.find().sort({x:-1}) 1 表示升序，-1 表示降序。 好了，MongoDB 中的查询我们就说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0907/mongodb-documents-query.html"},{"title":"MongoDB 查看执行计划","text":"MongoDB 中的 explain() 函数可以帮助我们查看查询相关的信息，这有助于我们快速查找到搜索瓶颈进而解决它，本文我们就来看看 explain() 的一些用法及其查询结果的含义。 本文是 MongoDB 系列的第八篇文章，了解前面的文章有助于更好的理解本文： 整体来说，explain() 的用法和 sort()、limit() 用法差不多，不同的是 explain() 必须放在最后面。 基本用法先来看一个基本用法： 1db.sang_collect.find({x:1}).explain() 直接跟在 find() 函数后面，表示查看 find() 函数的执行计划，结果如下： 1234567891011121314151617181920212223242526272829{ \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"sang.sang_collect\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"x\" : { \"$eq\" : 1.0 } }, \"winningPlan\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"x\" : { \"$eq\" : 1.0 } }, \"direction\" : \"forward\" }, \"rejectedPlans\" : [] }, \"serverInfo\" : { \"host\" : \"localhost.localdomain\", \"port\" : 27017, \"version\" : \"3.4.9\", \"gitVersion\" : \"876ebee8c7dd0e2d992f36a848ff4dc50ee6603e\" }, \"ok\" : 1.0} 返回结果包含两大块信息，一个是 queryPlanner，即查询计划，还有一个是 serverInfo，即 MongoDB 服务的一些信息。那么这里涉及到的参数比较多，我们来一一看一下： 参数 含义 plannerVersion 查询计划版本 namespace 要查询的集合 indexFilterSet 是否使用索引 parsedQuery 查询条件，此处为x=1 winningPlan 最佳执行计划 stage 查询方式，常见的有**COLLSCAN/全表扫描、IXSCAN/索引扫描、FETCH/根据索引去检索文档、SHARD_MERGE/合并分片结果、IDHACK/针对123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960|filter|过滤条件||direction|搜索方向||rejectedPlans|拒绝的执行计划||serverInfo|MongoDB服务器信息|## 添加不同参数explain() 也接收不同的参数，通过设置不同参数我们可以查看更详细的查询计划。### queryPlannerqueryPlanner 是默认参数，添加 queryPlanner 参数的查询结果就是我们上文看到的查询结果，so，这里不再赘述。### executionStatsexecutionStats 会返回最佳执行计划的一些统计信息，如下：```json{ &quot;queryPlanner&quot; : { &quot;plannerVersion&quot; : 1, &quot;namespace&quot; : &quot;sang.sang_collect&quot;, &quot;indexFilterSet&quot; : false, &quot;parsedQuery&quot; : {}, &quot;winningPlan&quot; : { &quot;stage&quot; : &quot;COLLSCAN&quot;, &quot;direction&quot; : &quot;forward&quot; }, &quot;rejectedPlans&quot; : [] }, &quot;executionStats&quot; : { &quot;executionSuccess&quot; : true, &quot;nReturned&quot; : 10000, &quot;executionTimeMillis&quot; : 4, &quot;totalKeysExamined&quot; : 0, &quot;totalDocsExamined&quot; : 10000, &quot;executionStages&quot; : { &quot;stage&quot; : &quot;COLLSCAN&quot;, &quot;nReturned&quot; : 10000, &quot;executionTimeMillisEstimate&quot; : 0, &quot;works&quot; : 10002, &quot;advanced&quot; : 10000, &quot;needTime&quot; : 1, &quot;needYield&quot; : 0, &quot;saveState&quot; : 78, &quot;restoreState&quot; : 78, &quot;isEOF&quot; : 1, &quot;invalidates&quot; : 0, &quot;direction&quot; : &quot;forward&quot;, &quot;docsExamined&quot; : 10000 } }, &quot;serverInfo&quot; : { &quot;host&quot; : &quot;localhost.localdomain&quot;, &quot;port&quot; : 27017, &quot;version&quot; : &quot;3.4.9&quot;, &quot;gitVersion&quot; : &quot;876ebee8c7dd0e2d992f36a848ff4dc50ee6603e&quot; }, &quot;ok&quot; : 1.0} 这里除了我们上文介绍到的一些参数之外，还多了 executionStats 参数，含义如下： 参数 含义 executionSuccess 是否执行成功 nReturned 返回的结果数 executionTimeMillis 执行耗时 totalKeysExamined 索引扫描次数 totalDocsExamined 文档扫描次数 executionStages 这个分类下描述执行的状态 stage 扫描方式，具体可选值与上文的相同 nReturned 查询结果数量 executionTimeMillisEstimate 预估耗时 works 工作单元数，一个查询会分解成小的工作单元 advanced 优先返回的结果数 docsExamined 文档检查数目，与totalDocsExamined一致 allPlansExecutionallPlansExecution 用来获取所有执行计划，结果参数基本与上文相同，这里就不再细说了。 好了，MongoDB 中的 explain() 我们就说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》 MongoDB执行计划获取(db.collection.explain()) mongodb .explain(‘executionStats’) 查询性能分析(没找到原文出处)","link":"/2019/0908/mongodb-query-planner.html"},{"title":"初识 MongoDB 中的索引","text":"索引就像图书的目录一样，可以让我们快速定位到需要的内容，关系型数据库中有索引，NoSQL 中当然也有，本文我们就先来简单介绍下 MongoDB 中的索引。 索引创建默认情况下，集合中的 _id 字段就是索引，我们可以通过 getIndexes() 方法来查看一个集合中的索引： 1db.sang_collect.getIndexes() 结果如下： 12345678910[ { \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\", \"ns\" : \"sang.sang_collect\" }] 我们看到这里只有一个索引，就是 _id。 现在我的集合中有 10000 个文档，我想要查询 x 为 1 的文档，我的查询操作如下： 1db.sang_collect.find({x:1}) 这种查询默认情况下会做全表扫描，我们可以用上篇文章介绍的 explain() 来查看一下查询计划，如下： 1db.sang_collect.find({x:1}).explain(&quot;executionStats&quot;) 结果如下： 12345678910111213141516171819202122232425262728293031323334{ \"queryPlanner\" : { }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1, \"executionTimeMillis\" : 15, \"totalKeysExamined\" : 0, \"totalDocsExamined\" : 10000, \"executionStages\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"x\" : { \"$eq\" : 1.0 } }, \"nReturned\" : 1, \"executionTimeMillisEstimate\" : 29, \"works\" : 10002, \"advanced\" : 1, \"needTime\" : 10000, \"needYield\" : 0, \"saveState\" : 78, \"restoreState\" : 78, \"isEOF\" : 1, \"invalidates\" : 0, \"direction\" : \"forward\", \"docsExamined\" : 10000 } }, \"serverInfo\" : { }, \"ok\" : 1.0} 结果比较长，我摘取了关键的一部分。我们可以看到查询方式是全表扫描，一共扫描了 10000 个文档才查出来我要的结果。实际上我要的文档就排第二个，但是系统不知道这个集合中一共有多少个 x 为 1 的文档，所以会把全表扫描完，这种方式当然很低效，但是如果我加上 limit，如下： 1db.sang_collect.find({x:1}).limit(1) 此时再看查询计划发现只扫描了两个文档就有结果了，但是如果我要查询 x 为 9999 的记录，那还是得把全表扫描一遍，此时，我们就可以给该字段建立索引，索引建立方式如下： 1db.sang_collect.ensureIndex({x:1}) 1 表示升序，-1 表示降序。当我们给 x 字段建立索引之后，再根据 x 字段去查询，速度就非常快了，我们看下面这个查询操作的执行计划： 1db.sang_collect.find({x:9999}).explain(&quot;executionStats&quot;) 这个查询计划过长我就不贴出来了，我们可以重点关注查询要耗费的时间大幅度下降。 此时调用 getIndexes() 方法可以看到我们刚刚创建的索引，如下： 123456789101112131415161718[ { \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\", \"ns\" : \"sang.sang_collect\" }, { \"v\" : 2, \"key\" : { \"x\" : 1.0 }, \"name\" : \"x_1\", \"ns\" : \"sang.sang_collect\" }] 我们看到每个索引都有一个名字，默认的索引名字为 字段名_排序值，当然我们也可以在创建索引时自定义索引名字，如下： 1db.sang_collect.ensureIndex({x:1},{name:&quot;myfirstindex&quot;}) 此时创建好的索引如下： 12345678{ \"v\" : 2, \"key\" : { \"x\" : 1.0 }, \"name\" : \"myfirstindex\", \"ns\" : \"sang.sang_collect\"} 当然索引在创建的过程中还有许多其他可选参数，如下： 1db.sang_collect.ensureIndex({x:1},{name:&quot;myfirstindex&quot;,dropDups:true,background:true,unique:true,sparse:true,v:1,weights:99999}) 关于这里的参数，我说一下： name 表示索引的名称 dropDups 表示创建唯一性索引时如果出现重复，则将重复的删除，只保留第一个 background 是否在后台创建索引，在后台创建索引不影响数据库当前的操作，默认为 false unique 是否创建唯一索引，默认 false sparse 对文档中不存在的字段是否不起用索引，默认 false v 表示索引的版本号，默认为 2 weights 表示索引的权重 此时创建好的索引如下： 123456789101112{ \"v\" : 1, \"unique\" : true, \"key\" : { \"x\" : 1.0 }, \"name\" : \"myfirstindex\", \"ns\" : \"sang.sang_collect\", \"background\" : true, \"sparse\" : true, \"weights\" : 99999.0} 查看索引上文我们介绍了 getIndexes() 可以用来查看索引，我们还可以通过 totalIndexSize() 来查看索引的大小，如下： 1db.sang_collect.totalIndexSize() 删除索引我们可以按名称删除索引，如下： 1db.sang_collect.dropIndex(&quot;xIndex&quot;) 表示删除一个名为xIndex的索引，当然我们也可以删除所有索引，如下： 1db.sang_collect.dropIndexes() 总结索引是个好东西，可以有效的提高查询速度，但是索引会降低插入、更新和删除的速度，因为这些操作不仅要更新文档，还要更新索引，MongoDB 限制每个集合上最多有 64 个索引，我们在创建索引时要仔细斟酌索引的字段。 好了，MongoDB 中的索引入门我们就说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0909/mongodb-index-basic.html"},{"title":"我来啦","text":"Hello 老铁们，写了几年博客之后，我终于建立起自己的站点了，这个 Hexo 使用起来还是挺方便的，半天时间就弄好了，以后我的文章将在我的公众号上首发，然后也会同步到这里，欢迎老铁们关注！","link":"/2019/0406/helloworld.html"},{"title":"一键部署 Spring Boot 到远程 Docker 容器，就是这么秀！","text":"不知道各位小伙伴在生产环境都是怎么部署 Spring Boot 的，打成 jar 直接一键运行？打成 war 扔到 Tomcat 容器中运行？不过据松哥了解，容器化部署应该是目前的主流方案。 不同于传统的单体应用，微服务由于服务数量众多，在部署的时候出问题的可能性更大，这个时候，结合 Docker 来部署，就可以很好的解决这个问题，这也是目前使用较多的方案之一。 将 Spring Boot 项目打包到 Docker 容器中部署，有很多不同的方法，今天松哥主要来和大家聊一聊如何将 Spring Boot 项目一键打包到远程 Docker 容器，然后通过运行一个镜像的方式来启动一个 Spring Boot 项目。 至于其他的 Spring Boot 结合 Docker 的用法，大家不要着急，后续的文章，松哥会和大家慢慢的一一道来。 1.准备工作1.1 准备 Docker我这里以 CentOS7 为例来给大家演示。 首先需要在 CentOS7 上安装好 Docker，这个安装方式网上很多，我就不多说了，我自己去年写过一个 Docker 入门教程，大家可以在公众号后台回复 Docker 获取教程下载地址。 Docker 安装成功之后，我们首先需要修改 Docker 配置，开启允许远程访问 Docker 的功能，开启方式很简单，修改 /usr/lib/systemd/system/docker.service 文件，加入如下内容： 1-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 如下图： 配置完成后，保存退出，然后重启 Docker： 12systemctl daemon-reload service docker restart Docker 重启成功之后，Docker 的准备工作就算是 OK 了。 1.2 准备 IDEAIDEA 上的准备工作，主要是安装一个 Docker 插件，点击 File-&gt;Settings-&gt;Plugins-&gt;Browse Repositories 如下： 点击右边绿色的 Install 按钮，完成安装，安装完成之后需要重启一下 IDEA。 IDEA 重启成功之后，我们依次打开 File-&gt;Settings-&gt;Build,Execution,Deployment-&gt;Docker ，然后配置一下 Docker 的远程连接地址： 配置一下 Docker 的地址，配置完成后，可以看到下面有一个 Connection successful 提示，这个表示 Docker 已经连接上了。 如此之后，我们的准备工作就算是 OK 了。 2.准备项目接下来我们来创建一个简单的 Spring Boot 项目（只需要引入 spring-boot-starter-web 依赖即可），项目创建成功之后，我们再创建一个普通的 HelloDockerController，用来做测试，如下： 1234567@RestControllerpublic class HelloDockerController { @GetMapping(\"/hello\") public String hello() { return \"hello docker!\"; }} 这是一个很简单的接口，无需多说。 3.配置 Dockerfile接下来，在项目的根目录下，我创建一个 Dockerfile ，作为我镜像的构建文件，具体位置如下图： 文件内容如下： 1234FROM hub.c.163.com/library/java:latestVOLUME /tmpADD target/docker-0.0.1-SNAPSHOT.jar app.jarENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"] 这里只有简单的四行，我说一下： Spring Boot 项目的运行依赖 Java 环境，所以我自己的镜像基于 Java 镜像来构建。 考虑到 Docker 官方镜像下载较慢，我这里使用了网易提供的 Docker 镜像。 由于 Spring Boot 运行时需要 tmp 目录，这里数据卷配置一个 /tmp 目录出来。 将本地 target 目录中打包好的 .jar 文件复制一份新的 到 /app.jar。 最后就是配置一下启动命令，由于我打包的 jar 已经成为 app.jar 了，所以启动命令也是启动 app.jar。 这是我们配置的一个简单的 Dockerfile。 4.配置 Maven 插件接下来在 pom.xml 文件中，添加如下插件： 123456789101112131415161718192021222324252627282930&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;build-image&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;dockerHost&gt;http://192.168.66.131:2375&lt;/dockerHost&gt; &lt;imageName&gt;javaboy/${project.artifactId}&lt;/imageName&gt; &lt;imageTags&gt; &lt;imageTag&gt;${project.version}&lt;/imageTag&gt; &lt;/imageTags&gt; &lt;forceTags&gt;true&lt;/forceTags&gt; &lt;dockerDirectory&gt;${project.basedir}&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt;&lt;/plugin&gt; 这个插件的配置不难理解： 首先在 execution 节点中配置当执行 mvn package 的时候，顺便也执行一下 docker:build 然后在 configuration 中分别配置 Docker 的主机地址，镜像的名称，镜像的 tags，其中 dockerDirectory 表示指定 Dockerfile 的位置。 最后 resource 节点中再配置一下 jar 的位置和名称即可。 OK，做完这些我们就算大功告成了。 5.打包运行接下来对项目进行打包，打包完成后，项目会自动构建成一个镜像，并且上传到 Docker 容器中，打包方式如下： 打包过程会稍微有一点旧，因为还包含了镜像的构建，特别是第一次打包，需要下载基础镜像，会更慢一些。 部分打包日志如下（项目构建过程）: 项目打包成功之后，我们就可以在 Docker 容器中看到我们刚刚打包成的镜像了，如下： 5.1 运行方式一此时，我们可以直接在 Linux 上像创建普通容器一样创建这个镜像的容器，然后启动，执行如下命令即可： 1docker run -d --name javaboy -p 8080:8080 javaboy/docker:0.0.1 启动成功之后，我们就可以访问容器中的接口了。 但是这种操作显然还是有点麻烦，结合我们一开始安装的 Docker 插件，这个运行步骤还可以做进一步的简化。 5.2 运行方式二大家注意，此时我们的 IDEA 中多了一个选项，就是 docker，如下： 点击左边的绿色启动按钮，连接上 Docker 容器，连接成功之后，我们就可以看到目前 Docker 中的所有容器和镜像了，当然也包括我们刚刚创建的 Docker 镜像，如下： 此时，我们选中这个镜像，右键单击，即可基于此镜像创建出一个容器，如下图： 我们选择 Create container，然后填入容器的一些必要信息，配置一下容器名称，镜像 ID 会自动填上，暴露的端口使用 Specify 即可，然后写上端口的映射关系： 配置完成后，点击下方的 run 按钮，就可以开始运行了。运行日志如下： 注意，这个日志是在 Docker 的那个窗口里打印出来的。 项目运行成功之后，在浏览器输入远程服务器的地址，就可以访问了： 如此之后，我们的 Spring Boot 项目就算顺利发布到远程 Docker 容器中了。 好玩吗？试试！ 本文案例我已经上传到 GitHub，小伙伴们可以参考：https://github.com/lenve/javaboy-code-samples​","link":"/2019/0819/springboot-docker.html"},{"title":"七个开源的 Spring Boot 前后端分离项目，一定要收藏！","text":"前后端分离已经在慢慢走进各公司的技术栈，根据松哥了解到的消息，不少公司都已经切换到这个技术栈上面了。即使贵司目前没有切换到这个技术栈上面，松哥也非常建议大家学习一下前后端分离开发，以免在公司干了两三年，SSH 框架用的滚瓜烂熟，出来却发现自己依然没有任何优势！ 其实前后端分离本身并不难，后段提供接口，前端做数据展示，关键是这种思想。很多人做惯了前后端不分的开发，在做前后端分离的时候，很容易带进来一些前后端不分时候的开发思路，结果做出来的产品不伦不类，因此松哥这里给大家整理了几个开源的前后端分离项目，帮助大家快速掌握前后端分离开发技术栈。 美人鱼 star 数 3499 项目地址： https://gitee.com/mumu-osc/NiceFish 听名字就知道这是个不错的项目，事实上确实不赖。NiceFish（美人鱼） 是一个系列项目，目标是示范前后端分离的开发模式:前端浏览器、移动端、Electron 环境中的各种开发模式；后端有两个版本：SpringBoot 版本和 SpringCloud 版本，前端有 Angular 、React 以及 Electron 等版本。 项目效果图： 微人事 star 数 9313 项目地址：https://github.com/lenve/vhr 微人事是一个前后端分离的人力资源管理系统，项目采用 SpringBoot + Vue 开发。项目打通了前后端，并且提供了非常详尽的文档，从 Spring Boot 接口设计到前端 Vue 的开发思路，作者全部都记录在项目的 wiki 中，是不可多得的 Java 全栈学习资料。 项目效果图: 项目部分文档截图： bootshiro star 数 1370 项目地址： https://gitee.com/tomsun28/bootshiro bootshiro 是基于 Spring Boot + Shiro + JWT 的真正 RESTful URL 资源无状态认证权限管理系统的后端,前端 usthe 。区别于一般项目，该项目提供页面可配置式的、动态的 RESTful api 安全管理支持，并且实现数据传输动态秘钥加密，jwt 过期刷新，用户操作监控等，加固应用安全。 项目效果图： open-capacity-platform star 数 2643 项目地址：https://gitee.com/owenwangwen/open-capacity-platform open-capacity-platform 微服务能力开放平台，简称 ocp ，是基于 layui + springcloud 的企业级微服务框架(用户权限管理，配置中心管理，应用管理，….)，其核心的设计目标是分离前后端，快速开发部署，学习简单，功能强大，提供快速接入核心接口能力，其目标是帮助企业搭建一套类似百度能力开放平台的框架。 项目效果图： V 部落 star 数 2902 项目地址：https://github.com/lenve/VBlog V部落是一个多用户博客管理平台，采用 Vue + SpringBoot + ElementUI 开发。这个项目最大的优势是简单，属于功能完整但是又非常简单的那种，非常非常适合初学者。 项目效果图： 悟空 CRM star 数 650 项目地址：https://gitee.com/wukongcrm/72crm-java 悟空 CRM 是基于 jfinal + vue + ElementUI 的前后端分离 CRM 系统。 老实说，jfinal 了解下就行了，没必要认真研究，Vue + ElementUI 的组合可以认真学习下、前后端交互的方式可以认真学习下。 paascloud-master star 数 5168 项目地址：https://github.com/paascloud/paascloud-master paascloud-master 核心技术为 SpringCloud + Vue 两个全家桶实现，采取了取自开源用于开源的目标，所以能用开源绝不用收费框架，整体技术栈只有阿里云短信服务是收费的，都是目前 java 前瞻性的框架，可以为中小企业解决微服务架构难题，可以帮助企业快速建站。由于服务器成本较高，尽量降低开发成本的原则，本项目由 10 个后端项目和 3 个前端项目共同组成。真正实现了基于 RBAC、jwt 和 oauth2 的无状态统一权限认证的解决方案，实现了异常和日志的统一管理，实现了 MQ 落地保证 100% 到达的解决方案。 项目效果图： 总结他山之石，可以攻玉。当我们学会了很多知识点之后，需要一个项目来将这些知识点融会贯通，这些开源项目就是很好的资料。现在前后端分离开发方式日渐火热，松哥也强烈建议大家有空学习下这种开发方式。虽然我们身为 Java 工程师，可是也不能固步自封，看看前端单页面应用怎么构建，看看前端工程化是怎么回事，这些都有助于我们开发出更加合理好用的后端接口。好了，七个开源项目，助力大家在全栈的路上更进一步！","link":"/2019/0815/springboot-vue.html"},{"title":"你真的理解 Spring Boot 项目中的 parent 吗？","text":"前面和大伙聊了 Spring Boot 项目的三种创建方式，这三种创建方式，无论是哪一种，创建成功后，pom.xml 坐标文件中都有如下一段引用： 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; 对于这个 parent 的作用，你是否完全理解？有小伙伴说，不就是依赖的版本号定义在 parent 里边吗？是的，没错，但是 parent 的作用可不仅仅这么简单哦！本文松哥就来和大伙聊一聊这个 parent 到底有什么作用。 基本功能当我们创建一个 Spring Boot 工程时，可以继承自一个 spring-boot-starter-parent ，也可以不继承自它，我们先来看第一种情况。先来看 parent 的基本功能有哪些？ 定义了 Java 编译版本为 1.8 。 使用 UTF-8 格式编码。 继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。 执行打包操作的配置。 自动化的资源过滤。 自动化的插件配置。 针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。 请注意，由于application.properties和application.yml文件接受Spring样式占位符 $ {...} ，因此 Maven 过滤更改为使用 @ .. @ 占位符，当然开发者可以通过设置名为 resource.delimiter 的Maven 属性来覆盖 @ .. @ 占位符。 源码分析当我们创建一个 Spring Boot 项目后，我们可以在本地 Maven 仓库中看到看到这个具体的 parent 文件，以 2.1.4 这个版本为例，松哥 这里的路径是 C:\\Users\\sang\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-parent\\2.1.4.RELEASE\\spring-boot-starter-parent-2.1.4.RELEASE.pom ,打开这个文件，快速阅读文件源码，基本上就可以证实我们前面说的功能，如下图： 我们可以看到，它继承自 spring-boot-dependencies ，这里保存了基本的依赖信息，另外我们也可以看到项目的编码格式，JDK 的版本等信息，当然也有我们前面提到的数据过滤信息。最后，我们再根据它的 parent 中指定的 spring-boot-dependencies 位置，来看看 spring-boot-dependencies 中的定义： 在这里，我们看到了版本的定义以及 dependencyManagement 节点，明白了为啥 Spring Boot 项目中部分依赖不需要写版本号了。 不用 parent但是并非所有的公司都需要这个 parent ，有的时候，公司里边会有自己定义的 parent ，我们的 Spring Boot 项目要继承自公司内部的 parent ，这个时候该怎么办呢？ 一个简单的办法就是我们自行定义 dependencyManagement 节点，然后在里边定义好版本号，再接下来在引用依赖时也就不用写版本号了，像下面这样： 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 这样写之后，依赖的版本号问题虽然解决了，但是关于打包的插件、编译的 JDK 版本、文件的编码格式等等这些配置，在没有 parent 的时候，这些统统要自己去配置。 总结好了，一篇简单的文章，向大伙展示一下 Spring Boot 项目中 parent 的作用，有问题欢迎留言讨论。","link":"/2019/0413/spring-boot-parent.html"},{"title":"使用 Docker 搭建 MySQL 主从复制","text":"hello 各位小伙伴大家好，今年 5 月份的时候，松哥和大家聊过如何搭建 MySQL 主从复制： 提高性能，MySQL 读写分离环境搭建(一) 提高性能，MySQL 读写分离环境搭建(二) 不过很多小伙伴反映在 Linux 中安装 MySQL 比较费劲，弄错了想从头开始都难，因此今天松哥就来和大家分享一下如何通过 Docker 快速搭建 MySQL 主从复制。 关于 Docker关于 Docker，松哥这里就不再多说了，松哥之前有出过一个 Docker 教程，大家在公众号后台回复 Docker ，就可以获取教程下载地址。 主从规划首先规划两个 MySQL 实例： 192.168.66.131:33061/主机 192.168.66.131:33062/从机 当然大家可以准备多个从机，从机的配置步骤是一样的。 在 Docker 中创建两个 MySQL 实例的命令如下： 12docker run --name mysql1 -p 33061:3306 -e MYSQL_ROOT_PASSWORD=123 -d mysql:5.7 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_cidocker run --name mysql2 -p 33062:3306 -e MYSQL_ROOT_PASSWORD=123 -d mysql:5.7 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci 创建完成后，通过 docker ps 命令可以查看 MySQL 实例的运行情况： 如此，表示 MySQL 实例已经在运行了。使用 Docker 配置 MySQL 主从最方便的莫过于配错了可以非常方便的从头开始。 接下来，我们就开始主从的配置。 配置主机主机上的配置，主要是两个地方： 第一个是配置一个从机登录用户 第二个配置开启 binlog。 Docker 中创建的 MySQL 实例，默认只有一个用户，就是 root，这里我们需要进入到 MySQL 命令行，再给它分配一个用户。在宿主机中通过如下命令连接上主机： 1mysql -u root -h 192.168.66.131 -P 33061 -p 输入密码后，进入到主机的命令行。然后给从机分配用户(因为我的宿主机上也安装了 MySQL，所以可以直接执行 mysql 命令，如果宿主机没有安装 MySQL，建议通过 docker exec 进入到 MySQL 容器中，然后执行如下命令)： 1GRANT REPLICATION SLAVE ON *.* to &apos;rep1&apos;@&apos;%&apos; identified by &apos;123&apos;; 这个表示从机一会使用 rep1/123 来登录主机，% 表示这个账户可以从任意地址登录，也可以给一个固定的 IP，表示这个账户只能从某一个 IP 登录。 接下来开启 binlog。 binlog 的开启，需要修改 MySQL 的配置，因此，我们需要进入到容器内部去执行。 首先进入到容器内部： 1docker exec -it mysql1 /bin/bash 然后找到 MySQL 配置文件的位置： 1/etc/mysql/mysql.conf.d/mysqld.cnf 这就是 MySQL 的配置文件。我们要在这里进行修改操作。因为 MySQL 容器中，默认没有 VI 编辑器，安装费事，所以我们可以在宿主机中将配置文件写好，然后拷贝到 MySQL 容器中，覆盖原有配置。我们主要在该配置文件中添加如下内容： 123log-bin=/var/lib/mysql/binlogserver-id=1binlog-do-db = cmdb 第一行表示配置 binlog 的位置，理论上 binlog 可以放在任意位置，但是该位置，MySQL 一定要有操作权限。 server-id 表示集群中，每个实例的唯一标识符。 bindlog-do-db 表示要同步的数据库有哪些。当从机连上主机后，并不是主机中的每一个库都要同步，这里表示配置哪些库需要同步。 配置完成后，保存退出。 接下来执行命令，将宿主机中的 mysqld.cnf 拷贝到容器中： 1docker cp ./mysqld.cnf mysql1:/etc/mysql/mysql.conf.d/ 拷贝完成后，重启容器。 1docker restart mysql1 容器重启完成后，进入到主机的命令行中，查看配置是否成功： File 和 Position 需要记着，这两个标记了二进制日志的起点位置，在从机的配置中将使用到这两个参数。 至此，主机的配置就算完成了。 配置从机从机的配置比较简单，不用开启 binlog，也不用配置要同步的库，只需要在配置文件中，添加一个 server-id 即可。 这是从机的 mysqld.cnf 配置： 配置完成后，一样拷贝到容器中。拷贝方式和主机一样： 1docker cp ./mysqld.cnf mysql2:/etc/mysql/mysql.conf.d/ 配置完成后，重启从机容器： 1docker restart mysql2 重启完成后，进入到 mysql2 的命令行，执行如下命令，开启数据同步： 1change master to master_host=&apos;192.168.66.131&apos;,master_port=33061,master_user=&apos;rep1&apos;,master_password=&apos;123&apos;,master_log_file=&apos;binlog.000001&apos;,master_log_pos=154; 配置完成后，开启从机进程。在从机命令行执行如下命令： 1start slave; 接下来，执行 show slave status\\G；查看从机状态： 这里重点查看 Slave_IO_Running 和 Slave_SQL_Running ，这两个的值必须为 Yes。如果有一个的值不为 Yes，表示配置失败，一般情况下，配置失败，下面会有失败提示。 至此，我们的 MySQL 主从就算是配置成功了。 检验配置成功之后，我们可以通过 Navicat 或者 SQLyog 等工具连接上我们的两个 MySQL 实例，然后在主机中创建一个名为 db1 的库，你会发现从机中也会自动同步这个库。 OK，本文就说这么多，有问题欢迎大家留言讨论。","link":"/2019/1101/docker-mysql.html"},{"title":"分布式数据库中间件 MyCat 搞起来！","text":"关于 MyCat 的铺垫文章已经写了三篇了： MySQL 只能做小项目？松哥要说几句公道话！ 北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！ What？Tomcat 竟然也算中间件？ 今天终于可以迎接我们的大 Boss 出场了！ MyCat 简介前面文章我们提到，如果数据量比较大的话，我们需要对数据进行分库分表，分完之后，原本存在一个数据库中的数据，现在就存在多个数据库中了，就像下面这样： 那么此时 MyCat 所扮演的角色就是分布式数据库中间件！ MyCat 是一个开源的分布式数据库中间件，它实现了 MySQL 协议，在开发者眼里，他就是一个数据库代理，我们甚至可以使用 MySQL 的客户端工具以及命令行来访问 MyCat 。 MyCat 现在已经不仅仅只支持 MySQL 了，同时也支持 MSSQL、Oracle、DB2、以及 PostgreSQL等主流数据库。甚至像 MongoDB 这种 NoSQL 也支持。 快速入门搭建读写分离要搞 MyCat ，一般要先搭建好 MySQL 的读写分离，MySQL 的读写分离可以参考松哥之前的这篇文章： 提高性能，MySQL 读写分离环境搭建(二) MyCat 安装环境： CentOS7 JDK1.8 MyCat 使用 Java 开发，因此，运行 MyCat ，一定要具备 Java 环境，配置 Java 运行环境这个比较容易，网上资料也很多，我就不详细介绍了。 Java 环境安装好之后，首先下载 MyCat： 1wget http://dl.mycat.io/1.6.7.1/Mycat-server-1.6.7.1-release-20190213150257-linux.tar.gz 下载完成后，对下载文件进行解压。 1tar -zxvf Mycat-server-1.6.7.1-release-20190213150257-linux.tar.gz 解压成功后，会出现一个 mycat 目录，进入到 mycat/conf 目录，对 mycat 进行配置： 首先来配置 schema.xml 文件： 首先在 schema 中指定逻辑库的名字，逻辑库是指 MyCat 中的库，这个库不存储数据，数据存储在 MySQL 中的物理库中。 逻辑库中配置逻辑表，配置逻辑表时，需要指定 dataNode 节点， dataNode 就是指数据库存储的位置 配置 dataNode ，dataNode 指定 dataHost 和物理库的名字。 dataHost 则配置 MySQL 的主机和从机的位置，登录密码等。主机和从机都可以配置多个。 配置完 schema.xml 后 ，接下来配置 server.xml。 server.xml 中主要配置 MyCat 的登录用户名和密码，以及需要操作的逻辑库。 配置完成后，接下来就可以启动 MyCat 了 。 执行 MyCat 解压目录下的 bin 目录下的 mycat 命令，可以启动 MyCat 1./bin/mycat start 如果启动后，提示无法创建 mycat.pid 文件，就自己手动创建一个 mycat.pid 文件。启动成功之后，就可以在本地连接 MyCat 了，连接方式和 MySQL 一样，唯一的区别在于端口号不同。 在连接 MyCat 之前，先在 MySQL 物理库中创建 db1、db2 以及 db3 三个数据库。 使用 SQLyog 连接： 也可以在 cmd 命令行登录 MyCat ： 登录成功后 ，在 MyCat 的窗口中，执行如下命令，创建表： 1create table t_user (id integer primary key,username varchar(255)) 执行成功后，我们会发现物理库中出现了相应的表。接下来，手动往各个物理库的物理表中存储一条数据，然后在 MyCat 窗口中查询： 这样就可以查询到 三个库中的三个表中的数据。 问题分析整个过程不难，但是有的小伙伴在第一次配置的过程中还是容易出错，因此我这里还是来说两句，出错了要如何定位。 一般来说，配置 MyCat 出错，问题可能发生在两个阶段。第一个阶段就是客户端连接 MyCat 出错，第二个阶段就是 MyCat 连接 MySQL 出错。 无论你是使用 SQLyog 还是 Navicat ，我们在连接数据库的过程中，都可以先测试连接，很多人卡在这一步。 如果在测试连接的时候就连接不通，说明是 MyCat 的问题，这个时候检查步骤如下： 首先当然是查看日志信息，看能不能找出端倪 通过 jps 命令查看 mycat 是否成功启动 检查 server.xml 中配置是否正确，用户名密码是否输入正确 这是第一种可能的问题，第二种问题就是测试连接没问题，但是测试完后，却连接不上。反映到 Navicat 上，就是测试连接没问题，测完之后，点击连接名要打开连接时，Navicat 就崩了，出现这个问题一般是 MyCat 在连接 MySQL 出问题了，这个时候就要去检查 schema.xml 文件中关于 MySQL 主机和从机的配置是否正确，数据库地址是否正确，用户名密码是否正确。 结语好了，本文主要简单介绍了下 MyCat 的安装问题，下篇文章我们来看 MyCat 中的分片规则问题。 参考资料： MyCat 官方文档","link":"/2019/0628/mycat-install.html"},{"title":"分布式数据库如何实现主键全局自增？","text":"前面和大家介绍了 MyCat 中数据库不同的分片规则，从留言中看出大家对分布式数据库中间件还挺感兴趣，因此今天就再来一篇，聊一聊主键全局自增要如何实现。 关于数据库分库分表的问题，我们前面还有几篇铺垫的文章，阅读前面的文章有助于更好的理解本文： 提高性能，MySQL 读写分离环境搭建(一) 提高性能，MySQL 读写分离环境搭建(二) MySQL 只能做小项目？松哥要说几句公道话！ 北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！ What？Tomcat 竟然也算中间件？ 分布式数据库中间件 MyCat 搞起来！ 数据库分库分表，分片配置轻松入门！ 问题主键自增这应该算是一个非常常见的需求，在单机数据库中，这个需求一个 auto_increment 就能实现，但是在数据库集群中，这个需求却变复杂了，因为存在多个数据库实例 ，各自都是主键自增，合在一起就不是主键自增了。 最简单的思路最简单的办法莫过于通过设置主键自增的步长和起始偏移量来处理这个问题。默认情况下，主键自增步长为 1 ，如果我们有三个数据库实例，我们可以将主键自增步长设置为 3 ，这样对于第一个数据库实例而言，主键自增就是 1、4、7、10…，对于第二个数据库实例而言，主键自增就是 2、5、8、11…，对于第三个数据库实例而言，主键自增就是 3、6、9、12….。 MSSQL 可以直接在 SQL 中指定主键的自增步长和起始偏移量，但是 MySQL 则需要修改数据库配置才能实现，因此这里不推荐使用这种方式。 MyCat 的办法MyCat 作为一个分布式数据库中间，屏蔽了数据库集群的操作，让我们操作数据库集群就像操作单机版数据库一样，对于主键自增，它有自己的方案： 通过本地文件实现 通过数据库实现 通过本地时间戳实现 通过分布式 ZK ID 生成器实现 通过 ZK 递增方式实现 今天我们就先来看看看如何通过 ZK 递增的方式实现主键全局自增。 配置步骤如下： 首先修改主键自增方式为 4 ，4 表示使用 zookeeper 实现主键自增。 server.xml 配置表自增，并且设置主键 schema.xml 设置主键自增，并且设置主键为 id 。 配置 zookeeper 的信息 在 myid.properties 中配置 zookeeper 信息： 配置要自增的表 sequence_conf.properties 注意，这里表名字要大写。 TABLE.MINID 某线程当前区间内最小值 TABLE.MAXID 某线程当前区间内最大值 TABLE.CURID 某线程当前区间内当前值 文件配置的MAXID以及MINID决定每次取得区间，这个对于每个线程或者进程都有效 文件中的这三个属性配置只对第一个进程的第一个线程有效，其他线程和进程会动态读取 ZK 重启 MyCat 测试 最后重启 MyCat ，删掉之前创建的表，然后创建新表进行测试即可。 好了，本文主要向大家介绍了 MyCat 实现主键全局自增的方案。不知道大家有没有 GET 到呢？有问题欢迎留言讨论。","link":"/2019/0712/mycat-id-autoincrement.html"},{"title":"前后端分离开发中动态菜单的两种实现方案","text":"关于前后端分离开发中的权限处理问题，松哥之前写过一篇文章和大家聊这个问题： Spring Boot + Vue 前后端分离开发，权限管理的一点思路 但是最近有小伙伴在学习微人事项目时，对动态菜单这一块还是有疑问（即不同用户登录成功后会看到不同的菜单项），因此松哥打算再来写一篇文章和大家聊一聊前后端分离开发中的动态菜单问题。 1. 一个原则做权限管理，一个核心思想就是后端做权限控制，前端做的所有工作都只是为了提高用户体验，我们不能依靠前端展示或者隐藏一个按钮来实现权限控制，这样肯定是不安全的。 就像用户注册时需要输入邮箱地址，前端校验之后，后端还是要校验，两个校验目的不同，前端校验是为了提高响应速度，优化用户体验，后端校验则是为了确保数据完整性。权限管理也是如此，前端按钮的展示/隐藏都只是为了提高用户体验，真正的权限管理需要后端来实现。 这是非常重要的一点，做前后端分离开发中的权限管理，我们首先要建立上面这样的思考框架，然后在这样的框架下，去考虑其他问题。 因此，下文我会和大家分享两种方式实现动态菜单，这两种方式仅仅只是探讨如何更好的给用户展示菜单，而不是探讨权限管理，因为权限管理是在后端完成的，也必须在后端完成。 2. 具体实现一旦建立起这样的思考框架，你会发现动态菜单的实现办法太多了。 动态菜单就是用户登录之后看到的菜单，不用角色的用户登录成功之后，会看到不用的菜单项，这个动态菜单要怎么实现呢？整体来说，有两种不同的方案，松哥曾经做过的项目中，两种方案也都有用过，这里分别来和大家分享一下。 2.1 后端动态返回后端动态返回，这是我在微人事中采用的方案。微人事中，权限管理相关的表一共有五张表，如下： 其中 hr 表就是用户表，用户登录成功之后，可以查询到用户的角色，再根据用户角色去查询出来用户可以操作的菜单（资源），然后把这些可以操作的资源，组织成一个 JSON 数据，返回给前端，前端再根据这个 JSON 渲染出相应的菜单。以微人事为例，我们返回的 JSON 数据格式如下： 1234567891011121314151617181920212223242526272829[ { \"id\":2, \"path\":\"/home\", \"component\":\"Home\", \"name\":\"员工资料\", \"iconCls\":\"fa fa-user-circle-o\", \"children\":[ { \"id\":null, \"path\":\"/emp/basic\", \"component\":\"EmpBasic\", \"name\":\"基本资料\", \"iconCls\":null, \"children\":[ ], \"meta\":{ \"keepAlive\":false, \"requireAuth\":true } } ], \"meta\":{ \"keepAlive\":false, \"requireAuth\":true } }] 这样的 JSON 在前端中再进行二次处理之后，就可以使用了，前端的二次处理主要是把 component 属性的字符串值转为对象。这一块具体操作大家可以参考微人事项目（具体在：https://github.com/lenve/vhr/blob/master/vuehr/src/utils/utils.js），我就不再赘述了。 这种方式的一个好处是前端的判断逻辑少一些，后端也不算复杂，就是一个 SQL 操作，前端拿到后端的返回的菜单数据，稍微处理一下就可以直接使用了。另外这种方式还有一个优势就是可以动态配置资源-角色以及用户-角色之间的关系，进而调整用户可以操作的资源(菜单)。 2.2 前端动态渲染另一种方式就是前端动态渲染，这种方式后端的工作要轻松一些，前端处理起来麻烦一些，松哥去年年末帮一个律所做的一个管理系统，因为权限上比较容易，我就采用了这种方案。 这种方式就是我直接在前端把所有页面都在路由表里边定义好，然后在 meta 属性中定义每一个页面需要哪些角色才能访问，例如下面这样： 123456789101112131415161718192021222324252627282930[ { \"id\":2, \"path\":\"/home\", \"component\":Home, \"name\":\"员工资料\", \"iconCls\":\"fa fa-user-circle-o\", \"children\":[ { \"id\":null, \"path\":\"/emp/basic\", \"component\":EmpBasic, \"name\":\"基本资料\", \"iconCls\":null, \"children\":[ ], \"meta\":{ \"keepAlive\":false, \"requireAuth\":true, \"roles\":['admin','user'] } } ], \"meta\":{ \"keepAlive\":false, \"requireAuth\":true } }] 这样定义表示当前登录用户需要具备 admin 或者 user 角色，才可以访问 EmpBasic 组件，当然这里不是说我这样定义了就行，这个定义只是一个标记，在项目首页中，我会遍历这个数组做菜单动态渲染，然后根据当前登录用户的角色，再结合当前组件需要的角色，来决定是否把当前组件所对应的菜单项渲染出来。 这样的话，后端只需要在登录成功后返回当前用户的角色就可以了，剩下的事情则交给前端来做。不过这种方式有一个弊端就是菜单和角色的关系在前端代码中写死了，以后如果想要动态调整会有一些不方便，可能需要改代码。特别是大项目，权限比较复杂的时候，调整就更麻烦了，所以这种方式我一般建议在一些简单的项目中使用。 3. 结语虽然我在微人事中使用了第一种方式，不过如果小伙伴是一个新项目，并且权限问题不是很复杂的话，我还是建议尝试一下第二种方式，感觉要方便一些。 不过在公司中，动态菜单到底在前端做还是后端做，可能会有一个前后端团队沟（si）通（bi）的过程，赢了的一方就可以少写几行代码了。","link":"/2019/1016/vue-router.html"},{"title":"前后端分离时代，Java 程序员的变与不变！","text":"事情的起因是这样的，有个星球的小伙伴向邀请松哥在知乎上回答一个问题，原题是： 前后端分离的时代，Java后台程序员的技术建议？ 松哥认真看了下这个问题，感觉对于初次接触前后端分离的小伙伴来说，可能都会存在这样的疑问，于是决定通过这篇文章和大家聊一聊这个话题。 我这里还是尽量从一个 Java 程序员的角度来说说这个问题，这样大家可能更好理解。 从一个题外话开始很多小伙伴可能知道，松哥本科是经管学院的，亚当•斯密的《国富论》多多少少还是了解一点。书中提到人类社会的本质就是分工协作，亚当•斯密认为人类之间的专业分工可以极大的提高生产力、创造财富，专业分工也是工业革命的基础。人类社会的发展过程就是一个专业分工不断细化、不断深化的过程，从最早的农牧分家到手工业农业分家再到商人的出现，其实都是专业分工不断细化深化的体现。 我们的开发世界也是一个小宇宙，专业分工不断细化也是一个趋势，从这个角度来说，前后端分离，都是值得积极拥抱的。 前后端分离的开发方式在最近几年突然火起来，松哥认为有两方面的原因： 前端的发展。前端经过近几年的发展，已经不再是我们传统所说的HTML+画图了，各种概念层出不穷，webpack、RxJs、Node、Redux、ssr、NuxtJs等，前端已经可以胜任很多事情，也能够完成更加丰富的用户交互。 移动互联网的发展。前两年移动互联网的火爆，很多公司的产品都要在多个平台上线，Android、iOS、小程序、公众号、PC 等等各个平台都要展示，不可能针对不同的设备开发一套后端，应该是多个前端共用同一个后端，这是就不能采用传统的前后端不分的方式来开发后端程序了。正是这样的业务需求，促进了前后端分离的发展。 变与不变程序员之间的分工协作方式有所变化，开发方式当然也会随着一起变化。但是这种变化其实是非常细微的，很容易上手的。 变 工作内容变 老实说，前后端分离之后，对 Java 程序员的要求变低了，以前大家大家出去面试 Java 工程师，如果是前后端不分的话，前端基本上也是必问的，常见的问题就是各种元素选择器，这也很好理解，因为在前后端不分的开发方式中，后端工程师多多少少是要写一点前端代码的，你很难完完全全的只写 Java 代码。但是在这种情况下，你要写的前端代码其实都是很简单的，不会是特别难的。 前后端分离之后，Java 程序员只需要专注于后台业务逻辑，对外接收前台传来的参数，根据参数给出不同的响应即可，基本上不需要写前端代码。因为这个时候的前端不同于前后端不分时候的前端，前后端分离之后，前端还是有一定的难度，较为常见的是 SPA 应用，涉及到 NodeJS、Webpack 等，此时如果还要让后端工程师写前端代码，对后端工程师的技术要求就会比较高。 不过话说回来，前后端分离后，如果你还能即写前端又写后端，那可以让老板加薪了。 接口变 前后端不分的时候，很少会涉及到接口设计，以 SpringMVC 为例，你可能返回的始终是 ModelAndView 一类的东西，前后端分离之后，我们基本上不需要返回页面了，后端主要是返回 JSON 数据，所以关键是设计好各种接口。 一个比较好的实践方案是设计满足 RESTful 规范的接口，语义明确，简洁明了，看到 URL 就知道你想干嘛！ 开发流程变化 前后端分离之后，前端不可能等后端开发好接口之后再去开发，如果这样，原本两个月做完的项目可能就得 4 个月才能完成。 一般在开发之前，整个项目组需要先设计好一个接口文档，一般可以采用 Swagger 来做接口文档(SpringBoot整合Swagger2，再也不用维护接口文档了！)，文档中约定了接口的详细信息，前后端分别按照既定的接口规范去开发，在尚未开发完成时，可以借助 Mock 来进行测试。 前端也是使用模拟数据进行测试，开发完成之后，前后端接口联调，完成测试。 不变其实除了前后端交互方式发生变化之外，其他的地方都是不变的。 前后端分离，一般来说是不会影响后端技术架构的，你使用了 SSM 或者 Spring Boot 或者 Dubbo 或者微服务，无论什么，这些技术架构既可以支撑你前后端不分的项目，也可以支撑你前后端分离的项目。 因此我说后端技术架构不受前后端分离影响。 另一方面，技术的根本不变，例如你做 Java 开发，该会的 SSM/SpringBoot/Redis/Nginx/Dubbo/SpringCloud/MySQL/MyCat/ELK/…等等，都还得会。 所以，还是去老老实实撸代码吧！ 结语如果仅仅从一个 Java 程序员的角度来说，前后端分离开发这种方式，其实是解放了 Java 程序员，可以让我们专注于后端的工作，不用再去写前端代码，术业有专攻，可以写出更优质的后端代码。不过话说回来，如果想保持一个良好的竞争力，还是有必要去了解一下目前流行的前端开发方式。 前后端分离不是终点，只是软件开发方式演变大潮下的一个点而已，未来的路还很长，还有很多东西需要我们去学习，这只是一个其中一个而已。 最后再给大家推荐几篇松哥之前写的前后端分离的文章： 一个Java程序猿眼中的前后端分离以及Vue.js入门 Spring Boot + Vue 前后端分离，两种文件上传方式总结！ Spring Boot + Vue 前后端分离开发，前端网络请求封装与配置 Spring Boot + Vue 前后端分离开发，权限管理的一点思路 再推荐两个入门项目： 微人事 V部落 好了，本文就说到这里，有问题欢迎留言讨论。","link":"/2019/0701/fronted-backend.html"},{"title":"北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！","text":"千万量级的数据，用 MySQL 要怎么存？ 初学者在看到这个问题的时候，可能首先想到的是 MySQL 一张表到底能存放多少条数据？ 根据 MySQL 官方文档的介绍，MySQL 理论上限是 (232)2 条数据，然而实际操作中，往往还受限于下面两条因素： myisam_data_pointer_size，MySQL 的 myisam_data_pointer_size 一般默认是 6，即 48 位，那么对应的行数就是 248-1。 表的存储大小 256TB 那有人会说，只要我的数据大小不超过上限，数据行数也不超过上限，是不是就没有问题了？其实不尽然。 在实际项目中，一般没有哪个项目真的触发到 MySQL 数据的上限了，因为当数据量变大了之后，查询速度会慢的吓人，而一般这个时候，你的数据量离 MySQL 的理论上限还远着呢！ 传统的企业应用一般数据量都不大，数据也都比较容易处理，但是在互联网项目中，上千万、上亿的数据量并不鲜见。在这种时候，还要保证数据库的操作效率，我们就不得不考虑数据库的分库分表了。 那么接下来就和大家简单聊一聊数据库分库分表的问题。 数据库切分看这个名字就知道，就是把一个数据库切分成 N 多个数据库，然后存放在不同的数据库实例上面，这样做有两个好处： 降低单台数据库实例的负载 可以方便的实现对数据库的扩容 一般来说，数据库的切分有两种不同的切分规则： 水平切分 垂直切分 接下来我们就对这两种不同的切分规则分别进行介绍。 水平切分先来一张简单的示意图，大家感受一下什么是水平切分： 假设我的 DB 中有 table-1、table-2 以及 table-3 三张表，水平切分就是拿着我的绝世好剑，对准黑色的线条，砍一剑或者砍 N 剑！ 砍完之后，将砍掉的部分放到另外一个数据库实例中，变成下面这样： 这样，原本放在一个 DB 中的 table 现在放在两个 DB 中了，观察之后我们发现： 两个 DB 中表的个数都是完整的，就是原来 DB 中有几张表，现在还是几张。 每张表中的数据是不完整的，数据被拆分到了不同的 DB 中去了。 这就是数据库的水平切分，也可以理解为按照数据行进行切分，即按照表中某个字段的某种规则来将表数据分散到多个库之中，每个表中包含一部分数据。 这里的某种规则都包含哪些规则呢？这就涉及到数据库的分片规则问题了，这个松哥在后面的文章中也会和大家一一展开详述。这里先简单说几个常见的分片规则： 按照日期划分：不容日期的数据存放到不同的数据库中。 对 ID 取模：对表中的 ID 字段进行取模运算，根据取模结果将数据保存到不同的实例中。 使用一致性哈希算法进行切分。 详细的用法，将在后面的文章中和大家仔细说。 垂直切分先来一张简单的示意图，大家感受一下垂直切分： 所谓的垂直切分就是拿着我的屠龙刀，对准了黑色的线条砍。砍完之后，将不同的表放到不同的数据库实例中去，变成下面这个样子： 这个时候我们发现如下几个特点： 每一个数据库实例中的表的数量都是不完整的。 每一个数据库实例中表的数据是完整的。 这就是垂直切分。一般来说，垂直切分我们可以按照业务来划分，不同业务的表放到不同的数据库实例中。 老实说，在实际项目中，数据库垂直切分并不是一件容易的事，因为表之间往往存在着复杂的跨库 JOIN 问题，那么这个时候如何取舍，就要考验架构师的水平了！ 优缺点分析通过上面的介绍，相信大家对于水平切分和垂直切分已经有所了解，优缺点其实也很明显了，松哥再来和大家总结一下。 水平切分 优点 水平切分最大的优势在于数据库的扩展性好，提前选好切分规则，数据库后期可以非常方便的进行扩容。 有效提高了数据库稳定性和系统的负载能力。拆分规则抽象好， join 操作基本可以数据库做。 缺点 水平切分后，分片事务一致性不容易解决。 拆分规则不易抽象，对架构师水平要求很高。 跨库 join 性能较差。 垂直切分 优点 一般按照业务拆分，拆分后业务清晰，可以结合微服务一起食用。 系统之间整合或扩展相对要容易很多。 数据维护相对简单。 缺点 最大的问题在于存在单库性能瓶颈，数据表扩展不易。 跨库 join 不易。 事务处理复杂。 结语虽然 MySQL 中数据存储的理论上限比较高，但是在实际开发中我们不会等到数据存不下的时候才去考虑分库分表问题，因为在那之前，你就会明显的感觉到数据库的各项性能在下降，就要开始考虑分库分表了。 好了，今天主要是向大家介绍一点概念性的东西，算是我们分布式数据库中间件正式出场前的一点铺垫。 参考资料： MySQL 官方文档","link":"/2019/0625/mysql.html"},{"title":"原创的离线版 Redis 教程，给力！","text":"嗯，你没看错，松哥又给大家送干货来了。这次是可以离线阅读的 PDF 版教程哦。 之前一直有小伙伴问我有没有 Redis 的电子书，老实说，有是有，但是公开给大家分享，其实有一点点风险，毕竟这都是有版权的东西，特别是松哥自己写书之后，深知作者的不易。 一般情况下，出版社给作者的稿酬是 8%，一本售价 ￥70 的书，到作者手里就是 ￥5.6。 不过松哥今天要分享的资源，不牵涉任何版权问题，因为是我一个字一个字码出来的，我要和大家堂堂正正的分享这个资源。 这个就是松哥自己码出来的 4w 余字的 Redis 教程。其实这个教程发布有一段时间了，只是一直没有电子版的，有不少小伙伴希望我能把这些教程整理成电子版的，包括以前写的 MongoDB、Git、Spring Cloud 以及今年一直在持续更新的 Spring Boot，大家有需求，松哥当然就要行动，于是前两天费了一番功夫，把 md 转成 word ，然后整理了下做成了 pdf，这样大家以后在没有网络的环境下也可以学习了。 先来看看目录吧： 这个教程从 Redis 的安装开始，到基本数据类型操作、数据持久化、主从复制、集群搭建直到最后的用 Java 操作 Redis ，可以说覆盖到了 Redis 常见的各种用法，当然还有更多的实际项目中 Redis 的用法，大家可以留意公众号的内容，这些我还会继续更新。 好了，资源我已经上传到百度云盘了，大家在公众号后台回复 redis 就可以获取到下载链接。如果这个资料帮助到你了，也欢迎分享给更多的小伙伴哦。 未来，我还会整理更多的电子版出来，包括 Spring Boot 系列。请小伙伴们留意公众号消息哦。","link":"/2019/1016/redis-guide.html"},{"title":"喜大普奔，两个开源的 Spring Boot + Vue 前后端分离项目可以在线体验了","text":"折腾了一周的域名备案昨天终于搞定了。 松哥第一时间想到赶紧把微人事和 V 部落部署上去，我知道很多小伙伴已经等不及了。 1. 也曾经上过线其实这两个项目当时刚做好的时候，我就把它们部署到服务器上了，以帮助小伙伴们更好的查看效果。但是那个是一台国外服务器，之所以购买国外服务器，主要是嫌国内备案麻烦，当然也有其他大家都懂的原因。 国外服务器有方便的地方，同时也有很多不便，例如网络不稳，随时有失联的风险。所以我在 2018 年年初，虽然把这两个项目都部署在服务器上，但是很多小伙伴的访问体验都不好，主要还是网络的问题。后来一段时间，经过几轮围剿与反围剿，这台服务器就彻底和松哥失联了。 失联之后，因为工作比较忙，我也就懒得去折腾了，所以导致微人事和 V 部落大家在很长一段时间内无法在线查看效果。 2. 重新上线最近因为有一些其他的计划，于是购买了阿里云服务，完事之后就是备案，所有东西都搞定之后，想着先把微人事和 V 部落部署起来，方便大家查看效果。 说干就干，我首先规划了两个二级域名： vblog.itboyhub.com vhr.itboyhub.com 这两个二级域名分别用来部署 V 部落和微人事。 大家可以通过这两个地址查看效果： 微人事 V 部落 为了确保每位小伙伴都能看到完整的演示效果，防止有的小伙伴不慎把所有数据清空了，导致其他小伙伴啥都看不到，我只开通了演示账户的查询和部分字段的更新权限，因此大家在查看演示效果时，可能会有一些涉及到增删改的操作会执行失败，请勿见怪，将项目部署到本地运行之后，就可以查看完整效果了。 3. 技能树既然都写到这儿了，就和大家聊一聊这两个部署是怎么实现的。 3.1 部署方案选择大家知道前后端分离部署的时候，我们有两种不同的方案： 一种就是将前端项目打包编译之后，放到后端项目中（例如 Spring Boot 项目的 src/main/resources/static 目录下） 另外一种则是将前端打包之后的静态资源用 Nginx 来部署，后端单独部署只需要单纯的提供接口即可。 一般在公司项目中，我们更多的是采用后者。不过松哥这里部署为了省事，我采用了第一种方案。（以后抽空我会和大家聊聊第二种部署方案） 3.2 域名映射域名映射这块简单，登录阿里云后台，添加两个 A 记录即可。 3.3 启动 Spring Boot将微人事和 V 部落分别打包上传到服务器，这个过程应该就不用我多说了吧，然后分别启动这两个项目，两个项目的默认端口分别是 8081 和 8082，命令如下： 12nohup java -jar vblog.jar &gt; vblog.log &amp;nohup java -jar vhr.jar &gt; vhr.log &amp; 将两个项目的运行日志分别写入到 vblog.log 和 vhr.log 文件中。 启动成功之后，我们就可以通过 itboyhub.com:8081 和 itboyhub.com:8082 两个端口来分别访问这两个项目了。但是这还没达到松哥的目标，我想通过二级域名来访问，并且想通过 80 端口来访问，这就要借助 Nginx 了。 注意 启动完成后，大家需要登录阿里云后台，确认 8081 和 8082 端口已经开启。 3.4 Nginx 配置Nginx 的基本用法，大家可以参考松哥的这篇旧文： Nginx 极简入门教程！ 这里我们主要来看看 Nginx 的配置。 由于有两个二级域名，而且未来服务器还要配置其他域名，因此域名要能够做到动态解析，因此在具体配置如下： 123456789server { listen 80; server_name *.itboyhub.com; if ($http_host ~* &quot;^(.*?)\\.itboyhub\\.com$&quot;) { set $domain $1; } # 其他配置...} 首先监听的端口为 80 二级域名则用一个通配符 * 代替 接下来在 if 语句用，通过正则表达式提取出二级域名的名字，交给变量 $domain，以备后用。 接下来配置转发规则： 1234567891011121314location / { if ($domain ~* &quot;vhr&quot;) { proxy_pass http://itboyhub.com:8082; } if ($domain ~* &quot;vblog&quot;) { proxy_pass http://itboyhub.com:8081; } tcp_nodelay on; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; index index.html;} 当定义的 $domain 中包含 vhr 字符时，将请求转发到 http://itboyhub.com:8082 当定义的 $domain 中包含 vblog 字符时，将请求转发到 http://itboyhub.com:8081 最后再配置将代理服务器收到的用户的信息传到 real server 上 另一方面，由于默认的后端首页是 /index.html，如果用户直接访问 vblog.itboyhub.com 或者 vhr.itboyhub.com，会被权限管理机制拦截（会自动重定向到 /login_p），因此，如果用户访问地址中没有 /index.html ，则自动添加上 /index.html，配置如下： 12345678location /login_p { if ($domain ~* &quot;vhr&quot;) { rewrite ^/(.*)$ http://vhr.itboyhub.com/index.html permanent; } if ($domain ~* &quot;vblog&quot;) { rewrite ^/(.*)$ http://vblog.itboyhub.com/index.html permanent; }} 注意，这行配置在 location / 之前进行配置，这里两个 if 的含义和前面的一样，不再赘述。 OK，如此之后我们的配置就算是完成了（上面 nginx 完整的配置文件小伙伴可以在公众号后台回复 nginx.conf 获取​）。 接下来我们就可以通过如下两个二级域名访问这两个开源项目了，小伙伴们赶紧试一把吧。 vblog.itboyhub.com vhr.itboyhub.com 4. 结语最后，再向小伙伴们安利一把这两个开源项目： https://github.com/lenve/vhr https://github.com/lenve/VBlog 如果你要学习 Spring Boot + Vue 前后端分离项目，这两个是不可多得的好资料。 其中 V 部落无论是从技术点还是业务上来说，都要简单一些，所以如果你是新手，可以先看看 V 部落。微人事虽然稍微复杂一点，但好在松哥配有完整的开发文档，照着开发文档，相信大家也能理解大部分的功能。文档如下： 如果大家在部署的过程中遇到问题，也可以参考松哥手把手的部署视频： 微人事项目部署视频教程 好了，本文说到这里，小伙伴们有问题欢迎留言讨论。","link":"/2019/0920/springboot-vue-online.html"},{"title":"天天吹微服务，单体应用有啥不好？","text":"单体应用确实有问题！ 最近在研究微服务架构，有一点点心得，打算在公众号上写几篇文章和大家慢慢分享下。 这个话题有点大，我会分几篇文章和大家慢慢说，今天就先来说说传统的单体应用有哪些弊端，正是因为单体应用存在的弊端，使得我们不得不考虑发展微服务。 人类发展的历史就是一个社会分工不断细化的历史，从这个角度来讲，微服务这种将一个复杂的大项目拆分为众多小项目，然后程序员分工合作，共同完成项目，这种协作方式是符合历史潮流的。 这是我们站在今天的角度来说的，曾经的单体应用也是先进生产力的代表。 但是，随着互联网的发展，我们对一个系统的要求越来越高，单体应用已经很难适应当前的开发，因此在回答我们为什么要使用微服务这个问题之前，我们有必要来聊一聊单体应用目前都面临哪些问题。 面临的问题1.项目过度复杂你要创建一个简单的用户管理系统，二话不说，直接创建 Maven 项目然后开干就完事了，这没问题，因为这很简单。 但是你要说想搞一个淘宝网站，或者你想搞一个用友 U8 系统，那你恐怕就得先慢慢设计系统架构了。单体应用，由于就是一个项目，所有的功能都是写在一个项目中，不可避免的出现项目过度复杂的情况。而且这种复杂情况会不断恶化。 有的小伙伴可能有这样的经验，刚入职了一家公司，新接手了一个项目，上面催的很急，让你赶快修复几个 bug ，项目复杂，光是实体类的包就有好几个 bean、model、pojo 等，一个项目被很多人经手之后，到你手里，早已经一团乱麻，你小心翼翼尽量不碰触到已有的功能，终于修完了几个 bug，搞了俩礼拜，你觉得这个项目太坑爹了，不想干了，于是接盘侠从你手里接到了一个复杂度又上升了一步的项目。 就这样，一个原本简简单单的单体项目，在变复杂的路上一去不复返。 2.开发速度缓慢单体应用开发速度缓慢，因为单体应用复杂了之后，项目变得异常臃肿而且庞大，每一次编译构建、运行以及测试，都需要花费大量时间，而且如果测试有问题，又得从头来一遍，注意，这里的每一次从头编译构建等都是整个项目的从头编译构建。 即使你可能只要修改某一个参数，你也得把上面整个流程走一遍，相当于每一次的修改都是牵一发而动全身的操作。 速度没法快。 3.不易扩展项目中不同模块对计算机的性能要求不一样，例如使用 Redis 来保存了大量的热点数据，那么我们希望服务器的内存非常大，另外有一个模块涉及到了图片处理，我们又希望服务器的 CPU 非常强，如果是单体应用部署的话，那么这些条件服务器都要满足。 4.技术栈不易扩展单体应用还有一个劣势就是技术栈不易扩展，一旦你选定了某一个技术栈来开发项目，以后很难在技术栈上做切换。有的公司还会自己搞一套系统，这种在当时看起来好像都没有啥问题，可是经过几年之后，回头再看，已经很过时了，很 low 了，当初设计系统的人可能已经离职了，刚入职的新手也不敢动这个老古董，只能在这个老古董上面忍痛开发。 有的时候，有一个服务需要处理高并发，你很想用 Go 语言来做，可是做不到，没法引入其他语言。 这些都是单体应用的劣势，如果有微服务，上面这些问题都将得到解决。 曾经的优势当然，事物都是有两面性的，单体应用也有它自己的优势，例如： 开发简单，一个 IDE 就可以快速构建出一个单体应用 测试简单 部署简单，Tomcat 安装好之后，应用扔上去就行了 集群化部署也很容易，多个 Tomcat + 一个 Nginx 分分钟就搭建好集群环境了 这么多优势，还是难掩劣势。 不过大家在做项目的时候，还是要结合实际情况来选择，不能因为微服务厉害，所有项目都是微服务，如果你仅仅只想做一个用户的增删改查，那么很明显，创建一个简单的单体应用是最合适的。 好了，本文主要和大家分享了传统单体应用存在的一些问题，正是因为这些问题，我们需要引入微服务，下篇文章，我们就来看看微服务有哪些优势。 参考资料： [1] Chris Richardson.微服务架构设计模式[M].北京：机械工业出版社，2019.","link":"/2019/0729/microservice.html"},{"title":"学艺不精，总是掉坑！前后端分离历险记","text":"Spring Boot + Vue 这一对技术栈目前看来可以说是非常的火热，关于 Spring Boot 松哥已经写过多篇教程，如： 40 篇原创干货，带你进入 Spring Boot 殿堂！ 前后端分离的文章也写过好几篇了，例如： 一个Java程序猿眼中的前后端分离以及Vue.js入门 前后端分离开发思路探讨 前后端分离时代，Java 程序员的变与不变！ 相信大家也从中学到了不少干货。 老实说，前后端分离其实并不难，前后端分离之后，Java 工程师只需要专心写页面就可以了，在我看来工作可比以前轻松多了。 如果让一个专业的前端工程师来写前端页面，其实也不难，Vue 算是三大前端框架中最容易上手的了。 那怎么样就有难度了呢？ 让同一个人既写前端又写后端！ 我知道很多小伙伴在这里总是想不通，很多人 clone 了松哥在 GitHub 上的开源项目下来之后，问的最多的问题就是前后端是怎么通信的？跨域是怎么解决的？刚好松哥最近在这里踩了个坑，就来和大家聊一聊这个问题。 不是跨域的跨域如果你直接在项目中引入 Vue，像用 jQuery 那样用 Vue，那没什么问题，你应该也不会有跨域的疑问。但是如果你做的是单页面应用(SPA)，那么必然会有这样的疑问，跨域问题怎么搞！ 因为在单页面应用中，前端项目可以单独通过 node 启动，它单独占用一个端口，后端项目启动后也是另外一个端口，此时从前端发送请求到后端，由于两者处于不同的端口之上，因此必然存在一个跨域问题。 但是大家想想，这个跨域有可能只是在开发环境下存在，生产环境下有可能不存在。因为当项目开发完成之后，我们对前端项目进行打包，打包后部署在 Nginx 上或者直接拷贝到后端项目中运行都可以（一般使用前者）： 如果是前者，后端接口也通过 Nginx 进行映射，这个时候就不会存在跨域问题了 如果是后者，那就更简单了，部署的时候前后端代码放在一起，更不会有跨域问题了 因此，解决这个所谓的 “跨域” 问题，我们不能按照传统的思路来（通过 JSONP 或者 CORS），因为在项目真正上线后，所谓的跨域问题可能就会消失。 那么这个问题怎么解决呢？我们可以在前端 nodejs 中配置请求转发。 配置请求转发其实不难，不过 vue-cli2 和 vue-cli3 的写法稍有不同，这也是我前一段时间踩坑的地方。 vue-cli2 方案如果我们使用的 vue-cli2 来创建的 SPA 应用，创建成功之后，在项目的 config 目录下有一个 index.js 文件，在这个文件中，我们可以进行请求转发配置，如下图： 配置内容如下： 123456789101112131415161718192021module.exports = { dev: { // Paths assetsSubDirectory: 'static', assetsPublicPath: '/', proxyTable: { '/': { target: 'http://localhost:8082', changeOrigin: true, pathRewrite: { '^/': '' } }, '/ws/*': { target: 'ws://127.0.0.1:8082', ws: true } }, ... } proxyTable 就是我们配置的转发路由表。这个里边我们一共配置了两个规则： 第一个是拦截所有 HTTP 请求，将之转发到后端服务器上（前端默认端口是 8080），后端的端口是 8082。至于拦截规则 / ，大家可以自定义，根据实际情况来写，例如所有的 HTTP 请求都有一个统一的前缀 api，那么这里就可以写 /api。 第二个是拦截所有的 websocket 请求进行转发，我这里给所有的 websocket 请求取了一个统一的前缀 /ws 如果你有更多的拦截规则，继续在这里配置就可以了，这些配置只会在开发环境下生效，当项目编译打包时，这些配置是不会打包进去的，也就是说，项目发布的时候，这些配置是失效的，这个时候我们通过 Nginx 或者将前端代码拷贝到后端，就可以解决生产环境下的跨域问题了（相当于开发时候的跨域在生产环境下不存在）。 相对来说，vue-cli2 在这里的配置还比较容易。 vue-cli3 方案vue-cli3 去年出来后，当时就尝了一把鲜，但是可能 vue-cli2 用久了，一时半会还不愿意接受 vue-cli3 ，于是尝鲜完了之后就放下了，没怎么用了。直到前两天，新项目尝试了一下 vue-cli3，结果在请求转发这块就掉坑里了。 一开始没多想，还是 vue-cli2 里边的老办法，只不过是在 vue-cli3 创建的项目的 vue.config.js 文件中进行配置，文件位置如下图： 注意，使用 vue-cli3 创建的 SPA 应用，没有 config 目录了，因此请求转发的配置我们要在 vue.config.js 这个配置文件中来配置。 一开始我直接把 vue-cli2 中的请求转发配置拷贝过来，这样发送 HTTP 请求倒是没问题，但是 websocket 请求一直有问题，后来经过仔细分析，发现这两者在请求转发配置上有一点点差异，我们来看看 vue-cli3 中的请求转发配置(这也是我这里 vue.config.js 文件的完整内容)； 1234567891011121314151617181920let proxyObj = {};proxyObj['/ws'] = { ws: true, target: \"ws://localhost:8081\"};proxyObj['/'] = { ws: false, target: \"http://localhost:8081\", changeOrigin: true, pathRewrite: { '^/': '' }};module.exports = { devServer: { host: 'localhost', port: 8080, proxy: proxyObj }} 首先我们创建一个 proxyObj 用来放各种代理对象，至于代理的内容这里的则和 vue-cli2 中的没有太多差异。要注意的是，HTTP 请求代理中，多了一个属性 ws: false，用过 vue-cli3 同学可能发现了，如果不加这个属性，浏览器控制台会一直报连不上 socket 的错，加上就没事了。 最后在 devServer 中指定项目的 host 和 port ，然后再配置一下 proxy 对象就可以啦。 这就是我们在 vue-cli3 中请求的配置。 不过这里的配置老实说没有什么难度，做过一次就会啦，要是没做过，头一次可能得折腾半天。 结语很多小伙伴一直对于前后端分离开发，前后端请求是如何对接的一直有疑问，希望这篇文章能够给你一些启发。如果看懂了，可以点个在看或者转发支持下哦。","link":"/2019/0818/springboot-vue-axios.html"},{"title":"完结撒花！129 集 21 个小时，松哥自制的 Spring Boot2 系列视频教程杀青啦！","text":"松哥的 Spring Boot 教程分为几个阶段。 2016松哥最早在 2016 年底的时候开始写 Spring Boot 系列的教程，记得当时在广州上班，年底那段时间在深圳出差，在深圳人生地不熟，下班回到酒店，就开始写博客，写 Spring Boot 教程。 我写的 Spring Boot 教程，不敢说是顶呱呱，但是我相信对大家来说绝对是有用的。我在 CSDN 上写了 400 多篇原创干货，其中访问量最高的几篇竟然都是 Spring Boot 相关的： Spring Boot 火爆程度可见一斑。不过这些都是基于这是基于早期的 Spring Boot 版本写的（1.4.x）。 Spring Boot 也算是业界有名的版本帝，版本更新非常快，这也从侧面说明了 Spring Boot 发展速度之快。于是松哥的教程一直没有停。 2017我在 2017 年推出了两个 Spring Boot + Vue 前后端分离项目。目前在 GitHub 上 star 数分别超过 8.8k 和 2.8k（公司倒闭 1 年了，而我当年的项目上了 GitHub 热榜）。 https://github.com/lenve/vhr https://github.com/lenve/VBlog 为什么这两个项目这么火呢？我也分析过原因，单纯的 Spring Boot 并不难，单纯的 Vue 也不难，相对于 React 和 Angular ，Vue 算是最容易上手的前端框架了。但是要把前后端结合起来，这就有难度了，对前端工程师而言，数据库、Java、Redis 等等，都要花时间去学习，对后端工程师而言，前端的 ES6、webpack、前端工程化、Vue 等等也都要花时间去研究。 而我这两个开源项目，则打通了前后端，从一个 Java 工程师的角度，带领小伙伴既写后端接口，又写前端页面，快速实现一个常规的企业后台管理系统。 时代变了，单兵作战、快速迭代才有未来。从这个角度来讲，每个人都不应只专注于后端的 CRUD，我觉得这是这两个开源项目受欢迎的原因。 20182018 年，应清华社夏老师的邀请，出版了 《Spring Boot + Vue 全栈开发实战》 一书。把 Spring Boot 开发中的知识点做了一番仔细的整理，同时也在自己脑海中将 Spring Boot 教程体系化。 新书出版至今，已经加印多次，还被国内某一本大学选作教材（我的第一本书，被选作大学教材了！）。 加了很多读者的微信，也收到读者不少反馈。我发现一些很简单的知识点，大家照着书写还是有问题，虽然我也提供了很多配套案例，可是还是会收到不少小伙伴的求助，很多东西搞不定。 于是，继续出教程…. 2019时间到了 2019 年，Spring Boot 又经过了好几次版本变更，我自己也写了不少新版教程： 40 篇原创干货，带你进入 Spring Boot 殿堂！ 还利用业余时间整理了一个电子书出来： 可以说，在 Spring Boot 布道的路上从未停止。 除了这些图文教程之外，松哥最近也抽时间录制了一套 Spring Boot 视频教程，这套教程分为两个阶段： Spring Boot 精讲系列 Spring Boot + Vue 项目实战系列 目前第一阶段的视频已经录制完毕，共 129 集 21 个小时，全程高能无废话，可以说是满满的干货，大家可以看一下目录： 从 9 月份开始，我将开始录制第二阶段的内容，第二阶段我会手把手带领大家做一个 Spring Boot + Vue 的实战项目，具体的项目就是我在 GitHub 上的开源项目 vhr(https://github.com/lenve/vhr)，该项目目前已经超过 8.8k star。这个项目我会带领大家从头开始搭建 Spring Boot + Vue 前后端分离环境，权限设计，RESTful 接口设计等，预计两个月之内更新完毕。 看过我博客的小伙伴应该知道，我的博客的思路清晰，小伙伴按照我博客的思路都能够做出来效果，我的视频教程和博客的风格一致，一样也是思路清晰条理清楚，这不是我的自夸，有小伙伴的评价为证： 除了这两个视频之外，后期也会录制其他视频教程，目前确定的有 Cloud 和 Redis，其他的还在规划中，不过可以确定的是，每个月都会发布我自己录制的视频教程。 这些视频的录制，我花费了巨大的时间成本，很多时候我都是晚上十二点才到家，然后早上六点起来录视频,录到八点半，然后去上班： 晚上回到家，除了写博客，还要对录好的视频剪辑，去噪，这是一个细活： 巨大的时间付出，保证了视频的质量，当然也决定了这是一套付费视频。 我自己还在网上搜集了很多别人录制的视频，这些视频对我来说没有多大成本，都是网络上找的，因此我都免费送给大家了，在我公众号底部菜单里有免费视频，这些免费视频大家都可以领取，我不会拿这些随处可见的视频来卖钱。 如果大家想要试看视频，可以参考如下两篇文章： Spring Boot 整合 Spring Session Spring Boot 中 CORS 解决跨域 欢迎大家加入星球，一起学习进步！现在直接扫码加入星球需要 199，这里我提供另外一个优惠的方式，大家可以加我微信，发红包只要 119，然后我手动拉你进星球。星球上每有一个课程完结的时候，就会提升一次价格，早点加入就是优势。同时为了保证加入星球的小伙伴的权益，我可以向大家保证，你在其他地方不会看到一模一样的免费的整套视频教程，因为这些视频都是我自己录制的，全部都是加密之后发布的，所以请大家放心。 感谢大家信任。","link":"/2019/0825/springboot-video.html"},{"title":"干货最新版 Spring Boot2.1.5 教程+案例合集","text":"最近发了一系列的 Spring Boot 教程，但是发的时候没有顺序，有小伙伴反映不知道该从哪篇文章开始看起，刚好最近工作告一个小小段落，松哥就把这些资料按照学习顺序重新整理了一遍，给大家做一个索引，大家照着索引就可以由浅入深的学习了。 松哥刚开始写这个系列的时候最新版是 Spring Boot2.1.4 ，后来写着写着版本升级了变成 Spring Boot2.1.5 了，于是我又用 Spring Boot2.1.5 接着写，因此索引中的教程主要是这两个版本的教程。 可能有人觉得小版本的变化差异不大，事实上也确实如此，不过变化不大不意味着没有变化，给大家随便举两个例子： 在整合 Redis 时，Spring Boot2.1.4 不用引入 Spring Security，而 Spring Boot2.1.5 则需要引入 Spring Security。 再比如 Spring Security 中的角色继承，在 Spring Boot2.0.8 之前和之后的写法完全不同，这些差异松哥也给大家细细剖析了。 这一系列教程不是终点，而是一个起点，松哥后期还会不断完善这个教程，也会持续更新 Spring Boot 最新版本的教程，希望能帮到大家。教程索引如下： 创建一个 Spring Boot 项目，你会几种方法？ 这一次，我连 web.xml 都不要了，纯 Java 搭建 SSM 环境 你真的理解 Spring Boot 项目中的 parent 吗？ 一文读懂 Spring Boot 配置文件 application.properties ！ Spring Boot中的yaml配置简介 Spring Boot 中的静态资源到底要放在哪里？ 极简 Spring Boot 整合 Thymeleaf 页面模板 Spring Boot 中关于自定义异常处理的套路！ Spring Boot中通过CORS解决跨域问题 SpringMVC 中 @ControllerAdvice 注解的三种使用场景！ Spring Boot中，Redis缓存还能这么用！ Spring Boot 操作 Redis，三种方案全解析！ Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！ 另一种缓存，Spring Boot 整合 Ehcache 徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！ Spring Boot 定义系统启动任务，你会几种方式？ 干货|一文读懂 Spring Data Jpa！ Spring Boot数据持久化之JdbcTemplate Spring Boot多数据源配置之JdbcTemplate 最简单的SpringBoot整合MyBatis教程 极简Spring Boot整合MyBatis多数据源 Spring Boot 中 10 行代码构建 RESTful 风格应用 Spring Boot 整合 Shiro ，两种方式全总结！ 干货|一个案例学会Spring Security 中使用 JWT! Spring Security 中的角色继承问题 Spring Security 登录添加验证码 SpringSecurity登录使用JSON格式数据 Spring Boot 中实现定时任务的两种方式! SpringBoot整合Swagger2，再也不用维护接口文档了！ 整理了八个开源的 Spring Boot 学习资源 另外，还有一件重要的事，就是松哥把微信公众号中文章的案例，都整理到 GitHub 上了，每个案例都对应了一篇解读的文章，方便大家学习。松哥以前写博客没养成好习惯，有的案例丢失了，现在在慢慢整理补上。 GitHub 仓库地址：https://github.com/lenve/javaboy-code-samples，欢迎大家 star。已有的案例如下图： 好了，这就是松哥说的干货，大家撸起袖子加油学吧！","link":"/2019/0614/springboot-resources.html"},{"title":"微人事项目视频教程已经开始更新，国庆节有事情做啦！","text":"在正式介绍松哥最近在连载的微人事视频之前，我想要和大家先聊聊前后端分离到底难在哪里？ 老实说，前后端分离不难。 相反，前后端分离之后，对于后端工程师而言，事情反而变简单了。 在前后端不分的年代，很少有纯粹的后端工程师，一个 Java 工程师，多多少少得会一点 js、会一点 jQuery 吧？我相信大部分做过开发的小伙伴都有这样的经验。然而，当前后端分离之后，后端工程师的工作大大的减轻了，后端的工作也更加纯粹了，如果没有其他抱负，大概可以安安静静做一个接口仔了。 但是我相信，在程序员这个群体中，应该没有人愿意做一个安安静静的接口仔。想要提高自己的技能，全栈就是一个方向。然而前后端分离时代的全栈工程师，门槛可比从前的前后端不分时代的全栈工程师高多了。 为什么这么说呢？ 前端经过这几年的发展，早已经变得非常强大，前端工程化、webpack、单页面应用/SPA，各种概念层出不穷。这种情况下，我们后端工程师想要去前端领域分一杯羹，进而提升自己的竞争力，着实得花点功夫。其实我一直觉得，目前前端的这些东西，对一个专业的前端工程师而言，其实应该都不算难，我们后端的 Spring Boot 这些，对一个专业的 Java 工程师也不算难，但是如果让一个人既写前端又写后端，多多少少有点难度，这些难度松哥从一个 Java 工程师的角度总结了，主要集中在以下几个方面： 前后端分离开发，开发环境下前后端的通信到底是怎么进行的？因为前端在 nodejs 上启动，有自己的端口，后端在 Tomcat 或者 Jetty 上启动，也有自己的端口，这种看似跨域的问题要怎么解决？ 权限管理要怎么做？以前大家做权限管理就是各种请求拦截，如果请求权限不足，则转发到某一个页面去，前后端分离之后，无论什么情况，后端都是只返回 JSON，那么此时权限管理要怎么做？ 前端的单页面应用（SPA）到底是怎么回事？有哪些优势？ 前端组件化又是什么鬼？ webpack 到底是什么？ 前后端分离项目的部署 这里的很多问题其实并不难，只是你缺乏一个令人恍然大悟的解释。且听松哥一一道来。 作为一个专业的后端工程师，当我们去学习很多前端概念的时候，如果能够把这些概念和我们 Java 中的某一个概念或者工具对应起来，你就会有一种恍然大悟的感觉，原来是这么回事！其实开发本语言和工具本来就是相似的，大部分语言不就是互相借鉴产生的么（例如 C# 和 Java），因此这些开发中的概念、工具等也都具有高度的相似性，如果能把前端的概念和 Java 中的一些概念对应起来理解，那就太棒啦！ 然而我们在网上看到的大部分教程，前端就是专业的前端，后端就是专业的后端，很少有把这两个联合起来讲，不免有点遗憾。 松哥在 2018 年年初做微人事项目，就是考虑到这种前后端分离的案例太少了，我想打通前后端的任督二脉，很多小伙伴也亟需一个前后端分离的案例来练手，因此才有了后来的 V 部落和微人事项目。其中微人事项目我还写了一个非常详细的开发文档，当时心想着小伙伴们对照的开发文档应该就能搞出来微人事。 但是事实证明，还是有点乐观了。有不少小伙伴通过微人事项目添加了我的微信，问了很多问题，其实大部分问题都不难，但是要给大家解释清楚又非三言两语能说清，因此，我才决定在 Spring Boot 系列完结之后，继续录制这一套微人事项目的手把手教程，到目前为止已经录制了接近 50 集了，以下是部分视频： 在这套视频中，松哥从零开始，带领大家做一个和我的开源项目微人事一模一样的项目。我会从一个 Java 工程师的角度来和大家聊一聊全栈开发（Spring Boot + Vue）中的弯弯绕绕，很多前端的概念我会对照着 Java 中的一些工具来跟大家解释，这样可以帮助大家更好更快的理解并掌握前端的概念。我觉得这是非常难得的，你可以在网上找到专业的后端教程，也可以找到专业的前端教程，但是前后端结合的，老实说，并不多。 看过我博客的小伙伴都知道，松哥的博客力求简单易懂，让大家都看得明白，做得出效果，视频也是延续了这样的风格。无论是已经连载完毕的 Spring Boot 还是目前正在连载的微人事，都受到小伙伴的好评： 还有很多小伙伴把松哥视频教程推荐给他们身边的同事朋友，我想这足以说明松哥视频的质量了吧。（要是很坑的话，估计也没人推荐吧） 真的挺感动，这都是满满的信任。 当然视频的录制，还是和以前一样辛苦，早上六点起来录视频，录到八点半去上班，晚上回到家里，对视频进行二次处理，去噪，重新渲染，只是为了给小伙伴们一个更加清晰的视频教程： 当然这些付出也决定了这是一套付费教程。 我自己还在网上搜集了很多别人录制的视频，这些视频对我来说没有多大成本，都是网络上找的，因此我都免费送给大家了，在我公众号底部菜单里有免费视频，这些免费视频大家都可以领取，我不会拿这些随处可见的视频来卖钱。 欢迎大家加入星球，一起学习进步！趁着国庆节弯道超车。现在直接扫码加入星球需要 199，这里我提供另外一个优惠的方式，大家可以加我微信，发红包只要 119，然后我手动拉你进星球。星球上每有一个课程完结的时候，就会提升一次价格，早点加入就是优势。同时为了保证加入星球的小伙伴的权益，我可以向大家保证，你在其他地方不会看到一模一样的免费的整套视频教程，因为这些视频都是我自己录制的，全部都是加密之后发布的，所以请大家放心。 感谢大家信任。","link":"/2019/1014/vhr-video.html"},{"title":"想让自己的项目6666，可是 Chrome 不答应！","text":"读万卷书，行万里路！有的技能可以从书里学会，有的技能却需要在实战中打怪升级慢慢掌握，今天就来和大家聊一个很多小伙伴经常遇到的问题。 缘起有人向松哥反映，在搭建微服务分布式配置中心 Spring Cloud Config 时，如果将端口设置为 6000，总是访问不成功，像下面这样： 如果换成 Tomcat 默认的 8080 就可以访问了。 其实不止 6000，如果你配置成 6666 ，也是无法访问成功的！ 分析刚入行或者经验欠缺的小伙伴应该很容易遇到这个问题，松哥就来和大家稍微说一说这个问题。 首先，当我们将项目的端口设置为 6000 之后，并非仅仅只有 Chrome 无法访问，Firefox、Safari 也是无法访问的，反而是经常被大家忽略的坐在角落的 IE/Edge 这对难兄难弟可以访问！看看 Safari 访问 6000 端口怎么说： 再看看 Firefox 访问 6000 端口怎么说： 但是 Edge 就可以访问，如下： 看到这里，大家首先可以确认出现这个问题，和你的代码没有关系！是不是可以松一口气了！ 这个问题实际上是由 Chrome 默认的非安全端口限制导致的，除了上文说的 6000，还有其他端口也无法在 Chrome 、Firefox 以及 Safari 中访问（具体端口见文末列表）。 这些无法访问的端口大部分都是小于 1024，小于 1024 的端口大家应该会很少使用，基本上不会在这个上面栽跟头。大于 1024 的端口也并非每一个都可以使用，这才是容易犯错的地方。 解决那么问题要怎么解决呢？两个思路： 修改项目端口（推荐） 修改浏览器配置，使之允许访问非安全端口 推荐大家使用第一种方案，省事！ 如果要使用第二种方案： Chrome 修改办法如下： 右键单击Chrome快捷方式 -&gt; 目标 -&gt; 末尾添加参数：--explicitly-allowed-ports=6000 Firefox 修改办法如下： 浏览器地址栏输入 about:config 打开配置页面，然后搜索 network.security.ports.banned.override ，将其值设为 6000 即可(如果没有则右键单击新建即可)。 受限端口列表： 端口 原因 1 tcpmux 7 echo 9 discard 11 systat 13 daytime 15 netstat 17 qotd 19 chargen 20 ftp data 21 ftp access 22 ssh 23 telnet 25 smtp 37 time 42 name 43 nicname 53 domain 77 priv-rjs 79 finger 87 ttylink 95 supdup 101 hostriame 102 iso-tsap 103 gppitnp 104 acr-nema 109 pop2 110 pop3 111 sunrpc 113 auth 115 sftp 117 uucp-path 119 nntp 123 NTP 135 loc-srv /epmap 139 netbios 143 imap2 179 BGP 389 ldap 465 smtp+ssl 512 print / exec 513 login 514 shell 515 printer 526 tempo 530 courier 531 chat 532 netnews 540 uucp 556 remotefs 563 nntp+ssl 587 stmp? 601 ?? 636 ldap+ssl 993 ldap+ssl 995 pop3+ssl 2049 nfs 3659 apple-sasl / PasswordServer 4045 lockd 6000 X11 6665 Alternate IRC [Apple addition] 6666 Alternate IRC [Apple addition] 6667 Standard IRC [Apple addition] 6668 Alternate IRC [Apple addition] 6669 Alternate IRC [Apple addition] 好了，这就是本文和大家聊的浏览器的一个小小的坑，希望能帮助到经验欠缺的小伙伴。有问题欢迎留言讨论。 最新版 Spring Boot2.1.5 案例：https://github.com/lenve/javaboy-code-samples ，欢迎大家 star。","link":"/2019/0617/chrome-port.html"},{"title":"我的第一本书，被选作大学教材了！","text":"这真是意料之外！ 去年年初，接受清华大学出版社夏老师的邀请，下定决心开始写我的第一本书，战战兢兢早起晚睡，好几个月都没有陪女票出去玩，辛辛苦苦终于在年底的时候新书顺利上市！ 销量出乎意料新书上市之后，刚开始想的是首印的能卖完就不错了，千万别砸到出版社手里了。松哥的公众号当时也只有 4000 多人，而且一直断断续续更新，没啥影响力，新书出来后，宣传了一波之后就偃旗息鼓没再管了，结果后来发生的事情证明我的担心多余了。 2 月份回家过年，3 月份的时候收到出版社的消息第二次加印，4 月份第三次加印，5 月份第四次加印，下个月第五次加印！这一波操作真是亮瞎了。 这个销量真是出乎我的意料！从写书的第一天开始，从来不敢想会有这一天。 不停的加印，也让出版社的编辑老师又找我约稿了，希望能再来一本，可是最近几个月，每天敲键盘时间从早九点到晚十一点，有点快扛不住了，没敢答应，但是这让我看到了未来更多的可能性。 选作大学教材前天，收到了出版社编辑老师的消息，说是有高校老师想把这本书选作下学期的教材，希望我能提供一份配套的 PPT 课件，我立马就答应了，熬夜赶制了一份出来。 其实新书出版第一周就有一个老师加我微信，后来又陆陆续续有好几位高校老师加我微信，一起探讨书中的内容，我自己也获益良多。这次是一个山东的一本大学将这本书选作教材，我还是有些意外，具体是哪个学校我就不透露了，我看了下，这个高校去年在陕西招生分数高一本线 50 分左右。 其实对于经常写博客的人来说，写书真是一个信手拈来的机会，遍地都是。但是并不是每一本书都有机会进入大学，成为学生的课本，甚至最后成为学生考试的噩梦（松哥的书估计不会成为考试的噩梦）！ 现在国内技术图书市场，很多书生命力很短，当然原因是多方面的。但是据出版社那边的反馈，如果这本书被选作高校教材，老师讲熟练了，后面还会继续使用，也就是说后面还会继续印刷，我觉得这是最有价值的地方，相信没有作者希望自己的书昙花一现吧！ 为什么写书书卖了这么多，有人要问了，松哥一定赚的盆满钵满了吧？老实说，目前拿到手的稿费还没有一个月的薪水多。当然也因为有一部分稿费还没结算，不过写书不挣钱，这个倒是真的。如果奔着赚钱，我去年应该不会写这本书，因为写书是一个非常花费时间精力的事情，写书的时间要是拿去做其他事，会有更大的收益。 那为什么要写呢？ 首先，我觉得这个世界不能白白走一遭，总得留下一点东西，几年之后，和孩子吹牛，咱也得有个话题啊！ 第二，我觉得这是对自己的一个证明，跨行做计算机，其实我一直都不是很自信，虽然我觉得自己的代码写的还不赖，但是总觉得差点什么！通过这本书我想告诉别人，我不是计算机科班，我也不是一流名校，但是我能在这个行业中做的很好，能够得到多数同行的认可，这就够了。 第三，作为一个农村出来的孩子，能写一本书，也能让逐渐老去的父母脸上有光，这算是当时的写作的一点私心吧。 结语写书很辛苦，但是获得大家的认可还是很开心。因为《Spring Boot + Vue 全栈开发实战》这本书，认识了很多优秀的人，半年里，自己也提高了很多。因此我强烈建议每一个技术人都应该有自己的博客，记录自己技术成长的点点滴滴，说不定博客哪天给你带来一个意料之外的机会，这谁说的准呢？","link":"/2019/0626/springboot-vue.html"},{"title":"手把手教你用 Java 发送邮件，不用框架！","text":"邮件发送也是一个老生常谈的问题了，代码虽然简单，但是许多小伙伴对过程不太理解，所以还是打算和各位小伙伴聊聊这个话题。 邮件协议我们经常会听到各种各样的邮件协议，比如 SMTP、POP3、IMAP ，那么这些协议有什么作用，有什么区别？我们先来讨论一下这个问题。 SMTP 是一个基于 TCP/IP 的应用层协议，江湖地位有点类似于 HTTP，SMTP 服务器默认监听的端口号为 25 。看到这里，小伙伴们可能会想到既然 SMTP 协议是基于 TCP/IP 的应用层协议，那么我是不是也可以通过 Socket 发送一封邮件呢？回答是肯定的。 生活中我们投递一封邮件要经过如下几个步骤： 深圳的小王先将邮件投递到深圳的邮局 深圳的邮局将邮件运送到上海的邮局 上海的小张来邮局取邮件 这是一个缩减版的生活中邮件发送过程。这三个步骤可以分别对应我们的邮件发送过程，假设从 aaa@qq.com 发送邮件到 111@163.com ： aaa@qq.com 先将邮件投递到腾讯的邮件服务器 腾讯的邮件服务器将我们的邮件投递到网易的邮件服务器 111@163.com 登录网易的邮件服务器查看邮件 邮件投递大致就是这个过程，这个过程就涉及到了多个协议，我们来分别看一下。 SMTP 协议全称为 Simple Mail Transfer Protocol，译作简单邮件传输协议，它定义了邮件客户端软件于 SMTP 服务器之间，以及 SMTP 服务器与 SMTP 服务器之间的通信规则。 也就是说 aaa@qq.com 用户先将邮件投递到腾讯的 SMTP 服务器这个过程就使用了 SMTP 协议，然后腾讯的 SMTP 服务器将邮件投递到网易的 SMTP 服务器这个过程也依然使用了 SMTP 协议，SMTP 服务器就是用来收邮件。 而 POP3 协议全称为 Post Office Protocol ，译作邮局协议，它定义了邮件客户端与 POP3 服务器之间的通信规则，那么该协议在什么场景下会用到呢？当邮件到达网易的 SMTP 服务器之后， 111@163.com 用户需要登录服务器查看邮件，这个时候就该协议就用上了：邮件服务商都会为每一个用户提供专门的邮件存储空间，SMTP 服务器收到邮件之后，就将邮件保存到相应用户的邮件存储空间中，如果用户要读取邮件，就需要通过邮件服务商的 POP3 邮件服务器来完成。 最后，可能也有小伙伴们听说过 IMAP 协议，这个协议是对 POP3 协议的扩展，功能更强，作用类似，这里不再赘述。 发送QQ邮件准备工作首先我们需要先登录 QQ 邮箱网页版，点击上方的设置按钮： 然后点击账户选项卡： 在账户选项卡中找到开启POP3/SMTP选项，如下： 点击开启，开启相关功能，开启过程需要手机号码验证，按照步骤操作即可，不赘述。开启成功之后，即可获取一个授权码，将该号码保存好，一会使用。 然后我们需要 JavaxMail 这个 jar 包，小伙伴可以直接去Maven中央仓库下载，这里不再赘述。 发送简单邮件如果我们只发送一个简单的文本，发送方式就比较简单，整个过程可以分为三步如下： 第一步：构造 SMTP 邮件服务器的基本环境 12345678Properties properties = new Properties();properties.setProperty(\"mail.host\", \"smtp.qq.com\");properties.setProperty(\"mail.transport.protocol\", \"smtp\");properties.setProperty(\"mail.smtp.auth\", \"true\");properties.setProperty(\"mail.smtp.socketFactory.class\", \"javax.net.ssl.SSLSocketFactory\");properties.setProperty(\"mail.smtp.port\", \"465\");Session session = Session.getDefaultInstance(properties);session.setDebug(true); 第二步：构造邮件 123456MimeMessage mimeMessage = new MimeMessage(session);mimeMessage.addRecipients(Message.RecipientType.TO, \"111@qq.com\");//设置收信人mimeMessage.addRecipients(Message.RecipientType.CC, \"222@qq.com\");//抄送mimeMessage.setFrom(\"1510161612@qq.com\");//邮件发送人mimeMessage.setSubject(\"测试邮件主题\");//邮件主题mimeMessage.setContent(\"Hello,这是一封测试邮件\", \"text/html;charset=utf-8\");//正文 第三步：发送邮件 1234Transport transport = session.getTransport();transport.connect(\"smtp.qq.com\", \"333@qq.com\", \"刚刚申请到的授权码\");transport.sendMessage(mimeMessage, mimeMessage.getAllRecipients());//发送邮件，第二个参数为收件人transport.close(); 复杂邮件发送复杂邮件，第一步和第三步也是一样的，只有第二步构造邮件的过程比较麻烦，那么接下来给小伙伴们演示一个发送一封图文+两个附件的邮件。要发送复杂邮件，得先熟悉三个概念，如下： MimeMessage：该类是个能理解MIME类型和头的电子邮件消息 MimeMultipart：该类定义了增加、删除以及获取邮件不同部分内容的方法 MimeBodyPart：该对象代表一个MimeMessage对象内容的一部分。每个MimeBodyPart被认为有两部分：MIME类型和匹配这个类型的内容 完整的邮件生成过程如下(第一步和第三步参考上文)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344MimeMessage mimeMessage = new MimeMessage(session);mimeMessage.addRecipients(Message.RecipientType.TO, \"111@qq.com\");//设置收信人mimeMessage.addRecipients(Message.RecipientType.CC, \"222@qq.com\");//抄送mimeMessage.setFrom(\"333@qq.com\");//邮件发送人mimeMessage.setSubject(\"测试邮件主题\");//邮件主题MimeMultipart mixed = new MimeMultipart(\"mixed\");mimeMessage.setContent(mixed);//设置整封邮件的MIME消息体为混合的组合关系MimeBodyPart attach1 = new MimeBodyPart();//创建附件1MimeBodyPart attach2 = new MimeBodyPart();//创建附件2MimeBodyPart content = new MimeBodyPart();//创建邮件正文mixed.addBodyPart(attach1);//将附件一添加到MIME消息体中mixed.addBodyPart(attach2);//将附件二添加到MIME消息体中mixed.addBodyPart(content);//将正文添加到消息体中FileDataSource fds1 = new FileDataSource(new File(\"C:\\\\Users\\\\sang\\\\Desktop\\\\1.png\"));//构造附件一的数据源DataHandler dh1 = new DataHandler(fds1);//数据处理attach1.setDataHandler(dh1);//设置附件一的数据源attach1.setFileName(\"1.png\");//设置附件一的文件名//附件二的操作与附件一类似，这里就不一一注释了FileDataSource fds2 = new FileDataSource(new File(\"C:\\\\Users\\\\sang\\\\Desktop\\\\博客笔记.xlsx\"));DataHandler dh2 = new DataHandler(fds2);attach2.setDataHandler(dh2);attach2.setFileName(MimeUtility.encodeText(\"博客笔记.xlsx\"));//设置文件名时，如果有中文，可以通过MimeUtility类中的encodeText方法进行编码，避免乱码MimeMultipart bodyMimeMultipart = new MimeMultipart(\"related\");//设置正文的MIME类型content.setContent(bodyMimeMultipart);//将bodyMimeMultipart添加到正文消息体中MimeBodyPart bodyPart = new MimeBodyPart();//正文的HTML部分bodyPart.setContent(\"&lt;h1&gt;Hello大家好，这是一封测试邮件&lt;img src='cid:2.png'/&gt;&lt;/h1&gt;\",\"text/html;charset=utf-8\");MimeBodyPart picPart = new MimeBodyPart();//正文的图片部分DataHandler dataHandler = new DataHandler(new FileDataSource(\"C:\\\\Users\\\\sang\\\\Desktop\\\\2.png\"));picPart.setDataHandler(dataHandler);picPart.setContentID(\"2.png\");//将正文的HTML和图片部分分别添加到bodyMimeMultipart中bodyMimeMultipart.addBodyPart(bodyPart);bodyMimeMultipart.addBodyPart(picPart);mimeMessage.saveChanges(); OK，Java Mail 发送 QQ 邮件就是这么简单，至于其他的如 163，sina 等，写法类似，这里我就不赘述了。 有问题欢迎留言讨论。","link":"/2019/0709/java-mail.html"},{"title":"提高性能，MySQL  读写分离环境搭建(一)","text":"这是松哥之前一个零散的笔记，整理出来分享给大伙！ MySQL 读写分离在互联网项目中应该算是一个非常常见的需求了。受困于 Linux 和 MySQL 版本问题，很多人经常会搭建失败，今天松哥就给大伙举一个成功的例子，后面有时间再和大家分享下使用 Docker 搭建环境，那样就 100% 成功了。 CentOS 安装 MySQL自己玩 Linux 松哥一般首选 Ubuntu，不过公司里边使用一般还是 CentOS 为主，因此这里松哥就以 CentOS 为例来向大家演示整个过程，今天这篇文章主要来看看 MySQL 的安装。 环境： CentOS7 MySQL5.7 具体的安装步骤如下： 检查是否安装了 mariadb，如果已经安装了则卸载： 1yum list installed | grep mariadb 如果执行结果如下，表示已经安装了 mariadb，将之卸载： 1mariadb-libs.x86_64 1:5.5.52-1.el7 @anaconda 卸载命令如下： 1yum -y remove mariadb* 接下来下载官方提供的 rpm 包 如果 CentOS 上没有 wget 命令，首先通过如下命令安装 wget： 1yum install wget 然后执行如下操作下载 rpm 包： 1wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm 下载完成后，安装rpm包： 1rpm -ivh mysql57-community-release-el7-11.noarch.rpm 检查 MySQL 的 yum 源是否安装成功： 1yum repolist enabled | grep &quot;mysql.*-community.*&quot; 执行结果如下表示安装成功： 安装 MySQL 1yum install mysql-server 安装完成后，启动MySQL： 1systemctl start mysqld.service 停止MySQL： 1systemctl stop mysqld.service 登录 MySQL： 1mysql -u root -p 默认无密码。有的版本有默认密码，查看默认密码，首先去 /etc/my.cnf 目录下查看 MySQL 的日志位置，然后打开日志文件，可以看到日志中有一个提示，生成了一个临时的默认密码，使用这个密码登录，登录成功后修改密码即可。 改密码 首先修改密码策略(这一步不是必须的，如果不修改密码策略，需要取一个比较复杂的密码，松哥这里简单起见，就修改下密码策略)： 1set global validate_password_policy=0; 然后重置密码： 12set password=password(&quot;123&quot;); flush privileges; 授权远程登录同方式一： 12grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123&apos; with grant option;flush privileges; 授权远程登录同方式二： 修改 mysql 库中的 user 表，将 root 用户的 Host 字段的值改为 % ，然后重启 MySQL 即可。 关闭防火墙MySQL 要能远程访问，还需要关闭防火墙： 1systemctl stop firewalld.service 禁止firewall开机启动: 1systemctl disable firewalld.service 总结装了这么多 MySQL ，还是 Ubuntu 下 MySQL 最好弄，其他系统多多少少总有点麻烦，本文主要和大家分享了 CentOS7 中 MySQL 的安装步骤，大伙有问题欢迎留言讨论。下篇文章和大伙分享 MySQL 读写分离环境搭建。","link":"/2019/0509/mysql-read-write.html"},{"title":"提高性能，MySQL  读写分离环境搭建(二)","text":"上篇文章和大家聊了 CentOS7 安装 MySQL5.7 ，这个大家一般装在虚拟机里边，装好了，把虚拟拷贝一份，这样我们就有两个 MySQL ，就可以开始今天的主从搭建了。 准备工作我这里有一张简单的图向大伙展示 MySQL 主从的工作方式： 这里，我们准备两台机器： 主机：192.168.248.128 从机：192.168.248.139 主机配置主机的配置就三个步骤，比较容易： 1.授权给从机服务器 12GRANT REPLICATION SLAVE ON *.* to &apos;rep1&apos;@&apos;192.168.248.139&apos; identified by &apos;123&apos;;FLUSH PRIVILEGES; 这里表示配置从机登录用户名为 rep1，密码为 123，并且必须从 192.168.248.139这个地址登录，登录成功之后可以操作任意库中的任意表。其中，如果不需要限制登录地址，可以将 IP 地址更换为一个 %。 2.修改主库配置文件，开启 binlog ，并设置 server-id ，每次修改配置文件后都要重启 MySQL 服务才会生效 1vi /etc/my.cnf 修改的文件内容如下： 1234[mysqld]log-bin=/var/lib/mysql/binlogserver-id=128binlog-do-db = cmdb 如下图： log-bin：同步的日志路径及文件名，一定注意这个目录要是 MySQL 有权限写入的（我这里是偷懒了，直接放在了下面那个datadir下面）。 binlog-do-db：要同步的数据库名，当从机连上主机后，只有这里配置的数据库才会被同步，其他的不会被同步。 server-id: MySQL 在主从环境下的唯一标志符，给个任意数字，注意不能和从机重复。 配置完成后重启 MySQL 服务端： 1systemctl restart mysqld 3.查看主服务器当前二进制日志名和偏移量，这个操作的目的是为了在从数据库启动后，从这个点开始进行数据的恢复： 1show master status; 至此，主机配置完成。 从机配置从机的配置也比较简单，我们一步一步来看： 1.在/etc/my.cnf 添加下面配置： 注意从机这里只需要配置一下 server-id 即可。 注意：如果从机是从主机复制来的，即我们通过复制 CentOS 虚拟机获取了 MySQL 实例 ，此时两个 MySQL 的 uuid 一样（正常安装是不会相同的），这时需要手动修改，修改位置在 /var/lib/mysql/auto.cnf ，注意随便修改这里几个字符即可，但也不可太过于随意，例如修改了 uuid 的长度。 2.使用命令来配置从机： 1change master to master_host=&apos;192.168.248.128&apos;,master_port=3306,master_user=&apos;rep1&apos;,master_password=&apos;123&apos;,master_log_file=&apos;binlog.000001&apos;,master_log_pos=120; 这里配置了主机地址、端口以及从机登录主机的用户名和密码，注意最后两个参数要和 master 中的保持一致。 3.启动 slave 进程 1start slave; 启动之后查看从机状态： 1show slave status\\G; 4.查看 slave 的状态 主要是下面两项值都要为为 YES，则表示配置正确： 12Slave_IO_Running: YesSlave_SQL_Running: Yes 至此，配置完成，主机创建库，添加数据，从机会自动同步。 如果这两个有一个不为 YES ，表示主从环境搭建失败，此时可以阅读日志，查看出错的原因，再具体问题具体解决。 总结本文主要和大伙说了 MySQL 主从环境搭建，这几个步骤松哥反反复复操作过很多遍，小伙伴只要按照松哥的步骤一般来说都能成功，有问题欢迎留言讨论。","link":"/2019/0513/mysql-read-write.html"},{"title":"整理了八个开源的 Spring Boot 学习资源","text":"Spring Boot 算是目前 Java 领域最火的技术栈了，松哥年初出版的 《Spring Boot + Vue 全栈开发实战》迄今为止已经加印了 3 次，Spring Boot 的受欢迎程度可见一斑。经常有人问松哥有没有推荐的 Spring Boot 学习资料？当然有！买松哥书就对了，哈哈。除了书呢？当然就是开源项目了，今天松哥整理了几个优质 Spring Boot 开源项目给大家参考，希望能够帮助到正在学习 Spring Boot 的小伙伴！ spring-boot-examples star 数 14821 项目地址：https://github.com/ityouknow/spring-boot-examples 这个项目中整合了 Spring Boot 使用的各种示例，以最简单、最实用为标准，此开源项目中的每个示例都以最小依赖，最简单为标准，帮助初学者快速掌握 Spring Boot 各组件的使用。基本上涉及到了 Spring Boot 使用的方方面面。 项目部分 demo 截图： 微人事 star 数 3333 项目地址：https://github.com/lenve/vhr 微人事是一个前后端分离的人力资源管理系统，项目采用 SpringBoot + Vue 开发。项目打通了前后端，并且提供了非常详尽的文档，从 Spring Boot 接口设计到前端 Vue 的开发思路，作者全部都记录在项目的 wiki 中，是不可多得的 Java 全栈学习资料。 项目效果图: 项目部分文档截图： mall star 数 12668 项目地址：https://github.com/macrozheng/mall mall 项目是一套电商系统，包括前台商城系统及后台管理系统，基于 Spring Boot + MyBatis 实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 项目效果图： spring-boot-pay star 数 2931 项目地址：https://gitee.com/52itstyle/spring-boot-pay 这是一个支付案例，提供了包括支付宝、微信、银联在内的详细支付代码案例，对于有支付需求的小伙伴来说，这个项目再合适不过了。 项目效果图： V 部落 star 数 1060 项目地址：https://github.com/lenve/VBlog V部落是一个多用户博客管理平台，采用 Vue + SpringBoot + ElementUI 开发。这个项目最大的优势是简单，属于功能完整但是又非常简单的那种，非常非常适合初学者。 项目效果图： springboot-plus star 数 2546 项目地址：https://gitee.com/xiandafu/springboot-plus 一个基于SpringBoot 2 的管理后台系统,包含了用户管理，组织机构管理，角色管理，功能点管理，菜单管理，权限分配，数据权限分配，代码生成等功能 相比其他开源的后台系统，SpringBoot-Plus 具有一定的复杂度。系统基于Spring Boot2.1技术，前端采用了Layui2.4。数据库以MySQL/Oracle/Postgres/SQLServer为实例，理论上是跨数据库平台。 项目效果图： litemall star 数 6436 项目地址：https://github.com/linlinjava/litemall 一个商城项目，包括Spring Boot后端 + Vue管理员前端 + 微信小程序用户前端 + Vue用户移动端，功能包括、分类列表、分类详情、品牌列表、品牌详情、新品首发、人气推荐、优惠券列表、优惠券选择、团购（团购业务有待完善）、搜索、商品详情、商品评价、商品分享、购物车、下单、订单列表、订单详情、地址、收藏、足迹、意见反馈以及客服；管理平台功能包括会员管理、商城管理、商品管理、推广管理、系统管理、配置管理、统计报表等。 项目效果图： 其他另外再向大家推荐两个优质的 Spring Boot 和 Spring Cloud 学习网站，如下： http://www.springboot.wiki http://www.springcloud.wiki 总结好了，一点点整理的资源，希望能够帮助到大家。","link":"/2019/0517/springboot-samples.html"},{"title":"是什么驱动着你不断前进?","text":"工作这些年，从来没有一点安全感，担心自己会落后，会被这个不断更新的技术世界抛弃，因此不停的写博客，写博客有人看，有人点赞有人评论，我就知道我的技术还没过时，换言之，写博客让我有一点点的安全感！ 这是输出，另一方面在不停的输入。不停的学习才让我有不断输出的资本，下面都是松哥自己购买的极客时间专栏，极客时间的课程质量都还是非常 OK 的，我自己就经常学习，大家有需要的话，可以扫描我分享的二维码购买，购买后添加我的微信（微信：ws584991843），我将极客时间给我的返现都返给大家。","link":"/2019/0528/geek-time.html"},{"title":"是时候彻底搞清楚 Spring Boot 的配置文件 application.properties 了！","text":"在 Spring Boot 中，配置文件有两种不同的格式，一个是 properties ，另一个是 yaml 。 虽然 properties 文件比较常见，但是相对于 properties 而言，yaml 更加简洁明了，而且使用的场景也更多，很多开源项目都是使用 yaml 进行配置（例如 Hexo）。除了简洁，yaml 还有另外一个特点，就是 yaml 中的数据是有序的，properties 中的数据是无序的，在一些需要路径匹配的配置中，顺序就显得尤为重要（例如我们在 Spring Cloud Zuul 中的配置），此时我们一般采用 yaml。关于 yaml ，松哥之前写过一篇文章：Spring Boot 中的 yaml 配置简介。 本文主要来看看 properties 的问题。 位置问题首先，当我们创建一个 Spring Boot 工程时，默认 resources 目录下就有一个 application.properties 文件，可以在 application.properties 文件中进行项目配置，但是这个文件并非唯一的配置文件，在 Spring Boot 中，一共有 4 个地方可以存放 application.properties 文件。 当前项目根目录下的 config 目录下 当前项目的根目录下 resources 目录下的 config 目录下 resources 目录下 按如上顺序，四个配置文件的优先级依次降低。如下： 这四个位置是默认位置，即 Spring Boot 启动，默认会从这四个位置按顺序去查找相关属性并加载。但是，这也不是绝对的，我们也可以在项目启动时自定义配置文件位置。 例如，现在在 resources 目录下创建一个 javaboy 目录，目录中存放一个 application.properties 文件，那么正常情况下，当我们启动 Spring Boot 项目时，这个配置文件是不会被自动加载的。我们可以通过 spring.config.location 属性来手动的指定配置文件位置，指定完成后，系统就会自动去指定目录下查找 application.properties 文件。 此时启动项目，就会发现，项目以 classpath:/javaboy/application.propertie 配置文件启动。 这是在开发工具中配置了启动位置，如果项目已经打包成 jar ，在启动命令中加入位置参数即可： 1java -jar properties-0.0.1-SNAPSHOT.jar --spring.config.location=classpath:/javaboy/ 文件名问题对于 application.properties 而言，它不一定非要叫 application ，但是项目默认是去加载名为 application 的配置文件，如果我们的配置文件不叫 application ，也是可以的，但是，需要明确指定配置文件的文件名。 方式和指定路径一致，只不过此时的 key 是 spring.config.name 。 首先我们在 resources 目录下创建一个 app.properties 文件，然后在 IDEA 中指定配置文件的文件名： 指定完配置文件名之后，再次启动项目，此时系统会自动去默认的四个位置下面分别查找名为 app.properties 的配置文件。当然，允许自定义文件名的配置文件不放在四个默认位置，而是放在自定义目录下，此时就需要明确指定 spring.config.location 。 配置文件位置和文件名称可以同时自定义。 普通的属性注入由于 Spring Boot 源自 Spring ，所以 Spring 中存在的属性注入，在 Spring Boot 中一样也存在。由于 Spring Boot 中，默认会自动加载 application.properties 文件，所以简单的属性注入可以直接在这个配置文件中写。 例如，现在定义一个 Book 类： 123456public class Book { private Long id; private String name; private String author; //省略 getter/setter} 然后，在 application.properties 文件中定义属性： 123book.name=三国演义book.author=罗贯中book.id=1 按照传统的方式（Spring中的方式），可以直接通过 @Value 注解将这些属性注入到 Book 对象中： 12345678910@Componentpublic class Book { @Value(\"${book.id}\") private Long id; @Value(\"${book.name}\") private String name; @Value(\"${book.author}\") private String author; //省略getter/setter} 注意 Book 对象本身也要交给 Spring 容器去管理，如果 Book 没有交给 Spring 容器，那么 Book 中的属性也无法从 Spring 容器中获取到值。 配置完成后，在 Controller 或者单元测试中注入 Book 对象，启动项目，就可以看到属性已经注入到对象中了。 一般来说，我们在 application.properties 文件中主要存放系统配置，这种自定义配置不建议放在该文件中，可以自定义 properties 文件来存在自定义配置。 例如在 resources 目录下，自定义 book.properties 文件，内容如下： 123book.name=三国演义book.author=罗贯中book.id=1 此时，项目启动并不会自动的加载该配置文件，如果是在 XML 配置中，可以通过如下方式引用该 properties 文件： 1&lt;context:property-placeholder location=\"classpath:book.properties\"/&gt; 如果是在 Java 配置中，可以通过 @PropertySource 来引入配置： 1234567891011@Component@PropertySource(\"classpath:book.properties\")public class Book { @Value(\"${book.id}\") private Long id; @Value(\"${book.name}\") private String name; @Value(\"${book.author}\") private String author; //getter/setter} 这样，当项目启动时，就会自动加载 book.properties 文件。 这只是 Spring 中属性注入的一个简单用法，和 Spring Boot 没有任何关系。 类型安全的属性注入Spring Boot 引入了类型安全的属性注入，如果采用 Spring 中的配置方式，当配置的属性非常多的时候，工作量就很大了，而且容易出错。 使用类型安全的属性注入，可以有效的解决这个问题。 123456789@Component@PropertySource(\"classpath:book.properties\")@ConfigurationProperties(prefix = \"book\")public class Book { private Long id; private String name; private String author; //省略getter/setter} 这里，主要是引入 @ConfigurationProperties(prefix = “book”) 注解，并且配置了属性的前缀，此时会自动将 Spring 容器中对应的数据注入到对象对应的属性中，就不用通过 @Value 注解挨个注入了，减少工作量并且避免出错。 总结application.properties 是 Spring Boot 中配置的一个重要载体，很多组件的属性都可以在这里定制。它的用法和 yaml 比较类似，关于 yaml 配置，大家可以参考 Spring Boot 中的 yaml 配置简介 。 本文案例我已上传到 GitHub：https://github.com/lenve/javaboy-code-samples 好了，有问题欢迎留言讨论。","link":"/2019/0530/application.properties.html"},{"title":"晚十点半之前下班是耻辱？老板干嘛老爱和员工谈创业精神？","text":"这两天又有一个话题触动了广大程序员的神经，某互联网公司高管在内部邮件中公开指责员工晚上 10:30 后没有加班，没有创业精神，表示这是公司的耻辱日。 刚看到这个消息我也是虎躯一震，经过今年年中 996.ICU 那次风波之后，很少有公司再去公开发表支持加班、支持 996 了，没想到这才没过多久，又有人往枪口上撞。 很多时候员工烦的不是加班，而是无偿加班，比无偿加班还可恶的就是再把人当傻子画个饼，制造一种这好像不是无偿加班的错觉。 这位孙姓高管说 Day One 创业精神是我们印在骨子里的基因 这话用在她自己身上可能没错，或者她对她的合伙人讲可能也没错，问题是这话是给员工讲的。你在招聘的时候写的是员工招聘还是合伙人招募呢？员工和合伙人的利益不同，要承担的责任当然就不同。不愿意分享利益却要共同承担风险，这不是耍流氓么。 有人会说，不想当将军的士兵不是好士兵，员工也应该把公司的事业当成自己的事业，努力奋斗就会有回报。 这话不假，松哥曾经就是一个想当将军的好士兵，松哥的老东家，节假日加班三倍工资，每逢节假日那些单身的同事都来公司加班，日常加班可以调休或者折算成工资，松哥离职时加的二十多天班全部折算成工资发了，其他的福利就不说了，反正公司不把员工当傻子，不会白嫖员工。在这样的公司里干活很有劲头，大部分员工都是想当将军的好士兵。 所以，做一个想当将军的好士兵也是有条件的，公司的环境决定了这个士兵是否有可能成为将军。老板如果压榨员工，总是想从员工身上薅羊毛，这样的环境下，当将军的希望很小，那也就没人想当将军了。 其实大部分程序员不缺创业精神与奋斗精神，要调动员工的积极性，就得公司给一点实在的东西，空手套白狼这种操作简直就是侮辱一个程序员的智商。 程序员谁不知道菊厂加班猛，谁不知道去阿里修福报，可是每年秋招很多人不还是挤破头皮想进去，为啥？付出和回报成比例啊！这种回报可能是物质上的，也可能是对未来职业生涯有益的一些虚拟的东西，反正你得让大家觉得我虽然辛苦工作，但是值。总不能让员工用爱发电，这不现实。 所以这位孙姓高管说员工不具备创业精神，问题可能不是出在员工身上，她应该问问自己为什么没能激发员工的创业精神和奋斗精神？ 话说回来，为什么有些老板老爱和员工谈创业精神？ 因为不想花钱。 老板不想花钱，就得和员工谈创业精神、谈奋斗，这些饼吃多了，员工一个个虚胖，又因为这些饼没营养，员工的头都秃了。这些东西还是太虚，作用有限。 领导希望员工能跟自己一条心，共同奋斗，这能理解，如果我是领导，我当然希望员工跟我一条心，但是姿势要对，该给的东西要给足，不能光画大饼，你觉得呢？","link":"/2019/1021/996.html"},{"title":"条件注解，Spring Boot 的基石！","text":"Spring Boot 中的自动化配置确实够吸引人，甚至有人说 Spring Boot 让 Java 又一次焕发了生机，这话虽然听着有点夸张，但是不可否认的是，曾经臃肿繁琐的 Spring 配置确实让人感到头大，而 Spring Boot 带来的全新自动化配置，又确实缓解了这个问题。 你要是问这个自动化配置是怎么实现的，很多人会说不就是 starter 嘛！那么 starter 的原理又是什么呢？松哥以前写过一篇文章，介绍了自定义 starter： 徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！ 这里边有一个非常关键的点，那就是条件注解，甚至可以说条件注解是整个 Spring Boot 的基石。 条件注解并非一个新事物，这是一个存在于 Spring 中的东西，我们在 Spring 中常用的 profile 实际上就是条件注解的一个特殊化。 想要把 Spring Boot 的原理搞清，条件注解必须要会用，因此今天松哥就来和大家聊一聊条件注解。 定义Spring4 中提供了更加通用的条件注解，让我们可以在满足不同条件时创建不同的 Bean，这种配置方式在 Spring Boot 中得到了广泛的使用，大量的自动化配置都是通过条件注解来实现的，查看松哥之前的 Spring Boot 文章，凡是涉及到源码解读的文章，基本上都离不开条件注解： 干货|最新版 Spring Boot2.1.5 教程+案例合集 有的小伙伴可能没用过条件注解，但是开发环境、生产环境切换的 Profile 多多少少都有用过吧？实际上这就是条件注解的一个特例。 实践抛开 Spring Boot，我们来单纯的看看在 Spring 中条件注解的用法。 首先我们来创建一个普通的 Maven 项目，然后引入 spring-context，如下： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.5.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 然后定义一个 Food 接口： 123public interface Food { String showName();} Food 接口有一个 showName 方法和两个实现类： 12345678910public class Rice implements Food { public String showName() { return \"米饭\"; }}public class Noodles implements Food { public String showName() { return \"面条\"; }} 分别是 Rice 和 Noodles 两个类，两个类实现了 showName 方法，然后分别返回不同值。 接下来再分别创建 Rice 和 Noodles 的条件类，如下： 12345678910public class NoodlesCondition implements Condition { public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { return context.getEnvironment().getProperty(\"people\").equals(\"北方人\"); }}public class RiceCondition implements Condition { public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { return context.getEnvironment().getProperty(\"people\").equals(\"南方人\"); }} 在 matches 方法中做条件属性判断，当系统属性中的 people 属性值为 ‘北方人’ 的时候，NoodlesCondition 的条件得到满足，当系统中 people 属性值为 ‘南方人’ 的时候，RiceCondition 的条件得到满足，换句话说，哪个条件得到满足，一会就会创建哪个 Bean 。 接下来我们来配置 Rice 和 Noodles ： 12345678910111213@Configurationpublic class JavaConfig { @Bean(\"food\") @Conditional(RiceCondition.class) Food rice() { return new Rice(); } @Bean(\"food\") @Conditional(NoodlesCondition.class) Food noodles() { return new Noodles(); }} 这个配置类，大家重点注意两个地方： 两个 Bean 的名字都为 food，这不是巧合，而是有意取的。两个 Bean 的返回值都为其父类对象 Food。 每个 Bean 上都多了 @Conditional 注解，当 @Conditional 注解中配置的条件类的 matches 方法返回值为 true 时，对应的 Bean 就会生效。 配置完成后，我们就可以在 main 方法中进行测试了： 12345678910public class Main { public static void main(String[] args) { AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.getEnvironment().getSystemProperties().put(\"people\", \"南方人\"); ctx.register(JavaConfig.class); ctx.refresh(); Food food = (Food) ctx.getBean(\"food\"); System.out.println(food.showName()); }} 首先我们创建一个 AnnotationConfigApplicationContext 实例用来加载 Java 配置类，然后我们添加一个 property 到 environment 中，添加完成后，再去注册我们的配置类，然后刷新容器。容器刷新完成后，我们就可以从容器中去获取 food 的实例了，这个实例会根据 people 属性的不同，而创建出来不同的 Food 实例。 这个就是 Spring 中的条件注解。 进化条件注解还有一个进化版，那就是 Profile。我们一般利用 Profile 来实现在开发环境和生产环境之间进行快速切换。其实 Profile 就是利用条件注解来实现的。 还是刚才的例子，我们用 Profile 来稍微改造一下： 首先 Food、Rice 以及 Noodles 的定义不用变，条件注解这次我们不需要了，我们直接在 Bean 定义时添加 @Profile 注解，如下： 12345678910111213@Configurationpublic class JavaConfig { @Bean(\"food\") @Profile(\"南方人\") Food rice() { return new Rice(); } @Bean(\"food\") @Profile(\"北方人\") Food noodles() { return new Noodles(); }} 这次不需要条件注解了，取而代之的是 @Profile 。然后在 Main 方法中，按照如下方式加载 Bean： 12345678910public class Main { public static void main(String[] args) { AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.getEnvironment().setActiveProfiles(\"南方人\"); ctx.register(JavaConfig.class); ctx.refresh(); Food food = (Food) ctx.getBean(\"food\"); System.out.println(food.showName()); }} 效果和上面的案例一样。 这样看起来 @Profile 注解貌似比 @Conditional 注解还要方便，那么 @Profile 注解到底是什么实现的呢？ 我们来看一下 @Profile 的定义： 1234567@Target({ElementType.TYPE, ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(ProfileCondition.class)public @interface Profile { String[] value();} 可以看到，它也是通过条件注解来实现的。条件类是 ProfileCondition ，我们来看看： 123456789101112131415class ProfileCondition implements Condition { @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { MultiValueMap&lt;String, Object&gt; attrs = metadata.getAllAnnotationAttributes(Profile.class.getName()); if (attrs != null) { for (Object value : attrs.get(\"value\")) { if (context.getEnvironment().acceptsProfiles(Profiles.of((String[]) value))) { return true; } } return false; } return true; }} 看到这里就明白了，其实还是我们在条件注解中写的那一套东西，只不过 @Profile 注解自动帮我们实现了而已。 @Profile 虽然方便，但是不够灵活，因为具体的判断逻辑不是我们自己实现的。而 @Conditional 则比较灵活。 结语两个例子向大家展示了条件注解在 Spring 中的使用，它的一个核心思想就是当满足某种条件的时候，某个 Bean 才会生效，而正是这一特性，支撑起了 Spring Boot 的自动化配置。 好了，本文就说到这里，有问题欢迎留言讨论。","link":"/2019/0802/springboot-conditional.html"},{"title":"极简Spring Boot整合MyBatis多数据源","text":"关于多数据源的配置，前面和大伙介绍过JdbcTemplate多数据源配置，那个比较简单，本文来和大伙说说MyBatis多数据源的配置。其实关于多数据源，我的态度还是和之前一样，复杂的就直接上分布式数据库中间件，简单的再考虑多数据源。这是项目中的建议，技术上的话，当然还是各种技术都要掌握的。 工程创建首先需要创建MyBatis项目，项目创建和前文的一样，添加MyBatis、MySQL以及Web依赖： 项目创建完成后，添加Druid依赖，和JdbcTemplate一样，这里添加Druid依赖也必须是专为Spring boot打造的Druid，不能使用传统的Druid。完整的依赖如下： 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 多数据源配置接下来配置多数据源，这里基本上还是和JdbcTemplate多数据源的配置方式一致，首先在application.properties中配置数据库基本信息，然后提供两个DataSource即可，这里我再把代码贴出来，里边的道理条条框框的，大伙可以参考前面的文章，这里不再赘述。 application.properties中的配置： 123456789spring.datasource.one.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=utf-8spring.datasource.one.username=rootspring.datasource.one.password=rootspring.datasource.one.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.two.url=jdbc:mysql:///test02?useUnicode=true&amp;characterEncoding=utf-8spring.datasource.two.username=rootspring.datasource.two.password=rootspring.datasource.two.type=com.alibaba.druid.pool.DruidDataSource 然后再提供两个DataSource，如下： 12345678910111213@Configurationpublic class DataSourceConfig { @Bean @ConfigurationProperties(prefix = \"spring.datasource.one\") DataSource dsOne() { return DruidDataSourceBuilder.create().build(); } @Bean @ConfigurationProperties(prefix = \"spring.datasource.two\") DataSource dsTwo() { return DruidDataSourceBuilder.create().build(); }} MyBatis配置接下来则是MyBatis的配置，不同于JdbcTemplate，MyBatis的配置要稍微麻烦一些，因为要提供两个Bean，因此这里两个数据源我将在两个类中分开来配置，首先来看第一个数据源的配置： 1234567891011121314151617181920212223@Configuration@MapperScan(basePackages = \"org.sang.mybatis.mapper1\",sqlSessionFactoryRef = \"sqlSessionFactory1\",sqlSessionTemplateRef = \"sqlSessionTemplate1\")public class MyBatisConfigOne { @Resource(name = \"dsOne\") DataSource dsOne; @Bean SqlSessionFactory sqlSessionFactory1() { SqlSessionFactory sessionFactory = null; try { SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dsOne); sessionFactory = bean.getObject(); } catch (Exception e) { e.printStackTrace(); } return sessionFactory; } @Bean SqlSessionTemplate sqlSessionTemplate1() { return new SqlSessionTemplate(sqlSessionFactory1()); }} 创建MyBatisConfigOne类，首先指明该类是一个配置类，配置类中要扫描的包是org.sang.mybatis.mapper1，即该包下的Mapper接口将操作dsOne中的数据，对应的SqlSessionFactory和SqlSessionTemplate分别是sqlSessionFactory1和sqlSessionTemplate1，在MyBatisConfigOne内部，分别提供SqlSessionFactory和SqlSessionTemplate即可，SqlSessionFactory根据dsOne创建，然后再根据创建好的SqlSessionFactory创建一个SqlSessionTemplate。 这里配置完成后，依据这个配置，再来配置第二个数据源即可： 1234567891011121314151617181920212223@Configuration@MapperScan(basePackages = \"org.sang.mybatis.mapper2\",sqlSessionFactoryRef = \"sqlSessionFactory2\",sqlSessionTemplateRef = \"sqlSessionTemplate2\")public class MyBatisConfigTwo { @Resource(name = \"dsTwo\") DataSource dsTwo; @Bean SqlSessionFactory sqlSessionFactory2() { SqlSessionFactory sessionFactory = null; try { SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dsTwo); sessionFactory = bean.getObject(); } catch (Exception e) { e.printStackTrace(); } return sessionFactory; } @Bean SqlSessionTemplate sqlSessionTemplate2() { return new SqlSessionTemplate(sqlSessionFactory2()); }} 好了，这样MyBatis多数据源基本上就配置好了，接下来只需要在org.sang.mybatis.mapper1和org.sang.mybatis.mapper2包中提供不同的Mapper，Service中注入不同的Mapper就可以操作不同的数据源。 mapper创建org.sang.mybatis.mapper1中的mapper： 123public interface UserMapperOne { List&lt;User&gt; getAllUser();} 对应的XML文件： 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"org.sang.mybatis.mapper1.UserMapperOne\"&gt; &lt;select id=\"getAllUser\" resultType=\"org.sang.mybatis.model.User\"&gt; select * from t_user; &lt;/select&gt;&lt;/mapper&gt; org.sang.mybatis.mapper2中的mapper： 123public interface UserMapper { List&lt;User&gt; getAllUser();} 对应的XML文件： 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"org.sang.mybatis.mapper2.UserMapper\"&gt; &lt;select id=\"getAllUser\" resultType=\"org.sang.mybatis.model.User\"&gt; select * from t_user; &lt;/select&gt;&lt;/mapper&gt; 接下来，在Service中注入两个不同的Mapper，不同的Mapper将操作不同的数据源。 好了，欢迎加入我的星球，关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0407/mybatis-multi.html"},{"title":"每次回西安，都会动摇我继续留在深圳的决心","text":"hello 小伙伴们国庆节快乐吖。 松哥国庆节回老家了，这两天一直忙哥没停。和女票在一起八年了，结婚的事情该慢慢准备了，国庆节趁机带女票家人来西安走走看看，好多近在眼前但是以前从来没有去过的景点如城墙、大唐不夜城等，这次都去走了走。不过老实说，西安的文化，单纯从一些典型的景点中其实不太容易体会到，西安的文化和历史沉淀在这个城市的每一个细节中，随便走两步，看见一个地名，这就是一个故事，这些以后松哥在再和大家细细聊。 回家呆这几天，真是感慨颇多，甚至又一次动摇了我继续留在深圳的决心。回家真是太舒服了，老家亲戚多朋友多，很多事情都可以热热闹闹的去做，村子里的晚上超级安静，只有远处村子的狗吠，没有了大城市的嘈杂，每天晚上都能安安稳稳的睡好久。这次回家，西安的房子也收拾好了可以住人了，小区旁边每逢 2/4/6 有集，今天去看了下，蘑菇十块钱一筐五块钱半框、提子一块五一斤十块八斤…等等，这价格真的太感人了，我心里思量着要是回西安生活，日子一定美滋滋（其实我也明白，要是真的回来了，日子也不见得就一定美滋滋。 在深圳就不一样了，完全是另外一种状态，周一到周五白天上班，晚上写公众号，周末录视频教程+写公众号，很少有自己的生活，甚至可以说只有工作没有生活。在深圳租的房子，我一般习惯称之为宿舍，我不太喜欢称之为家，因为我觉得那就是一个晚上睡觉的地方，而家，不仅仅指房子，也是心的寄托，将租的房子称之为家，我有点不太习惯，没有家，所以总有一种漂着的感觉。 每次过完年离家其实也是这样，刚走的时候，挺想家，感觉呆在西安真不错，时间久了，这种感觉就慢慢变淡了，开始谋划如何在广深两地立足，直到过年回到家，又开始思想上新一轮的轮回。 既然这样，那干嘛还要留在深圳或者广州呢？ 对于一个农村出来的孩子，我有太多的不甘心，留在深圳，代表着奋斗，希望就有可能实现。而回老家，在我看来有一点点认命的感觉，就是不打算继续奋斗了，和一帮好朋友去吃吃喝喝了，生活乐悠悠，事业上能糊口就行了。 大学毕业后，我就羡慕两种人，一种人就是留在学校所在地工作的，可以随时见到老师同学，可以随时回学校看看，还有一种就是回老家工作的，虽然不知道他们真实的生活状态，不过我自己想象着应该是很美好的生活吧… 每次想到以后要不要回西安，就会很纠结，我想知道小伙伴们都在哪里工作呢？是老家还是一线城市？为什么作出这样的选择？欢迎大家来留言说说.","link":"/2019/1006/go-home.html"},{"title":"微人事 star 数超 10k，如何打造一个 star 数超 10k 的开源项目","text":"看了下，微人事(https://github.com/lenve/vhr)项目 star 数超 10k 啦，松哥第一个 star 数过万的开源项目就这样诞生了。 两年前差不多就是现在这个时候，松哥所在的公司业绩下滑严重，关门倒闭已成定局，很多同事在谋划的新的出路，松哥则被公司留下来善后，在一段并不太忙碌的日子里，做了两个 Spring Boot + Vue 的前后端分离开源项目，以期能给自己来年找工作增加一点筹码，没想到这两个项目后来受到很多关注，也帮助了很多人。有不少小伙伴在公司使用微人事项目做脚手架开发项目，也有国内 top20 的高校研究生借鉴微人事做毕设，我自己也因此收到一些大厂的橄榄枝，可以说还是收获满满。 关于这个项目诞生的故事，松哥之前写过一篇文章，感兴趣的小伙伴可以看看： 公司倒闭 1 年了，而我当年的项目上了 GitHub 热榜 今天，我想和小伙伴们聊聊如何从零开始打造一个 star 数过万的开源项目。松哥把这些经验总结为三点： 文档详细 项目有料 适当宣传 这些经验不是什么惊世骇俗的大道理，都很普通，关键在于执行。 1. 文档详细其实在做微人事和 V 部落之前，松哥在 GitHub 上已经做过多个开源项目了，比较有意思的一个是一个 Android 上的自定义控件，我做了一个歌词展示的控件，这个控件引入到自己的项目中以后，可以根据当前歌曲的播放进度动态滚动歌词，效果如如下： 还有一个比较好玩的就是 Android 上自动抢红包的 App。不过这些开源工具和项目最终都石沉大海了。 究其原因，我觉得是自己对待这些项目不够认真，项目开源之后基本上都没有再继续维护了，一个项目提交次数一般都是个位数，项目做完之后，写一篇博客介绍下就算完事了。一个自己都不怎么重视的项目，其实很难引起别人的重视。 所以在 V 部落和微人事中，我就吸取教训，尽量把项目的文档写的详细一些，让不懂前后端分离开发的小伙伴看到我写的开发文档后，就能够快速理清项目的思路。就这样，我每写一个功能点，就写一篇技术文档，微人事项目前前后后一共写了 30 多篇文档： 同时我考虑到很多小伙伴第一次接触到这个项目，一个庞然大物不好处理，因此我在每一次项目提交之前，都会对项目打一个 tag，这样大家通过 git clone 命令获取到项目之后，就可以通过 tag 非常方便的定位到项目的任意时刻，例如只想看登录设计的，可以根据文档介绍回到 v20180107 这个版本： 小伙伴也可以点击 GitHub 上的 release 下载不同时期的项目。我一开始担心有的小伙伴不熟悉 Git 上的 tag 操作，还针对此写了个教程，就是上面文档的第 17 篇。 可以说，这个项目我从头到尾考虑了很多小伙伴们可能遇到的问题，不是自嗨，确实是希望能够带着小伙伴们一起飞。 这么详细的文档当然也得到了小伙伴和一些平台的认可，项目刚刚发布的时候，也就是 2018 年年初的时候，慕课网在它的官方知乎账号、微博账号上都有推过我的介绍项目的文章，当时这个项目就受到了很多小伙伴的关注，收获了不少 star，小伙伴们的关注也鼓励我继续把这个项目向前推进。 这是我介绍的第一点经验，文档详细。 2. 项目有料项目有料，也就是这个项目对大家而言是有价值的，能够真正帮到大家。 很多小伙伴看到微人事会觉得奇怪，这个项目还有一些功能没有实现，怎么就有这么多人关注呢？其实原因很简单，这个项目的价值不在于它的业务，而在于它解决了很多小伙伴在前后端分离开发中遇到的问题，这也是我做这个项目的初衷之一。 作为一个 Java 攻城狮，我非常明白很多小伙伴去接触 Vue、接触 Nodejs、接触 SPA 以及接触前端工程化这些概念时所面临的困惑，因为这些困惑我也曾经遇到过。 例如前后端分离后，开发环境下前后端如何进行数据交互、权限管理怎么做、文件上传怎么做、项目怎么部署等等，事无巨细，我觉得小伙伴们可能会困惑的地方，我在微人事中直接用代码做出样例，然后再辅以详细的文字解释，这样对于大部分小伙伴而言，都能够快速理解这个项目了。 这个项目建立之初，一开始就定位是一个学习项目，目的就是帮助大家建立前后端分离开发知识体系，搞定前后端分离开发中常见的坑，因此，一旦将项目的框架搭建成功，大家理解了前后端分离架构中的各个细节，剩下的业务不过是堆代码，技术上已经没有挑战了，这也是这个项目最近一段时间更新慢的原因。 这是我和大家分享的第二点经验，项目要有价值。 3. 适当宣传微人事和 V 部落刚刚上线的时候，我的相关文章被慕课网猛推过几次，当时就让微人事受到了比较多的关注，印象中，大概不到一个月的时间，star 数就超过 1k 了。 由于文档比较详细，很多大佬在整理相关资料的时候都会加上微人事项目，很多小伙伴可能见过标题类似下面这样的文章： 13个优秀的 Spring Boot 学习项目 开源的13个Spring Boot 优秀学习项目！ 六月份 GitHub 上最受欢迎的开源项目 七月份 GitHub 上最受欢迎的开源项目 …. 这一类的文章大部分都会收录微人事项目，进而将微人事项目曝光给更多小伙伴去学习。 今年七月份，有感于 1 年前公司倒闭时的无助，松哥写了一篇文章介绍了微人事项目的诞生过程： 公司倒闭 1 年了，而我当年的项目上了 GitHub 热榜 没想到这篇文章被很多大佬转发，单单在知乎上这篇文章的阅读量就超过 12w，那一段时间，微人事项目也被很多有需要的小伙伴关注到了。 宣传这一块，我的经验是如果项目确实帮助到了很多小伙伴，对很多人而言有价值，你会发现平台，一些有影响力的技术牛人会自然的帮你推。所以，我还是建议将重心放在项目上。所谓家有梧桐树，引得凤凰来。 这是我介绍的第三点经验，适当宣传。 好了，一点点不太成熟的经验分享给小伙伴们，希望能帮到大家。","link":"/2019/1016/vhr.html"},{"title":"使用 Nginx 部署前后端分离项目，解决跨域问题","text":"前后端分离这个问题其实松哥和大家聊过很多了，上周松哥把自己的两个开源项目部署在服务器上以帮助大家可以快速在线预览（喜大普奔，两个开源的 Spring Boot + Vue 前后端分离项目可以在线体验了），然后群里就有小伙伴想让松哥来聊聊如何结合 Nginx 来部署前后端分离项目？今天我们就来聊一聊这个话题。 不得不说的跨域很多人对前后端分离部署感到困惑，其实主要是困惑跨域问题怎么解决。因为前后端分离项目在开发的时候，前端通过 nodejs 来运行，需要一个单独的端口，后端通过 Tomcat 或者 Jetty 来运行，也需要端口，两个不同的端口，就造成了跨域。 但是松哥之前多次和大家聊过这个问题，这种跨域并不是我们传统开发中真正的跨域，这个所谓的跨域只在开发环境中存在，生产环境下就不存在这个跨域问题了。所以我们不能按照以往的通过 JSONP 或者 CORS 之类的手段来解决这个跨域问题。 前后端分离开发中，前端为了能够模拟出测试数据，并且模拟出请求，一般需要借助于 nodejs 来运行，这是开发时候的状态，开发时候的配置大家可以参考这篇文章： 前后端分离历险记 等开发完成后，我们会对前端项目编译打包，编译打包完成之后，就只剩下一堆 js、css 以及 html 文件了，我们把这些编译打包后的文件拷贝到后端项目中，这样再去运行就不存在跨域问题了（例如将编译打包后的静态文件拷贝到 Spring Boot 项目的 src/main/resources/static 目录下）。这种方式我就不再多说了，相信大家都会，今天咱们主要来看看如何结合 Nginx 来部署。 Nginx 大杀器结合 Nginx 来部署前后端分离项目算是目前的主流方案。一来部署方便，二来通过动静分离也可以有效提高项目的运行效率。 大家知道我们项目中的资源包含动态资源和静态资源两种，其中： 动态资源就是那些需要经过容器处理的资源，例如 jsp、freemarker、各种接口等。 静态资源则是那些不需要经过容器处理，收到客户端请求就可以直接返回的资源，像 js、css、html 以及各种格式的图片，都属于静态资源。 将动静资源分开部署，可以有效提高静态资源的加载速度以及整个系统的运行效率。 在前后端分离项目部署中，我们用 Nginx 来做一个反向代理服务器，它既可以代理动态请求，也可以直接提供静态资源访问。我们来一起看下。建议大家先阅读松哥以前关于 Nginx 的一篇旧文，可以有效帮助大家理解后面的配置： Nginx 极简入门教程！ 后端部署后端接口的部署，主要看项目的形式，如果就是普通的 SSM 项目，那就提前准备好 Tomcat ，在 Tomcat 中部署项目，如果是 Spring Boot 项目，可以通过命令直接启动 jar，如果是微服务项目，存在多个 jar 的话，可以结合 Docker 来部署（参考一键部署 Spring Boot 到远程 Docker 容器），无论是那种形式，对于我们 Java 工程师来说，这都不是问题，我相信这一步大家都能搞定。 后端项目可以在一个非 80 端口上部署，部署成功之后，因为这个后端项目只是提供接口，所以我们并不会直接去访问他。而是通过 Nginx 请求转发来访问这个后端接口。 松哥这里以我去年为一个律所的小程序为例，后端是一个 Spring Boot 工程，那么我可以通过 Docker 部署，也可以直接通过命令来启动，这里简单点，直接通过命令来启动 jar ，如下： 1nohup java -jar jinlu.jar &gt; vhr.log &amp; 后端启动成功之后，我并不急着直接去访问后端，而是安装并且去配置一个 Nginx，通过 Nginx 来转发请求，Nginx 的基本介绍与安装，大家可以参考（Nginx 极简入门教程！），我这里就直接来说相关的配置了。 这里我们在 nginx.conf 中做出如下配置： 首先配置上游服务器： 123upstream zqq.com{ server 127.0.0.1:9999 weight=2;} 在这里主要是配置服务端的地址，如果服务端是集群化部署，那么这里就会有多个服务端地址，然后可以通过权重或者 ip hash 等方式进行请求分发。 然后我们在 server 中配置转发规则： 1234567location /jinlu/ { proxy_pass http://zqq.com; tcp_nodelay on; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;} 这样配置完成后，假设我目前的域名是 javaboy.org，那么用户通过 http://www.javaboy.org/jinlu/** 格式的地址就可以访问到我服务端的接口。 前端部署以 Vue 为例，如果是 SPA 应用，项目打包之后，就是一个 index.html 还有几个 js、css、images 以及 fonts ，这些都是静态文件，我们将静态文件首先上传到服务器，然后在 nginx.conf 中配置静态资源访问，具体配置如下： 1234location ~ .*\\.(js|css|ico|png|jpg|eot|svg|ttf|woff|html|txt|pdf|) { root /usr/local/nginx/html/;#所有静态文件直接读取硬盘 expires 30d; #缓存30天} ​​​​ 当然我这里是按照资源类型来拦截的，即后缀为 js、css、ico 等的文件，统统都不进行请求分发，直接从本地的 /usr/local/nginx/html/ 目录下读取并返回到前端（我们需要将静态资源文件上传到 /usr/local/nginx/html/ 目录下）。 如果我们的服务器上部署了多个项目，这种写法就不太合适，因为多个项目的前端静态文件肯定要分门别类，各自放好的，这个时候我们一样可以通过路径来拦截，配置如下： 1234location /jinlu-admin/ { root /usr/local/nginx/html/jinlu-admin/;#所有静态文件直接读取硬盘 expires 30d; #缓存30天} ​​​​ 这样，请求路径是 /jinlu-admin/ 格式的请求，则不会进行请求分发，而是直接从本机的 /usr/local/nginx/html/jinlu-admin/ 目录下返回相关资源。采用这方方式配置静态资源，我们就可以部署多个项目了，多个项目的部署方式和上面的一样。 这样部署完成之后，假设我的域名是 javaboy.org ，那么用户通过 http://www.javaboy.org/jinlu-admin/**格式的请求就可以访问到前端资源了。 此时大家发现，前端的静态资源和后端的接口现在处于同一个域之中了，这样就不存在跨域问题，所以我一开始基说不必用 JSONP 或者 CORS 去解决跨域。特殊情况可能需要在 nginx 中配置跨域，这个松哥以后再和大家细聊~​ 好了，不知道小伙伴有没有看懂呢？有问题欢迎留言讨论。","link":"/2019/1016/nginx.html"},{"title":"浏览 GitHub 太卡了？教你两招！","text":"老实说，GitHub 在国内的使用体验并不算太好，这其中最大的原因就是网络了。 GitHub 访问起来比较卡，这个看起来貌似无解。国内的 gitee 网速倒是可以，但是无法代替 GitHub，个人感觉 gitee 上还是开源项目多一些，工具类库要少一些。 在网络比较卡的情况下，如果我们想在线查看 GitHub 上项目的源码，是非常的不方便，我们需要不断的进入到某个目录中，然后再退出来，再进入到其他目录中，这样每一次都要加载页面，查看起来非常不便。 今天松哥就给大家介绍两款浏览器插件 SourceGraph 和 Octotree ，使用这两个浏览器插件可以非常方便的在线查看 GitHub 上项目的源码。Chrome 或者 Firefox 都可以安装此插件。 SourceGraph简介SourceGraph 是一个依据 Apache2.0 开源协议开源的一个工具，集代码查看、review PR、代码搜索等功能于一身。开发者必备。 支持的浏览器虽然我们大部分情况下可能都是在 Chrome 或者 Firefox 上使用 SourceGraph，不过实际上，SourceGraph 支持的平台可不止 Chrome 和 Firefox ，包括 Vscode 在内的大多数工具都支持，如下： 可以看到有 Chrome、Firefox、VsCode、Jet Brains全家桶(IDEA、WebStorm 等)、Vi、Sublime、Atom。 支持的平台那我们用这个都可以浏览哪些网站的代码呢？除了 GitHub 还有哪些平台也受到支持呢？ 可以看到，常见的 GitHub、GitLab、Bitbucket 都获得了支持，还有像 Phabricator、Azure DevOps、AWS CodeCommit 也都有较好的支持。 安装Firefox 大家可以直接搜索安装，Chrome，要是上网不方便，松哥已经帮大家下载好了，公众号后台回复 sourcegraph，获取 SourceGraph 离线包下载地址，离线包下载后之后，直接拖到浏览器上安装即可。 这里我就来和大家演示下正常安装，在 Chrome 的 Store 中搜索 SourceGraph ，如下： 搜到之后点击右边的 添加至 Chrome 按钮即可，我这边因为已经装过了，所以按钮是评分，安装完成之后，浏览器上会多出一个 SourceGraph 图标，如下： 这样 SourceGraph 就安装好了。 使用此时我们在 GitHub 上打开任意一个项目，以松哥的微人事为例，如下： 可以看到，在项目的 Watch 旁边多了一个按钮，这个按钮就是 SourceGraph，我们点击一下，就可以进入到 SourceGraph 页面： 这就像一个开发工具一样，我们在右边可以轻松的展开项目的包，想看哪个文件就看哪个。上面的搜索框支持正则表达式搜索，可以方便的查找到项目中相关变量的定义。 SourceGraph 中还可以查看变量被定义或者被引用的位置(这个功能要登录 SourceGraph 后才可以使用)： 是不是都可以当成 IDE 来用了。 有了这些功能，我们再在 GitHub 上浏览项目就方便多了，小伙伴们不妨试试。 Octotree简介Octotree(https://www.octotree.io/) 是一款浏览器插件，Chrome 和 Firefox 、Opera 对此都有很好的支持。这个插件可以将 GitHub 上项目代码以树形格式展示，而且在展示的列表中，我们可以下载指定的文件，而不需要下载整个项目。 安装Firefox 安装 Octotree 很容易，问题不大，但是 Chrome 上安装，可能有的小伙伴不太方便，松哥已经将 Chrome 上 Octotree 的离线安装包下载好了，大家在我公众号后台回复 octotree 就可以获取下载链接。 当然，如果大家方便的话，可以直接在 Chrome 商店中搜索，如下： 选择第一个免费版的安装就可以了。 装好之后，浏览器右上角就会有一个相应的图标，如下： 使用Octotree 安装成功之后，打开一个 GitHub 网页，我们在左边就可以看到 Octotree 插件，如下： 这样就可以快速打开并浏览一个 GitHub 文件了。 点击文件名前面的图标，就可以在一个单独的网页中打开这个文件。 好了，这两个神器就说到这里，小伙伴们不妨试试。","link":"/2019/0817/github-chrome.html"},{"title":"给大家整理了几个开源免费的 Spring Boot + Vue 学习资料","text":"最近抽空在整理前面的文章案例啥的，顺便把手上的几个 Spring Boot + Vue 的学习资料推荐给各位小伙伴。这些案例有知识点的讲解，也有项目实战，正在做这一块的小伙伴们可以收藏下。 案例学习javaboy-video-samples 项目地址：https://github.com/lenve/javaboy-video-samples 这个是松哥录制的 Spring Boot2 系列视频教程的案例，视频是加密的，但是案例从一开始就是开源的，这个可以毫无保留的共享给大家，案例可以说是非常全面，这个仓库会随着视频的录制而继续完善。 javaboy-code-samples 项目地址：https://github.com/lenve/javaboy-code-samples 这个是我平时公众号上文章的案例，因为公众号的文章大部分都是以 Spring Boot + Vue 前后端分离开发为主，所以这些文章也是这一方面的，不同于上面的那个仓库，这里的每个案例都有对应的文章进行讲解，这个仓库中的内容会随着公众号文章的增加而继续增加： spring-boot-vue-samples 项目地址：https://github.com/lenve/spring-boot-vue-samples 这个是松哥《Spring Boot + Vue 全栈开发实战》一书的官方案例，但是因为书里给出来的地址是一个百度云盘的地址，所以这个仓库很少受到小伙伴们的关注。不过这套案例的整理的不是很满意，另外这套案例也比较旧了，是去年的，所以建议小伙伴们关注第一个，里边的案例比较新，也比较整齐。 Spring Boot2 项目地址：http://springboot.javaboy.org 这个不是 GitHub 上的仓库，是松哥一系列 Spring Boot 相关文章的集合，我把它做成了电子书的形式，但是有两个不太满意的地方：一个是文章排版不太满意，另一个则是网站托管在国外服务器上，访问速度较慢，因此这个在线的电子书我最近也在整理，整理完成后，我会分享出来给大家免费下载，小伙伴们多多关注公众号上的消息哦。 awesome-github-vue 项目地址：https://github.com/opendigg/awesome-github-vue 这个不是松哥的仓库，这是我刚开始学习 Vue 的时候收藏的一个仓库，感觉非常棒，很多常用的 Vue 插件这里都有，不过稍微遗憾的是，这个仓库有两年没有更新了，不过对于刚刚开始接触 Vue 的小伙伴而言，这个仓库够用了。 项目项目就不用多说了，V 部落和微人事。 微人事 项目地址：https://github.com/lenve/vhr 关于微人事我已经写过好多文章了，这里就不再赘述了，要告诉大家的一个好消息是，大概在 12 月，微人事会进行一次全面的版本升级，Spring Boot 切换到当前最新版，Vue 构建工具切换到 vue-cli3，而且还会引入消息中间件 RabbitMQ 等一些外部工具，进一步扩展微人事所涉及到的知识点。微人事项目最新体验地址： http://vhr.itboyhub.com 当然，松哥之前也录过一个微人事部署视频，大家可以参考： V 部落 项目地址：https://github.com/lenve/VBlog V 部落没有微人事那么丰富的文档，但是比较简单，业务简单，用到的技术点也简单，不过好多小伙伴竟然反映部署不起来，以后我看时间抽空也可以录一个部署教程吧，V 部落目前最新的体验地址是： http://vblog.itboyhub.com 最后再悄悄告诉大家，公众号后台回复 2TB 可以获取 2TB Java 学习资源下载地址哦。","link":"/2019/1120/sprinkgboot-vue.html"},{"title":"给数据库减负的八个思路","text":"传统的企业级应用，其实很少会有海量应用，因为企业的规模本身就摆在那里，能有多少数据？高并发？海量数据？不存在的！ 不过在互联网公司中，因为应用大多是面向广大人民群众，数据量动辄上千万上亿，那么这些海量数据要怎么存储？光靠数据库吗？肯定不是。 今天松哥和大家简单的聊一聊这个话题。 海量数据，光用数据库肯定是没法搞定的，即使不读松哥这篇文章，相信大家也能凝聚这样的共识，海量数据，不是说一种方案、两种方案就能搞定，它是一揽子方案。那么这一揽子方案都包含哪些东西呢？松哥从以下八个方面来和大家聊聊。 1.缓存首先第一种解决方案就是缓存了。 缓存，我们可以将数据直接缓存在内从中，例如 Map、也可以使用缓存框架如 Redis 等，将一些需要频繁使用的热点数据保存在缓存中，每当用户来访问的时候，就可以直接将缓存中的数据返回给用户，这样可以有效降低服务器的压力。 可以缓存起来使用的数据，一般都不能对实时性要求太高。 2.页面静态化页面静态化其实可以算作是缓存的另外一种形式，相当于直接将相关的页面渲染结果缓存起来。首先大家知道，在我们的 Web 项目中，资源分为两大类： 静态资源 动态资源 静态资源就是我们常见的 HTML、CSS、JavaScript、图片等资源，这些资源可以不经过服务端处理，就可以直接返回给前端浏览器，浏览器就可以直接显示出来。 动态资源则是指我们项目中的 Servlet 接口、Jsp 文件、Freemarker 等，这些需要经过服务端渲染之后，才可以返回前端的资源。 在实际项目中，静态资源的访问速度要远远高于动态资源，动态资源往往很容易遇到服务器瓶颈、数据库瓶颈，因此，对于一些不经常更新的页面，或者说更新比较缓慢的页面，我们可以通过页面静态化，将一个动态资源保存为静态资源，这样当服务端需要访问的时候，直接将静态资源返回，就可以避免去操作数据库了，降低数据库的压力。 例如松哥以前做过的一个电商项目，系统根据大数据统计，自动统计出用户当前搜索的热点商品，这些热点商品，10 分钟更新一次，也就是说，在十分钟内，用户登录上来看到的热点商品都是相同的。那么就没有必要每次都去查询数据库，而是将热点数据的页面，通过输出流自动写到服务器上，写成一个普通的 HTML 文件，下次用户来访问，在 10 分钟有效期内，直接将 HTML 页面返回给用户，就不必操作数据库了。 一般来说，Freemarker、Velocity 等都有相关的方法可以帮助我们快速将动态页面生成静态页面。 这就是页面静态化。 3.数据库优化很多时候程序跑得慢，不是因为设备落后，而是因为数据库 SQL 写的太差劲。 要解决海量数据的问题，数据库优化肯定也是不可避免的。一般来说，我们可以从 SQL 优化、表结构优化、以及数据库分区分表等多个方面来对数据库进行优化。数据库优化其实也是一门巨大的学问，松哥以后看有时间写个连载和大家仔细聊聊这个话题。 4.热点数据分离数据库中的数据，虽然是海量数据，但是这些数据并不见得所有数据都是活跃数据，例如用户注册，有的用户注册完就消失的无影无踪了，而有的用户则在不停的登录，因此，对于这两种不同的用户，我们可以将活跃用户分离出来，在主要操作的数据表中只保存活跃用户数据。每次用户登录，先去主表中查看有没有记录，有的话，直接登录，没有的话，再去查看其他表。 通过判断用户在某一段时间内的登录次数，就可以很快分离出热点数据。 5.合并数据库操作这个方案的宗旨其实是减少数据库操作的次数，例如多次插入操作，我们可以合并成一条 SQL 搞定。多个不同条件的查询，如果条件允许的话，也可以合并成为一个查询，尽量减少数据库的操作，减少在网络上消耗，同时也降低数据库的压力。 6.数据库读写分离数据库的读写分离其实松哥在之前的 MyCat 中也和大伙聊过了（MyCat 系列），读写分离之后，一方面可以提高数据库的操作效率，另一方面也算是对数据库的一个备份。这一块的具体操作大家可以参考松哥前面的文章。 7.分布式数据库数据库读写分离之后，无形中增大了代码的复杂度，所以一般还需要借助分布式数据库中间件，这样可以有效提高数据库的弹性，可以方便的随时为数据库扩容，同时也降低代码的耦合度。 8.NoSQL 和 Hadoop另外，引入 NoSQL 和 Hadoop 也是解决方案之一。NoSQL 突破了关系型数据库中对表结构、字段等定义的条条框框，使用户可以非常灵活方便的操作，另外 NoSQL 通过多个存储块存储数据的特点，使得天然具备操作大数据的优势（快）。不过，老实说，NoSQL 目前还是在互联网项目中比较常见，在传统的企业级应用中还是比较少见。 Hadoop 就不必说了，大数据处理利器。 很多时候技术和架构只是一个工具，所有的东西都摆在你面前，关键是如何把这些东西组合在一起，使之产生最大化收益，这就需要大家慢慢琢磨，松哥后面也尽量和大家多分享一些这方面的经验。 好了，简单的从 8 个方面和大家聊一聊这个问题，大家在工作中有没有遇到类似问题呢？你都是怎么处理的？欢迎留言讨论。 参考资料： [1] 韩路彪.看透Spring MVC：源代码分析与实践[M].北京：机械工业出版社，2015.","link":"/2019/0921/mysql.html"},{"title":"跟着平台混了四年，现在要单飞了！","text":"我记得是2015年4月15在CSDN上发表了我的第一篇博客，是一个学习笔记，从那之后开启了我博客写作之路，到今天为止即将4年，这4年时间我在CSDN上发表的博客最多，共有372篇原创，CSDN是我的大本营，不过在这期间也有断断续续在其他公共平台上发过博客，例如 sf、博客园、掘金、慕课网等，但是都是非常零散，2016年的时候，利用我的 GitHub 也搭建了一个个人站点，但是只是试验了几个页面，并没有好好去维护，前两天清明节，一时心血来潮，花了半天时间搞了一个自己的独立博客 http://www.javaboy.org ，以后将在这个站点上和大伙分享技术。 实际上搭建一个个人站点并不费什么事，唯一的资金投入就是域名，一年也就几十块钱，其他的套用现成的技术即可，接下来我就来和大伙分享下独立博客搭建过程，给小伙伴一个参考。 准备工作博客搭建实际上现在搭建一个个人独立博客，可选方案很多，我这里用了久闻大名的 Hexo 来搭建，用 Hexo 搭建，要是有一点点前端 Node 的使用经验更佳，没有当然也没关系，因为与之相关的命令并不多。使用 Hexo 需要提前在电脑上安装好 Node 和 Git ，安装成功后，就可以开始 Hexo 的安装了。步骤如下： 安装 Hexo 1npm install -g hexo-cli 在本地创建一个博客目录 1hexo init blog 上面这个命令执行完后，会在本地创建一个 blog 目录，这里边就是独立博客所必须的一些文件，然后进入到这个目录中，执行 npm install 命令，安装相关的依赖。 安装完成后，会生成如下目录： 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 这里几个文件/文件夹，我们先来关注其中两个 _config.yml 和 themes 目录， _config.yml 文件中，我们可以做网站的一些基本配置，例如 网站的 title，描述，关键字、图标等，这些配置大都见名知意。如下： 配置完成后，定位到 blog 目录，执行 hexo s 就可以在本地启动项目了，启动成功后，浏览器中输入 http://localhost:4000 就可以看到网站了。 说到 hexo s 命令，这里有几个常用命令需要给大家介绍下，分别是： 命令 简写 中文含义 hexo server hexo s 本地启动 hexo generate hexo g 生成静态文件 hexo deploy hexo d 部署网站 hexo clean 清除缓存和已经生成的静态文件 这四个算是松哥这两天使用最多的命令，其他的命令，大伙可以参考这里。 修改主题一般来说，主题都会自己配置一个，个人感觉 Hexo 的生态还是比较丰富的，有很多可选的主题，Hexo 默认使用的主题是 landscape ，我这里使用了 hexo-theme-next 主题。博客在本地跑起来之后，接下来就是修改主题，主题修改的第一步就是先选一个自己认为好看的主题，选好之后，首先将之克隆到 ./themes 目录下，这个目录下原本有有一个 landscape 文件夹，里边放的默认的样式，当然开发者也可以直接将主题文件下载好拷贝进来，但是我还是建议使用 clone ，使用 clone ，假如有一天这个主题更新了，只需要 pull 一下就可以获取到最新样式了。 以 hexo-theme-next 主题为例， clone 命令如下： 12cd your-hexo-sitegit clone https://github.com/iissnan/hexo-theme-next themes/next 克隆成功后，修改 hexo 的 _config.yml 文件，将主题修改为 next，如下： 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 主题创建好之后，接下来就是对主题的配置了，这个比较容易，直接参考官方文档即可。配置完成后，执行如下命令，即可看到新的主题效果： 123hexo cleanhexo ghexo s 命令含义可以参考上面的表格，这里不再赘述。 绑定到 GitHub大家可能已经迫不及待想要把博客上传到 GitHub 了，绑定到 Github 步骤也很简单，首先以 自己的GitHub ID.github.io 为名创建一个 public 仓库，例如我的 ID 为 lenve，创建的仓库如下： 创建成功之后，修改 hexo 的 _config.yml 文件，配置 GitHub 地址，如下： 1234deploy: type: git repo: git@github.com:lenve/lenve.github.io.git branch: master 这里根据自己的地址来配置即可，配置完成后，执行如下命令： 12hexo ghexo d 执行完成后，就可以将数据上传到 GitHub 了（当然这里需要大家提前配置一下 GitHub 的公钥，具体可以参考Git关联远程仓库）。 上传成功后，访问 https://lenve.github.io 就可以看到自己的个人站点了。 如果你对 GitHub 提供的域名不满意，也可以自己申请一个域名，分分钟就配置好了。 域名申请域名申请建议使用国外的域名提供商，不用备案（一个字，快！不用等），松哥使用了 godaddy ，主要是因为这个服务商支持支付宝付款，域名申请就比较容易了，无需多说。 域名和 GitHub 绑定域名申请成功之后，接下来的配置，也分为两部分。 GitHub 配置首先在博客所在目录下的 source 目录中，创建一个 CNAME 文件，文件内容就是你的域名，如下： 然后执行 hexo d 命令将这个文件上传到 GitHub就可以了。 在网上看到有人直接在 GitHub 上配置这个，如下图： 这种方式也可以，这种方式会自动生成一个CNAME文件到当前仓库中，但是松哥在这里不推荐大家使用这种方式，因为如果你在本地执行了 hexo clean ，然后再去上传，就会丢失掉 CNAME 文件，然后又得重新配置。 域名解析配置域名解析这块，当时遇到了一些问题，因为是在清明节假期，也没法联系客服，后来松哥使用了 DNSPod 去做域名解析了，没有使用 godaddy 提供的域名解析。所以首先要做的，就是修改 godaddy 提供的域名解析服务，登录自己的 godaddy 账号，找到域名管理，修改域名解析服务为 DNSPod ，如下： 然后登录到 DNSPod（没有账号注册一个），然后添加自己的域名解析，如下图： 添加两条 A 记录，指向 GitHub 的 IP 地址，再添加一条 CNAME ，指向你的 GitHub 域名就可以了。 如此之后，大功告成！ 总结因为是第一次做，比较顺利，也很简单，不用花很多钱，就是一个域名的费用而已，不需要额外买服务器，hexo 的使用也很简单，有兴趣小伙伴赶快实践下吧！","link":"/2019/0411/hexo-install.html"},{"title":"身边的人都说微服务好，好在哪？","text":"微服务这么火，多少人多少公司都想试试水。 松哥了解到很多小伙伴在找 Java 开发工作时，如果这个公司用的微服务架构，就觉得很牛逼，进去了很有前景，如果没用微服务，甚者还用的是以前的 SSH ，就会觉得没前景，不想去。由此可见微服务在大家心中的分量。 不过话说回来，并非每一个项目都是适合用微服务架构，也并非每一个公司都需要微服务架构。松哥有个朋友在某网红茶公司做微服务开发，新项目架构师强行上马微服务，结果项目上线后，一个小小的变更都要修改许多服务才能解决，没办法，架构师只能卷铺盖走人了，项目又变回了单体应用。 我觉得这样的例子不是个案，项目要不要上马微服务，还是要看项目和公司的具体情况，不盲目，不跟风。 上周和大家聊了单体应用存在的问题： 天天吹微服务，单体应用有啥不好？ 今天我就来和大家聊一聊微服务到底有哪些好处，又有哪些弊端。 微服务的优势大项目可以持续交付微服务将一个大系统拆分成很多个互相独立的服务，每一个服务都可以由一个团队去完成，并且配备自己的开发、部署，而且可以独立于其他的团队。每一个团队开发的微服务都可以由自己的代码仓库、以及部署流水线等，互不相扰。 在微服务中，一个大项目被拆分成 n 多个小项目，每一个小项目都可以非常方便的进行测试、部署，而不会牵一发而动全身，原本需要全员高度警戒的项目上线，现在分散到不同的团队中去完成。 松哥六月底参加深圳的一个线下技术活动，某在线编程的 CEO 谈到他们公司的发版，说：“我说话的这会儿，我们可能就有新版本在发布。”，这句话令我印象深刻。传统的单体应用，没人敢这么搞，微服务时代，这一切才变得可能。 易于维护这个不必多说，相信大家都理解。 一个传统的单体应用，如果你新接手，一时半会还不一定能理出一个头绪，而如果是微服务，由于比较小巧玲珑，一个微服务只负责一件事情，很容易理出头绪，然后上手开发。 并且相对于单体应用，微服务规模都比较小，无论你用 Eclipse 还是 IDEA，项目启动、测试速度都比较快。 服务可以独立扩展独立扩展，可以让我们充分使用硬件资源。 传统的单体应用，所有的功能模块都写在一起，有的模块是 CPU 运算密集型的，有的模块则是对内存需求更大的，这些模块的代码写在一起，部署的时候，我们只能选择 CPU 运算更强，内存更大的机器，如果采用了了微服务架构，不同的系统独立部署，压力大的时候，可以独立进行集群化部署，这些操作都不会影响到已经运行的其他微服务，非常灵活。 更强的容错性由于每一个微服务都是独立运行的，处理得当，我们在微服务架构中可以实现更好的故障隔离。当一个微服务发生问题时，例如内存泄漏，不会影响到其他的微服务。 可以灵活的采用最新技术传统的单体应用一个非常大的弊端就是技术栈升级非常麻烦，这也是为什么你经常会见到用 10 年前的技术栈做的项目，现在还需要继续开发维护。不是他们不愿意升级，而是升级实在是太麻烦了，伤筋动骨。 而在微服务架构中，每一个服务都是独立运行的，单个微服务的技术升级则非常容易。你可以随意去尝试你喜欢的最新技术。因为试错成本很低，因此大家可以尽情的玩耍。 微服务的弊端事物都有两面性，微服务也有一些挑战，这些挑战性问题如果处理不好，你使用微服务可能反而适得其反。那么都有哪些问题呢？ 服务的拆分 个人觉得，这是最大的挑战，我了解到一些公司做微服务，但是服务拆分的乱七八糟。这样到后期越搞越乱，越搞越麻烦，你可能会觉得微服务真坑爹，后悔当初信了松哥的说微服务好的鬼话。 分布式系统带来的挑战 记得以前在网上看到过一个段子： 没用分布式架构之前，你只有一个问题：并发性能不足。用了分布式架构，多出了一堆问题：数据如何同步、主键如何产生、如何熔断、分布式事务如何处理……。 这个段子形象的说明了分布式系统带来的挑战。 多个研发团队的协调管理 传统的单体应用开发，一个团队管理好就行了，现在不同的团队开发不同的微服务，要协调多个团队共同配合，才能做好微服务开发，这对项目管理提出了挑战。 好了，本文就先说这么多，大伙可以留言说说你的项目有没有使用微服务，出于什么样的考虑而使用了目前的架构呢？ 参考资料： [1] Chris Richardson.微服务架构设计模式[M].北京：机械工业出版社，2019.","link":"/2019/0805/microservice.html"},{"title":"还在用 Dockerfile 部署 Spring Boot？out 啦！试试谷歌的大杀器 Jib","text":"之前松哥和大家分享过一篇将 Spring Boot 项目部署到远程 Docker 上的文章： 一键部署 Spring Boot 到远程 Docker 容器 但是这种部署有一个问题，就是一个小小的 helloworld 构建成镜像之后，竟然都有 660 MB+，这就有点过分了；而且这种方式步骤繁琐，很多人看了头大。 因此松哥今天想再和大家聊一聊另外一种方案 Jib，这是谷歌开源的一个容器化运行方案，使用它我们将 Spring Boot 进行容器化部署只要两步： 第一步配置 Maven Plugin 第二步构建 我们一起来看看。 Jib在之前那篇文章中，我们将 Spring Boot 项目进行容器化部署，要求开发人员要有一定的 Docker 技能作为支撑，然而在实际开发中，并非每个人都是 Docker 专家，或者说会用 Docker。 有鉴于此，Google 搞出来一个 Jib，使 Spring Boot 容器化部署变得更加简便，开发人员可以不需要任何 Docker 相关的技能，就能将 Spring Boot 项目构建成 Docker 中的镜像，而且还可以“顺便”将镜像 push 到 register 上，极大的简化了部署过程。 Jib 使用 Java 开发，使用也非常简单，可以作为 Maven 或者 Gradle 的插件直接集成到我们的项目中。它利用镜像分层和注册表缓存来实现快速、增量的构建。Jib 会自动读取项目的构建配置，代码组织到不同的层（依赖项、资源、类）中，然后它只会重新构建和推送发生变更的层。在项目进行快速迭代时，Jib 只将发生变更的层推送到 registers 来缩短构建时间。 好了，大致了解了 Jib 之后，接下来我们来看看 Jib 要怎么使用。 准备工作Jib 可以直接将构建好的镜像 push 到 registers 上，如果公司有自己的私有镜像站的话，可以直接推送到私有镜像站上，本文我就将构建好的镜像推送到官方的 Docker Hub 上，因此需要大家提前准备一个 Docker Hub 的账号，账号大家可以直接去 Docker Hub 上面注册（https://hub.docker.com/），大家要是对 Docker Hub 这些东西不了解，可以在公众号后台回复 docker，获取松哥自制的 Docker 教程。 牛刀小试首先我们来创建一个 Spring Boot 工程，创建时只需要添加一个 Web 依赖即可： 项目创建成功后，添加一个 HelloController 用来做测试： 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello jib\"; }} 然后，在 pom.xml 中添加上 Jib 的插件，如下： 12345678910111213141516171819202122232425262728&lt;plugin&gt; &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt; &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;configuration&gt; &lt;from&gt; &lt;image&gt;openjdk:alpine&lt;/image&gt; &lt;/from&gt; &lt;to&gt; &lt;image&gt;docker.io/wongsung/dockerjib&lt;/image&gt; &lt;tags&gt; &lt;tag&gt;v1&lt;/tag&gt; &lt;/tags&gt; &lt;auth&gt; &lt;username&gt;wongsung&lt;/username&gt; &lt;password&gt;你的密码&lt;/password&gt; &lt;/auth&gt; &lt;/to&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 关于这段配置，我说如下几点： 首先就是版本号的问题，我这里使用的是 1.7.0 ，网上有的教程比较老，用的 0.x 的版本，老的版本在配置 Docker 认证的时候非常麻烦，所以版本这块建议大家使用当前最新版。 from 中的配置表示本镜像构建所基于的根镜像为 openjdk:alpine to 中的配置表示本镜像构建完成后，要发布到哪里去，如果是发布到私有镜像站，就写自己私有镜像站的地址，如果是发布到 Docker Hub 上，就参考我这里的写法 docker.io/wongsung/dockerjib，其中 wongsung 表示你在 Docker Hub 上注册的用户名，dockerjib 表示你镜像的名字，可以随意取。 tags 中配置的是自己镜像的版本。 auth 中配置你在 Docker Hub 上的用户名/密码。 executions 节点中的就是常规配置了，我就不再多说了。 配置完成后，在命令行执行如下命令将当前下项目构建成一个 Docker 镜像并 push 到 Docker Hub： 1mvn compile jib:build 构建完成后，我们在 Docker Hub 上就能看到自己的镜像了： 接下来，启动 Docker ，在 Docker 中执行如下命令拉取镜像下来并运行： 1docker run -d --name mydockerjib -p 8080:8080 docker.io/wongsung/dockerjib:v1 启动成功后，我们在浏览器中就可以直接访问我们刚才的 Spring Boot 项目中的 hello 接口了： 是不是很方便？比我第一次给大家介绍的方案要方便很多。 注意 这种方式是将项目构建成镜像后并 push 到 registers 上，这种构建方式不需要你本地安装 Docker，如果你需要在本地运行镜像，那当然需要 Docker，单纯的构建是不需要 Docker 环境的。 本地构建如果你电脑本地刚好安装了 Docker ，有 Docker 环境，那么也可以将项目构建成本地 Docker 的镜像， 首先我们来查看一下本地镜像： 可以看到只有 MySQL 镜像，然后我们执行如下命令构建本地镜像： 1mvn compile jib:dockerBuild 看到如下构建日志信息表示构建成功： 构建完成后，我们再来看本地镜像： 可以都看到，已经构建成功了，接下来启动命令和上面一样，我就不重复展示了。 结语容器的出现，让我们的 Java 程序比任何时候都接近“一次编写，到处运行”，Spring Boot 容器化部署也是越来越方便，后面有空松哥再和大家聊聊结合 jenkins 的用法，好了，本文的案例我已经上传到 GitHub：https://github.com/lenve/javaboy-code-samples，有问题欢迎留言讨论。","link":"/2019/1107/docker-springboot.html"},{"title":"Docker 容器基本操作[Docker 系列-2]","text":"docker 中的容器就是一个轻量级的虚拟机，是镜像运行起来的一个状态，本文就先来看看容器的基本操作。 镜像就像是一个安装程序，而容器则是程序运行时的一个状态。 查看容器查看容器启动 docker 后，使用 docker ps 命令可以查看当前正在运行的容器： 查看所有容器\b上面这条命令是查看当前正在运行的容器，如果需要查看所有容器，则可以通过 docker ps -a 命令查看： 在查看容器时，涉及到几个查看参数，含义分别如下： CONTAINER ID:CONTAINER ID是指容器的id，是一个唯一标识符,这是一个64位的十六进制整数，在不会混淆的情况下可以只采用id的前几位进行\b标识一个容器。 IMAGE:IMAGE表示创建容器时使用的镜像。 COMMAND:COMMAND表示容器最后运行的命令。 CREATED:创建容器的时间。 STATUS:容器的状态，这里可能显示一个容器启动时间，也能显示容器关闭时间。具体显示哪个要看容器当前的状态。 PORTS:容器对外开放的端口。 NAMES:容器的名字，如果不设置，会有一个默认的名字。 查看最新创建的容器使用 docker ps -l 可以查看最近创建的容器，如下： 查看最新创建的n个容器可以使用 docker ps -n=XXX 来查看最新创建的n个容器，如下： 创建容器创建容器整体上来说有两种不同的方式，可以先创建，再启动，也可以连创建带启动一步到位，无论是那种方式，流程都是相似的，当执行一个创建命令之后，docker 首先会去本地路径下查找是否有相应的镜像，如果没有，就去 docker hub 上\b搜索\b，如果搜索到了，则下载下来，然后利用该镜像\b创建一个容器并启动。容器的文件系统是\b在只读的镜像文件上添加一层可读写的文件层，这样可以使在不改变镜像的情况下，只记录改变的数据。下面对这两种方式分别予以介绍。 容器创建开发者可以首先使用\b docker create 命令\b创建一个容器，这个时候创建出来的容器是处于停止状态，\b没有运行，\b例如要创建一个 nginx 容器，\b创建命令如下： 1docker create nginx 创建成功后，可以查看容器是否创建成功： 此时创建的容器并未运行，处于停止状态，\b容器的 name 是随机生成的，开发者也可以在创建容器时指定 name ，如下： 1docker create --name=nginx nginx 运行结果如下： 此时的 name 属性就不是随机生成的，而是用户指定的 name。 这种方式只是单纯的创建了一个用户，并未启动。 容器创建+启动如果开发者需要既创建又启动容器，则可以使用 docker run 命令。 docker run 命令又可以启动两种不同模式的容器：\b后台型容器和交互型容器，\b顾名思义，\b后台型容器就是一个在后台运行的容器，默默的在后台执行计算就行了，不需要和开发者进行交互，而交互型容器则\b需要接收开发者的输入进行处理给出反馈。对于开发者而言，大部分情况下创建的都是后台型容器，不过在很多时候，即使是后台型容器也不可避免的需要进行交互，\b下面分别来看。 \b后台型容器后台型容器以 nginx 为例，一般 nginx 在后台运行即可： 1docker run --name nginx1 -d -p 8080:80 nginx --name 含义和上文一样，表示创建的容器的名字，-d 表示容器在后台运行，-p 表示将容器的 80 端口映射到宿主机的 8080 端口，\b创建过程如下图： 首先依然会去本地检查，本地\b没有相应的容器，则会去 Docker Hub 上查找，\b查找到了下载并运行，并且生成了一个容器 id。运行成功后，在浏览器中输入 http://localhost:8080 就能看到 Nginx 的默认页面了，如下： 这是一个后台型容器的基本创建方式。 交互型\b容器也可以创建交互型容器，例如创建一个 ubuntu 容器，开发者可能需要在 ubuntu 上面输入\b命令执行相关操作，\b交互型\b容器创建方式如下： 1docker run --name ubuntu -it ubuntu /bin/bash 参数含义都和上文一致，除了 -it，-it\b 参数，i 表示开发容器的标准输入（STDIN），t 则表示告诉 \bdocker，为容器创建一个\b命令行终端。执行结果如下： 该命令执行完后，会打开一个输入终端，读者就可以在这个终端里愉快的操作 ubuntu 了。 想要退出该终端，只需要输入 exit 命令即可。 容器启动启动如果开发者使用了 docker run 命令创建了容器，则创建完成后容器就已经启动了，如果使用了 docker create 命令创建了容器，则需要再执行 docker start 命令来启动容器，使用 docker start 命令结合容器 id 或者容器 name 可以启动一个容器，如下： docker start 启动的是一个已经存在的容器，要使用该命令启动一个容器，必须要先知道容器的 id 或者 name ，\b开发者可以通过这两个属性启动一个容器（案例中，nginx 是通过 name 启动，而 ubuntu 则是通过 id 启动）。一般来说，第一次可以使用 docker run 启动一个容器，以后直接使用 docker start 即可。 重启容器在\b\b运行过程中，会不可避免的出问题，出了问题时，需要能够自动重启，在容器启动时使用\b –restart 参数可以实现这一需求。根据 docker 官网的解释，docker 的重启策略可以分为 4 种，如下： 四种的含义分别如下： no表示不自动重启容器，默认即此。 on:failure:[max-retries]表示在退出\b状态为非0时才会重启（非正常退出），有一个可选择参数：最大重启次数，可以设置\b最大重启次数，重启次数达到上限后就会放弃重启。 always表示始终重启容器，当docker守护进程启动时，也会\b无论容器当时的状态为何，都会尝试重启\b容器。 unless-stopped表示始终重启容器，但是当docker守护进程启动时，如果\b容器已经停止运行，则不会去重启它。 容器停止通过\b docker stop 命令可以终止一个容器，如下： 可以通过 name 或者 id 终止一个容器。 容器删除单个删除容器停止\b后还依然存在，如果需要\b，还可以通过 docker start 命令再次重启一个容器，\b\b如果不需要一个容器，则可以通过 docker rm 命令删除一个容器。删除容器时，只能删除已经停止运行的容器，不能删除正在运行的容器。如下： 可以通过 name 或者 id 删除一个容器。如果非要删除一个\b正在运行的容器，可以通过 -f 参数实现，如下： 批量删除容器也可以批量删除，命令如下： 1docker rm $(docker ps -a -q) docker ps -a -q 会列出所有容器的 id ，供 rm 命令删除。 如下命令也\b\b支持删除已退出的孤立的容器： 1docker container prune 总结本文主要向大家介绍了 Docker 容器的基本操作，更多高级操作我们将在下篇文章中介绍。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0524/docker-container-basic.html"},{"title":"Docker 镜像基本操作[Docker 系列-4]","text":"镜像也是 docker 的核心组件之一，镜像时容器运行的基础，容器是镜像运行后的形态。前面我们介绍了容器的用法，今天来和大家聊聊镜像的问题。 总体来说，镜像是一个包含程序运行必要以来环境和代码的只读文件，它采用分层的文件系统，将每一层的改变以读写层的形式增加到原来的只读文件上。这有点像洋葱，一层一层的，当我们后面学习了 Dockerfile ，相信大家对于这样的架构理解将更为准确。 镜像与容器的关系前文已经向读者介绍过容器的使用了，细心的读者可能已经发现，容器在启动或者创建时，必须指定一个镜像的名称或者 id ，其实，这时镜像所扮演的角色就是容器的模版，不同的镜像可以构造出不同的容器，同一个镜像，我们也可以通过配置不同参数来构造出不通的容器。如下命令： 1docker run -itd --name nginx nginx 命令中的最后一个 nginx 即表示创建该容器所需要的镜像（模版），当然这里还省略了一些信息，例如版本号等，这些我们后文会详细介绍。 镜像的体系结构镜像的最底层是一个启动文件系统（bootfs）镜像，bootfs 的上层镜像叫做根镜像，一般来说，根镜像是一个操作系统，例如 Ubuntu、CentOS 等，用户的镜像必须构建于根镜像之上，在根镜像之上，用户可以构建出各种各样的其他镜像。从上面的介绍读者可以看出，镜像的本质其实就是一系列文件的集合，一层套一层的结构有点类似于 Git ，也有点类似于生活中的洋葱。 镜像的写时复制机制通过 docker run 命令指定一个容器创建镜像时，实际上是在该镜像之上创建一个空的可读写的文件系统层级，可以将这个文件系统层级当成一个临时的镜像来对待，而命令中所指的模版镜像则可以称之为父镜像。父镜像的内容都是以只读的方式挂载进来的，容器会读取共享父镜像的内容，用户所做的所有修改都是在文件系统中，不会对父镜像造成任何影响。当然用户可以通过其他一些手段使修改持久化到父镜像中，这个我们后面会详细介绍到。 简而言之，镜像就是一个固定的不会变化的模版文件，容器是根据这个模版创建出来的，容器会在模版的基础上做一些修改，这些修改本身并不会影响到模版，我们还可以根据模版（镜像）创建出来更多的容器。 如果有必要，我们是可以修改模版（镜像）的。 镜像查看用户可以通过 docker images 命令查看本地所有镜像，如下： 这里一共有五个参数，含义分别如下： TAG: TAG用于区分同一仓库中的不同镜像，默认为latest。 IMAGE ID: IMAGE ID是镜像的一个唯一标识符。 CREATED: CREATED表示镜像的创建时间。 SIZE: SIZE表示镜像的大小。 REPOSITORY:仓库名称，仓库一般用来\b存放同一类型的镜像。仓库的名称由其创建者指定。如果没有指定则为 &lt;none&gt; 。一般来说，仓库名称有如下几种不同的形式: [namespace\\ubuntu]:这种仓库名称由命名空间和实际的仓库名组成，中间通过 \\ 隔开。当开发者在 Docker Hub 上创建一个用户时，用户名就是默认的命名空间，这个命令空间是用来区分 Docker Hub 上注册的不同用户或者组织（类似于 GitHub 上用户名的作用），如果读者想将自己的\b镜像上传到 Docker Hub 上供别人使用，则必须指定命名空间，否则上传会失败。 [ubuntu]:这种只有仓库名，对于这种没有命名空间的仓库名，可以认为其属于顶级命名空间，该空间的仓库只用于官方的镜像，由 Docker 官方进行管理，但一般会授权给第三方进行开发维护。当然用户自己创建的镜像也可以使用这种命名方式，但是将无法\b上传到 Docker Hub 上共享。 [hub.c.163.com/library/nginx]:这种指定 url 路径的方式，一般用于非\b Docker Hub 上的镜像命名，例如一个第三方服务商提供的镜像或者开发者\b自己搭建的镜像中心，都可以使用这种命名方式命名。 使用 docker images 命令可以查看本地所有的镜像，如果镜像过多，可以通过通配符进行匹配，如下： 如果需要查看镜像的详细信息，也可以通过上文提到的 docker inspect 命令来查看。 镜像下载当用户执行 docker run 命令时，就会自动去 Docker Hub 上下载相关的镜像，这个就不再重复演示，开发者也可以通过 search 命令去 \bDocker Hub 上搜索符合要求的镜像，如下： 其中： NAME：表示镜像的名称。 DESCRIPTION：表示镜像的简要描述。 STARS：表示用户对镜像的评分，评分越高越可以放心使用。 OFFICIAL：是否为\b官方镜像。 AUTOMATED：是否使用了自动构建。 在执行 docker run 命令时再去下载，速度会有点慢，如果希望该命令能够快速执行，可以在执行之前，先利用 docker pull 命令\b将\b镜像先下载下来，然后再\b运行。 运行命令如下： 镜像删除镜像可以\b通过 docker rmi 命令进行删除，参数为镜像的id或者镜像名，参数可以有多个，多个参数之间用空格隔开。如下： 有的时候，无法删除一个镜像，大部分原因是因为该镜像被一个容器所依赖，此时需要先删除容器，然后就可以删除镜像了，删除容器的命令可以参考本系列前面的文章。 通过前面文章的阅读，读者已经了解到所谓的容器实际上是在父镜像的基础上创建了一个可读写的文件层级，所有的修改操作都在这个文件层级上进行，而父镜像并未受影响，如果读者需要根据这种修改创建一个新的本地镜像，有两种不同的方式，先来看第一种方式：commit。 创建容器首先，根据本地镜像运行一个容器，如下： 命令解释： 首先执行 docker images 命令，查看本地镜像。 根据本地镜像中\b的 nginx 镜像，创建一个名为 nginx 的容器，并启动。 将宿主机中一个名为 index.html 的文件\b拷贝到容器中。 访问容器，发现改变已经生效。 接下来再重新创建一个容器，名为 nginx2. 访问 nginx2 ，发现 nginx2 中默认的页面还是 nginx 的默认页面，并未发生改变。 commint 创建本地镜像接下来，根据\b\b刚刚创建的第一个容器，创建一个本地镜像，如下： 命令解释： 参数 -m 是对创建的该镜像的一个简单描述。 –author 表示该镜像的作者。 ce1fe32739402 表示创建镜像所依据的容器的 id。 sang/nginx 则表示仓库名，sang 是名称空间，nginx 是镜像名。 v1 表示仓库的 tag。 创建完成后，通过 docker images 命令就可以查看到刚刚创建的镜像。 通过刚刚创建的镜像运行一个容器，访问该容器，发现 nginx 默认的首页已经发生改变。 这是我们通过 commint 方式创建本地镜像的方式，但是 commit 方式存在一些问题，比如不够透明化，无法重复，体积较大，为了解决这些问题，可以考虑使用 Dockerfile ，实际上，主流方案也是 Dockerfile。 DockerfileDockerfile 就是一个普通的文本文件，其内包含了一条条的指令，每一条指令都会构建一层。先来看一个简单的例子。 首先在一个空白目录下创建一个名为 Dockerfile 的文件，内容如下： 命令解释： FROM nginx 表示该镜像的构建，以已有的 nginx 镜像为基础，在该镜像的基础上构建。 MAINTAINER 指令用来声明创建镜像的作者信息以及邮箱信息，这个命令不是必须的。 RUN 指令用来修改镜像，算是使用比较频繁的一个指令了，\b该指令可以用来安装程序、安装库以及配置应用程序等，一个 RUN 指令执行会在当前镜像的基础上创建一个新的镜像层，接下来的指令将在这个新的镜像层上执行，RUN 语句有两种不同的形式：shell 格式和 exec 格式。本案例采用的 shell 格式，shell 格式就像 linux 命令一样，exec 格式则是一个 JSON 数组，将命令放到数组中即可。在使用 RUN 命令时，适当的时候可以将多个 RUN 命令合并成一个，这样可以避免在创建镜像时创建过多的层。 COPY 语句则是将镜像上下文中的 hello.html 文件拷贝到镜像中。 文件创建完成后，执行如下命令进行构建： 命令解释： -t 参数用来指定镜像的命名空间，仓库名以及 TAG 等信息。 最后面的 . 是指镜像构建上下文。 注意： Docker 采用了 C/S 架构，分为 Docker 客户端（Docker 可执行程序）与 Docker 守护进程，Docker 客户端通过命令行和 API 的形式与 Docker 守护进程进行通信，Docker 守护进程则提供 Docker 服务。因此，我们操作的各种 docker 命令实际上都是由 docker\b 客户端发送到 docker 守护进程上去执行。我们在构建一个镜像时，不可避免的需要将一些本地文件拷贝到镜像中，例如上文提到的 COPY 命令，用户在构建镜像时，需要指定构建镜像的上下文路径（即前文的 . ）, docker build 在获得这个路径之后，会将路径下的所有内容打包，然后上传给 Docker 引擎。 镜像\b构建成功后，可以通过 docker images 命令查看，如下： 然后创建\b容器并启动，就可以看到之前的内容都生效了。 总结本文主要向大家介绍了 Docker 中镜像的基本操作，操作其实并不难，关键是理解好镜像和容器的关系，以及镜像洋葱式的文件结构。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0526/docker-images.html"},{"title":"IDEA 神器入坑指南！17 个常用快捷键奉上！","text":"我前几年写过一个 Android Studio 中的快捷键教程，发表在 CSDN 上，最近有小伙伴看我讲 Spring Boot 视频，IDEA 用的还比较溜，问我有没有整理好的快捷键，我一般就直接把当时写的 Android Studio 的快捷键发给他。因为大家知道 as 其实就是基于 IDEA 来做的。 不过鉴于很多小伙伴有快速掌握 IDEA 的需求 ，因此我打算抽空写一个系列的教程，带着大家从头到尾来学习下 IDEA 的使用。欢迎小伙伴们搬好小板凳持续关注。 本文就当作是一个引子吧。 我为什么用 IDEA曾经我也是 Eclipse 坚定的拥趸者。 刚开始学 Java 那会，用了一个非常小巧的开发工具 JCreator，只有几 MB 大小，分为社区版和专业版，专业版是收费的。校公选课上老师就用了这个工具，我也跟着用这个，用这个学完了整个 JavaSE，很多基础的算法题我都是用这个工具完成的。比起现在动辄几百 MB 几 G 的 IDE 而言，这个真的可以算作是小巧玲珑。今天还特意去网上搜了一下 JCreator，发现已经好久没有更新了，估计是凉了。 后来接触到 JavaWeb 之后，就用了 Eclipse 系的 IDE 了。 Eclipse、MyEclipse 以及 Spring Tool Suite，这些都算是 Eclipse 系的 IDE，其中 STS 使用时间最久了。大学毕业后做过一段时间的 Android 开发，当时用的也是 Eclipse。当时的 Eclipse 用的滚瓜烂熟，从 Eclipse 切换到 IDEA 上犹豫了好久，后来想想我还年轻，程序员之路才刚刚开始，不应该放弃尝试新事物，于是就尝试切换到 Android Studio 上，大概一周时间，各种快捷键就用顺手了，并且慢慢喜欢上了这个开发工具，后来又做回老本行 Java 后端，就顺手用了 IntelliJ IDEA，这两年前后端都做，前端果断选择 WebStorm，快捷键都是一样的，也是很顺手。 另外 Java 官方还有一个 IDE ，NetBeans，这个工具这几年存在感愈来愈弱。我自己也只是在刚开始学习 Java 的时候好奇尝试过，基本上没用这个做过项目。 IDEA 介绍IDEA 全称 IntelliJ IDEA，由 JetBrains 公司开发，公司总部位于捷克共和国的首都布拉格，开发人员以严谨著称的东欧程序员为主。对于 IDEA ，该公司致力于提供一个面向 JVM，功能强大且符合人体工程学的 IDE，所谓的面向 JVM，就是 IDEA 要支持所有运行在 JVM 上的开发语言，例如 Scala，Kotlin 等。 IDEA 在业界被公认为最好的 Java 开发工具之一，尤其在智能代码助手、代码自动提示、重构、J2EE支持、Maven、JUnit、Svn、Git、代码审查等方面的功能可以说是超常的。 IDEA 中的版本问题IDEA 目前共分为两个版本，社区版和旗舰版。社区版功能单一但是免费，使用社区版直接创建 Maven 项目或者 Spring Boot 项目都不支持，旗舰版功能丰富但是收费。我自己因为在 GitHub 上有几个比较受欢迎的开源项目（https://github.com/lenve），因为这些项目，我申请到 JetBrains 一整套的官方授权。 大家安装办法很多，这个应该不用我多说，大伙都有办法搞定。 IDEA 各种特性IDEA 中有很多好玩的特性，我们来逐个看下： 智能的选取 有的时候我们可能需要从某个变量到表达式到方法甚至到类，扩充者选取，这个时候就可以使用 Ctrll+W 来实现： 丰富的导航模式 IDEA 提供了丰富的导航查看模式，例如 Ctrll+E 显示最近打开过的文件: Ctrll+N 或者连按两下 Shift，会出现一个类名搜索框，有的时候查看源码用这个非常方便。 历史记录功能 在 IDEA 中，你可以不用借助 Git 或者 Svn 之类的工具，就可以查看文件修改历史。 辅助编码 这个其实算不上两点，因为基本上各种 Java 开发工具都有。Java Bean 中常见的 toString()、hashCode()、equals() 以及所有的get/set 方法都可以自动生成（Windows 是 Alt+Insert，Mac 是 Command +N）。 XML 的完美支持 这个算是最赞的功能之一了。Eclipse 中做 Spring 开发，多多少少你得提前准备好一套 Spring 配置模板，而在 IDEA 中则不需要。引入 Spring 依赖之后，就会有 Spring 的 XML 模板，可以直接用。因此，如果使用 IDEA，每个人都可以不用参考任何外部文件配置 SSM。 列编辑模式 这个也是我经常使用的功能之一，在有的场景下非常方便，按住 Alt 键就可以快速实现列编辑： 预置模板 这个基本上大部分 IDE 都有，只不过在 IDEA 中，提供的内置模板更加丰富，例如 main 方法的生成： 这是系统自带的，按下 Ctrll+J，然后点击右上角的小灯泡，我们也可以自定义代码模板。 对 Git 的友好支持 IDEA 集成了目前大部分的版本工智工具插件，例如 CVS、Svn、Git 等，包括 GitHub 也可以在 IDEA 中非常愉快的使用。 智能代码 自动检查代码，发现与预置规范有出入的代码给出提示，自动完成修改。 当然还有很多其他特性，我就不一一列举了。本系列后面的文章会向大家逐个介绍。 从 Eclipse 切换过来注意事项如果大家是从 Eclipse 上切换到 IDEA ，我总结了以下几个常见问题： 快捷键问题 IDEA 支持使用 Eclipse 那一套快捷键，但是个人非常不建议切换，感觉没有必要，增加以后的使用成本，可能还会错过一些 IDEA 中非常棒的功能。当然如果你只是想稍微的尝试一下 IDEA，那就无所谓了。 保存问题 IDEA 中没有保存按钮，当然你也不用 Ctrll+S 了，工具会自动帮我们保存，这一点也和 Eclipse 不同。不知道大家有没有在工作中遇到突然停电的问题，我以前遇到过一次，幸好当时使用的是 IDEA，损失不大。 项目目录 这个可能是很多初次接触 IDEA 的小伙伴最不习惯的地方了。Eclipse 中一个窗口中可以打开多个 Project，也可以打开多个 Module，但是在 IDEA 中，一个窗口只能打开一个 Project（当然 Module 也是可以打开多个的），如果在 IDEA 中要打开一个新的 Project ，只能再打开一个新的窗口。 很多人将 IDEA 中的 Project 类比为 Eclipse 中的 workspace，将 IDEA 中的 module 类比为 Eclipse 中的 Project，我认为这个虽然形象但是极为不恰当的，Project 和 Module 就当成正常的 Project 和 Module 就行了。 常用快捷键这里主要是 Windows 上的快捷键，Mac 上部分快捷键会有一点点小小差异。 Ctrl+N 这个可以用来快速搜索类。 Ctrl+Shift+N 这个可以用来快速搜索文件。 Ctrl+ALT+SPACE 代码提示（类似于ALT+/），这个快捷键使用场景并不多，大部分情况下 IDEA 都会主动提示。 ALT+F7 这个用来查看某一个方法或者变量在哪里被使用了。 Ctrl+Q 查看代码提示及内容，这个说起来并没有 Eclipse 方便，Eclipse 中将光标放在类名或者方法名上就会出现 doc，但是在 IDEA 中需要按下 Ctrl+q 才会出现。 Ctrl+B 查看类的定义，也可以像 Eclipse 一样，按下 Ctrl 再鼠标左键单击。 Ctrl+F12 这个可以列出类中的所有方法。 SHIFT+F6 变量或者类名重命名。 ALT+INSERT 生成 get/set/toString/hashCode/equals 等方法 Ctrl+ALT+T 代码包裹，选中代码后，可以被 for/if/trycache 等代码块包裹： Ctrl+ALT+B 查看接口或者抽象类的子类。也可以通过 Ctrl+H 来查看。 Ctrl+D 代码复制到新的一行 Ctrl+Y 删除当前行 Ctrl+Alt+↑/↓ 代码向上或者向下移动 Ctrl+Alt+enter 在当前行的上面创建新的一行 Ctrl+enter 在当前行的下面新建一行（光标不用移动到当前行的最末尾处）。 好了，本文就当是一个引子吧，后面再来和大家详细聊聊 IDEA 中的其他细节。","link":"/2019/0827/intellij-idea.html"},{"title":"JavaWeb 乱码问题终极解决方案！","text":"经常有读者在公众号上问 JavaWeb 乱码的问题，昨天又有一个小伙伴问及此事，其实这个问题很简单，但是想要说清楚却并不容易，因为每个人乱码的原因都不一样，给每位小伙伴都把乱码的原因讲一遍也挺费时间的，因此，松哥今天决定写一篇文章，和大伙好好捋捋 JavaWeb 中的乱码问题。 对于一些老司机而言，其实并不太容易遇到乱码问题，但是对于一些新手来说，乱码几乎是家常便饭，而且每当乱码时，网上搜了一大堆解决方案，发现自己的问题还是没能解决，其实这就是平时研究代码不求甚解导致的，乱码问题，也要去分析，然后才能对症下药，才能药到病除。 整体思路首先出现乱码之后，要先去确认乱码的地方，当一个网页上出现乱码，有可能是浏览器显示问题，也有可能是 Java 编码问题，也有可能数据库中的数据本身就是乱码的，所以我们要做的第一件事就是确认乱码发生的位置，缩小 bug 范围，通过打印日志或者 debug 首先去确认乱码发生的位置，然后再去进一步解决，一般来说，乱码的原因大致上可以分为两类： 请求乱码 响应乱码 请求乱码，可能是因为参数放在 URL 地址中乱码，也有可能是参数放在请求体中乱码，不同传参方案也对应了不同的乱码解决方案。如果是响应乱码，那么原因就会比较多了，一般来说，有如下几种可能的原因： 数据库本身乱码 数据在 Java 代码中乱码 数据在浏览器显示的时候乱码 数据在从 Java 应用传到数据库的过程中乱码 对于不同的乱码原因，会有不同的解决方案，对症下药，才能药到病除，所以当出现乱码时，大家要做的第一件事就是分析乱码发生的原因，找到原因了，才能找到解决方案。 基本原则发生乱码是因为各自编码不同导致的，所以，大家首先要有一个良好的开发习惯，项目编码，文件编码都要统一起来，松哥有个同事就因为 Freemarker 乱码，找了半天没找到原因，后来在松哥建议下修改了项目编码，乱码问题才解决了，一般来说，公司制度稍微成熟一些，都会对项目编码，文件编码有硬性规定的。在Eclipse 中，设置项目编码方式如下（工程的编码要提前设置，如果项目已经开发一半再去设置，已有的中文就会乱码）： Window-&gt;Preferences-&gt;General 然后对于 JSP 文件也需要提前设置好编码方式，如下： 这是在 Eclipse 中设置文件编码，如果是在 IntelliJ IDEA中，则不需要设置JSP文件编码，因为默认就是 UTF-8，只需要提前设置下工程编码即可： 除了开发工具的编码，数据库的编码也要统一，一般来说，主要是设置一下数据库的编码和数据表的编码，如下： 设置数据库编码： 1CREATE DATABASE `vhr` DEFAULT CHARACTER SET utf8; 设置数据表编码： 123456DROP TABLE IF EXISTS `adjustsalary`;CREATE TABLE `adjustsalary` ( `id` int(11) NOT NULL AUTO_INCREMENT, `eid` int(11) DEFAULT NULL, PRIMARY KEY (`id`),) ENGINE=InnoDB DEFAULT CHARSET=utf8; 这些是准备工作，这些工作做好了，还是有可能会遇到乱码问题，接下来我们就具体问题具体分析。 请求乱码请求乱码，就是说数据在浏览器中显示是正常的，但是传到 Java 后端之后，就乱码了，这种乱码一般来说，分为两种： 参数放在 URL 地址中导致的乱码 参数放在请求体中导致的乱码 两种乱码原因，对应了两种不同的解决方案。分别来看。 URL 地址中的参数乱码这种乱码主要发生在 GET 请求中，因为在 GET 请求中我们一般通过 URL 来传递参数，这个问题可以在代码中解决，但是太过于麻烦，因此一般我们直接在Tomcat配置中解决，修改 Tomcat的conf/server.xml 文件，修改 URL 编码格式，如下： 这样就可以搞定 URL 地址中的参数乱码。 请求体中的参数乱码请求体中的参数乱码，我们可以在解析参数之前通过设置 HttpServletRequest 的编码来解决，如下： 1request.setCharacterEncoding(\"UTF-8\"); 但是一样也太过于麻烦，所以如果是普通的 Servlet/JSP 项目，我们就可以直接定义一个过滤器来处理，如下： 1234567public class EncodingFilter implements Filter { @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { request.setCharacterEncoding(\"UTF-8\"); chain.doFilter(request, response); }} 过滤器配置： 12345678&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.sang.filter.EncodingFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 在工程编码和JSP/HTML编码都没问题的情况下，请求乱码基本上就是这两种情况。 响应乱码如果在浏览器上加载页面看到了乱码，大家首先要确认在从服务端往浏览器写数据的前一刻，这个数据还没有乱码（即数据库中查询出来的数据是OK的，没有发生乱码的问题），那么对于这种乱码，我们只需要设置响应数据的 ContentType 就可以了，如下： 1response.setContentType(\"text/html;charset=UTF-8\"); 如果从数据库中查询出来的数据就是乱码的，那么就需要去确认数据库中的编码是否 OK 。 框架处理前面提到的方案，都是在 Servlet/JSP 项目中我们可以采用的方案，在 SSM 框架中当然也可以使用，但是，SpringMVC 框架本身也提供了一个过滤器，我们可以借用这个过滤器更加高效的解决响应乱码问题，如下： 1234567891011121314151617181920&lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceRequestEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceResponseEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 当然，上面这段配置并不能代替 Tomcat 中 conf/server.xml 中的编码配置，如果是在 Spring Boot 中，配置可以更加简单，只需要在 application.properties 中添加如下配置即可： 123server.tomcat.uri-encoding=UTF-8spring.http.encoding.force-request=truespring.http.encoding.force-response=true 其他乱码其他乱码主要是指使用一些第三方框架导致的乱码，例如使用 Alibaba 的 fastjson，开发者就需要在配置 HttpMessageConverter 时指定编码格式，否则就有可能出现乱码，这种第三方框架的乱码松哥没法穷举，大伙在使用时需要注意看官方文档，fastjson 的 HttpMessageConverter 配置如下： 123456789@BeanFastJsonHttpMessageConverter fastJsonHttpMessageConverter() { FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); FastJsonConfig config = new FastJsonConfig(); config.setCharset(Charset.forName(\"UTF-8\")); converter.setFastJsonConfig(config); converter.setDefaultCharset(Charset.forName(\"UTF-8\")); return converter;} 一个隐蔽的乱码除了前面介绍的这几种乱码之外，还有一个比较隐蔽的乱码，容易被很多初学者忽略的地方，就是数据在从 Java 应用传递到 MySQL 的过程中，发生了乱码，这种问题一般在 Windows 上不易发生，如果数据库装在 Linux 上，则这个问题就很容易发生，数据在代码中命名没有乱码，存到 MySQL 上就乱码了，但是如果直接使用 Navicat 等工具往 MySQL 上存储数据，又不会乱码，或者 MySQL 中数据没有乱码，但是用 Java 查询出来就乱码了，这种都是数据在 应用 和 数据库 之间传递时发生了乱码，解决方式很简单，在数据库连接地址上指定编码即可，如下： 1db.url=jdbc:mysql:///yuetong?useUnicode=true&amp;characterEncoding=UTF-8 大致就这些，还有一些非常偶尔的情况可能会用到 @RequestMapping 注解中的 produces 属性，在这里指定数据类型即可。 好了，差不多就这些，下次有人问你为啥我的又乱码了，直接把这篇文章甩给他。大伙有什么解决乱码的独门密器也可以一起来讨论。","link":"/2019/0409/javaweb-encoding.html"},{"title":"Redis 散列与有序集合","text":"前面文章我们介绍了列表与集合中的基本命令，本文我们来看看Redis中的散列与有序集合。 本文是 Redis 系列的第六篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合 散列很多时候，散列就像一个微缩版的 redis ，在本文中，小伙伴们对看到的许多散列命令都会有似曾相识的感觉。 HSETHSET 命令可以用来设置 key 指定的哈希集中指定字段的值，如下： 12127.0.0.1:6379&gt; HSET k1 h1 v1(integer) 1 HGETHGET 命令可以用来返回 key 指定的哈希集中该字段所关联的值，如下： 12127.0.0.1:6379&gt; HGET k1 h1&quot;v1&quot; HMSETHMSET 命令可以批量设置 key 指定的哈希集中指定字段的值，如下： 12127.0.0.1:6379&gt; HMSET k2 h1 v1 h2 v2 h3 v3OK HMGETHMGET 可以批量返回 key 指定的哈希集中指定字段的值，如下： 1234127.0.0.1:6379&gt; HMGET k2 h1 h2 h31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot; HDELHDEL 命令可以从 key 指定的哈希集中移除指定的域，在哈希集中不存在的域将被忽略，如下： 12345678910127.0.0.1:6379&gt; HMGET k2 h1 h2 h31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; HDEL k2 h1(integer) 1127.0.0.1:6379&gt; HMGET k2 h1 h2 h31) (nil)2) &quot;v2&quot;3) &quot;v3&quot; HSETNXHSETNX 命令只在 key 指定的哈希集中不存在指定的字段时，设置字段的值，如果字段已存在，该操作无效果。如下： 1234127.0.0.1:6379&gt; HSETNX k2 h3 1(integer) 0127.0.0.1:6379&gt; HSETNX k2 h4 1(integer) 1 HVALSHVALS 命令可以返回 key 指定的哈希集中所有字段的值，如下： 1234127.0.0.1:6379&gt; HVALS k21) &quot;v2&quot;2) &quot;v3&quot;3) &quot;1&quot; HKEYSHKEYS 命令可以返回 key 指定的哈希集中所有字段的名字，如下： 1234127.0.0.1:6379&gt; HKEYS k21) &quot;h2&quot;2) &quot;h3&quot;3) &quot;h4&quot; HGETALLHGETALL 命令可以返回 key 指定的哈希集中所有的字段和值。返回值中，每个字段名的下一个是它的值，所以返回值的长度是哈希集大小的两倍，如下： 1234567127.0.0.1:6379&gt; HGETALL k21) &quot;h2&quot;2) &quot;v2&quot;3) &quot;h3&quot;4) &quot;v3&quot;5) &quot;h4&quot;6) &quot;1&quot; HEXISTSHEXISTS 命令可以返回 hash 里面 field 是否存在，如下： 12127.0.0.1:6379&gt; HEXISTS k2 h3(integer) 1 HINCRBYHINCRBY 可以增加 key 指定的哈希集中指定字段的数值。如果 key 不存在，会创建一个新的哈希集并与 key 关联。如果字段不存在，则字段的值在该操作执行前被设置为 0， HINCRBY 支持的值的范围限定在 64 位有符号整数，如下： 123456789101112131415127.0.0.1:6379&gt; HEXISTS k2 h3(integer) 1127.0.0.1:6379&gt;127.0.0.1:6379&gt; HGET k2 h4&quot;1&quot;127.0.0.1:6379&gt; HINCRBY k2 h4 5(integer) 6127.0.0.1:6379&gt; HGET k2 h4&quot;6&quot;127.0.0.1:6379&gt; HGET k2 h5(nil)127.0.0.1:6379&gt; HINCRBY k2 h5 99(integer) 99127.0.0.1:6379&gt; HGET k2 h5&quot;99&quot; HINCRBYFLOATHINCRBYFLOAT 与 HINCRBY 用法基本一致，只不过这里允许 float 类型的数据，不赘述。 HLENHLEN 返回 key 指定的哈希集包含的字段的数量，如下： 12127.0.0.1:6379&gt; HLEN k2(integer) 4 HSTRLENHSTRLEN 可以返回 hash 指定 field 的 value 的字符串长度，如果 hash 或者 field 不存在，返回 0 ，如下： 12127.0.0.1:6379&gt; HSTRLEN k2 h2(integer) 2 有序集合有序集合类似 Sets ,但是每个字符串元素都关联到一个叫 score 浮动数值。里面的元素总是通过 score 进行着排序，因此它是可以检索的一系列元素。 ZADDZADD 命令可以将所有指定成员添加到键为 key 的有序集合里面。添加时可以指定多个分数/成员（score/member）对。 如果指定添加的成员已经是有序集合里面的成员，则会更新该成员的分数（scrore）并更新到正确的排序位置。如下： 12127.0.0.1:6379&gt; ZADD k1 60 v1(integer) 1 ZSCOREZSCORE 命令可以返回有序集 key 中，成员 member 的score 值。如下： 12127.0.0.1:6379&gt; ZSCORE k1 v1&quot;60&quot; ZRANGEZRANGE 命令可以根据 index 返回 member ，该命令在执行时加上 withscores 参数可以连同 score 一起返回： 1234567891011121314127.0.0.1:6379&gt; ZRANGE k1 0 31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;4) &quot;v4&quot;127.0.0.1:6379&gt; ZRANGE k1 0 3 withscores1) &quot;v1&quot;2) &quot;60&quot;3) &quot;v2&quot;4) &quot;70&quot;5) &quot;v3&quot;6) &quot;80&quot;7) &quot;v4&quot;8) &quot;90&quot; ZREVRANGEZREVRANGE 和 ZRANGE 功能基本一致，不同的是 ZREVRANGE 是反着来的，如下： 1234567891011121314127.0.0.1:6379&gt; ZREVRANGE k1 0 31) &quot;v5&quot;2) &quot;v4&quot;3) &quot;v3&quot;4) &quot;v2&quot;127.0.0.1:6379&gt; ZREVRANGE k1 0 3 withscores1) &quot;v5&quot;2) &quot;100&quot;3) &quot;v4&quot;4) &quot;90&quot;5) &quot;v3&quot;6) &quot;80&quot;7) &quot;v2&quot;8) &quot;70&quot; ZCARDZCARD 命令可以返回 key 的有序集元素个数。如下： 12127.0.0.1:6379&gt; ZCARD k1(integer) 5 ZCOUNTZCOUNT 命令可以返回有序集 key 中，score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员。如下： 12127.0.0.1:6379&gt; ZCOUNT k1 60 90(integer) 4 如果在统计时，不需要包含 60 或者 90 ，则添加一个 ( 即可，如下： 12127.0.0.1:6379&gt; ZCOUNT k1 60 (90(integer) 3 ZRANGEBYSCOREZRANGEBYSCORE 命令可以按照 score 范围范围元素，加上 withscores 可以连 score 一起返回。如下： 12345678910111213141516127.0.0.1:6379&gt; ZRANGEBYSCORE k1 60 801) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE k1 60 80 withscores1) &quot;v1&quot;2) &quot;60&quot;3) &quot;v2&quot;4) &quot;70&quot;5) &quot;v3&quot;6) &quot;80&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE k1 (60 80 withscores1) &quot;v2&quot;2) &quot;70&quot;3) &quot;v3&quot;4) &quot;80&quot; ZRANKZRANK 命令可以返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。排名以 0 为底，即 score 值最小的成员排名为 0 。如下： 1234127.0.0.1:6379&gt; ZRANK k1 v1(integer) 0127.0.0.1:6379&gt; ZRANK k1 v2(integer) 1 ZREVRANKZREVRANK 和 ZRANK 命令功能基本一致，不同的是，ZREVRANK 中的排序是从大到小： 1234127.0.0.1:6379&gt; ZREVRANK k1 v1(integer) 4127.0.0.1:6379&gt; ZREVRANK k1 v2(integer) 3 ZINCRBYZINCRBY 命令可以为有序集 key 的成员 member 的 score 值加上增量 increment 。如果 key 中不存在 member ，就在 key 中添加一个 member ，score 是 increment（就好像它之前的 score 是0.0）。如果 key 不存在，就创建一个只含有指定 member 成员的有序集合： 12345127.0.0.1:6379&gt; ZINCRBY k1 3 v1&quot;63&quot;127.0.0.1:6379&gt; ZRANGE k1 0 0 withscores1) &quot;v1&quot;2) &quot;63&quot; ZINTERSTOREZINTERSTORE 命令可以计算给定的 numkeys 个有序集合的交集，并且把结果放到 destination 中。 在给定要计算的 key 和其它参数之前，必须先给定 key 个数( numberkeys )。该命令也可以在执行的过程中给原 score 乘以 weights 后再求和，如下： 1234567891011121314151617181920212223242526127.0.0.1:6379&gt; ZADD k2 2 v1(integer) 1127.0.0.1:6379&gt; ZADD k2 3 v2(integer) 1127.0.0.1:6379&gt; ZADD k2 4 v3(integer) 1127.0.0.1:6379&gt; ZADD k3 9 v2(integer) 1127.0.0.1:6379&gt; ZADD k3 10 v3(integer) 1127.0.0.1:6379&gt; ZADD k3 11 v4(integer) 1127.0.0.1:6379&gt; ZINTERSTORE k4 2 k2 k3(integer) 2127.0.0.1:6379&gt; ZRANGE k4 0 -1 withscores1) &quot;v2&quot;2) &quot;12&quot;3) &quot;v3&quot;4) &quot;14&quot;127.0.0.1:6379&gt; ZINTERSTORE k5 2 k2 k3 weights 3 1(integer) 2127.0.0.1:6379&gt; ZRANGE k5 0 -1 withscores1) &quot;v2&quot;2) &quot;18&quot;3) &quot;v3&quot;4) &quot;22&quot; ZREMZREM 命令可以从集合中弹出一个元素，如下： 1234567891011121314127.0.0.1:6379&gt; ZRANGE k2 0 -1 withscores1) &quot;v1&quot;2) &quot;2&quot;3) &quot;v2&quot;4) &quot;3&quot;5) &quot;v3&quot;6) &quot;4&quot;127.0.0.1:6379&gt; ZREM k2 v1(integer) 1127.0.0.1:6379&gt; ZRANGE k2 0 -1 withscores1) &quot;v2&quot;2) &quot;3&quot;3) &quot;v3&quot;4) &quot;4&quot; ZLEXCOUNTZLEXCOUNT 命令用于计算有序集合中指定成员之间的成员数量。如下： 1234127.0.0.1:6379&gt; ZLEXCOUNT k2 - +(integer) 2127.0.0.1:6379&gt; ZLEXCOUNT k2 [v2 [v4(integer) 2 注意：可以用 - 和 + 表示得分最小值和最大值，如果使用成员名的话，一定要在成员名之前加上 [ 。 ZRANGEBYLEXZRANGEBYLEX 返回指定成员区间内的成员，按成员字典正序排序, 分数必须相同。如下： 1234567127.0.0.1:6379&gt; ZRANGEBYLEX k2 [v2 [v41) &quot;v2&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; ZRANGEBYLEX k2 - +1) &quot;v2&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; 注意 min 和 max 参数的写法和 ZLEXCOUNT 一致。 OK,散列和有序集合的命令我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-hash-zset.html"},{"title":"Redis 集群搭建","text":"主从的搭建差不多说完了，本文我们来看看集群如何搭建。 本文是 Redis 系列的第十二篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一)11.Redis 主从复制(二) 集群原理Redis 集群架构如下图： Redis 集群运行原理如下： 所有的 Redis 节点彼此互联( PING-PONG 机制),内部使用二进制协议优化传输速度和带宽 节点的 fail 是通过集群中超过半数的节点检测失效时才生效 客户端与 Redis 节点直连,不需要中间 proxy 层，客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 Redis-cluster 把所有的物理节点映射到 [0-16383] slot 上, cluster (簇)负责维护 node&lt;-&gt;slot&lt;-&gt;value 。Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，Redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，Redis 会根据节点数量大致均等的将哈希槽映射到不同的节点 怎么样投票投票过程是集群中所有 master 参与,如果半数以上 master 节点与 master 节点通信超过 cluster-node-timeout 设置的时间,认为当前 master 节点挂掉。 怎么样判定节点不可用1.如果集群任意 master 挂掉,且当前 master 没有 slave， 集群进入 fail 状态,也可以理解成集群的 slot 映射 [0-16383] 不完整时进入 fail 状态。2.如果集群超过半数以上 master 挂掉，无论是否有 slave ,集群进入 fail 状态，当集群不可用时,所有对集群的操作做都不可用，收到 ((error) CLUSTERDOWN The cluster is down) 错误。 ruby 环境Redis 集群管理工具 redis-trib.rb 依赖 ruby 环境，首先需要安装 ruby 环境： 安装 ruby: 12yum install rubyyum install rubygems 但是这种安装方式装好的 ruby 版本可能不适用，如果安装失败，可以参考这篇文章解决 redis requires Ruby version &gt;= 2.2.2。 集群搭建首先我们对集群做一个简单规划，假设我的集群中一共有三个节点，每个节点一个主机一个从机，这样我一共需要 6 个 Redis 实例。首先创建 redis-cluster 文件夹，在该文件夹下分别创建 7001、7002、7003、7004、7005、7006 文件夹，用来存放我的 Redis 配置文件，如下： 将 Redis 也在 redis-cluster 目录下安装一份，然后将 redis.conf 文件向 7001-7006 这 6 个文件夹中分别拷贝一份，拷贝完成后，分别修改如下参数： 123456port 7001#bind 127.0.0.1cluster-enabled yescluster-config-XX XXX7001.confprotected nodaemonize yes 这是 7001 目录下的配置，其他的文件夹将 7001 改为对应的数字即可。修改完成后，进入到 redis 安装目录中，分别启动各个 redis ，使用刚刚修改过的配置文件，如下： 启动成功后，我们可以查看 redis 进程，如下： 这个表示各个节点都启动成功了。接下来我们就可以进行集群的创建了，首先将 redis/src 目录下的 redis-trib.rb 文件拷贝到 redis-cluster 目录下，然后在 redis-cluster 目录下执行如下命令： 1./redis-trib.rb create --replicas 1 192.168.248.128:7001 192.168.248.128:7002 192.168.248.128:7003 192.168.248.128:7004 192.168.248.128:7005 192.168.248.128:7006 注意，replicas 后面的 1 表示每个主机都带有 1 个从机，执行过程如下： 注意创建过程的日志，每个 redis 都获得了一个编号，同时日志也说明了哪些实例做主机，哪些实例做从机，每个从机的主机是谁，每个主机所分配到的 hash 槽范围等等。 查询集群信息集群创建成功后，我们可以登录到 Redis 控制台查看集群信息，注意登录时要添加 -c 参数，表示以集群方式连接，如下： 添加主节点首先我们准备一个端口为 7007 的主节点并启动，准备方式和前面步骤一样，启动成功后，通过如下命令添加主节点： 1./redis-trib.rb add-node 127.0.0.1:7007 127.0.0.1:7001 主节点添加之后，我们可以通过 cluster nodes 命令查看主节点是否添加成功，此时我们发现新添加的节点没有分配到 slot ，如下： 没有分配到 slot 将不能存储数据，此时我们需要手动分配 slot，分配命令如下： 1./redis-trib.rb reshard 127.0.0.1:7001 后面的地址为任意一个节点地址，在分配的过程中，我们一共要输入如下几个参数： 1.一共要划分多少个 hash 槽出来？就是我们总共要给新添加的节点分多少 hash 槽，这个参数依实际情况而定，如下： 2.这些划分出来的槽要给谁，这里输入 7007 节点的编号，如下： 3.要让谁出血？因为 hash 槽目前已经全部分配完毕，要重新从已经分好的节点中拿出来一部分给 7007 ，必然要让另外三个节点把吃进去的吐出来，这里我们可以输入多个节点的编号，每次输完一个点击回车，输完所有的输入 done 表示输入完成，这样就让这几个节点让出部分 slot，如果要让所有具有 slot 的节点都参与到此次 slot 重新分配的活动中，那么这里直接输入 all 即可，如下： OK，主要就是这几个参数，输完之后进入到 slot 重新分配环节，分配完成后，通过 cluster nodes 命令，我们可以发现 7007 已经具有 slot 了，如下： OK,刚刚我们是添加主节点，我们也可以添加从节点，比如我要把 7008 作为 7007 的从节点，添加方式如下： 1./redis-trib.rb add-node --slave --master-id 79bbb30bba66b4997b9360dd09849c67d2d02bb9 192.168.31.135:7008 192.168.31.135:7007 其中 79bbb30bba66b4997b9360dd09849c67d2d02bb9 是 7007 的编号。 删除节点删除节点也比较简单，如下： 1./redis-trib.rb del-node 127.0.0.1:7005 4b45eb75c8b428fbd77ab979b85080146a9bc017 注意 4b45eb75c8b428fbd77ab979b85080146a9bc017 是要删除节点的编号。 再注意：删除已经占有 hash 槽的结点会失败，报错如下： 1[ERR] Node 127.0.0.1:7005 is not empty! Reshard data away and try again. 需要将该结点占用的 hash 槽分配出去（分配方式与上文一致，不赘述）。 好了，redis 集群搭建我们先说这么多，有问题欢迎留言讨论。","link":"/2019/0615/redis-cluster.html"},{"title":"Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！","text":"有的人可能会觉得题目有点夸张，其实不夸张，题目没有使用任何修辞手法！认真读完本文，你就知道松哥说的是对的了！ 在传统的单服务架构中，一般来说，只有一个服务器，那么不存在 Session 共享问题，但是在分布式/集群项目中，Session 共享则是一个必须面对的问题，先看一个简单的架构图： 在这样的架构中，会出现一些单服务中不存在的问题，例如客户端发起一个请求，这个请求到达 Nginx 上之后，被 Nginx 转发到 Tomcat A 上，然后在 Tomcat A 上往 session 中保存了一份数据，下次又来一个请求，这个请求被转发到 Tomcat B 上，此时再去 Session 中获取数据，发现没有之前的数据。对于这一类问题的解决，思路很简单，就是将各个服务之间需要共享的数据，保存到一个公共的地方（主流方案就是 Redis）： 当所有 Tomcat 需要往 Session 中写数据时，都往 Redis 中写，当所有 Tomcat 需要读数据时，都从 Redis 中读。这样，不同的服务就可以使用相同的 Session 数据了。 这样的方案，可以由开发者手动实现，即手动往 Redis 中存储数据，手动从 Redis 中读取数据，相当于使用一些 Redis 客户端工具来实现这样的功能，毫无疑问，手动实现工作量还是蛮大的。 一个简化的方案就是使用 Spring Session 来实现这一功能，Spring Session 就是使用 Spring 中的代理过滤器，将所有的 Session 操作拦截下来，自动的将数据 同步到 Redis 中，或者自动的从 Redis 中读取数据。 对于开发者来说，所有关于 Session 同步的操作都是透明的，开发者使用 Spring Session，一旦配置完成后，具体的用法就像使用一个普通的 Session 一样。 1 实战1.1 创建工程首先 创建一个 Spring Boot 工程，引入 Web、Spring Session 以及 Redis: 创建成功之后，pom.xml 文件如下： 1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 注意： 这里我使用的 Spring Boot 版本是 2.1.4 ，如果使用当前最新版 Spring Boot2.1.5 的话，除了上面这些依赖之外，需要额外添加 Spring Security 依赖（其他操作不受影响，仅仅只是多了一个依赖，当然也多了 Spring Security 的一些默认认证流程）。 1.2 配置 Redis1234spring.redis.host=192.168.66.128spring.redis.port=6379spring.redis.password=123spring.redis.database=0 这里的 Redis ，我虽然配置了四行，但是考虑到端口默认就是 6379 ，database 默认就是 0，所以真正要配置的，其实就是两行。 1.3 使用配置完成后 ，就可以使用 Spring Session 了，其实就是使用普通的 HttpSession ，其他的 Session 同步到 Redis 等操作，框架已经自动帮你完成了： 1234567891011121314@RestControllerpublic class HelloController { @Value(\"${server.port}\") Integer port; @GetMapping(\"/set\") public String set(HttpSession session) { session.setAttribute(\"user\", \"javaboy\"); return String.valueOf(port); } @GetMapping(\"/get\") public String get(HttpSession session) { return session.getAttribute(\"user\") + \":\" + port; }} 考虑到一会 Spring Boot 将以集群的方式启动 ，为了获取每一个请求到底是哪一个 Spring Boot 提供的服务，需要在每次请求时返回当前服务的端口号，因此这里我注入了 server.port 。 接下来 ，项目打包： 打包之后，启动项目的两个实例： 12java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8080java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8081 然后先访问 localhost:8080/set 向 8080 这个服务的 Session 中保存一个变量，访问完成后，数据就已经自动同步到 Redis 中 了 ： 然后，再调用 localhost:8081/get 接口，就可以获取到 8080 服务的 session 中的数据： 此时关于 session 共享的配置就已经全部完成了，session 共享的效果我们已经看到了，但是每次访问都是我自己手动切换服务实例，因此，接下来我们来引入 Nginx ，实现服务实例自动切换。 1.4 引入 Nginx很简单，进入 Nginx 的安装目录的 conf 目录下（默认是在 /usr/local/nginx/conf），编辑 nginx.conf 文件: 在这段配置中： upstream 表示配置上游服务器 javaboy.org 表示服务器集群的名字，这个可以随意取名字 upstream 里边配置的是一个个的单独服务 weight 表示服务的权重，意味者将有多少比例的请求从 Nginx 上转发到该服务上 location 中的 proxy_pass 表示请求转发的地址，/ 表示拦截到所有的请求，转发转发到刚刚配置好的服务集群中 proxy_redirect 表示设置当发生重定向请求时，nginx 自动修正响应头数据（默认是 Tomcat 返回重定向，此时重定向的地址是 Tomcat 的地址，我们需要将之修改使之成为 Nginx 的地址）。 配置完成后，将本地的 Spring Boot 打包好的 jar 上传到 Linux ，然后在 Linux 上分别启动两个 Spring Boot 实例： 12nohup java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8080 &amp;nohup java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8081 &amp; 其中 nohup 表示当终端关闭时，Spring Boot 不要停止运行 &amp; 表示让 Spring Boot 在后台启动 配置完成后，重启 Nginx： 1/usr/local/nginx/sbin/nginx -s reload Nginx 启动成功后，我们首先手动清除 Redis 上的数据，然后访问 192.168.66.128/set 表示向 session 中保存数据，这个请求首先会到达 Nginx 上，再由 Nginx 转发给某一个 Spring Boot 实例： 如上，表示端口为 8081 的 Spring Boot 处理了这个 /set 请求，再访问 /get 请求： 可以看到，/get 请求是被端口为 8080 的服务所处理的。 2 总结本文主要向大家介绍了 Spring Session 的使用，另外也涉及到一些 Nginx 的使用 ，虽然本文较长，但是实际上 Spring Session 的配置没啥。 我们写了一些代码，也做了一些配置，但是全都和 Spring Session 无关，配置是配置 Redis，代码就是普通的 HttpSession，和 Spring Session 没有任何关系！ 唯一和 Spring Session 相关的，可能就是我在一开始引入了 Spring Session 的依赖吧！ 如果大家没有在 SSM 架构中用过 Spring Session ，可能不太好理解我们在 Spring Boot 中使用 Spring Session 有多么方便，因为在 SSM 架构中，Spring Session 的使用要配置三个地方 ，一个是 web.xml 配置代理过滤器，然后在 Spring 容器中配置 Redis，最后再配置 Spring Session，步骤还是有些繁琐的，而 Spring Boot 中直接帮我们省去了这些繁琐的步骤！不用再去配置 Spring Session。 好了 ，本文就说到这里，本文相关案例我已经上传到 GitHub ，大家可以自行下载:https://github.com/lenve/javaboy-code-samples","link":"/2019/0604/springboot-springsession.html"},{"title":"Spring Boot 中 10 行代码构建 RESTful 风格应用","text":"RESTful ，到现在相信已经没人不知道这个东西了吧！关于 RESTful 的概念，我这里就不做过多介绍了，传统的 Struts 对 RESTful 支持不够友好 ，但是 SpringMVC 对于 RESTful 提供了很好的支持，常见的相关注解有： 1234567@RestController@GetMapping@PutMapping@PostMapping@DeleteMapping@ResponseBody... 这些注解都是和 RESTful 相关的，在移动互联网中，RESTful 得到了非常广泛的使用。RESTful 这个概念提出来很早，但是以前没有移动互联网时，我们做的大部分应用都是前后端不分的，在这种架构的应用中，数据基本上都是在后端渲染好返回给前端展示的，此时 RESTful 在 Web 应用中基本就没用武之地，移动互联网的兴起，让我们一套后台对应多个前端项目，因此前后端分离，RESTful 顺利走上前台。 Spring Boot 继承自 Spring + SpringMVC， SpringMVC 中对于 RESTful 支持的特性在 Spring Boot 中全盘接收，同时，结合 Jpa 和 自动化配置，对于 RESTful 还提供了更多的支持，使得开发者几乎不需要写代码（很少几行），就能快速实现一个 RESTful 风格的增删改查。 接下来，松哥通过一个简单的案例，来向大家展示 Spring Boot 对于 RESTful 的支持。 实战创建工程首先创建一个 Spring Boot 工程，引入 Web 、 Jpa 、 MySQL 、Rest Repositories 依赖： 创建完成后，还需要锁定 MySQL 驱动的版本以及加入 Druid 数据库连接池，完整依赖如下： 12345678910111213141516171819202122232425&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-rest&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;version&gt;5.1.27&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置数据库主要配置两个，一个是数据库，另一个是 Jpa： 12345678910spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.username=rootspring.datasource.password=rootspring.datasource.url=jdbc:mysql:///test01spring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL57Dialectspring.jpa.show-sql=truespring.jpa.hibernate.ddl-auto=updatespring.jpa.database-platform=mysqlspring.jpa.database=mysql 这里的配置，和 Jpa 中的基本一致。 前面五行配置了数据库的基本信息，包括数据库连接池、数据库用户名、数据库密码、数据库连接地址以及数据库驱动名称。 接下来的五行配置了 JPA 的基本信息，分别表示生成 SQL 的方言、打印出生成的 SQL 、每次启动项目时根据实际情况选择是否更新表、数据库平台是 MySQL。 这两段配置是关于 MySQL + JPA 的配置，没用过 JPA 的小伙伴可以参考松哥之前的 JPA 文章：http://www.javaboy.org/2019/0407/springboot-jpa.html 构建实体类123456789101112@Entity(name = \"t_book\")public class Book { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(name = \"book_name\") private String name; private String author; //省略 getter/setter}public interface BookRepository extends JpaRepository&lt;Book,Long&gt; {} 这里一个是配置了一个实体类 Book，另一个则是配置了一个 BookRepository ，项目启动成功后，框架会根据 Book 类的定义，在数据库中自动创建相应的表，BookRepository 接口则是继承自 JpaRepository ，JpaRepository 中自带了一些基本的增删改查方法。 好了，代码写完了。 啥？你好像啥都没写啊？是的，啥都没写，啥都不用写，一个 RESTful 风格的增删改查应用就有了，这就是 Spring Boot 的魅力！ 测试此时，我们就可以启动项目进行测试了，使用 POSTMAN 来测试（大家也可以自行选择趁手的 HTTP 请求工具）。 此时我们的项目已经默认具备了一些接口，我们分别来看： 根据 id 查询接口 http://127.0.0.1:8080/books/{id} 这个接口表示根据 id 查询某一本书： 分页查询 http://127.0.0.1:8080/books 这是一个批量查询接口，默认请求路径是类名首字母小写，并且再加一个 s 后缀。这个接口实际上是一个分页查询接口，没有传参数，表示查询第一页，每页 20 条数据。 查询结果中，除了该有的数据之外，也包含了分页数据： 分页数据中： size 表示每页查询记录数 totalElements 表示总记录数 totalPages 表示总页数 number 表示当前页数，从0开始计 如果要分页或者排序查询，可以使用 _links 中的链接。http://127.0.0.1:8080/books?page=1&amp;size=3&amp;sort=id,desc 。 添加也可以添加数据，添加是 POST 请求，数据通过 JSON 的形式传递，如下： 添加成功之后，默认会返回添加成功的数据。 修改修改接口默认也是存在的，数据修改请求是一个 PUT 请求，修改的参数也是通过 JSON 的形式传递： 默认情况下，修改成功后，会返回修改成功的数据。 删除当然也可以通过 DELETE 请求根据 id 删除数据： 删除成功后，是没有返回值的。 不需要几行代码，一个基本的增删改查就有了。 这些都是默认的配置，这些默认的配置实际上都是在 JpaRepository 的基础上实现的，实际项目中，我们还可以对这些功能进行定制。 查询定制最广泛的定制，就是查询，因为增删改操作的变化不像查询这么丰富。对于查询的定制，非常容易，只需要提供相关的方法即可。例如根据作者查询书籍： 123public interface BookRepository extends JpaRepository&lt;Book,Long&gt; { List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author);} 注意，方法的定义，参数要有 @Param 注解。 定制完成后，重启项目，此时就多了一个查询接口，开发者可以通过 http://localhost:8080/books/search 来查看和 book 相关的自定义接口都有哪些： 查询结果表示，只有一个自定义接口，接口名就是方法名，而且查询结果还给出了接口调用的示例。我们来尝试调用一下自己定义的查询接口： 开发者可以根据实际情况，在 BookRepository 中定义任意多个查询方法，查询方法的定义规则和 Jpa 中一模一样（不懂 Jpa 的小伙伴，可以参考干货|一文读懂 Spring Data Jpa！，或者在松哥个人网站 www.javaboy.org 上搜索 JPA，有相关教程参考）。但是，这样有一个缺陷，就是 Jpa 中方法名太长，因此，如果不想使用方法名作为接口名，则可以自定义接口名： 1234public interface BookRepository extends JpaRepository&lt;Book, Long&gt; { @RestResource(rel = \"byauthor\",path = \"byauthor\") List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author);} @RestResource 注解中，两个参数的含义： rel 表示接口查询中，这个方法的 key path 表示请求路径 这样定义完成后，表示接口名为 byauthor ，重启项目，继续查询接口： 除了 rel 和 path 两个属性之外，@RestResource 中还有一个属性，exported 表示是否暴露接口，默认为 true ，表示暴露接口，即方法可以在前端调用，如果仅仅只是想定义一个方法，不需要在前端调用这个方法，可以设置 exported 属性为 false 。 如果不想暴露官方定义好的方法，例如根据 id 删除数据，只需要在自定义接口中重写该方法，然后在该方法上加 @RestResource 注解并且配置相关属性即可。 1234567public interface BookRepository extends JpaRepository&lt;Book, Long&gt; { @RestResource(rel = \"byauthor\",path = \"byauthor\") List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author); @Override @RestResource(exported = false) void deleteById(Long aLong);} 另外生成的 JSON 字符串中的集合名和单个 item 的名字都是可以自定义的： 12345678@RepositoryRestResource(collectionResourceRel = \"bs\",itemResourceRel = \"b\",path = \"bs\")public interface BookRepository extends JpaRepository&lt;Book, Long&gt; { @RestResource(rel = \"byauthor\",path = \"byauthor\") List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author); @Override @RestResource(exported = false) void deleteById(Long aLong);} path 属性表示请求路径，请求路径默认是类名首字母小写+s，可以在这里自己重新定义。 其他配置最后，也可以在 application.properties 中配置 REST 基本参数： 12345678spring.data.rest.base-path=/apispring.data.rest.sort-param-name=sortspring.data.rest.page-param-name=pagespring.data.rest.limit-param-name=sizespring.data.rest.max-page-size=20spring.data.rest.default-page-size=0spring.data.rest.return-body-on-update=truespring.data.rest.return-body-on-create=true 配置含义，从上往下，依次是： 给所有的接口添加统一的前缀 配置排序参数的 key ，默认是 sort 配置分页查询时页码的 key，默认是 page 配置分页查询时每页查询页数的 key，默认是size 配置每页最大查询记录数，默认是 20 条 分页查询时默认的页码 更新成功时是否返回更新记录 添加成功时是否返回添加记录 总结本文主要向大家介绍了 Spring Boot 中快速实现一个 RESTful 风格的增删改查应用的方案，整体来说还是比较简单的，并不难。相关案例我已上传到 GitHub 上了，小伙伴可以自行下载：https://github.com/lenve/javaboy-code-samples。 关于本文，有问题欢迎留言讨论。","link":"/2019/0606/springboot-restful.html"},{"title":"Spring Boot 中实现定时任务的两种方式","text":"在 Spring + SpringMVC 环境中，一般来说，要实现定时任务，我们有两中方案，一种是使用 Spring 自带的定时任务处理器 @Scheduled 注解，另一种就是使用第三方框架 Quartz ，Spring Boot 源自 Spring+SpringMVC ，因此天然具备这两个 Spring 中的定时任务实现策略，当然也支持 Quartz，本文我们就来看下 Spring Boot 中两种定时任务的实现方式。 @Scheduled使用 @Scheduled 非常容易，直接创建一个 Spring Boot 项目，并且添加 web 依赖 spring-boot-starter-web，项目创建成功后，添加 @EnableScheduling 注解，开启定时任务： 123456789@SpringBootApplication@EnableSchedulingpublic class ScheduledApplication { public static void main(String[] args) { SpringApplication.run(ScheduledApplication.class, args); }} 接下来配置定时任务： 123456789101112@Scheduled(fixedRate = 2000)public void fixedRate() { System.out.println(\"fixedRate&gt;&gt;&gt;\"+new Date()); }@Scheduled(fixedDelay = 2000)public void fixedDelay() { System.out.println(\"fixedDelay&gt;&gt;&gt;\"+new Date());}@Scheduled(initialDelay = 2000,fixedDelay = 2000)public void initialDelay() { System.out.println(\"initialDelay&gt;&gt;&gt;\"+new Date());} 首先使用 @Scheduled 注解开启一个定时任务。 fixedRate 表示任务执行之间的时间间隔，具体是指两次任务的开始时间间隔，即第二次任务开始时，第一次任务可能还没结束。 fixedDelay 表示任务执行之间的时间间隔，具体是指本次任务结束到下次任务开始之间的时间间隔。 initialDelay 表示首次任务启动的延迟时间。 所有时间的单位都是毫秒。 上面这是一个基本用法，除了这几个基本属性之外，@Scheduled 注解也支持 cron 表达式，使用 cron 表达式，可以非常丰富的描述定时任务的时间。cron 表达式格式如下： [秒] [分] [小时] [日] [月] [周] [年] 具体取值如下： 序号 说明 是否必填 允许填写的值 允许的通配符 1 秒 是 0-59 - * / 2 分 是 0-59 - * / 3 时 是 0-23 - * / 4 日 是 1-31 - * ? / L W 5 月 是 1-12 or JAN-DEC - * / 6 周 是 1-7 or SUN-SAT - * ? / L # 7 年 否 1970-2099 - * / 这一块需要大家注意的是，月份中的日期和星期可能会起冲突，因此在配置时这两个得有一个是 ? 通配符含义： ? 表示不指定值，即不关心某个字段的取值时使用。需要注意的是，月份中的日期和星期可能会起冲突，因此在配置时这两个得有一个是 ? * 表示所有值，例如:在秒的字段上设置 *,表示每一秒都会触发 , 用来分开多个值，例如在周字段上设置 “MON,WED,FRI” 表示周一，周三和周五触发 - 表示区间，例如在秒上设置 “10-12”,表示 10,11,12秒都会触发 / 用于递增触发，如在秒上面设置”5/15” 表示从5秒开始，每增15秒触发(5,20,35,50) # 序号(表示每月的第几个周几)，例如在周字段上设置”6#3”表示在每月的第三个周六，(用 在母亲节和父亲节再合适不过了) 周字段的设置，若使用英文字母是不区分大小写的 ，即 MON 与mon相同 L 表示最后的意思。在日字段设置上，表示当月的最后一天(依据当前月份，如果是二月还会自动判断是否是润年), 在周字段上表示星期六，相当于”7”或”SAT”（注意周日算是第一天）。如果在”L”前加上数字，则表示该数据的最后一个。例如在周字段上设置”6L”这样的格式,则表示”本月最后一个星期五” W 表示离指定日期的最近工作日(周一至周五)，例如在日字段上设置”15W”，表示离每月15号最近的那个工作日触发。如果15号正好是周六，则找最近的周五(14号)触发, 如果15号是周未，则找最近的下周一(16号)触发，如果15号正好在工作日(周一至周五)，则就在该天触发。如果指定格式为 “1W”,它则表示每月1号往后最近的工作日触发。如果1号正是周六，则将在3号下周一触发。(注，”W”前只能设置具体的数字,不允许区间”-“) L 和 W 可以一组合使用。如果在日字段上设置”LW”,则表示在本月的最后一个工作日触发(一般指发工资 ) 例如，在 @Scheduled 注解中来一个简单的 cron 表达式，每隔5秒触发一次，如下： 1234@Scheduled(cron = \"0/5 * * * * *\")public void cron() { System.out.println(new Date());} 上面介绍的是使用 @Scheduled 注解的方式来实现定时任务，接下来我们再来看看如何使用 Quartz 实现定时任务。 Quartz一般在项目中，除非定时任务涉及到的业务实在是太简单，使用 @Scheduled 注解来解决定时任务，否则大部分情况可能都是使用 Quartz 来做定时任务。在 Spring Boot 中使用 Quartz ，只需要在创建项目时，添加 Quartz 依赖即可： 项目创建完成后，也需要添加开启定时任务的注解： 1234567@SpringBootApplication@EnableSchedulingpublic class QuartzApplication { public static void main(String[] args) { SpringApplication.run(QuartzApplication.class, args); }} Quartz 在使用过程中，有两个关键概念，一个是JobDetail（要做的事情），另一个是触发器（什么时候做），要定义 JobDetail，需要先定义 Job，Job 的定义有两种方式： 第一种方式，直接定义一个Bean： 123456@Componentpublic class MyJob1 { public void sayHello() { System.out.println(\"MyJob1&gt;&gt;&gt;\"+new Date()); }} 关于这种定义方式说两点： 首先将这个 Job 注册到 Spring 容器中。 这种定义方式有一个缺陷，就是无法传参。 第二种定义方式，则是继承 QuartzJobBean 并实现默认的方法： 123456789101112131415161718public class MyJob2 extends QuartzJobBean { HelloService helloService; public HelloService getHelloService() { return helloService; } public void setHelloService(HelloService helloService) { this.helloService = helloService; } @Override protected void executeInternal(JobExecutionContext jobExecutionContext) throws JobExecutionException { helloService.sayHello(); }}public class HelloService { public void sayHello() { System.out.println(\"hello service &gt;&gt;&gt;\"+new Date()); }} 和第1种方式相比，这种方式支持传参，任务启动时，executeInternal 方法将会被执行。 Job 有了之后，接下来创建类，配置 JobDetail 和 Trigger 触发器，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Configurationpublic class QuartzConfig { @Bean MethodInvokingJobDetailFactoryBean methodInvokingJobDetailFactoryBean() { MethodInvokingJobDetailFactoryBean bean = new MethodInvokingJobDetailFactoryBean(); bean.setTargetBeanName(\"myJob1\"); bean.setTargetMethod(\"sayHello\"); return bean; } @Bean JobDetailFactoryBean jobDetailFactoryBean() { JobDetailFactoryBean bean = new JobDetailFactoryBean(); bean.setJobClass(MyJob2.class); JobDataMap map = new JobDataMap(); map.put(\"helloService\", helloService()); bean.setJobDataMap(map); return bean; } @Bean SimpleTriggerFactoryBean simpleTriggerFactoryBean() { SimpleTriggerFactoryBean bean = new SimpleTriggerFactoryBean(); bean.setStartTime(new Date()); bean.setRepeatCount(5); bean.setJobDetail(methodInvokingJobDetailFactoryBean().getObject()); bean.setRepeatInterval(3000); return bean; } @Bean CronTriggerFactoryBean cronTrigger() { CronTriggerFactoryBean bean = new CronTriggerFactoryBean(); bean.setCronExpression(\"0/10 * * * * ?\"); bean.setJobDetail(jobDetailFactoryBean().getObject()); return bean; } @Bean SchedulerFactoryBean schedulerFactoryBean() { SchedulerFactoryBean bean = new SchedulerFactoryBean(); bean.setTriggers(cronTrigger().getObject(), simpleTriggerFactoryBean().getObject()); return bean; } @Bean HelloService helloService() { return new HelloService(); }} 关于这个配置说如下几点： JobDetail 的配置有两种方式：MethodInvokingJobDetailFactoryBean 和 JobDetailFactoryBean 。 使用 MethodInvokingJobDetailFactoryBean 可以配置目标 Bean 的名字和目标方法的名字，这种方式不支持传参。 使用 JobDetailFactoryBean 可以配置 JobDetail ，任务类继承自 QuartzJobBean ，这种方式支持传参，将参数封装在 JobDataMap 中进行传递。 Trigger 是指触发器，Quartz 中定义了多个触发器，这里向大家展示其中两种的用法，SimpleTrigger 和 CronTrigger 。 SimpleTrigger 有点类似于前面说的 @Scheduled 的基本用法。 CronTrigger 则有点类似于 @Scheduled 中 cron 表达式的用法。 全部定义完成后，启动 Spring Boot 项目就可以看到定时任务的执行了。 总结这里主要向大家展示了 Spring Boot 中整合两种定时任务的方法，整合成功之后，剩下的用法基本上就和在 SSM 中使用一致了，不再赘述。","link":"/2019/0418/springboot-schedule-task.html"},{"title":"Spring Boot 中关于自定义异常处理的套路！","text":"在 Spring Boot 项目中 ，异常统一处理，可以使用 Spring 中 @ControllerAdvice 来统一处理，也可以自己来定义异常处理方案。Spring Boot 中，对异常的处理有一些默认的策略，我们分别来看。 默认情况下，Spring Boot 中的异常页面 是这样的： 我们从这个异常提示中，也能看出来，之所以用户看到这个页面，是因为开发者没有明确提供一个 /error 路径，如果开发者提供了 /error 路径 ，这个页面就不会展示出来，不过在 Spring Boot 中，提供 /error 路径实际上是下下策，Spring Boot 本身在处理异常时，也是当所有条件都不满足时，才会去找 /error 路径。那么我们就先来看看，在 Spring Boot 中，如何自定义 error 页面，整体上来说，可以分为两种，一种是静态页面，另一种是动态页面。 静态异常页面自定义静态异常页面，又分为两种，第一种 是使用 HTTP 响应码来命名页面，例如 404.html、405.html、500.html ….，另一种就是直接定义一个 4xx.html，表示400-499 的状态都显示这个异常页面，5xx.html 表示 500-599 的状态显示这个异常页面。 默认是在 classpath:/static/error/ 路径下定义相关页面： 此时，启动项目，如果项目抛出 500 请求错误，就会自动展示 500.html 这个页面，发生 404 就会展示 404.html 页面。如果异常展示页面既存在 5xx.html，也存在 500.html ，此时，发生500异常时，优先展示 500.html 页面。 动态异常页面动态的异常页面定义方式和静态的基本 一致，可以采用的页面模板有 jsp、freemarker、thymeleaf。动态异常页面，也支持 404.html 或者 4xx.html ，但是一般来说，由于动态异常页面可以直接展示异常详细信息，所以就没有必要挨个枚举错误了 ，直接定义 4xx.html（这里使用thymeleaf模板）或者 5xx.html 即可。 注意，动态页面模板，不需要开发者自己去定义控制器，直接定义异常页面即可 ，Spring Boot 中自带的异常处理器会自动查找到异常页面。 页面定义如下： 页面内容如下： 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;5xx&lt;/h1&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;path&lt;/td&gt; &lt;td th:text=\"${path}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;error&lt;/td&gt; &lt;td th:text=\"${error}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;message&lt;/td&gt; &lt;td th:text=\"${message}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;timestamp&lt;/td&gt; &lt;td th:text=\"${timestamp}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;status&lt;/td&gt; &lt;td th:text=\"${status}\"&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 默认情况下，完整的异常信息就是这5条，展示 效果如下 ： 如果动态页面和静态页面同时定义了异常处理页面，例如 classpath:/static/error/404.html 和 classpath:/templates/error/404.html 同时存在时，默认使用动态页面。即完整的错误页面查找方式应该是这样： 发生了500错误–&gt;查找动态 500.html 页面–&gt;查找静态 500.html –&gt; 查找动态 5xx.html–&gt;查找静态 5xx.html。 自定义异常数据默认情况下，在Spring Boot 中，所有的异常数据其实就是上文所展示出来的5条数据，这5条数据定义在 org.springframework.boot.web.reactive.error.DefaultErrorAttributes 类中，具体定义在 getErrorAttributes 方法中 ： 1234567891011121314@Overridepublic Map&lt;String, Object&gt; getErrorAttributes(ServerRequest request, boolean includeStackTrace) { Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;&gt;(); errorAttributes.put(\"timestamp\", new Date()); errorAttributes.put(\"path\", request.path()); Throwable error = getError(request); HttpStatus errorStatus = determineHttpStatus(error); errorAttributes.put(\"status\", errorStatus.value()); errorAttributes.put(\"error\", errorStatus.getReasonPhrase()); errorAttributes.put(\"message\", determineMessage(error)); handleException(errorAttributes, determineException(error), includeStackTrace); return errorAttributes;} DefaultErrorAttributes 类本身则是在org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration 异常自动配置类中定义的，如果开发者没有自己提供一个 ErrorAttributes 的实例的话，那么 Spring Boot 将自动提供一个ErrorAttributes 的实例，也就是 DefaultErrorAttributes 。 基于此 ，开发者自定义 ErrorAttributes 有两种方式 ： 直接实现 ErrorAttributes 接口 继承 DefaultErrorAttributes（推荐），因为 DefaultErrorAttributes 中对异常数据的处理已经完成，开发者可以直接使用。 具体定义如下： 1234567891011@Componentpublic class MyErrorAttributes extends DefaultErrorAttributes { @Override public Map&lt;String, Object&gt; getErrorAttributes(WebRequest webRequest, boolean includeStackTrace) { Map&lt;String, Object&gt; map = super.getErrorAttributes(webRequest, includeStackTrace); if ((Integer)map.get(\"status\") == 500) { map.put(\"message\", \"服务器内部错误!\"); } return map; }} 定义好的 ErrorAttributes 一定要注册成一个 Bean ，这样，Spring Boot 就不会使用默认的 DefaultErrorAttributes 了，运行效果如下图： 自定义异常视图异常视图默认就是前面所说的静态或者动态页面，这个也是可以自定义的，首先 ，默认的异常视图加载逻辑在 org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController 类的 errorHtml 方法中，这个方法用来返回异常页面+数据，还有另外一个 error 方法，这个方法用来返回异常数据（如果是 ajax 请求，则该方法会被触发）。 12345678910@RequestMapping(produces = MediaType.TEXT_HTML_VALUE)public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) { HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView != null) ? modelAndView : new ModelAndView(\"error\", model);} 在该方法中 ，首先会通过 getErrorAttributes 方法去获取异常数据（实际上会调用到 ErrorAttributes 的实例 的 getErrorAttributes 方法），然后调用 resolveErrorView 去创建一个 ModelAndView ，如果这里创建失败，那么用户将会看到默认的错误提示页面。 正常情况下， resolveErrorView 方法会来到 DefaultErrorViewResolver 类的 resolveErrorView 方法中： 123456789@Overridepublic ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) { ModelAndView modelAndView = resolve(String.valueOf(status.value()), model); if (modelAndView == null &amp;&amp; SERIES_VIEWS.containsKey(status.series())) { modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); } return modelAndView;} 在这里，首先以异常响应码作为视图名分别去查找动态页面和静态页面，如果没有查找到，则再以 4xx 或者 5xx 作为视图名再去分别查找动态或者静态页面。 要自定义异常视图解析，也很容易 ，由于 DefaultErrorViewResolver 是在 ErrorMvcAutoConfiguration 类中提供的实例，即开发者没有提供相关实例时，会使用默认的 DefaultErrorViewResolver ，开发者提供了自己的 ErrorViewResolver 实例后，默认的配置就会失效，因此，自定义异常视图，只需要提供 一个 ErrorViewResolver 的实例即可： 12345678910@Componentpublic class MyErrorViewResolver extends DefaultErrorViewResolver { public MyErrorViewResolver(ApplicationContext applicationContext, ResourceProperties resourceProperties) { super(applicationContext, resourceProperties); } @Override public ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) { return new ModelAndView(\"/aaa/123\", model); }} 实际上，开发者也可以在这里定义异常数据（直接在 resolveErrorView 方法重新定义一个 model ，将参数中的model 数据拷贝过去并修改，注意参数中的 model 类型为 UnmodifiableMap，即不可以直接修改），而不需要自定义MyErrorAttributes。定义完成后，提供一个名为123的视图，如下图： 如此之后，错误试图就算定义成功了。 总结实际上也可以自定义异常控制器 BasicErrorController ，不过松哥觉得这样太大动干戈了，没必要，前面几种方式已经可以满足我们的大部分开发需求了。","link":"/2019/0417/springboot-exception.html"},{"title":"Spring Boot 开发微信公众号后台(二)","text":"hello 各位小伙伴，今天我们来继续学习如何通过 Spring Boot 开发微信公众号。还没阅读过上篇文章的小伙伴建议先看看上文，有助于理解本文： Spring Boot 开发微信公众号后台 上篇文章中我们将微信服务器和我们自己的服务器对接起来了，并且在自己的服务器上也能收到微信服务器发来的消息，本文我们要看的就是如何给微信服务器回复消息。 消息分类在讨论如何给微信服务器回复消息之前，我们需要先来了解下微信服务器发来的消息主要有哪些类型以及我们回复给微信的消息都有哪些类型。 在上文中大家了解到，微信发送来的 xml 消息中有一个 MsgType 字段，这个字段就是用来标记消息的类型。这个类型可以标记出这条消息是普通消息还是事件消息还是图文消息等。 普通消息主要是指： 文本消息 图片消息 语音消息 视频消息 小视频消息 地址位置消息 链接消息 不同的消息类型，对应不同的 MsgType，这里我还是以普通消息为例，如下： 消息类型 MsgType 文本消息 text 图片消息 image 语音消息 voice 视频消息 video 小视频消息 shortvideo 地址位置消息 location 链接消息 link 大家千万不要以为不同类型消息的格式是一样的，其实是不一样的，也就是说，MsgType 为 text 的消息和 MsgType 为 image 的消息，微信服务器发给我们的消息内容是不一样的，这样带来一个问题就是我无法使用一个 Bean 去接收不同类型的数据，因此这里我们一般使用 Map 接收即可。 这是消息的接收，除了消息的接收之外，还有一个消息的回复，我们回复的消息也有很多类型，可以回复普通消息，也可以回复图片消息，回复语音消息等，不同的回复消息我们可以进行相应的封装。因为不同的返回消息实例也是有一些共同的属性的，例如消息是谁发来的，发给谁，消息类型，消息 id 等，所以我们可以将这些共同的属性定义成一个父类，然后不同的消息再去继承这个父类。 返回消息类型定义首先我们来定义一个公共的消息类型： 12345678public class BaseMessage { private String ToUserName; private String FromUserName; private long CreateTime; private String MsgType; private long MsgId; //省略 getter/setter} 在这里： ToUserName 表示开发者的微信号 FromUserName 表示发送方账号（用户的 OpenID） CreateTime 消息的创建时间 MsgType 表示消息的类型 MsgId 表示消息 id 这是我们的基本消息类型，就是说，我们返回给用户的消息，无论是什么类型的消息，都有这几个基本属性。然后在此基础上，我们再去扩展出文本消息、图片消息 等。 我们来看下文本消息的定义： 1234public class TextMessage extends BaseMessage { private String Content; //省略 getter/setter} 文本消息在前面消息的基础上多了一个 Content 属性，因此文本消息继承自 BaseMessage ，再额外添加一个 Content 属性即可。 其他的消息类型也是类似的定义，我就不一一列举了，至于其他消息的格式，大家可以参考微信开放文档（http://1t.click/aPXK）。 返回消息生成消息类型的 Bean 定义完成之后，接下来就是将实体类生成 XML。 首先我们定义一个消息工具类，将常见的消息类型枚举出来： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * 返回消息类型：文本 */public static final String RESP_MESSAGE_TYPE_TEXT = \"text\";/** * 返回消息类型：音乐 */public static final String RESP_MESSAGE_TYPE_MUSIC = \"music\";/** * 返回消息类型：图文 */public static final String RESP_MESSAGE_TYPE_NEWS = \"news\";/** * 返回消息类型：图片 */public static final String RESP_MESSAGE_TYPE_Image = \"image\";/** * 返回消息类型：语音 */public static final String RESP_MESSAGE_TYPE_Voice = \"voice\";/** * 返回消息类型：视频 */public static final String RESP_MESSAGE_TYPE_Video = \"video\";/** * 请求消息类型：文本 */public static final String REQ_MESSAGE_TYPE_TEXT = \"text\";/** * 请求消息类型：图片 */public static final String REQ_MESSAGE_TYPE_IMAGE = \"image\";/** * 请求消息类型：链接 */public static final String REQ_MESSAGE_TYPE_LINK = \"link\";/** * 请求消息类型：地理位置 */public static final String REQ_MESSAGE_TYPE_LOCATION = \"location\";/** * 请求消息类型：音频 */public static final String REQ_MESSAGE_TYPE_VOICE = \"voice\";/** * 请求消息类型：视频 */public static final String REQ_MESSAGE_TYPE_VIDEO = \"video\";/** * 请求消息类型：推送 */public static final String REQ_MESSAGE_TYPE_EVENT = \"event\";/** * 事件类型：subscribe(订阅) */public static final String EVENT_TYPE_SUBSCRIBE = \"subscribe\";/** * 事件类型：unsubscribe(取消订阅) */public static final String EVENT_TYPE_UNSUBSCRIBE = \"unsubscribe\";/** * 事件类型：CLICK(自定义菜单点击事件) */public static final String EVENT_TYPE_CLICK = \"CLICK\";/** * 事件类型：VIEW(自定义菜单 URl 视图) */public static final String EVENT_TYPE_VIEW = \"VIEW\";/** * 事件类型：LOCATION(上报地理位置事件) */public static final String EVENT_TYPE_LOCATION = \"LOCATION\";/** * 事件类型：LOCATION(上报地理位置事件) */public static final String EVENT_TYPE_SCAN = \"SCAN\"; 大家注意这里消息类型的定义，以 RESP 开头的表示返回的消息类型，以 REQ 表示微信服务器发来的消息类型。然后在这个工具类中再定义两个方法，用来将返回的对象转换成 XML： 123456789101112131415161718192021222324public static String textMessageToXml(TextMessage textMessage) { xstream.alias(\"xml\", textMessage.getClass()); return xstream.toXML(textMessage);}private static XStream xstream = new XStream(new XppDriver() { public HierarchicalStreamWriter createWriter(Writer out) { return new PrettyPrintWriter(out) { boolean cdata = true; @SuppressWarnings(\"rawtypes\") public void startNode(String name, Class clazz) { super.startNode(name, clazz); } protected void writeText(QuickWriter writer, String text) { if (cdata) { writer.write(\"&lt;![CDATA[\"); writer.write(text); writer.write(\"]]&gt;\"); } else { writer.write(text); } } }; }}); textMessageToXML 方法用来将 TextMessage 对象转成 XML 返回给微信服务器，类似的方法我们还需要定义 imageMessageToXml、voiceMessageToXml 等，不过定义的方式都基本类似，我就不一一列出来了。 返回消息分发由于用户发来的消息可能存在多种情况，我们需要分类进行处理，这个就涉及到返回消息的分发问题。因此我在这里再定义一个返回消息分发的工具类，如下： 1234567891011121314151617181920public class MessageDispatcher { public static String processMessage(Map&lt;String, String&gt; map) { String openid = map.get(\"FromUserName\"); //用户 openid String mpid = map.get(\"ToUserName\"); //公众号原始 ID if (map.get(\"MsgType\").equals(MessageUtil.REQ_MESSAGE_TYPE_TEXT)) { //普通文本消息 TextMessage txtmsg = new TextMessage(); txtmsg.setToUserName(openid); txtmsg.setFromUserName(mpid); txtmsg.setCreateTime(new Date().getTime()); txtmsg.setMsgType(MessageUtil.RESP_MESSAGE_TYPE_TEXT); txtmsg.setContent(\"这是返回消息\"); return MessageUtil.textMessageToXml(txtmsg); } return null; } public String processEvent(Map&lt;String, String&gt; map) { //在这里处理事件 }} 这里我们还可以多加几个 elseif 去判断不同的消息类型，我这里因为只有普通文本消息，所以一个 if 就够用了。 在这里返回值我写死了，实际上这里需要根据微信服务端传来的 Content 去数据中查询，将查询结果返回，数据库查询这一套相信大家都能搞定，我这里就不重复介绍了。 最后在消息接收 Controller 中调用该方法，如下： 1234567891011@PostMapping(value = \"/verify_wx_token\",produces = \"application/xml;charset=utf-8\")public String handler(HttpServletRequest request, HttpServletResponse response) throws Exception { request.setCharacterEncoding(\"UTF-8\"); Map&lt;String, String&gt; map = MessageUtil.parseXml(request); String msgType = map.get(\"MsgType\"); if (MessageUtil.REQ_MESSAGE_TYPE_EVENT.equals(msgType)) { return messageDispatcher.processEvent(map); }else{ return messageDispatcher.processMessage(map); }} 在 Controller 中，我们首先判断消息是否是事件，如果是事件，进入到事件处理通道，如果不是事件，则进入到消息处理通道。 注意，这里需要配置一下返回消息的编码，否则可能会出现中文乱码。 如此之后，我们的服务器就可以给公众号返回消息了。 上篇文章发出后，有小伙伴问松哥这个会不会开源，我可以负责任的告诉大家，肯定会开源，这个系列截稿后，我把代码处理下就上传到 GitHub。 好了，本文我们就先说到这里。","link":"/2019/1031/springboot-weixin.html"},{"title":"Spring Boot 开发微信公众号后台","text":"Hello 各位小伙伴，松哥今天要和大家聊一个有意思的话题，就是使用 Spring Boot 开发微信公众号后台。 很多小伙伴可能注意到松哥的个人网站（http://www.javaboy.org）前一阵子上线了一个公众号内回复口令解锁网站文章的功能，还有之前就有的公众号内回复口令获取超 2TB 免费视频教程的功能（免费视频教程），这两个都是松哥基于 Spring Boot 来做的，最近松哥打算通过一个系列的文章，来向小伙伴们介绍下如何通过 Spring Boot 来开发公众号后台。 1. 缘起今年 5 月份的时候，我想把我自己之前收集到的一些视频教程分享给公众号上的小伙伴，可是这些视频教程大太了，无法一次分享，单次分享分享链接立马就失效了，为了把这些视频分享给大家，我把视频拆分成了很多份，然后设置了不同的口令，小伙伴们在公众号后台通过回复口令就可以获取到这些视频，口令前前后后有 100 多个，我一个一个手动的在微信后台进行配置。这么搞工作量很大，前前后后大概花了三个晚上才把这些东西搞定。 于是我就在想，该写点代码了。 上个月买了服务器，也备案了，该有的都有了，于是就打算把这些资源用代码实现下，因为大学时候搞过公众号开发，倒也没什么难度，于是说干就干。 2. 实现思路其实松哥这个回复口令获取视频链接的实现原理很简单，说白了，就是一个数据查询操作而已，回复的口令是查询关键字，回复的内容则是查询结果。这个原理很简单。 另一方面大家需要明白微信公众号后台开发消息发送的一个流程，大家看下面这张图： 这是大家在公众号后台回复关键字的情况。那么这个消息是怎么样一个传递流程呢？我们来看看下面这张图： 这张图，我给大家稍微解释下： 首先 javaboy4096 这个字符从公众号上发送到了微信服务器 接下来微信服务器会把 javaboy4096 转发到我自己的服务器上 我收到 javaboy4096 这个字符之后，就去数据库中查询，将查询的结果，按照腾讯要求的 XML 格式进行返回 微信服务器把从我的服务器收到的信息，再发回到微信上，于是小伙伴们就看到了返回结果了 大致的流程就是这个样子。 接下来我们就来看一下实现细节。 3. 公众号后台配置开发的第一步，是微信服务器要验证我们自己的服务器是否有效。 首先我们登录微信公众平台官网后，在公众平台官网的 开发-基本设置 页面，勾选协议成为开发者，然后点击“修改配置”按钮，填写： 服务器地址（URL） Token EncodingAESKey 这里的 URL 配置好之后，我们需要针对这个 URL 开发两个接口，一个是 GET 请求的接口，这个接口用来做服务器有效性验证，另一个则是 POST 请求的接口，这个用来接收微信服务器发送来的消息。也就是说，微信服务器的消息都是通过 POST 请求发给我的。 Token 可由开发者可以任意填写，用作生成签名（该 Token 会和接口 URL 中包含的 Token 进行比对，从而验证安全性）。 EncodingAESKey 由开发者手动填写或随机生成，将用作消息体加解密密钥。 同时，开发者可选择消息加解密方式：明文模式、兼容模式和安全模式。明文模式就是我们自己的服务器收到微信服务器发来的消息是明文字符串，直接就可以读取并且解析，安全模式则是我们收到微信服务器发来的消息是加密的消息，需要我们手动解析后才能使用。 4. 开发公众号后台配置完成后，接下来我们就可以写代码了。 4.1 服务器有效性校验我们首先来创建一个普通的 Spring Boot 项目，创建时引入 spring-boot-starter-web 依赖，项目创建成功后，我们创建一个 Controller ，添加如下接口： 12345678910111213141516171819@GetMapping(\"/verify_wx_token\")public void login(HttpServletRequest request, HttpServletResponse response) throws UnsupportedEncodingException { request.setCharacterEncoding(\"UTF-8\"); String signature = request.getParameter(\"signature\"); String timestamp = request.getParameter(\"timestamp\"); String nonce = request.getParameter(\"nonce\"); String echostr = request.getParameter(\"echostr\"); PrintWriter out = null; try { out = response.getWriter(); if (CheckUtil.checkSignature(signature, timestamp, nonce)) { out.write(echostr); } } catch (IOException e) { e.printStackTrace(); } finally { out.close(); }} 关于这段代码，我做如下解释： 首先通过 request.getParameter 方法获取到微信服务器发来的 signature、timestamp、nonce 以及 echostr 四个参数，这四个参数中：signature 表示微信加密签名，signature 结合了开发者填写的 token 参数和请求中的timestamp参数、nonce参数；timestamp 表示时间戳；nonce 表示随机数；echostr 则表示一个随机字符串。 开发者通过检验 signature 对请求进行校验，如果确认此次 GET 请求来自微信服务器，则原样返回 echostr 参数内容，则接入生效，成为开发者成功，否则接入失败。 具体的校验就是松哥这里的 CheckUtil.checkSignature 方法，在这个方法中，首先将token、timestamp、nonce 三个参数进行字典序排序，然后将三个参数字符串拼接成一个字符串进行 sha1 加密，最后开发者获得加密后的字符串可与 signature 对比，标识该请求来源于微信。 校验代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142public class CheckUtil { private static final String token = \"123456\"; public static boolean checkSignature(String signature, String timestamp, String nonce) { String[] str = new String[]{token, timestamp, nonce}; //排序 Arrays.sort(str); //拼接字符串 StringBuffer buffer = new StringBuffer(); for (int i = 0; i &lt; str.length; i++) { buffer.append(str[i]); } //进行sha1加密 String temp = SHA1.encode(buffer.toString()); //与微信提供的signature进行匹对 return signature.equals(temp); }}public class SHA1 { private static final char[] HEX_DIGITS = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'}; private static String getFormattedText(byte[] bytes) { int len = bytes.length; StringBuilder buf = new StringBuilder(len * 2); for (int j = 0; j &lt; len; j++) { buf.append(HEX_DIGITS[(bytes[j] &gt;&gt; 4) &amp; 0x0f]); buf.append(HEX_DIGITS[bytes[j] &amp; 0x0f]); } return buf.toString(); } public static String encode(String str) { if (str == null) { return null; } try { MessageDigest messageDigest = MessageDigest.getInstance(\"SHA1\"); messageDigest.update(str.getBytes()); return getFormattedText(messageDigest.digest()); } catch (Exception e) { throw new RuntimeException(e); } }} OK，完成之后，我们的校验接口就算是开发完成了。接下来就可以开发消息接收接口了。 4.2 消息接收接口接下来我们来开发消息接收接口，消息接收接口和上面的服务器校验接口地址是一样的，都是我们一开始在公众号后台配置的地址。只不过消息接收接口是一个 POST 请求。 我在公众号后台配置的时候，消息加解密方式选择了明文模式，这样我在后台收到的消息直接就可以处理了。微信服务器给我发来的普通文本消息格式如下： 12345678&lt;xml&gt; &lt;ToUserName&gt;&lt;![CDATA[toUser]]&gt;&lt;/ToUserName&gt; &lt;FromUserName&gt;&lt;![CDATA[fromUser]]&gt;&lt;/FromUserName&gt; &lt;CreateTime&gt;1348831860&lt;/CreateTime&gt; &lt;MsgType&gt;&lt;![CDATA[text]]&gt;&lt;/MsgType&gt; &lt;Content&gt;&lt;![CDATA[this is a test]]&gt;&lt;/Content&gt; &lt;MsgId&gt;1234567890123456&lt;/MsgId&gt;&lt;/xml&gt; 这些参数含义如下： 参数 描述 ToUserName 开发者微信号 FromUserName 发送方帐号（一个OpenID） CreateTime 消息创建时间 （整型） MsgType 消息类型，文本为text Content 文本消息内容 MsgId 消息id，64位整型 看到这里，大家心里大概就有数了，当我们收到微信服务器发来的消息之后，我们就进行 XML 解析，提取出来我们需要的信息，去做相关的查询操作，再将查到的结果返回给微信服务器。 这里我们先来个简单的，我们将收到的消息解析并打印出来： 12345678910111213141516171819202122232425262728@PostMapping(\"/verify_wx_token\")public void handler(HttpServletRequest request, HttpServletResponse response) throws Exception { request.setCharacterEncoding(\"UTF-8\"); response.setCharacterEncoding(\"UTF-8\"); PrintWriter out = response.getWriter(); Map&lt;String, String&gt; parseXml = MessageUtil.parseXml(request); String msgType = parseXml.get(\"MsgType\"); String content = parseXml.get(\"Content\"); String fromusername = parseXml.get(\"FromUserName\"); String tousername = parseXml.get(\"ToUserName\"); System.out.println(msgType); System.out.println(content); System.out.println(fromusername); System.out.println(tousername);}public static Map&lt;String, String&gt; parseXml(HttpServletRequest request) throws Exception { Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); InputStream inputStream = request.getInputStream(); SAXReader reader = new SAXReader(); Document document = reader.read(inputStream); Element root = document.getRootElement(); List&lt;Element&gt; elementList = root.elements(); for (Element e : elementList) map.put(e.getName(), e.getText()); inputStream.close(); inputStream = null; return map;} 大家看到其实都是一些常规代码，没有什么难度。 做完这些之后，我们将项目打成 jar 包在服务器上部署启动。启动成功之后，确认微信的后台配置也没问题，我们就可以在公众号上发一条消息了，这样我们自己的服务端就会打印出来刚刚消息的信息。 好了，篇幅限制，今天就和大家先聊这么多，后面再聊不同消息类型的解析和消息的返回问题。 不知道小伙伴们看懂没？有问题欢迎留言讨论。 参考资料：微信开放文档","link":"/2019/1029/springboot-weixin.html"},{"title":"Spring Boot 整合 Freemarker，50 多行配置是怎么省略掉的？","text":"Spring Boot2 系列教程接近完工，最近进入修修补补阶段。Freemarker 整合貌似还没和大家聊过，因此今天把这个补充上。 已经完工的 Spring Boot2 教程，大家可以参考这里： 干货|最新版 Spring Boot2.1.5 教程+案例合集 Freemarker 简介这是一个相当老牌的开源的免费的模版引擎。通过 Freemarker 模版，我们可以将数据渲染成 HTML 网页、电子邮件、配置文件以及源代码等。Freemarker 不是面向最终用户的，而是一个 Java 类库，我们可以将之作为一个普通的组件嵌入到我们的产品中。 来看一张来自 Freemarker 官网的图片： 可以看到，Freemarker 可以将模版和数据渲染成 HTML 。 Freemarker 模版后缀为 .ftl(FreeMarker Template Language)。FTL 是一种简单的、专用的语言，它不是像 Java 那样成熟的编程语言。在模板中，你可以专注于如何展现数据， 而在模板之外可以专注于要展示什么数据。 好了，这是一个简单的介绍，接下来我们来看看 Freemarker 和 Spring Boot 的一个整合操作。 实践在 SSM 中整合 Freemarker ，所有的配置文件加起来，前前后后大约在 50 行左右，Spring Boot 中要几行配置呢？ 0 行！ 1.创建工程首先创建一个 Spring Boot 工程，引入 Freemarker 依赖，如下图： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 工程创建完成后，在 org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration 类中，可以看到关于 Freemarker 的自动化配置： 1234567@Configuration@ConditionalOnClass({ freemarker.template.Configuration.class, FreeMarkerConfigurationFactory.class })@EnableConfigurationProperties(FreeMarkerProperties.class)@Import({ FreeMarkerServletWebConfiguration.class, FreeMarkerReactiveWebConfiguration.class, FreeMarkerNonWebConfiguration.class })public class FreeMarkerAutoConfiguration {} 从这里可以看出，当 classpath 下存在 freemarker.template.Configuration 以及 FreeMarkerConfigurationFactory 时，配置才会生效，也就是说当我们引入了 Freemarker 之后，配置就会生效。但是这里的自动化配置只做了模板位置检查，其他配置则是在导入的 FreeMarkerServletWebConfiguration 配置中完成的。那么我们再来看看 FreeMarkerServletWebConfiguration 类，部分源码如下： 123456789101112131415161718192021222324@Configuration@ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.SERVLET)@ConditionalOnClass({ Servlet.class, FreeMarkerConfigurer.class })@AutoConfigureAfter(WebMvcAutoConfiguration.class)class FreeMarkerServletWebConfiguration extends AbstractFreeMarkerConfiguration { protected FreeMarkerServletWebConfiguration(FreeMarkerProperties properties) { super(properties); } @Bean @ConditionalOnMissingBean(FreeMarkerConfig.class) public FreeMarkerConfigurer freeMarkerConfigurer() { FreeMarkerConfigurer configurer = new FreeMarkerConfigurer(); applyProperties(configurer); return configurer; } @Bean @ConditionalOnMissingBean(name = \"freeMarkerViewResolver\") @ConditionalOnProperty(name = \"spring.freemarker.enabled\", matchIfMissing = true) public FreeMarkerViewResolver freeMarkerViewResolver() { FreeMarkerViewResolver resolver = new FreeMarkerViewResolver(); getProperties().applyToMvcViewResolver(resolver); return resolver; }} 我们来简单看下这段源码： @ConditionalOnWebApplication 表示当前配置在 web 环境下才会生效。 ConditionalOnClass 表示当前配置在存在 Servlet 和 FreeMarkerConfigurer 时才会生效。 @AutoConfigureAfter 表示当前自动化配置在 WebMvcAutoConfiguration 之后完成。 代码中，主要提供了 FreeMarkerConfigurer 和 FreeMarkerViewResolver。 FreeMarkerConfigurer 是 Freemarker 的一些基本配置，例如 templateLoaderPath、defaultEncoding 等 FreeMarkerViewResolver 则是视图解析器的基本配置，包含了viewClass、suffix、allowRequestOverride、allowSessionOverride 等属性。 另外还有一点，在这个类的构造方法中，注入了 FreeMarkerProperties： 12345678910@ConfigurationProperties(prefix = \"spring.freemarker\")public class FreeMarkerProperties extends AbstractTemplateViewResolverProperties { public static final String DEFAULT_TEMPLATE_LOADER_PATH = \"classpath:/templates/\"; public static final String DEFAULT_PREFIX = \"\"; public static final String DEFAULT_SUFFIX = \".ftl\"; /** * Well-known FreeMarker keys which are passed to FreeMarker's Configuration. */ private Map&lt;String, String&gt; settings = new HashMap&lt;&gt;();} FreeMarkerProperties 中则配置了 Freemarker 的基本信息，例如模板位置在 classpath:/templates/ ，再例如模板后缀为 .ftl，那么这些配置我们以后都可以在 application.properties 中进行修改。 如果我们在 SSM 的 XML 文件中自己配置 Freemarker ，也不过就是配置这些东西。现在，这些配置由 FreeMarkerServletWebConfiguration​ 帮我们完成了。 2.创建类首先我们来创建一个 User 类，如下： 123456public class User { private Long id; private String username; private String address; //省略 getter/setter} 再来创建 UserController： 12345678910111213141516@Controllerpublic class UserController { @GetMapping(\"/index\") public String index(Model model) { List&lt;User&gt; users = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { User user = new User(); user.setId((long) i); user.setUsername(\"javaboy&gt;&gt;&gt;&gt;\" + i); user.setAddress(\"www.javaboy.org&gt;&gt;&gt;&gt;\" + i); users.add(user); } model.addAttribute(\"users\", users); return \"index\"; }} 最后在 freemarker 中渲染数据： 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;用户编号&lt;/td&gt; &lt;td&gt;用户名称&lt;/td&gt; &lt;td&gt;用户地址&lt;/td&gt; &lt;/tr&gt; &lt;#list users as user&gt; &lt;tr&gt; &lt;td&gt;${user.id}&lt;/td&gt; &lt;td&gt;${user.username}&lt;/td&gt; &lt;td&gt;${user.address}&lt;/td&gt; &lt;/tr&gt; &lt;/#list&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 运行效果如下： 其他配置如果我们要修改模版文件位置等，可以在 application.properties 中进行配置： 12345678910spring.freemarker.allow-request-override=falsespring.freemarker.allow-session-override=falsespring.freemarker.cache=falsespring.freemarker.charset=UTF-8spring.freemarker.check-template-location=truespring.freemarker.content-type=text/htmlspring.freemarker.expose-request-attributes=falsespring.freemarker.expose-session-attributes=falsespring.freemarker.suffix=.ftlspring.freemarker.template-loader-path=classpath:/templates/ 配置文件按照顺序依次解释如下： HttpServletRequest的属性是否可以覆盖controller中model的同名项 HttpSession的属性是否可以覆盖controller中model的同名项 是否开启缓存 模板文件编码 是否检查模板位置 Content-Type的值 是否将HttpServletRequest中的属性添加到Model中 是否将HttpSession中的属性添加到Model中 模板文件后缀 模板文件位置 好了，整合完成之后，Freemarker 的更多用法，就和在 SSM 中使用 Freemarker 一样了，这里我就不再赘述。 结语本文和大家简单聊一聊 Spring Boot 整合 Freemarker，算是对 Spring Boot2 教程的一个补充（后面还会有一些补充），有问题欢迎留言讨论。 本项目案例，我已经上传到 GitHub 上，欢迎大家 star：https://github.com/lenve/javaboy-code-samples","link":"/2019/0705/springboot-freemarker.html"},{"title":"Spring Boot 整合 Shiro ，两种方式全总结！","text":"在 Spring Boot 中做权限管理，一般来说，主流的方案是 Spring Security ，但是，仅仅从技术角度来说，也可以使用 Shiro。 今天松哥就来和大家聊聊 Spring Boot 整合 Shiro 的话题！ 一般来说，Spring Security 和 Shiro 的比较如下： Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架 Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单 Spring Security 功能强大；Shiro 功能简单 … 虽然 Shiro 功能简单，但是也能满足大部分的业务场景。所以在传统的 SSM 项目中，一般来说，可以整合 Shiro。 在 Spring Boot 中，由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，当然也提供了 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。 这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。 在 Spring Boot 中整合 Shiro ，有两种不同的方案： 第一种就是原封不动的，将 SSM 整合 Shiro 的配置用 Java 重写一遍。 第二种就是使用 Shiro 官方提供的一个 Starter 来配置，但是，这个 Starter 并没有简化多少配置。 原生的整合 创建项目 创建一个 Spring Boot 项目，只需要添加 Web 依赖即可： 项目创建成功后，加入 Shiro 相关的依赖，完整的 pom.xml 文件中的依赖如下： 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 Realm 接下来我们来自定义核心组件 Realm： 1234567891011121314public class MyRealm extends AuthorizingRealm { @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) { return null; } @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { String username = (String) token.getPrincipal(); if (!\"javaboy\".equals(username)) { throw new UnknownAccountException(\"账户不存在!\"); } return new SimpleAuthenticationInfo(username, \"123\", getName()); }} 在 Realm 中实现简单的认证操作即可，不做授权，授权的具体写法和 SSM 中的 Shiro 一样，不赘述。这里的认证表示用户名必须是 javaboy ，用户密码必须是 123 ，满足这样的条件，就能登录成功！ 配置 Shiro 接下来进行 Shiro 的配置： 12345678910111213141516171819202122232425262728@Configurationpublic class ShiroConfig { @Bean MyRealm myRealm() { return new MyRealm(); } @Bean SecurityManager securityManager() { DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setRealm(myRealm()); return manager; } @Bean ShiroFilterFactoryBean shiroFilterFactoryBean() { ShiroFilterFactoryBean bean = new ShiroFilterFactoryBean(); bean.setSecurityManager(securityManager()); bean.setLoginUrl(\"/login\"); bean.setSuccessUrl(\"/index\"); bean.setUnauthorizedUrl(\"/unauthorizedurl\"); Map&lt;String, String&gt; map = new LinkedHashMap&lt;&gt;(); map.put(\"/doLogin\", \"anon\"); map.put(\"/**\", \"authc\"); bean.setFilterChainDefinitionMap(map); return bean; }} 在这里进行 Shiro 的配置主要配置 3 个 Bean ： 首先需要提供一个 Realm 的实例。 需要配置一个 SecurityManager，在 SecurityManager 中配置 Realm。 配置一个 ShiroFilterFactoryBean ，在 ShiroFilterFactoryBean 中指定路径拦截规则等。 配置登录和测试接口。 其中，ShiroFilterFactoryBean 的配置稍微多一些，配置含义如下： setSecurityManager 表示指定 SecurityManager。 setLoginUrl 表示指定登录页面。 setSuccessUrl 表示指定登录成功页面。 接下来的 Map 中配置了路径拦截规则，注意，要有序。 这些东西都配置完成后，接下来配置登录 Controller: 12345678910111213141516171819202122@RestControllerpublic class LoginController { @PostMapping(\"/doLogin\") public void doLogin(String username, String password) { Subject subject = SecurityUtils.getSubject(); try { subject.login(new UsernamePasswordToken(username, password)); System.out.println(\"登录成功!\"); } catch (AuthenticationException e) { e.printStackTrace(); System.out.println(\"登录失败!\"); } } @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @GetMapping(\"/login\") public String login() { return \"please login!\"; }} 测试时，首先访问 /hello 接口，由于未登录，所以会自动跳转到 /login 接口： 然后调用 /doLogin 接口完成登录： 再次访问 /hello 接口，就可以成功访问了： 使用 Shiro Starter上面这种配置方式实际上相当于把 SSM 中的 XML 配置拿到 Spring Boot 中用 Java 代码重新写了一遍，除了这种方式之外，我们也可以直接使用 Shiro 官方提供的 Starter 。 创建工程，和上面的一样 创建成功后，添加 shiro-spring-boot-web-starter ，这个依赖可以代替之前的 shiro-web 和 shiro-spring 两个依赖，pom.xml 文件如下： 1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring-boot-web-starter&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 Realm 这里的 Realm 和前面的一样，我就不再赘述。 配置 Shiro 基本信息 接下来在 application.properties 中配置 Shiro 的基本信息： 123456shiro.sessionManager.sessionIdCookieEnabled=trueshiro.sessionManager.sessionIdUrlRewritingEnabled=trueshiro.unauthorizedUrl=/unauthorizedurlshiro.web.enabled=trueshiro.successUrl=/indexshiro.loginUrl=/login 配置解释： 第一行表示是否允许将sessionId 放到 cookie 中 第二行表示是否允许将 sessionId 放到 Url 地址拦中 第三行表示访问未获授权的页面时，默认的跳转路径 第四行表示开启 shiro 第五行表示登录成功的跳转页面 第六行表示登录页面 配置 ShiroConfig 1234567891011121314151617181920@Configurationpublic class ShiroConfig { @Bean MyRealm myRealm() { return new MyRealm(); } @Bean DefaultWebSecurityManager securityManager() { DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setRealm(myRealm()); return manager; } @Bean ShiroFilterChainDefinition shiroFilterChainDefinition() { DefaultShiroFilterChainDefinition definition = new DefaultShiroFilterChainDefinition(); definition.addPathDefinition(\"/doLogin\", \"anon\"); definition.addPathDefinition(\"/**\", \"authc\"); return definition; }} 这里的配置和前面的比较像，但是不再需要 ShiroFilterFactoryBean 实例了，替代它的是 ShiroFilterChainDefinition ，在这里定义 Shiro 的路径匹配规则即可。 这里定义完之后，接下来的登录接口定义以及测试方法都和前面的一致，我就不再赘述了。大家可以参考上文。 总结本文主要向大家介绍了 Spring Boot 整合 Shiro 的两种方式，一种是传统方式的 Java 版，另一种则是使用 Shiro 官方提供的 Starter，两种方式，不知道大家有没有学会呢？ 本文案例，我已经上传到 GitHub ，欢迎大家 star：https://github.com/lenve/javaboy-code-samples 关于本文，有问题欢迎留言讨论。","link":"/2019/0611/springboot-shiro.html"},{"title":"Spring Boot 邮件发送的 5 种姿势！","text":"邮件发送其实是一个非常常见的需求，用户注册，找回密码等地方，都会用到，使用 JavaSE 代码发送邮件，步骤还是挺繁琐的，Spring Boot 中对于邮件发送，提供了相关的自动化配置类，使得邮件发送变得非常容易，本文我们就来一探究竟！看看使用 Spring Boot 发送邮件的 5 中姿势。 邮件基础我们经常会听到各种各样的邮件协议，比如 SMTP、POP3、IMAP ，那么这些协议有什么作用，有什么区别？我们先来讨论一下这个问题。 SMTP 是一个基于 TCP/IP 的应用层协议，江湖地位有点类似于 HTTP，SMTP 服务器默认监听的端口号为 25 。看到这里，小伙伴们可能会想到既然 SMTP 协议是基于 TCP/IP 的应用层协议，那么我是不是也可以通过 Socket 发送一封邮件呢？回答是肯定的。 生活中我们投递一封邮件要经过如下几个步骤： 深圳的小王先将邮件投递到深圳的邮局 深圳的邮局将邮件运送到上海的邮局 上海的小张来邮局取邮件 这是一个缩减版的生活中邮件发送过程。这三个步骤可以分别对应我们的邮件发送过程，假设从 aaa@qq.com 发送邮件到 111@163.com ： aaa@qq.com 先将邮件投递到腾讯的邮件服务器 腾讯的邮件服务器将我们的邮件投递到网易的邮件服务器 111@163.com 登录网易的邮件服务器查看邮件 邮件投递大致就是这个过程，这个过程就涉及到了多个协议，我们来分别看一下。 SMTP 协议全称为 Simple Mail Transfer Protocol，译作简单邮件传输协议，它定义了邮件客户端软件与 SMTP 服务器之间，以及 SMTP 服务器与 SMTP 服务器之间的通信规则。 也就是说 aaa@qq.com 用户先将邮件投递到腾讯的 SMTP 服务器这个过程就使用了 SMTP 协议，然后腾讯的 SMTP 服务器将邮件投递到网易的 SMTP 服务器这个过程也依然使用了 SMTP 协议，SMTP 服务器就是用来收邮件。 而 POP3 协议全称为 Post Office Protocol ，译作邮局协议，它定义了邮件客户端与 POP3 服务器之间的通信规则，那么该协议在什么场景下会用到呢？当邮件到达网易的 SMTP 服务器之后， 111@163.com 用户需要登录服务器查看邮件，这个时候就该协议就用上了：邮件服务商都会为每一个用户提供专门的邮件存储空间，SMTP 服务器收到邮件之后，就将邮件保存到相应用户的邮件存储空间中，如果用户要读取邮件，就需要通过邮件服务商的 POP3 邮件服务器来完成。 最后，可能也有小伙伴们听说过 IMAP 协议，这个协议是对 POP3 协议的扩展，功能更强，作用类似，这里不再赘述。 准备工作目前国内大部分的邮件服务商都不允许直接使用用户名/密码的方式来在代码中发送邮件，都是要先申请授权码，这里以 QQ 邮箱为例，向大家演示授权码的申请流程：首先我们需要先登录 QQ 邮箱网页版，点击上方的设置按钮： 然后点击账户选项卡： 在账户选项卡中找到开启POP3/SMTP选项，如下： 点击开启，开启相关功能，开启过程需要手机号码验证，按照步骤操作即可，不赘述。开启成功之后，即可获取一个授权码，将该号码保存好，一会使用。 项目创建接下来，我们就可以创建项目了，Spring Boot 中，对于邮件发送提供了自动配置类，开发者只需要加入相关依赖，然后配置一下邮箱的基本信息，就可以发送邮件了。 首先创建一个 Spring Boot 项目，引入邮件发送依赖： 创建完成后，项目依赖如下： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 配置邮箱基本信息 项目创建成功后，接下来在 application.properties 中配置邮箱的基本信息： 1234567spring.mail.host=smtp.qq.comspring.mail.port=587spring.mail.username=1510161612@qq.comspring.mail.password=ubknfzhjkhrbbabespring.mail.default-encoding=UTF-8spring.mail.properties.mail.smtp.socketFactoryClass=javax.net.ssl.SSLSocketFactoryspring.mail.properties.mail.debug=true 配置含义分别如下： 配置 SMTP 服务器地址 SMTP 服务器的端口 配置邮箱用户名 配置密码，注意，不是真正的密码，而是刚刚申请到的授权码 默认的邮件编码 配饰 SSL 加密工厂 表示开启 DEBUG 模式，这样，邮件发送过程的日志会在控制台打印出来，方便排查错误 如果不知道 smtp 服务器的端口或者地址的的话，可以参考 腾讯的邮箱文档 https://service.mail.qq.com/cgi-bin/help?subtype=1&amp;&amp;id=28&amp;&amp;no=371 做完这些之后，Spring Boot 就会自动帮我们配置好邮件发送类，相关的配置在 org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration 类中，部分源码如下： 12345678@Configuration@ConditionalOnClass({ MimeMessage.class, MimeType.class, MailSender.class })@ConditionalOnMissingBean(MailSender.class)@Conditional(MailSenderCondition.class)@EnableConfigurationProperties(MailProperties.class)@Import({ MailSenderJndiConfiguration.class, MailSenderPropertiesConfiguration.class })public class MailSenderAutoConfiguration {} 从这段代码中，可以看到，导入了另外一个配置 MailSenderPropertiesConfiguration 类，这个类中，提供了邮件发送相关的工具类： 123456789101112131415@Configuration@ConditionalOnProperty(prefix = \"spring.mail\", name = \"host\")class MailSenderPropertiesConfiguration { private final MailProperties properties; MailSenderPropertiesConfiguration(MailProperties properties) { this.properties = properties; } @Bean @ConditionalOnMissingBean public JavaMailSenderImpl mailSender() { JavaMailSenderImpl sender = new JavaMailSenderImpl(); applyProperties(sender); return sender; }} 可以看到，这里创建了一个 JavaMailSenderImpl 的实例， JavaMailSenderImpl 是 JavaMailSender 的一个实现，我们将使用 JavaMailSenderImpl 来完成邮件的发送工作。 做完如上两步，邮件发送的准备工作就算是完成了，接下来就可以直接发送邮件了。 具体的发送，有 5 种不同的方式，我们一个一个来看。 发送简单邮件简单邮件就是指邮件内容是一个普通的文本文档： 1234567891011121314@AutowiredJavaMailSender javaMailSender;@Testpublic void sendSimpleMail() { SimpleMailMessage message = new SimpleMailMessage(); message.setSubject(\"这是一封测试邮件\"); message.setFrom(\"1510161612@qq.com\"); message.setTo(\"25xxxxx755@qq.com\"); message.setCc(\"37xxxxx37@qq.com\"); message.setBcc(\"14xxxxx098@qq.com\"); message.setSentDate(new Date()); message.setText(\"这是测试邮件的正文\"); javaMailSender.send(message);} 从上往下，代码含义分别如下： 构建一个邮件对象 设置邮件主题 设置邮件发送者 设置邮件接收者，可以有多个接收者 设置邮件抄送人，可以有多个抄送人 设置隐秘抄送人，可以有多个 设置邮件发送日期 设置邮件的正文 发送邮件 最后执行该方法，就可以实现邮件的发送，发送效果图如下： 发送带附件的邮件邮件的附件可以是图片，也可以是普通文件，都是支持的。 1234567891011121314@Testpublic void sendAttachFileMail() throws MessagingException { MimeMessage mimeMessage = javaMailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage,true); helper.setSubject(\"这是一封测试邮件\"); helper.setFrom(\"1510161612@qq.com\"); helper.setTo(\"25xxxxx755@qq.com\"); helper.setCc(\"37xxxxx37@qq.com\"); helper.setBcc(\"14xxxxx098@qq.com\"); helper.setSentDate(new Date()); helper.setText(\"这是测试邮件的正文\"); helper.addAttachment(\"javaboy.jpg\",new File(\"C:\\\\Users\\\\sang\\\\Downloads\\\\javaboy.png\")); javaMailSender.send(mimeMessage);} 注意这里在构建邮件对象上和前文有所差异，这里是通过 javaMailSender 来获取一个复杂邮件对象，然后再利用 MimeMessageHelper 对邮件进行配置，MimeMessageHelper 是一个邮件配置的辅助工具类，创建时候的 true 表示构建一个 multipart message 类型的邮件，有了 MimeMessageHelper 之后，我们针对邮件的配置都是由 MimeMessageHelper 来代劳。 最后通过 addAttachment 方法来添加一个附件。 执行该方法，邮件发送效果图如下： 发送带图片资源的邮件图片资源和附件有什么区别呢？图片资源是放在邮件正文中的，即一打开邮件，就能看到图片。但是一般来说，不建议使用这种方式，一些公司会对邮件内容的大小有限制（因为这种方式是将图片一起发送的）。 123456789101112131415@Testpublic void sendImgResMail() throws MessagingException { MimeMessage mimeMessage = javaMailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); helper.setSubject(\"这是一封测试邮件\"); helper.setFrom(\"1510161612@qq.com\"); helper.setTo(\"25xxxxx755@qq.com\"); helper.setCc(\"37xxxxx37@qq.com\"); helper.setBcc(\"14xxxxx098@qq.com\"); helper.setSentDate(new Date()); helper.setText(\"&lt;p&gt;hello 大家好，这是一封测试邮件，这封邮件包含两种图片，分别如下&lt;/p&gt;&lt;p&gt;第一张图片：&lt;/p&gt;&lt;img src='cid:p01'/&gt;&lt;p&gt;第二张图片：&lt;/p&gt;&lt;img src='cid:p02'/&gt;\",true); helper.addInline(\"p01\",new FileSystemResource(new File(\"C:\\\\Users\\\\sang\\\\Downloads\\\\javaboy.png\"))); helper.addInline(\"p02\",new FileSystemResource(new File(\"C:\\\\Users\\\\sang\\\\Downloads\\\\javaboy2.png\"))); javaMailSender.send(mimeMessage);} 这里的邮件 text 是一个 HTML 文本，里边涉及到的图片资源先用一个占位符占着，setText 方法的第二个参数 true 表示第一个参数是一个 HTML 文本。 setText 之后，再通过 addInline 方法来添加图片资源。 最后执行该方法，发送邮件，效果如下： 在公司实际开发中，第一种和第三种都不是使用最多的邮件发送方案。因为正常来说，邮件的内容都是比较的丰富的，所以大部分邮件都是通过 HTML 来呈现的，如果直接拼接 HTML 字符串，这样以后不好维护，为了解决这个问题，一般邮件发送，都会有相应的邮件模板。最具代表性的两个模板就是 Freemarker 模板和 Thyemeleaf 模板了。 使用 Freemarker 作邮件模板首先需要引入 Freemarker 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt;&lt;/dependency&gt; 然后在 resources/templates 目录下创建一个 mail.ftl 作为邮件发送模板： 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;hello 欢迎加入 xxx 大家庭，您的入职信息如下：&lt;/p&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;td&gt;${username}&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;工号&lt;/td&gt; &lt;td&gt;${num}&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;薪水&lt;/td&gt; &lt;td&gt;${salary}&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;div style=\"color: #ff1a0e\"&gt;一起努力创造辉煌&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 接下来，将邮件模板渲染成 HTML ，然后发送即可。 123456789101112131415161718192021222324252627@Testpublic void sendFreemarkerMail() throws MessagingException, IOException, TemplateException { MimeMessage mimeMessage = javaMailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); helper.setSubject(\"这是一封测试邮件\"); helper.setFrom(\"1510161612@qq.com\"); helper.setTo(\"25xxxxx755@qq.com\"); helper.setCc(\"37xxxxx37@qq.com\"); helper.setBcc(\"14xxxxx098@qq.com\"); helper.setSentDate(new Date()); //构建 Freemarker 的基本配置 Configuration configuration = new Configuration(Configuration.VERSION_2_3_0); // 配置模板位置 ClassLoader loader = MailApplication.class.getClassLoader(); configuration.setClassLoaderForTemplateLoading(loader, \"templates\"); //加载模板 Template template = configuration.getTemplate(\"mail.ftl\"); User user = new User(); user.setUsername(\"javaboy\"); user.setNum(1); user.setSalary((double) 99999); StringWriter out = new StringWriter(); //模板渲染，渲染的结果将被保存到 out 中 ，将out 中的 html 字符串发送即可 template.process(user, out); helper.setText(out.toString(),true); javaMailSender.send(mimeMessage);} 需要注意的是，虽然引入了 Freemarker 的自动化配置，但是我们在这里是直接 new Configuration 来重新配置 Freemarker 的，所以 Freemarker 默认的配置这里不生效，因此，在填写模板位置时，值为 templates 。 调用该方法，发送邮件，效果图如下： 使用 Thymeleaf 作邮件模板推荐在 Spring Boot 中使用 Thymeleaf 来构建邮件模板。因为 Thymeleaf 的自动化配置提供了一个 TemplateEngine，通过 TemplateEngine 可以方便的将 Thymeleaf 模板渲染为 HTML ，同时，Thymeleaf 的自动化配置在这里是继续有效的 。 首先，引入 Thymeleaf 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 然后，创建 Thymeleaf 邮件模板： 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;hello 欢迎加入 xxx 大家庭，您的入职信息如下：&lt;/p&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;td th:text=\"${username}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;工号&lt;/td&gt; &lt;td th:text=\"${num}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;薪水&lt;/td&gt; &lt;td th:text=\"${salary}\"&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;div style=\"color: #ff1a0e\"&gt;一起努力创造辉煌&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 接下来发送邮件： 123456789101112131415161718192021@AutowiredTemplateEngine templateEngine;@Testpublic void sendThymeleafMail() throws MessagingException { MimeMessage mimeMessage = javaMailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); helper.setSubject(\"这是一封测试邮件\"); helper.setFrom(\"1510161612@qq.com\"); helper.setTo(\"25xxxxx755@qq.com\"); helper.setCc(\"37xxxxx37@qq.com\"); helper.setBcc(\"14xxxxx098@qq.com\"); helper.setSentDate(new Date()); Context context = new Context(); context.setVariable(\"username\", \"javaboy\"); context.setVariable(\"num\",\"000001\"); context.setVariable(\"salary\", \"99999\"); String process = templateEngine.process(\"mail.html\", context); helper.setText(process,true); javaMailSender.send(mimeMessage);} 调用该方法，发送邮件，效果图如下： 好了，这就是我们今天说的 5 种邮件发送姿势，不知道你掌握了没有呢？ 本文案例已经上传到 GitHub：https://github.com/lenve/javaboy-code-samples。 有问题欢迎留言讨论。","link":"/2019/0717/springboot-mail.html"},{"title":"Spring Boot数据持久化之JdbcTemplate","text":"在Java领域，数据持久化有几个常见的方案，有Spring自带的JdbcTemplate、有MyBatis，还有JPA，在这些方案中，最简单的就是Spring自带的JdbcTemplate了，这个东西虽然没有MyBatis那么方便，但是比起最开始的Jdbc已经强了很多了，它没有MyBatis功能那么强大，当然也意味着它的使用比较简单，事实上，JdbcTemplate算是最简单的数据持久化方案了，本文就和大伙来说说这个东西的使用。 基本配置JdbcTemplate基本用法实际上很简单，开发者在创建一个SpringBoot项目时，除了选择基本的Web依赖，再记得选上Jdbc依赖，以及数据库驱动依赖即可，如下： 项目创建成功之后，记得添加Druid数据库连接池依赖（注意这里可以添加专门为Spring Boot打造的druid-spring-boot-starter，而不是我们一般在SSM中添加的Druid），所有添加的依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 项目创建完后，接下来只需要在application.properties中提供数据的基本配置即可，如下： 1234spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.username=rootspring.datasource.password=123spring.datasource.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=UTF-8 如此之后，所有的配置就算完成了，接下来就可以直接使用JdbcTemplate了？咋这么方便呢？其实这就是SpringBoot的自动化配置带来的好处，我们先说用法，一会来说原理。 基本用法首先我们来创建一个User Bean，如下： public class User { private Long id; private String username; private String address; //省略getter/setter } 然后来创建一个UserService类，在UserService类中注入JdbcTemplate，如下： @Service public class UserService { @Autowired JdbcTemplate jdbcTemplate; } 好了，如此之后，准备工作就算完成了。 增JdbcTemplate中，除了查询有几个API之外，增删改统一都使用update来操作，自己来传入SQL即可。例如添加数据，方法如下： public int addUser(User user) { return jdbcTemplate.update(\"insert into user (username,address) values (?,?);\", user.getUsername(), user.getAddress()); } update方法的返回值就是SQL执行受影响的行数。 这里只需要传入SQL即可，如果你的需求比较复杂，例如在数据插入的过程中希望实现主键回填，那么可以使用PreparedStatementCreator，如下： public int addUser2(User user) { KeyHolder keyHolder = new GeneratedKeyHolder(); int update = jdbcTemplate.update(new PreparedStatementCreator() { @Override public PreparedStatement createPreparedStatement(Connection connection) throws SQLException { PreparedStatement ps = connection.prepareStatement(\"insert into user (username,address) values (?,?);\", Statement.RETURN_GENERATED_KEYS); ps.setString(1, user.getUsername()); ps.setString(2, user.getAddress()); return ps; } }, keyHolder); user.setId(keyHolder.getKey().longValue()); System.out.println(user); return update; } 实际上这里就相当于完全使用了JDBC中的解决方案了，首先在构建PreparedStatement时传入Statement.RETURN_GENERATED_KEYS，然后传入KeyHolder，最终从KeyHolder中获取刚刚插入数据的id保存到user对象的id属性中去。 你能想到的JDBC的用法，在这里都能实现，Spring提供的JdbcTemplate虽然不如MyBatis，但是比起Jdbc还是要方便很多的。 删删除也是使用update API，传入你的SQL即可： public int deleteUserById(Long id) { return jdbcTemplate.update(\"delete from user where id=?\", id); } 当然你也可以使用PreparedStatementCreator。 改public int updateUserById(User user) { return jdbcTemplate.update(\"update user set username=?,address=? where id=?\", user.getUsername(), user.getAddress(),user.getId()); } 当然你也可以使用PreparedStatementCreator。 查查询的话，稍微有点变化，这里主要向大伙介绍query方法，例如查询所有用户： public List&lt;User&gt; getAllUsers() { return jdbcTemplate.query(\"select * from user\", new RowMapper&lt;User&gt;() { @Override public User mapRow(ResultSet resultSet, int i) throws SQLException { String username = resultSet.getString(\"username\"); String address = resultSet.getString(\"address\"); long id = resultSet.getLong(\"id\"); User user = new User(); user.setAddress(address); user.setUsername(username); user.setId(id); return user; } }); } 查询的时候需要提供一个RowMapper，就是需要自己手动映射，将数据库中的字段和对象的属性一一对应起来，这样。。。。嗯看起来有点麻烦，实际上，如果数据库中的字段和对象属性的名字一模一样的话，有另外一个简单的方案，如下： public List&lt;User&gt; getAllUsers2() { return jdbcTemplate.query(\"select * from user\", new BeanPropertyRowMapper&lt;&gt;(User.class)); } 至于查询时候传参也是使用占位符，这个和前文的一致，这里不再赘述。 其他除了这些基本用法之外，JdbcTemplate也支持其他用法，例如调用存储过程等，这些都比较容易，而且和Jdbc本身都比较相似，这里也就不做介绍了，有兴趣可以留言讨论。 原理分析那么在SpringBoot中，配置完数据库基本信息之后，就有了一个JdbcTemplate了，这个东西是从哪里来的呢？源码在org.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration类中，该类源码如下： @Configuration @ConditionalOnClass({ DataSource.class, JdbcTemplate.class }) @ConditionalOnSingleCandidate(DataSource.class) @AutoConfigureAfter(DataSourceAutoConfiguration.class) @EnableConfigurationProperties(JdbcProperties.class) public class JdbcTemplateAutoConfiguration { @Configuration static class JdbcTemplateConfiguration { private final DataSource dataSource; private final JdbcProperties properties; JdbcTemplateConfiguration(DataSource dataSource, JdbcProperties properties) { this.dataSource = dataSource; this.properties = properties; } @Bean @Primary @ConditionalOnMissingBean(JdbcOperations.class) public JdbcTemplate jdbcTemplate() { JdbcTemplate jdbcTemplate = new JdbcTemplate(this.dataSource); JdbcProperties.Template template = this.properties.getTemplate(); jdbcTemplate.setFetchSize(template.getFetchSize()); jdbcTemplate.setMaxRows(template.getMaxRows()); if (template.getQueryTimeout() != null) { jdbcTemplate .setQueryTimeout((int) template.getQueryTimeout().getSeconds()); } return jdbcTemplate; } } @Configuration @Import(JdbcTemplateConfiguration.class) static class NamedParameterJdbcTemplateConfiguration { @Bean @Primary @ConditionalOnSingleCandidate(JdbcTemplate.class) @ConditionalOnMissingBean(NamedParameterJdbcOperations.class) public NamedParameterJdbcTemplate namedParameterJdbcTemplate( JdbcTemplate jdbcTemplate) { return new NamedParameterJdbcTemplate(jdbcTemplate); } } } 从这个类中，大致可以看出，当当前类路径下存在DataSource和JdbcTemplate时，该类就会被自动配置，jdbcTemplate方法则表示，如果开发者没有自己提供一个JdbcOperations的实例的话，系统就自动配置一个JdbcTemplate Bean（JdbcTemplate是JdbcOperations接口的一个实现）。好了，不知道大伙有没有收获呢？ 关于JdbcTemplate，我还有一个小小视频，加入我的知识星球，免费观看： 加入我的星球，和众多大牛一起切磋技术推荐一个技术圈子，Java技能提升就靠它了。","link":"/2019/0406/jdbctemplate.html"},{"title":"Spring Boot整合Jpa多数据源","text":"本文是Spring Boot整合数据持久化方案的最后一篇，主要和大伙来聊聊Spring Boot整合Jpa多数据源问题。在Spring Boot整合JbdcTemplate多数据源、Spring Boot整合MyBatis多数据源以及Spring Boot整合Jpa多数据源这三个知识点中，整合Jpa多数据源算是最复杂的一种，也是很多人在配置时最容易出错的一种。本文大伙就跟着松哥的教程，一步一步整合Jpa多数据源。 工程创建首先是创建一个Spring Boot工程，创建时添加基本的Web、Jpa以及MySQL依赖，如下： 创建完成后，添加Druid依赖，这里和前文的要求一样，要使用专为Spring Boot打造的Druid，大伙可能发现了，如果整合多数据源一定要使用这个依赖，因为这个依赖中才有DruidDataSourceBuilder，最后还要记得锁定数据库依赖的版本，因为可能大部分人用的还是5.x的MySQL而不是8.x。完整依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 如此之后，工程就创建成功了。 基本配置在基本配置中，我们首先来配置多数据源基本信息以及DataSource，首先在application.properties中添加如下配置信息： 123456789101112131415161718# 数据源一spring.datasource.one.username=rootspring.datasource.one.password=rootspring.datasource.one.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=UTF-8spring.datasource.one.type=com.alibaba.druid.pool.DruidDataSource# 数据源二spring.datasource.two.username=rootspring.datasource.two.password=rootspring.datasource.two.url=jdbc:mysql:///test02?useUnicode=true&amp;characterEncoding=UTF-8spring.datasource.two.type=com.alibaba.druid.pool.DruidDataSource# Jpa配置spring.jpa.properties.database=mysqlspring.jpa.properties.show-sql=truespring.jpa.properties.database-platform=mysqlspring.jpa.properties.hibernate.ddl-auto=updatespring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL57Dialect 这里Jpa的配置和上文相比key中多了properties，多数据源的配置和前文一致，然后接下来配置两个DataSource，如下： 1234567891011121314@Configurationpublic class DataSourceConfig { @Bean @ConfigurationProperties(prefix = \"spring.datasource.one\") @Primary DataSource dsOne() { return DruidDataSourceBuilder.create().build(); } @Bean @ConfigurationProperties(prefix = \"spring.datasource.two\") DataSource dsTwo() { return DruidDataSourceBuilder.create().build(); }} 这里的配置和前文的多数据源配置基本一致，但是注意多了一个在Spring中使用较少的注解@Primary，这个注解一定不能少，否则在项目启动时会出错，@Primary表示当某一个类存在多个实例时，优先使用哪个实例。 好了，这样，DataSource就有了。 多数据源配置接下来配置Jpa的基本信息，这里两个数据源，我分别在两个类中来配置，先来看第一个配置： 123456789101112131415161718192021222324252627@Configuration@EnableJpaRepositories(basePackages = &quot;org.sang.jpa.dao&quot;,entityManagerFactoryRef = &quot;localContainerEntityManagerFactoryBeanOne&quot;,transactionManagerRef = &quot;platformTransactionManagerOne&quot;)public class JpaConfigOne { @Autowired @Qualifier(value = &quot;dsOne&quot;) DataSource dsOne; @Autowired JpaProperties jpaProperties; @Bean @Primary LocalContainerEntityManagerFactoryBean localContainerEntityManagerFactoryBeanOne(EntityManagerFactoryBuilder builder) { return builder.dataSource(dsOne) .packages(&quot;org.sang.jpa.model&quot;) .properties(jpaProperties.getProperties()) .persistenceUnit(&quot;pu1&quot;) .build(); } @Bean PlatformTransactionManager platformTransactionManagerOne(EntityManagerFactoryBuilder builder) { LocalContainerEntityManagerFactoryBean factoryBeanOne = localContainerEntityManagerFactoryBeanOne(builder); return new JpaTransactionManager(factoryBeanOne.getObject()); }} 首先这里注入dsOne，再注入JpaProperties，JpaProperties是系统提供的一个实例，里边的数据就是我们在application.properties中配置的jpa相关的配置。然后我们提供两个Bean，分别是LocalContainerEntityManagerFactoryBean和PlatformTransactionManager事务管理器，不同于MyBatis和JdbcTemplate，在Jpa中，事务一定要配置。在提供LocalContainerEntityManagerFactoryBean的时候，需要指定packages，这里的packages指定的包就是这个数据源对应的实体类所在的位置，另外在这里配置类上通过@EnableJpaRepositories注解指定dao所在的位置，以及LocalContainerEntityManagerFactoryBean和PlatformTransactionManager分别对应的引用的名字。 好了，这样第一个就配置好了，第二个基本和这个类似，主要有几个不同点： dao的位置不同 persistenceUnit不同 相关bean的名称不同 注意实体类可以共用。 代码如下： 1234567891011121314151617181920212223242526@Configuration@EnableJpaRepositories(basePackages = \"org.sang.jpa.dao2\",entityManagerFactoryRef = \"localContainerEntityManagerFactoryBeanTwo\",transactionManagerRef = \"platformTransactionManagerTwo\")public class JpaConfigTwo { @Autowired @Qualifier(value = \"dsTwo\") DataSource dsTwo; @Autowired JpaProperties jpaProperties; @Bean LocalContainerEntityManagerFactoryBean localContainerEntityManagerFactoryBeanTwo(EntityManagerFactoryBuilder builder) { return builder.dataSource(dsTwo) .packages(\"org.sang.jpa.model\") .properties(jpaProperties.getProperties()) .persistenceUnit(\"pu2\") .build(); } @Bean PlatformTransactionManager platformTransactionManagerTwo(EntityManagerFactoryBuilder builder) { LocalContainerEntityManagerFactoryBean factoryBeanTwo = localContainerEntityManagerFactoryBeanTwo(builder); return new JpaTransactionManager(factoryBeanTwo.getObject()); }} 接下来，在对应位置分别提供相关的实体类和dao即可，数据源一的dao如下： 12345678package org.sang.jpa.dao;public interface UserDao extends JpaRepository&lt;User,Integer&gt; { List&lt;User&gt; getUserByAddressEqualsAndIdLessThanEqual(String address, Integer id); @Query(value = &quot;select * from t_user where id=(select max(id) from t_user)&quot;,nativeQuery = true) User maxIdUser();} 数据源二的dao如下： 12345678package org.sang.jpa.dao2;public interface UserDao2 extends JpaRepository&lt;User,Integer&gt; { List&lt;User&gt; getUserByAddressEqualsAndIdLessThanEqual(String address, Integer id); @Query(value = &quot;select * from t_user where id=(select max(id) from t_user)&quot;,nativeQuery = true) User maxIdUser();} 共同的实体类如下： 1234567891011package org.sang.jpa.model;@Entity(name = &quot;t_user&quot;)public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String username; private String address; //省略getter/setter} 到此，所有的配置就算完成了，接下来就可以在Service中注入不同的UserDao，不同的UserDao操作不同的数据源。 其实整合Jpa多数据源也不算难，就是有几个细节问题，这些细节问题解决，其实前面介绍的其他多数据源整个都差不多。 好了，欢迎大家加入松哥的星球，关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0407/springboot-jpa-multi.html"},{"title":"Spring 中用 XML 装配 Bean，竟然有五种姿势！","text":"Spring Boot 系列还在不断的更新，有小伙伴和松哥抱怨对 Spring 还不太懂，其实我 2016 年的时候写过一点点 Spring 的教程，但是不够详细，因此，最近决定再挖一个坑，和大家聊聊 Spring 的一些常见用法，也顺便聊聊源码。 Spring 和 Spring Boot 强相关，因此，相信这个系列不会烂尾。 本想写一些高大上的架构方面的，可是考虑到有很多读者是刚入行的状态，因此还是决定先把 Spring 也和大家过一遍,当然这些东西会穿插着来，尽量满足每一个小伙伴的需求。 今天就先来和大家聊一聊 Spring 中 XML 装配 Bean 的一些经常被人忽略的细节。 使用 XML 配置 Spring ，很多人都用过，可能有的小伙伴没认真总结过，今天我们就来稍微总结下，算是开启我们的 Spring 之旅。 基本配置XML 配置是最原始最古老的 Bean 的装配方案，曾经我们的项目离不开它，而如今，我们却在慢慢的抛弃它，没办法，时代在进步，我们也要进步呀。为了能看懂前辈们写的代码，我们还是有必要来看一下如何通过 XML 来装配 Bean。 首先我们来创建一个普通的 Maven 工程（不用创建成 web 工程），创建成功之后，引入 Spring 相关的依赖，这里只要引入 spring-context 即可，如下： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.5.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建成功之后，我们再来创建一个 Book 类，如下： 123456public class Book { private Integer id; private String name; private Double price; //省略 getter/setter} 然后再在 resources 目录下创建一个 beans.xml 文件，作为 Spring 的配置文件，然后在里边配置一个 Book bean，如下： 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"org.javaboy.spring.Book\" id=\"book\"/&gt;&lt;/beans&gt; 在这里，我们在 class 属性中配置类的全路径，id 则表示 bean 的名称，也可以通过 name 属性来指定 bean 的名称，大部分场景下两者无任何差别，会有一些特殊场景下（例如用,隔开多个实例名，两者的处理方案不同），两者有区别。 然后我们在 Java 代码中加载这个配置文件： 1ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(\"classpath:beans.xml\"); 题外话： ClassPathXmlApplicationContext 是配置文件众多的加载方式之一，表示从 classpath 下加载配置文件，这也是较常用的加载方式之一。其他常见的加载方式如下： 从这些不同的实现类中可以看到，我们也可以直接从文件系统中加载 Spring 的 XML 配置文件，使用 FileSystemXmlApplicationContext 类即可。 配置文件加载完成后，我们就可以从 Spring 容器中去获取这些 Bean 了，方式如下： 1Book book = (Book) ctx.getBean(\"book\"); 这个表示根据 id 获取相应的 Bean ，我们也可以通过类型来获取，方式如下： 1Book b1 = ctx.getBean(Book.class); 两种方式推荐第一种。 第二种通过类型获取 Bean 的方式存在一个问题，就是容器中同一个类如果存在多个实例，通过类型获取肯定会出错。 但是此时我们获取到的 Bean 中的属性全部为 null，没有值，这是因为我们在配置的时候没有给属性指定值。 在配置 Bean 时，给 Bean 指定相关的属性值，我们有几种不同的方式： 1.构造方法指定首先我们可以通过构造方法指定 bean 的属性值，前提是我们为 Book 类提供一个有参构造方法（大家在创建有参构造方法时，一定记得再顺手加一个无参构造方法）： 12345678910111213public class Book { private Integer id; private String name; private Double price; public Book() { } public Book(Integer id, String name, Double price) { this.id = id; this.name = name; this.price = price; } //省略 getter/setter} 然后在 XML 文件中，我们就可以通过构造方法注入相关值了： 12345&lt;bean class=\"org.javaboy.spring.Book\" id=\"book2\"&gt; &lt;constructor-arg name=\"id\" value=\"99\"/&gt; &lt;constructor-arg name=\"name\" value=\"三国演义\"/&gt; &lt;constructor-arg name=\"price\" value=\"99\"/&gt;&lt;/bean&gt; 使用构造方法注入相关值的时候，也可以使用下标来描述参数的顺序，注意如果使用下标，参数顺序不能错： 12345&lt;bean class=\"org.javaboy.spring.Book\" id=\"book3\"&gt; &lt;constructor-arg index=\"0\" value=\"99\"/&gt; &lt;constructor-arg index=\"1\" value=\"红楼梦\"/&gt; &lt;constructor-arg index=\"2\" value=\"100\"/&gt;&lt;/bean&gt; 注入成功之后，当我们再次去获取 Bean 的时候，就可以看到这些属性了。 2.通过属性注入当然也可以通过属性注入，这是一种更为常见的方式： 12345&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt;&lt;/bean&gt; 3.p名称空间注入p 名称空间本质上还是通过属性注入的，只不过写法有些差异，p 名称空间注入方式如下： 1&lt;bean class=\"org.javaboy.spring.Book\" id=\"book5\" p:id=\"100\" p:name=\"西游记\" p:price=\"40\"/&gt; 以上三种不同的属性注入方式，我给大家演示的都是注入基本数据类型，如果注入的是一个对象的话，只需要通过 ref 属性来指明对象的引用即可。 特殊属性注入除了这些基本属性之外，还有一些特殊属性，例如集合、数组、map 等。我们分别来看。 集合/数组集合/数组的注入方式基本一致，首先我们给项目添加一个集合属性，如下： 1234567public class Book { private Integer id; private String name; private Double price; private List&lt;String&gt; authors; //省略 getter/setter} 属性注入时，可以通过 array 节点注入值： 123456789101112&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt; &lt;property name=\"authors\"&gt; &lt;array&gt; &lt;value&gt;zhangsan&lt;/value&gt; &lt;value&gt;lisi&lt;/value&gt; &lt;value&gt;javaboy&lt;/value&gt; &lt;/array&gt; &lt;/property&gt;&lt;/bean&gt; 也可以通过 list 节点注入值： 123456789101112&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt; &lt;property name=\"authors\"&gt; &lt;list&gt; &lt;value&gt;zhangsan&lt;/value&gt; &lt;value&gt;lisi&lt;/value&gt; &lt;value&gt;javaboy&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 还有一个可能大家使用比较少的方式，就是通过 utils:list 来创建集合属性，然后配置到 Book 属性中去，即可： 12345678910&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt; &lt;property name=\"authors\" ref=\"authors\" /&gt;&lt;/bean&gt;&lt;utils:list id=\"authors\"&gt; &lt;value&gt;javaboy&lt;/value&gt; &lt;value&gt;zhangsan&lt;/value&gt;&lt;/utils:list&gt; 这种方式比较少见。 mapmap 的注入也有几种不同的方式，可以通过属性指定，也可以通过 utils 来搞定，先来看第一种： 12345678public class Book { private Integer id; private String name; private Double price; private List&lt;String&gt; authors; private Map&lt;String, Object&gt; info; //省略 getter/setter} 在 xml 文件中通过如下方式指定属性值： 123456789101112&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt; &lt;property name=\"authors\" ref=\"authors\" /&gt; &lt;property name=\"info\"&gt; &lt;map&gt; &lt;entry key=\"name\" value=\"zhangsan\"/&gt; &lt;entry key=\"age\" value=\"99\"/&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; 也可以通过 utils 来指定 map 的值，如下： 1234567891011&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt; &lt;property name=\"authors\" ref=\"authors\" /&gt; &lt;property name=\"info\" ref=\"info\"/&gt;&lt;/bean&gt;&lt;utils:map id=\"info\"&gt; &lt;entry key=\"name\" value=\"lisi\"/&gt; &lt;entry key=\"age\" value=\"98\"/&gt;&lt;/utils:map&gt; propertiesproperties 属性也是一样的配置方案。既可以通过 props 属性指定，也可以通过 utils 来指定，例如： 123456789public class Book { private Integer id; private String name; private Double price; private List&lt;String&gt; authors; private Map&lt;String, Object&gt; info; private Properties info2; //省略 getter/setter} 通过 props 属性指定方式如下： 12345678910111213&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt; &lt;property name=\"authors\" ref=\"authors\" /&gt; &lt;property name=\"info\" ref=\"info\"/&gt; &lt;property name=\"info2\"&gt; &lt;props&gt; &lt;prop key=\"name\"&gt;zhangsan&lt;/prop&gt; &lt;prop key=\"age\"&gt;99&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 通过 utils 指定方式如下： 123456789101112&lt;bean class=\"org.javaboy.spring.Book\" id=\"book4\"&gt; &lt;property name=\"id\" value=\"99\"/&gt; &lt;property name=\"name\" value=\"水浒传\"/&gt; &lt;property name=\"price\" value=\"99\"/&gt; &lt;property name=\"authors\" ref=\"authors\"/&gt; &lt;property name=\"info\" ref=\"info\"/&gt; &lt;property name=\"info2\" ref=\"info2\"/&gt;&lt;/bean&gt;&lt;utils:properties id=\"info2\"&gt; &lt;prop key=\"name\"&gt;zhangsan&lt;/prop&gt; &lt;prop key=\"age\"&gt;99&lt;/prop&gt;&lt;/utils:properties&gt; 除了这几种装配方式之外，我们也可以通过工厂方法装配。 工厂方法装配工厂方法装配可以分为静态工厂和实例工厂两种方式，我们分别来看。 静态工厂静态工厂方法装配需要我们先创建一个静态工厂方法，像下面这样： 12345public class BookFactory { public static Book getInstance() { return new Book(); }} 然后在 XML 文件中装配： 1&lt;bean class=\"org.javaboy.spring.BookFactory\" id=\"book6\" factory-method=\"getInstance\"/&gt; 此时我们去容器中获取 book6 这个实例，拿到的就是你在静态工厂中返回的 Book 实例。 实例工厂实例工厂方法则是指工厂方法是一个普通方法，不是静态的，像下面这样： 12345public class BookFactory2 { public Book getInstance() { return new Book(); }} 然后在 XML 文件中，我们需要首先配置 BookFactory2 的实例，然后才能调用实例中的方法获取 Book 对象，如下： 12&lt;bean class=\"org.javaboy.spring.BookFactory2\" id=\"bookFactory2\"&gt;&lt;/bean&gt;&lt;bean class=\"org.javaboy.spring.Book\" id=\"book7\" factory-bean=\"bookFactory2\" factory-method=\"getInstance\"/&gt; 工厂方法装配的价值在哪里呢？ 例如 Druid 中的 DataSource 对象，通过 DruidDataSourceBuilder.create().build() 方法来构建，如果我们想在 XML 中做这个配置，显然不太容易，此时就可以使用工厂方法装配了。 好了，这就是 XML 装配 Bean 的一个简单介绍，比较简单，但是为了知识的完整性，我还是稍微写了下，能看到这里的都是真爱啊！","link":"/2019/0801/spring-xml.html"},{"title":"Spring Security 登录使用 JSON 格式数据","text":"在使用 SpringSecurity 中，大伙都知道默认的登录数据是通过 key/value 的形式来传递的，默认情况下不支持 JSON格式的登录数据，如果有这种需求，就需要自己来解决，本文主要和小伙伴来聊聊这个话题。 基本登录方案在说如何使用 JSON 登录之前，我们还是先来看看基本的登录吧，本文为了简单，SpringSecurity 在使用中就不连接数据库了，直接在内存中配置用户名和密码，具体操作步骤如下： 创建 Spring Boot 工程 首先创建 SpringBoot 工程，添加 SpringSecurity 依赖，如下： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 添加 Security 配置 创建 SecurityConfig，完成 SpringSecurity 的配置，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Bean PasswordEncoder passwordEncoder() { return new BCryptPasswordEncoder(); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication().withUser(\"zhangsan\").password(\"$2a$10$2O4EwLrrFPEboTfDOtC0F.RpUMk.3q3KvBHRx7XXKUMLBGjOOBs8q\").roles(\"user\"); } @Override public void configure(WebSecurity web) throws Exception { } @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .anyRequest().authenticated() .and() .formLogin() .loginProcessingUrl(\"/doLogin\") .successHandler(new AuthenticationSuccessHandler() { @Override public void onAuthenticationSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { RespBean ok = RespBean.ok(\"登录成功！\",authentication.getPrincipal()); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(ok)); out.flush(); out.close(); } }) .failureHandler(new AuthenticationFailureHandler() { @Override public void onAuthenticationFailure(HttpServletRequest req, HttpServletResponse resp, AuthenticationException e) throws IOException, ServletException { RespBean error = RespBean.error(\"登录失败\"); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(error)); out.flush(); out.close(); } }) .loginPage(\"/login\") .permitAll() .and() .logout() .logoutUrl(\"/logout\") .logoutSuccessHandler(new LogoutSuccessHandler() { @Override public void onLogoutSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { RespBean ok = RespBean.ok(\"注销成功！\"); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(ok)); out.flush(); out.close(); } }) .permitAll() .and() .csrf() .disable() .exceptionHandling() .accessDeniedHandler(new AccessDeniedHandler() { @Override public void handle(HttpServletRequest req, HttpServletResponse resp, AccessDeniedException e) throws IOException, ServletException { RespBean error = RespBean.error(\"权限不足，访问失败\"); resp.setStatus(403); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(error)); out.flush(); out.close(); } }); }} 这里的配置虽然有点长，但是很基础，配置含义也比较清晰，首先提供 BCryptPasswordEncoder 作为 PasswordEncoder ，可以实现对密码的自动加密加盐，非常方便，然后提供了一个名为 zhangsan 的用户，密码是 123 ，角色是 user ，最后配置登录逻辑，所有的请求都需要登录后才能访问，登录接口是 /doLogin ，用户名的 key 是 username ，密码的 key 是 password ，同时配置登录成功、登录失败以及注销成功、权限不足时都给用户返回JSON提示，另外，这里虽然配置了登录页面为 /login ，实际上这不是一个页面，而是一段 JSON ，在 LoginController 中提供该接口，如下： 123456789101112@RestController@ResponseBodypublic class LoginController { @GetMapping(\"/login\") public RespBean login() { return RespBean.error(\"尚未登录，请登录\"); } @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} 这里 /login 只是一个 JSON 提示，而不是页面， /hello 则是一个测试接口。 OK，做完上述步骤就可以开始测试了，运行SpringBoot项目，访问 /hello 接口，结果如下： 此时先调用登录接口进行登录，如下： 登录成功后，再去访问 /hello 接口就可以成功访问了。 使用JSON登录上面演示的是一种原始的登录方案，如果想将用户名密码通过 JSON 的方式进行传递，则需要自定义相关过滤器，通过分析源码我们发现，默认的用户名密码提取在 UsernamePasswordAuthenticationFilter 过滤器中，部分源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class UsernamePasswordAuthenticationFilter extends AbstractAuthenticationProcessingFilter { public static final String SPRING_SECURITY_FORM_USERNAME_KEY = \"username\"; public static final String SPRING_SECURITY_FORM_PASSWORD_KEY = \"password\"; private String usernameParameter = SPRING_SECURITY_FORM_USERNAME_KEY; private String passwordParameter = SPRING_SECURITY_FORM_PASSWORD_KEY; private boolean postOnly = true; public UsernamePasswordAuthenticationFilter() { super(new AntPathRequestMatcher(\"/login\", \"POST\")); } public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException { if (postOnly &amp;&amp; !request.getMethod().equals(\"POST\")) { throw new AuthenticationServiceException( \"Authentication method not supported: \" + request.getMethod()); } String username = obtainUsername(request); String password = obtainPassword(request); if (username == null) { username = \"\"; } if (password == null) { password = \"\"; } username = username.trim(); UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken( username, password); // Allow subclasses to set the \"details\" property setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); } protected String obtainPassword(HttpServletRequest request) { return request.getParameter(passwordParameter); } protected String obtainUsername(HttpServletRequest request) { return request.getParameter(usernameParameter); } //... //...} 从这里可以看到，默认的用户名/密码提取就是通过 request 中的 getParameter 来提取的，如果想使用 JSON 传递用户名密码，只需要将这个过滤器替换掉即可，自定义过滤器如下： 12345678910111213141516171819202122232425public class CustomAuthenticationFilter extends UsernamePasswordAuthenticationFilter { @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException { if (request.getContentType().equals(MediaType.APPLICATION_JSON_UTF8_VALUE) || request.getContentType().equals(MediaType.APPLICATION_JSON_VALUE)) { ObjectMapper mapper = new ObjectMapper(); UsernamePasswordAuthenticationToken authRequest = null; try (InputStream is = request.getInputStream()) { Map&lt;String,String&gt; authenticationBean = mapper.readValue(is, Map.class); authRequest = new UsernamePasswordAuthenticationToken( authenticationBean.get(\"username\"), authenticationBean.get(\"password\")); } catch (IOException e) { e.printStackTrace(); authRequest = new UsernamePasswordAuthenticationToken( \"\", \"\"); } finally { setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); } } else { return super.attemptAuthentication(request, response); } }} 这里只是将用户名/密码的获取方案重新修正下，改为了从 JSON 中获取用户名密码，然后在 SecurityConfig 中作出如下修改： 123456789101112131415161718192021222324252627282930313233343536@Overrideprotected void configure(HttpSecurity http) throws Exception { http.authorizeRequests().anyRequest().authenticated() .and() .formLogin() .and().csrf().disable(); http.addFilterAt(customAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class);}@BeanCustomAuthenticationFilter customAuthenticationFilter() throws Exception { CustomAuthenticationFilter filter = new CustomAuthenticationFilter(); filter.setAuthenticationSuccessHandler(new AuthenticationSuccessHandler() { @Override public void onAuthenticationSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); RespBean respBean = RespBean.ok(\"登录成功!\"); out.write(new ObjectMapper().writeValueAsString(respBean)); out.flush(); out.close(); } }); filter.setAuthenticationFailureHandler(new AuthenticationFailureHandler() { @Override public void onAuthenticationFailure(HttpServletRequest req, HttpServletResponse resp, AuthenticationException e) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); RespBean respBean = RespBean.error(\"登录失败!\"); out.write(new ObjectMapper().writeValueAsString(respBean)); out.flush(); out.close(); } }); filter.setAuthenticationManager(authenticationManagerBean()); return filter;} 将自定义的 CustomAuthenticationFilter 类加入进来即可，接下来就可以使用 JSON 进行登录了，如下： 好了，本文就先介绍到这里，有问题欢迎留言讨论。","link":"/2019/0613/springsecurity-json.html"},{"title":"MongoDB 中各种类型的索引","text":"上篇文章中我们介绍了 MongoDB 中索引的简单操作，创建、查看、删除等基本操作，不过上文我们只介绍了一种类型的索引，本文我们来看看其他类型的索引。 _id 索引我们在上文介绍过，我们往集合中添加文档时，默认情况下 MongoDB 都会帮助我们创建一个名为 _id 的字段，这个字段就是一个索引。默认情况下，一般的集合都会帮我们创建这个字段作为索引，但也有一些集合不会将 _id 默认作为索引，比如固定集合，这个我们后面的文章会详细说到这个问题。 复合索引如果我们的查询条件有多个的话，我们可以对这多个查询条件都建立索引，比如我们可以对文档中的 x 和 y 字段都建立索引，如下： 1db.sang_collect.ensureIndex({x:1,y:-1}) 此时执行如下查询语句时就会用到这个复合索引： 1db.sang_collect.find({x:1,y:999}) 小伙伴们也可以通过查看查询计划来确定确实使用到了上文创建好的索引。 过期索引顾名思义，过期索引就是一种会过期的索引，在索引过期之后，索引对应的数据会被删除，创建方式如下： 1db.sang_collect.ensureIndex({time:1},{expireAfterSeconds:30}) expireAfterSeconds 表示索引的过期时间，单位为秒。time 表示索引的字段，time 的数据类型必须是 ISODate 或者 ISODate 数组，否则的话，当索引过期之后，time 的数据就不会被删除。 全文索引全文索引虽然好用，可惜不支持中文，我们这里就先做一个简单的了解。 比如，我的数据集如下： 1234567891011121314151617181920{ \"_id\" : ObjectId(\"59f5a3da1f9e8e181ffc3189\"), \"x\" : \"Java C# Python PHP\"}{ \"_id\" : ObjectId(\"59f5a3da1f9e8e181ffc318a\"), \"x\" : \"Java C#\"}{ \"_id\" : ObjectId(\"59f5a3da1f9e8e181ffc318b\"), \"x\" : \"Java Python\"}{ \"_id\" : ObjectId(\"59f5a3da1f9e8e181ffc318c\"), \"x\" : \"PHP Python\"}{ \"_id\" : ObjectId(\"59f5a4541f9e8e181ffc318d\"), \"x\" : \"C C++\"} 我们可以给 x 字段建立一个全文索引，创建方式如下： 1db.sang_collect.ensureIndex({x:&quot;text&quot;}) MongoDB 会自动对 x 字段的数据进行分词，然后我们就可以通过如下语句进行查询： 1db.sang_collect.find({$text:{$search:&quot;Java&quot;}}) 此时 x 中包含 Java 的文档都会被查询出来。如果想查询既包含 Java 又包含 C# 的文档，操作如下： 1db.sang_collect.find({$text:{$search:&quot;\\&quot;Java C#\\&quot;&quot;}}) 用一对双引号将查询条件括起来，如果想查询包含 PHP 或者 Python 的文档，操作如下： 1db.sang_collect.find({$text:{$search:&quot;PHP Python&quot;}}) 如果想查询既有 PHP，又有 Python，但是又不包括 Java 的文档，如下： 1db.sang_collect.find({$text:{$search:&quot;PHP Python -Java&quot;}}) 建立了全文索引之后，我们也可以查看查询结果的相似度，使用 $meta，如下： 1db.sang_collect.find({$text:{$search:&quot;PHP Python&quot;}},{score:{$meta:&quot;textScore&quot;}}) 此时查询结果中会多出一个 score 字段，该字段的值越大，表示相似度越高，我们可以根据 score 利用 sort 来对其进行排序，如下： 1db.sang_collect.find({$text:{$search:&quot;PHP Python&quot;}},{score:{$meta:&quot;textScore&quot;}}).sort({score:{$meta:&quot;textScore&quot;}}) 全文索引目前看起来功能还是很强大，可惜暂时不支持中文，不过网上对此也有很多解决方案，小伙伴们可以自行搜索查看。 地理空间索引地理空间索引类型地理空间索引可以分为两类： 2d 索引，可以用来存储和查找平面上的点。 2d sphere 索引，可以用来存储和查找球面上的点。 2d索引2d 索引一般我们可以用在游戏地图中。向集合中插入一条记录点的数据： 1db.sang_collect.insert({x:[90,0]}) 插入数据的格式为[经度,纬度]，取值范围，经度 [-180,180]，纬度 [-90,90]。数据插入成功之后，我们先通过如下命令创建索引： 1db.sang_collect.ensureIndex({x:&quot;2d&quot;}) 然后通过 $near 我们可以查询某一个点附近的点，如下: 1db.sang_collect.find({x:{$near:[90,0]}}) 默认情况下返回该点附近 100 个点，我们可以通过 $maxDistance 来设置返回的最远距离： 1db.sang_collect.find({x:{$near:[90,0],$maxDistance:99}}) 我们也可以通过 $geoWithin 查询某个形状内的点，比如查询矩形中的点： 1db.sang_collect.find({x:{$geoWithin:{$box:[[0,0],[91,1]]}}}) 两个坐标点用来确定矩形的位置。 查询圆中的点： 1db.sang_collect.find({x:{$geoWithin:{$center:[[0,0],90]}}}) 参数分别表示圆的圆心和半径。 查询多边形中的点： 1db.sang_collect.find({x:{$geoWithin:{$polygon:[[0,0],[100,0],[100,1],[0,1]]}}}) 这里可以填入任意多个点，表示多边形中的各个点。 2d sphere索引2dsphere 适用于球面类型的地图，它的数据类型是 GeoJSON 格式的，我们可以在 http://geojson.org/ 地址上查看 GeoJSON 格式的样式，比如我们描述一个点， GeoJSON 如下： 1234567891011{ \"_id\" : ObjectId(\"59f5e0571f9e8e181ffc3196\"), \"name\" : \"shenzhen\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ 90.0, 0.0 ] }} 描述线，GeoJSON 格式如下： 123456789101112131415161718192021{ \"_id\" : ObjectId(\"59f5e0d01f9e8e181ffc3199\"), \"name\" : \"shenzhen\", \"location\" : { \"type\" : \"LineString\", \"coordinates\" : [ [ 90.0, 0.0 ], [ 90.0, 1.0 ], [ 90.0, 2.0 ] ] }} 描述多边形，GeoJSON 格式如下： 123456789101112131415161718192021222324252627{ \"_id\" : ObjectId(\"59f5e3f91f9e8e181ffc31d0\"), \"name\" : \"beijing\", \"location\" : { \"type\" : \"Polygon\", \"coordinates\" : [ [ [ 0.0, 1.0 ], [ 0.0, 2.0 ], [ 1.0, 2.0 ], [ 0.0, 1.0 ] ] ] }} 还有其他的类型，具体小伙伴们可以参考 http://geojson.org/ 。有了数据之后，我们可以通过如下操作来创建地理空间索引了： 1db.sang_collect.ensureIndex({location:&quot;2dsphere&quot;}) 比如我想查询和深圳这个区域有交集的文档，如下： 12var shenzhen = db.sang_collect.findOne({name:&quot;shenzhen&quot;})db.sang_collect.find({location:{$geoIntersects:{$geometry:shenzhen.location}}}) 这里的查询结果是和深圳这个区域有交集的都会查到(比如经过深圳的高速公路、铁路等)，我们也可以只查询深圳市内的区域(比如深圳市内所有的学校)，如下： 12var shenzhen = db.sang_collect.findOne({name:&quot;shenzhen&quot;})db.sang_collect.find({location:{$within:{$geometry:shenzhen.location}}}) 也可以查询腾讯附近的其他位置，如下： 12var QQ = db.sang_collect.findOne({name:&quot;QQ&quot;})db.sang_collect.find({location:{$near:{$geometry:QQ.location}}}) 复合地理空间索引位置往往只是我们查询的一个条件，比如我要查询深圳市内所有的学校，那我得再增加一个查询条件，如下： 12var shenzhen = db.sang_collect.findOne({name:&quot;shenzhen&quot;})db.sang_collect.find({location:{$within:{$geometry:shenzhen.location}},name:&quot;QQ&quot;}) 其他的查询条件跟在后面就行了。 好了，MongoDB 中的索引问题我们就说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0910/mongodb-index-types.html"},{"title":"MongoDB 中 MapReduce 使用","text":"玩过 Hadoop 的小伙伴对 MapReduce 应该不陌生，MapReduce 的强大且灵活，它可以将一个大问题拆分为多个小问题，将各个小问题发送到不同的机器上去处理，所有的机器都完成计算后，再将计算结果合并为一个完整的解决方案，这就是所谓的分布式计算。本文我们就来看看 MongoDB 中 MapReduce 的使用。 mapReduceMongoDB 中的 MapReduce 可以用来实现更复杂的聚合命令，使用 MapReduce 主要实现两个函数：map 函数和 reduce 函数，map 函数用来生成键值对序列， map 函数的结果作为 reduce 函数的参数，reduce 函数中再做进一步的统计，比如我的数据集如下： 12345{\"_id\" : ObjectId(\"59fa71d71fd59c3b2cd908d7\"),\"name\" : \"鲁迅\",\"book\" : \"呐喊\",\"price\" : 38.0,\"publisher\" : \"人民文学出版社\"}{\"_id\" : ObjectId(\"59fa71d71fd59c3b2cd908d8\"),\"name\" : \"曹雪芹\",\"book\" : \"红楼梦\",\"price\" : 22.0,\"publisher\" : \"人民文学出版社\"}{\"_id\" : ObjectId(\"59fa71d71fd59c3b2cd908d9\"),\"name\" : \"钱钟书\",\"book\" : \"宋诗选注\",\"price\" : 99.0,\"publisher\" : \"人民文学出版社\"}{\"_id\" : ObjectId(\"59fa71d71fd59c3b2cd908da\"),\"name\" : \"钱钟书\",\"book\" : \"谈艺录\",\"price\" : 66.0,\"publisher\" : \"三联书店\"}{\"_id\" : ObjectId(\"59fa71d71fd59c3b2cd908db\"),\"name\" : \"鲁迅\",\"book\" : \"彷徨\",\"price\" : 55.0,\"publisher\" : \"花城出版社\"} 假如我想查询每位作者所出的书的总价，操作如下： 12345var map=function(){emit(this.name,this.price)}var reduce=function(key,value){return Array.sum(value)}var options={out:&quot;totalPrice&quot;}db.sang_books.mapReduce(map,reduce,options);db.totalPrice.find() emit 函数主要用来实现分组，接收两个参数，第一个参数表示分组的字段，第二个参数表示要统计的数据，reduce 来做具体的数据处理操作，接收两个参数，对应 emit 方法的两个参数，这里使用了 Array 中的 sum 函数对 price 字段进行自加处理，options 中定义了将结果输出的集合，届时我们将在这个集合中去查询数据，默认情况下，这个集合即使在数据库重启后也会保留，并且保留集合中的数据。查询结果如下： 123456789101112{ \"_id\" : \"曹雪芹\", \"value\" : 22.0}{ \"_id\" : \"钱钟书\", \"value\" : 165.0}{ \"_id\" : \"鲁迅\", \"value\" : 93.0} 再比如我想查询每位作者出了几本书，如下： 12345var map=function(){emit(this.name,1)}var reduce=function(key,value){return Array.sum(value)}var options={out:&quot;bookNum&quot;}db.sang_books.mapReduce(map,reduce,options);db.bookNum.find() 查询结果如下： 123456789101112{ \"_id\" : \"曹雪芹\", \"value\" : 1.0}{ \"_id\" : \"钱钟书\", \"value\" : 2.0}{ \"_id\" : \"鲁迅\", \"value\" : 2.0} 将每位作者的书列出来，如下： 12345var map=function(){emit(this.name,this.book)}var reduce=function(key,value){return value.join(&apos;,&apos;)}var options={out:&quot;books&quot;}db.sang_books.mapReduce(map,reduce,options);db.books.find() 结果如下： 123456789101112{ \"_id\" : \"曹雪芹\", \"value\" : \"红楼梦\"}{ \"_id\" : \"钱钟书\", \"value\" : \"宋诗选注,谈艺录\"}{ \"_id\" : \"鲁迅\", \"value\" : \"呐喊,彷徨\"} 比如查询每个人售价在 ￥40 以上的书： 12345var map=function(){emit(this.name,this.book)}var reduce=function(key,value){return value.join(&apos;,&apos;)}var options={query:{price:{$gt:40}},out:&quot;books&quot;}db.sang_books.mapReduce(map,reduce,options);db.books.find() query 表示对查到的集合再进行筛选。 结果如下： 12345678{ \"_id\" : \"钱钟书\", \"value\" : \"宋诗选注,谈艺录\"}{ \"_id\" : \"鲁迅\", \"value\" : \"彷徨\"} runCommand实现我们也可以利用 runCommand 命令来执行 MapReduce。格式如下： 1234567891011121314151617db.runCommand( { mapReduce: &lt;collection&gt;, map: &lt;function&gt;, reduce: &lt;function&gt;, finalize: &lt;function&gt;, out: &lt;output&gt;, query: &lt;document&gt;, sort: &lt;document&gt;, limit: &lt;number&gt;, scope: &lt;document&gt;, jsMode: &lt;boolean&gt;, verbose: &lt;boolean&gt;, bypassDocumentValidation: &lt;boolean&gt;, collation: &lt;document&gt; } ) 含义如下： 参数 含义 mapReduce 表示要操作的集合 map map函数 reduce reduce函数 finalize 最终处理函数 out 输出的集合 query 对结果进行过滤 sort 对结果排序 limit 返回的结果数 scope 设置参数值，在这里设置的值在map、reduce、finalize函数中可见 jsMode 是否将map执行的中间数据由javascript对象转换成BSON对象，默认为false verbose 是否显示详细的时间统计信息 bypassDocumentValidation 是否绕过文档验证 collation 其他一些校对 如下操作，表示执行 MapReduce 操作并对统计的集合限制返回条数，限制返回条数之后再进行统计操作，如下： 1234var map=function(){emit(this.name,this.book)}var reduce=function(key,value){return value.join(&apos;,&apos;)}db.runCommand({mapreduce:&apos;sang_books&apos;,map,reduce,out:&quot;books&quot;,limit:4,verbose:true})db.books.find() 执行结果如下： 123456789101112{ \"_id\" : \"曹雪芹\", \"value\" : \"红楼梦\"}{ \"_id\" : \"钱钟书\", \"value\" : \"宋诗选注,谈艺录\"}{ \"_id\" : \"鲁迅\", \"value\" : \"呐喊\"} 小伙伴们看到，鲁迅有一本书不见了，就是因为 limit 是先限制集合返回条数，然后再执行统计操作。 finalize 操作表示最终处理函数，如下： 12345var f1 = function(key,reduceValue){var obj={};obj.author=key;obj.books=reduceValue; return obj}var map=function(){emit(this.name,this.book)}var reduce=function(key,value){return value.join(&apos;,&apos;)}db.runCommand({mapreduce:&apos;sang_books&apos;,map,reduce,out:&quot;books&quot;,finalize:f1})db.books.find() f1 第一个参数 key 表示 emit 中的第一个参数，第二个参数表示 reduce 的执行结果，我们可以在 f1 中对这个结果进行再处理，结果如下： 123456789101112131415161718192021{ \"_id\" : \"曹雪芹\", \"value\" : { \"author\" : \"曹雪芹\", \"books\" : \"红楼梦\" }}{ \"_id\" : \"钱钟书\", \"value\" : { \"author\" : \"钱钟书\", \"books\" : \"宋诗选注,谈艺录\" }}{ \"_id\" : \"鲁迅\", \"value\" : { \"author\" : \"鲁迅\", \"books\" : \"呐喊,彷徨\" }} scope 则可以用来定义一个在 map、reduce 和 finalize 中都可见的变量，如下： 12345var f1 = function(key,reduceValue){var obj={};obj.author=key;obj.books=reduceValue;obj.sang=sang; return obj}var map=function(){emit(this.name,this.book)}var reduce=function(key,value){return value.join(&apos;,--&apos;+sang+&apos;--,&apos;)}db.runCommand({mapreduce:&apos;sang_books&apos;,map,reduce,out:&quot;books&quot;,finalize:f1,scope:{sang:&quot;haha&quot;}})db.books.find() 执行结果如下： 123456789101112131415161718192021222324{ \"_id\" : \"曹雪芹\", \"value\" : { \"author\" : \"曹雪芹\", \"books\" : \"红楼梦\", \"sang\" : \"haha\" }}{ \"_id\" : \"钱钟书\", \"value\" : { \"author\" : \"钱钟书\", \"books\" : \"宋诗选注,--haha--,谈艺录\", \"sang\" : \"haha\" }}{ \"_id\" : \"鲁迅\", \"value\" : { \"author\" : \"鲁迅\", \"books\" : \"呐喊,--haha--,彷徨\", \"sang\" : \"haha\" }} 好了，MongoDB 中的 MapReduce 我们就先说到这里，小伙伴们有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》 mongodb mapreduce小试 mongoDB–mapreduce用法详解(未找到原始出处)","link":"/2019/0914/mongodb-in-mapreduce.html"},{"title":"初识 MongoDB 分片","text":"分片是指将数据拆分，拆分后存放在不同的机器上的过程，以此来降低单个服务器的压力，同时也解决单个服务器硬盘空间不足的问题，让我们可以用廉价的机器实现高性能的数据架构。 有的小伙伴不理解分片和副本集的差异，一言以蔽之：副本集上每个备份节点存储的数据都是相同的，分片上存储的数据则是不同的。好了，本文我们就先来看看分片环境的搭建。 环境准备准备三台已经装好了 MongoDB 的服务器，地址分别是： 123192.168.248.128192.168.248.135192.168.248.136 本文使用的 MongoDB 版本为 3.4.9 规划首先我们需要三台配置服务器，配置服务器相当于集群的大脑，配置服务器中保存着集群和分片的元数据，即每个分片都包含了哪些数据信息，这些数据都是保存在配置服务器中的，我这里将开启三个配置服务器实例，这三个配置服务器将运行在三个 MongoDB 服务器上，地址分别如下： 123192.168.248.128:20000192.168.248.135:20000192.168.248.136:20000 接下来需要一个 Mongos 实例，Mongos 对请求进行路由，Mongos 扮演的角色有点类似于一个门面，我们以后访问的时候，直接访问 Mongos 即可，再由 Mongos 将请求路由到不同的分片上去，Mongos 在启动时会去访问配置服务器，它将从配置服务器中获取数据的存储信息，Mongos 我将启动在如下服务器上： 1192.168.248.128:30000 最后需要三个分片实例，三个分片依然运行在三台服务器上，如下： 123192.168.248.128:27017192.168.248.135:27017192.168.248.136:27017 搭建配置服务器配置服务器中不需要太多的空间和资源，因为配置服务器上保存的只是数据的分布表，不保存具体的数据，具体的数据都保存在分片上，配置服务器中 1KB 的空间约为 200MB 的真实数据。注意，从 MongoDB3.4 开始，配置服务器要做成集群的方式。 由于配置服务器是独立的 mongod 进程，所以我们可以像启动普通的 MongoDB 服务一样启动配置服务器，只是这里的配置不同罢了。 我这里以 192.168.248.128 服务器为例来讲述配置服务器的配置启动，另外两台服务器如法炮制即可： 在 mongodb 解压目录下创建 db20000 文件夹，用来存储配置服务器中的数据。 复制一份 mongodb.conf，命名为 mongodb20000.conf，修改文件内容如下： 123456dbpath=/opt/mongodb/db20000logpath=/opt/mongodb/logs/mongodb20000.logport=20000fork=trueconfigsvr=truereplSet=rs 注意 dbpath 改为我们第一步创建的目录，端口号改为 20000 （这个随意，只要该端口没被占用即可）， configsvr 表示这是一个配置服务器，另外由于我们的配置服务器要做成备份集，所以要设置 replSet。 3.做好前两步之后，执行如下命令启动配置服务器： 1mongod -f /opt/mongodb/bin/mongodb20000.conf 最后，在另外两台服务器上重复上面三个步骤。 三台服务器上都启动成功之后，参考我们之前的MongoDB 副本集搭建一文，将这三台配置服务器配成一个副本集，副本集的配置我这里就不再赘述。 搭建MongosMongos 实例我们可以启动在任意一台服务器上，我这里就启动在 192.168.248.128上，Mongos 的配置步骤如下： 1.复制一份 mongodb.conf，命名为 mongos.conf，修改内容： 1234logpath=/opt/mongodb/logs/mongos.logport=30000fork=trueconfigdb=rs/192.168.248.128:20000,192.168.248.135:20000,192.168.248.136:20000 因为 mongos 中不需要保存数据，所以不需要 dbpath，端口号改为 30000，configdb 表示三个配置服务器的地址，注意最前面的 rs 表示配置服务器副本集的名称。 2.配置完成后，执行如下命令启动 mongos: 1mongos -f /opt/mongodb/bin/mongos.conf 搭建三个分片三个分片实际上就是三个普通的 MongoDB 服务器，给大家看下我的配置文件： 12345dbpath=/opt/mongodb/dblogpath=/opt/mongodb/logs/mongodb.logport=27017fork=trueshardsvr=true 注意多了个 shardsvr 表示这是一个分片服务器。然后在三台服务器上分别执行如下命令启动分片： 1mongod -f /opt/mongodb/bin/mongodb.conf 添加分片上面三个步骤完成之后，我们就进入到 mongos 的 shell 命令行了，如下： 1mongo --port=30000 然后我们可以通过如下命令查看一下分片的当前状态： 1sh.status() 执行结果如下(部分)： 12345--- Sharding Status --- sharding version: {} shards: databases: shards 表示分片服务器，目前还没有，databases 表示分片的库，目前也还没有，接下来我们通过如下命令添加分片服务器： 123sh.addShard(&quot;192.168.248.128:27017&quot;)sh.addShard(&quot;192.168.248.135:27017&quot;)sh.addShard(&quot;192.168.248.136:27017&quot;) 添加三个分片服务器，然后再执行 sh.status(),结果如下(部分)： 123456789--- Sharding Status --- sharding version: {} shards: { &quot;_id&quot; : &quot;shard0000&quot;, &quot;host&quot; : &quot;192.168.248.128:27017&quot;, &quot;state&quot; : 1 } { &quot;_id&quot; : &quot;shard0001&quot;, &quot;host&quot; : &quot;192.168.248.135:27017&quot;, &quot;state&quot; : 1 } { &quot;_id&quot; : &quot;shard0002&quot;, &quot;host&quot; : &quot;192.168.248.136:27017&quot;, &quot;state&quot; : 1 } databases: { &quot;_id&quot; : &quot;test&quot;, &quot;primary&quot; : &quot;shard0000&quot;, &quot;partitioned&quot; : false } 设置集合分片接下来我们来设置集合的分片，首先执行如下命令表示给某个数据库分片： 1sh.enableSharding(&quot;sang&quot;) 对集合分片时，需要选择一个片键，片键实际上就是集合中的一个键，MongoDB 将根据这个片键来拆分数据，我们需要先对片键建立索引，如下： 1db.c1.ensureIndex({x:1}) 然后以 x 为片键，对 c1 集合进行分片，如下： 1sh.shardCollection(&quot;sang.c1&quot;,{x:1}) 做完这些之后，再执行 sh.status() 命令，查看目前状态，结果如下(部分)： 1234567891011121314151617--- Sharding Status --- sharding version: {} shards: { &quot;_id&quot; : &quot;shard0000&quot;, &quot;host&quot; : &quot;192.168.248.128:27017&quot;, &quot;state&quot; : 1 } { &quot;_id&quot; : &quot;shard0001&quot;, &quot;host&quot; : &quot;192.168.248.135:27017&quot;, &quot;state&quot; : 1 } { &quot;_id&quot; : &quot;shard0002&quot;, &quot;host&quot; : &quot;192.168.248.136:27017&quot;, &quot;state&quot; : 1 } databases: { &quot;_id&quot; : &quot;test&quot;, &quot;primary&quot; : &quot;shard0000&quot;, &quot;partitioned&quot; : false } { &quot;_id&quot; : &quot;sang&quot;, &quot;primary&quot; : &quot;shard0001&quot;, &quot;partitioned&quot; : true } sang.c1 shard key: { &quot;x&quot; : 1 } unique: false balancing: true chunks: shard0001 1 { &quot;x&quot; : { &quot;$minKey&quot; : 1 } } --&gt;&gt; { &quot;x&quot; : { &quot;$maxKey&quot; : 1 } } on : shard0001 Timestamp(1, 0) 做完上面这些之后，我们再做两个操作： 1.设置自动分片： 1sh.setBalancerState(true) 2.设置 chunksize,chunksize 这一项是用来指定 chunk 的大小的，为了方便测试分片效果，我们把 chunksize 指定为 1MB，即当这个分片中插入的数据大于 1M 时开始进行数据分片 1db.settings.save({_id:&quot;chunksize&quot;,value:1}) OK，做好这些之后，大功告成。 测试测试方式很简单，我们直接在 mongos 的命令行向 sang 的 c1 集合中插入 50000 条数据，然后再查看这些数据的分布，就知道分片有没有成功了： 1for(var i=0;i&lt;50000;i++){db.c1.insert({x:Math.random()*1000000,name:&quot;hahah&quot;+i})} 然后执行 db.c1.stats() ,结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859{\"sharded\" : true,\"capped\" : false,\"ns\" : \"sang.c1\",\"count\" : 50000,\"size\" : 2688890,\"storageSize\" : 1781760,\"totalIndexSize\" : 1978368,\"avgObjSize\" : 53,\"nindexes\" : 2,\"nchunks\" : 5,\"shards\" : { \"shard0000\" : { \"ns\" : \"sang.c1\", \"size\" : 926504, \"count\" : 17229, \"avgObjSize\" : 53, \"storageSize\" : 462848, \"capped\" : false, \"nindexes\" : 2, \"totalIndexSize\" : 516096, \"indexSizes\" : { \"_id_\" : 184320, \"x_1\" : 331776 }, \"ok\" : 1 }, \"shard0001\" : { \"ns\" : \"sang.c1\", \"size\" : 392593, \"count\" : 7299, \"avgObjSize\" : 53, \"storageSize\" : 667648, \"capped\" : false, \"nindexes\" : 2, \"totalIndexSize\" : 737280, \"indexSizes\" : { \"_id_\" : 253952, \"x_1\" : 483328 }, \"ok\" : 1 }, \"shard0002\" : { \"ns\" : \"sang.c1\", \"size\" : 1369793, \"count\" : 25472, \"avgObjSize\" : 53, \"storageSize\" : 651264, \"capped\" : false, \"nindexes\" : 2, \"totalIndexSize\" : 724992, \"indexSizes\" : { \"_id_\" : 237568, \"x_1\" : 487424 }, \"ok\" : 1 }}} OK，看到如上结果，说明我们的数据已经分布在三个分片服务器中了。 好了，MongoDB 中分片环境的搭建我们就先说到这里，小伙伴们有问题欢迎留言讨论。参考资料： 《MongoDB权威指南第2版》","link":"/2019/0918/mongodb-shard.html"},{"title":"MongoDB 文档更新操作","text":"我们在前面的文章中提到过文档的基本的增删改查操作，MongoDB 中提供的增删改查的语法非常丰富，本文我们主要来看看更新都有哪些好玩的语法。 文档替换假设我的集合中现在存了如下一段数据： 1234567{ \"_id\" : ObjectId(\"59f005402844ff254a1b68f6\"), \"name\" : \"三国演义\", \"authorName\" : \"罗贯中\", \"authorGender\" : \"男\", \"authorAge\" : 99.0} 这是一本书，有书名和作者信息，但是作者是一个独立的实体，所以我想将之提取出来，变成下面这样： 123456789{ \"_id\" : ObjectId(\"59f005402844ff254a1b68f6\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"罗贯中\", \"gender\" : \"男\", \"age\" : 99.0 }} 我可以采用如下操作： 另外一个问题是更新时，MongoDB 只会匹配第一个更新的文档，假设我的 MongoDB 中有如下数据： 1234{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68f7\"), \"x\" : 1 }{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68f8\"), \"x\" : 1 }{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68f9\"), \"x\" : 1 }{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68fa\"), \"x\" : 2 } 我想把所有 x 为 1 的数据改为 99，我们很容易想到如下命令： 1db.sang_collect.update({x:1},{x:99}) 但我们发现执行结果却是这样： 1234{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68f7\"), \"x\" : 99 }{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68f8\"), \"x\" : 1 }{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68f9\"), \"x\" : 1 }{ \"_id\" : ObjectId(\"59f00d4a2844ff254a1b68fa\"), \"x\" : 2 } 即只有第一条匹配的结果被更新了，其他的都没有变化。这是 MongoDB 的更新规则，即只更新第一条匹配结果。如果我们想将所有 x 为 1 的更新为 x 为 99，可以采用如下命令： 1db.sang_collect.update({x:1},{$set:{x:99}},false,true) 首先我们将要修改的数据赋值给 $set，$set 是一个修改器，我们将在下文详细讲解，然后后面多了两个参数，第一个 false 表示如果不存在 update 记录，是否将我们要更新的文档作为一个新文档插入，true 表示插入，false 表示不插入，默认为 false，第二个 true 表示是否更新全部查到的文档，false 表示只更新第一条记录，true 表示更新所有查到的文档。 使用修改器很多时候我们修改文档，只是要修改文章的某一部分，而不是全部，但是现在我面临这样一个问题，假设我有如下一个文档： 1{x:1,y:2,z:3} 我现在想把这个文档中 x 的值改为 99，我可能使用如下操作： 1db.sang_collect.update({x:1},{x:99}) 但是更新结果却变成了这样： 1{ &quot;_id&quot; : ObjectId(&quot;59f02dce95769f660c09955b&quot;), &quot;x&quot; : 99 } 如下图： MongoDB 帮我把整个文档更新了！要解决这个问题，我们可以使用修改器。 $set 修改器$set 可以用来修改一个字段的值，如果这个字段不存在，则创建它。如下： 如果该字段不存在，则创建，如下： 也可以利用 $unset 删除一个字段，如下： $set 也可以用来修改内嵌文档，还以刚才的书为例，如下： 12345678{ \"_id\" : ObjectId(\"59f042cfcafd355da9486008\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"罗贯中\", \"gender\" : \"男\" }} 想要修改作者的名字，操作如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$set:{&quot;author.name&quot;:&quot;明代罗贯中&quot;}}) 修改结果如下： 12345678{ \"_id\" : ObjectId(\"59f042cfcafd355da9486008\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"明代罗贯中\", \"gender\" : \"男\" }} $inc 修改器$inc 用来增加已有键的值，如果该键不存在就新创建一个。比如我想给上文的罗贯中增加一个年龄为 99，方式如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$inc:{&quot;author.age&quot;:99}}) 执行结果如下： 123456789{ \"_id\" : ObjectId(\"59f042cfcafd355da9486008\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"明代罗贯中\", \"gender\" : \"男\", \"age\" : 99.0 }} 加入我想给罗贯中增加 1 岁，执行如下命令： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$inc:{&quot;author.age&quot;:1}}) 这是会在现有值上加 1，结果如下： 123456789{ \"_id\" : ObjectId(\"59f042cfcafd355da9486008\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"明代罗贯中\", \"gender\" : \"男\", \"age\" : 100.0 }} 注意 $inc 只能用来操作数字，不能用来操作 null、布尔等。 数组修改器数组修改器有好几种，我们分别来看。$push 可以向已有数组末尾追加元素，要是不存在就创建一个数组，还是以我们的上面的 book 为例，假设 book 有一个字段为 comments，是一个数组，表示对这个 book 的评论，我们可以使用如下命令添加一条评论： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$push:{comments:&quot;好书666&quot;}}) 此时不存在 comments 字段，系统会自动帮我们创建该字段，结果如下： 123456789101112{ \"_id\" : ObjectId(\"59f042cfcafd355da9486008\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"明代罗贯中\", \"gender\" : \"男\", \"age\" : 100.0 }, \"comments\" : [ \"好书666\" ]} 此时我们可以追加评论，如下： 12345678910111213141516171819db.sang_collect.update({name:&quot;三国演义&quot;},{$push:{comments:&quot;好书666啦啦啦啦&quot;}})``` 结果如下：```json{ &quot;_id&quot; : ObjectId(&quot;59f042cfcafd355da9486008&quot;), &quot;name&quot; : &quot;三国演义&quot;, &quot;author&quot; : { &quot;name&quot; : &quot;明代罗贯中&quot;, &quot;gender&quot; : &quot;男&quot;, &quot;age&quot; : 100.0 }, &quot;comments&quot; : [ &quot;好书666&quot;, &quot;好书666啦啦啦啦&quot; ]} 如果想一次添加 3 条评论，可以结合 $each 一起来使用，如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$push:{comments:{$each:[&quot;111&quot;,&quot;222&quot;,&quot;333&quot;]}}}) 结果如下： 12345678910111213141516{ \"_id\" : ObjectId(\"59f042cfcafd355da9486008\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"明代罗贯中\", \"gender\" : \"男\", \"age\" : 100.0 }, \"comments\" : [ \"好书666\", \"好书666啦啦啦啦\", \"111\", \"222\", \"333\" ]} 我们可以使用 $slice 来固定数组的长度，假设我固定数组的长度为 5，如果数组中的元素不足 5 个，则全部保留，如果数组中的元素超过 5 个，则只会保留最新的 5 个，如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$push:{comments:{$each:[&quot;444&quot;,&quot;555&quot;],$slice:-5}}}) 注意 $slice 的值为负数，运行结果如下： 12345678910111213141516{ \"_id\" : ObjectId(\"59f042cfcafd355da9486008\"), \"name\" : \"三国演义\", \"author\" : { \"name\" : \"明代罗贯中\", \"gender\" : \"男\", \"age\" : 100.0 }, \"comments\" : [ \"111\", \"222\", \"333\", \"444\", \"555\" ]} 我们还可以在清理之前使用 $sort 对数据先进行排序，然后再清理比如我有一个 class 文档，如下： 1234{ \"_id\" : ObjectId(\"59f07f3649fc5c9c2412a662\"), \"class\" : \"三年级二班\"} 现在向这个文档中插入 student，每个 student 有姓名和成绩，然后按照成绩降序排列，只要前 5 条数据，如下： 1db.sang_collect.update({class:&quot;三年级二班&quot;},{$push:{students:{$each:[{name:&quot;张一百&quot;,score:100},{name:&quot;张九九&quot;,score:99},{name:&quot;张九八&quot;,score:98}],$slice:5,$sort:{score:-1}}}}) $sort 的取值为 -1 和 1，-1 表示降序，1 表示升序。上面的命令执行两次之后（即插入两次），结果如下： 1234567891011121314151617181920212223242526{ \"_id\" : ObjectId(\"59f07f3649fc5c9c2412a662\"), \"class\" : \"三年级二班\", \"students\" : [ { \"name\" : \"张一百\", \"score\" : 100.0 }, { \"name\" : \"张一百\", \"score\" : 100.0 }, { \"name\" : \"张九九\", \"score\" : 99.0 }, { \"name\" : \"张九九\", \"score\" : 99.0 }, { \"name\" : \"张九八\", \"score\" : 98.0 } ]} $slice和$sort不能只和$push一起使用，还要加上$each。 $addToSet我们可以在插入的时候使用 $addToSet，表示要插入的值如果存在则不插入，否则插入，如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$addToSet:{comments:&quot;好书&quot;}}) 上面的命令执行多次之后，发现只成功插入了一条数据。也可以将 $addToSet 和 $each 结合起来使用，如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$addToSet:{comments:{$each:[&quot;111&quot;,&quot;222&quot;,&quot;333&quot;]}}}) $pop$pop 可以用来删除数组中的数据，如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$pop:{comments:1}}) 1 表示从 comments 数组的末尾删除一条数据，-1 表示从 comments 数组的开头删除一条数据。 $pull使用 $pull 我们可以按条件删除数组中的某个元素，如下： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$pull:{comments:&quot;444&quot;}}) 表示删除数组中值为 444 的数据。 $既然是数组，我们当然可以通过下标来访问，如下一行操作表示将下标为 0 的(第一个 comments) comments 修改为 999： 1db.sang_collect.update({name:&quot;三国演义&quot;},{$set:{&quot;comments.0&quot;:&quot;999&quot;}}) 可是有的时候我并不知道我要修改的数据处于数组中的什么位置，这个时候可以使用 $ 符号来解决： 1db.sang_collect.update({comments:&quot;333&quot;},{$set:{&quot;comments.$&quot;:&quot;333-1&quot;}}) 查询条件查出来 333 的下标，$ 符号就代码这个下标，然后通过 $ 符号就能将之修改。 savesave 是 shell 中的一个函数，接收一个参数，这个参数就是文档，如果文档中有 _id 参数 save 会执行更新操作，否则执行插入操作，使用 save 操作我们可以方便的完成一些更新操作。 类似于如下命令则表示一个插入操作(因为没有 _id )： 1db.sang_collect.save({x:111}) 好了，MongoDB 的更新操作我们就先介绍这么多，有问题欢迎留言讨论。 参考资料： 《MongoDB权威指南第2版》","link":"/2019/0904/mongodb-documents-update.html"},{"title":"一个Java程序猿眼中的前后端分离以及Vue.js入门","text":"松哥的书里边，其实有涉及到 Vue，但是并没有详细说过，原因很简单，Vue 的资料都是中文的，把 Vue.js 官网的资料从头到尾浏览一遍该懂的基本就懂了，个人感觉这个是最好的 Vue.js 学习资料 ，因此在我的书里边就没有多说。但是最近总结小伙伴遇到的问题，感觉很多人对前后端分离开发还是两眼一抹黑，所以今天松哥想和大家聊一下前后端分离以及 Vue.js 的一点事，算是一个简单的入门科普吧。 前后端不分 后端模板：Jsp、FreeMarker、Velocity 前端模板：Thymeleaf 前后端不分，Jsp 是一个非常典型写法，Jsp 将 HTML 和 Java 代码结合在一起，刚开始的时候，确实提高了生产力，但是时间久了，大伙就发现 Jsp 存在的问题了，对于后端工程师来说，可能不太精通 css ，所以流程一般是这样前端设计页面–&gt;后端把页面改造成 Jsp –&gt; 后端发现问题 –&gt; 页面给前端 –&gt; 前端不会Jsp。这种方式效率低下。特别是在移动互联网兴起后，公司的业务，一般除了 PC 端，还有手机端、小程序等，通常，一套后台系统需要对应多个前端，此时就不可以继续使用前后端不分的开发方式了。 在前后端不分的开发方式中，一般来说，后端可能返回一个 ModelAndView ，渲染成 HTML 之后，浏览器当然可以展示，但是对于小程序、移动端来说，并不能很好的展示 HTML（实际上移动端也支持HTML，只不过运行效率低下）。这种时候，后端和前端数据交互，主流方案就是通过 JSON 来实现。 前后端分离前后端分离后，后端不再写页面，只提供 JSON 数据接口（XML数据格式现在用的比较少），前端可以移动端、小程序、也可以是 PC 端，前端负责 JSON 的展示，页面跳转等都是通过前端来实现的。前端后分离后，前端目前有三大主流框架： Vue 作者尤雨溪，Vue本身借鉴了 Angular，目前GitHubstar数最多，建议后端工程师使用这个，最大的原因是Vue上手容易，可以快速学会，对于后端工程师来说，能快速搭建页面解决问题即可，但是如果你是专业的前端工程师，我会推荐你三个都去学习 。就目前国内前端框架使用情况来说，Vue 算是使用最多的。而且目前来说，有大量 Vue 相关的周边产品，各种 UI 框架，开源项目，学习资料非常多。 React Facebook 的产品。是一个用于构建用户界面的 js 库，React 性能较好，代码逻辑简单。 Angular AngularJS 是一款由 Google 维护的开源 JavaScript 库，用来协助单一页面应用程序运行。它的目标是透过 MVC 模式（MVC）功能增强基于浏览器的应用，使开发和测试变得更加容易。 Vue简介Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。 只关注视图层 MVVM 框架 大家在使用 jQuery 过程中，掺杂了大量的 DOM 操作，修改视图或者获取 value ，都需要 DOM 操作，MVVM 是一种视图和数据模型双向绑定的框架，即数据发生变化，视图会跟着变化，视图发生变化，数据模型也会跟着变化，开发者再也不需要操作 DOM 节点。 如下一个简单的九九乘法表让大家感受一下 MVVM ： 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vue/dist/vue.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"app\"&gt; &lt;input type=\"text\" v-model=\"num\"&gt; &lt;table border=\"1\"&gt; &lt;tr v-for=\"i in parseInt(num)\"&gt; &lt;td v-for=\"j in i\"&gt;{{j}}*{{i}}={{i*j}}&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;&lt;script&gt; var app = new Vue({ el: \"#app\", data: { num:9 } });&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 用户修改输入框中的数据，引起变量的变化，进而实现九九乘法表的更新。 SPASPA（single page web application），单页面应用，是一种网络应用程序或网站的模型，它通过动态重写当前页面来与用户交互，而非传统的从服务器重新加载整个新页面。这种方法避免了页面之间切换打断用户体验，使应用程序更像一个桌面应用程序。在单页应用中，所有必要的代码（ HTML、JavaScript 和 CSS ）都通过单个页面的加载而检索，或者根据需要（通常是为响应用户操作）动态装载适当的资源并添加到页面。SPA 有一个缺点，因为 SPA 应用部署后只有1个页面，而且这个页面只是一堆 js 、css 引用，没有其他有效价值，因此，SPA 应用不易被搜索引擎收录，所以，一般来说，SPA 适合做大型企业后台管理系统。 Vue 使用方式大致上可以分为两大类： 直接将Vue在页面中引入，不做 SPA 应用 SPA应用 基本环境搭建首先需要安装两个东西： NodeJS npm 直接搜索下载 NodeJS 即可，安装成功之后，npm 也就有了。安装成功之后，可以 在 cmd 命令哈验证是否安装成功： NodeJS 安装成功之后，接下来安装 Vue的工具： 12345npm install -g vue-cli # 只需要第一次安装时执行vue init webpack my-project # 使用webpack模板创建一个vue项目cd my-project #进入到项目目录中npm install # 下载依赖（如果在项目创建的最后一步选择了自动执行npm install，则该步骤可以省略）npm run dev # 启动项目 启动成功后，浏览器输入 http://localhost:8080 就能看到如下页面： 执行 npm install 命令时，默认使用的是国外的下载源 ，可以通过如下代码配置为使用淘宝的镜像： 1npm config set registry https://registry.npm.taobao.org 修改完成后，就能有效提高下载的成功率。 Vue 项目结构介绍Vue 项目创建完成后，使用 Web Storm 打开项目，项目目录如下： build 文件夹，用来存放项目构建脚本 config 中存放项目的一些基本配置信息，最常用的就是端口转发 node_modules 这个目录存放的是项目的所有依赖，即 npm install 命令下载下来的文件 src 这个目录下存放项目的源码，即开发者写的代码放在这里 static 用来存放静态资源 index.html 则是项目的首页，入口页，也是整个项目唯一的HTML页面 package.json 中定义了项目的所有依赖，包括开发时依赖和发布时依赖 对于开发者来说，以后 99.99% 的工作都是在 src 中完成的，src 中的文件目录如下： assets 目录用来存放资产文件 components 目录用来存放组件（一些可复用，非独立的页面），当然开发者也可以在 components 中直接创建完整页面。 推荐在 components 中存放组件，另外单独新建一个 page 文件夹，专门用来放完整页面。 router 目录中，存放了路由的js文件 App.vue 是一个Vue组件，也是项目的第一个Vue组件 main.js相当于Java中的main方法，是整个项目的入口js main.js 内容如下： 1234567891011import Vue from 'vue'import App from './App'import router from './router'Vue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: '#app', router, components: { App }, template: '&lt;App/&gt;'}) 在main.js 中，首先导入 Vue 对象 导入 App.vue ，并且命名为 App 导入router，注意，由于router目录下路由默认文件名为 index.js ，因此可以省略 所有东西都导入成功后，创建一个Vue对象，设置要被Vue处理的节点是 ‘#app’，’#app’ 指提前在index.html 文件中定义的一个div 将 router 设置到 vue 对象中，这里是一个简化的写法，完整的写法是 router:router，如果 key/value 一模一样，则可以简写。 声明一个组件 App，App 这个组件在一开始已经导入到项目中了，但是直接导入的组件无法直接使用，必须要声明。 template 中定义了页面模板，即将 App 组件中的内容渲染到 ‘#app’ 这个div 中。 因此，可以猜测，项目启动成功后，看到的页面效果定义在 App.vue 中 123456789101112131415161718192021&lt;template&gt; &lt;div id=\"app\"&gt; &lt;img src=\"./assets/logo.png\"&gt; &lt;router-view/&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'App'}&lt;/script&gt;&lt;style&gt;#app { font-family: 'Avenir', Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; color: #2c3e50; margin-top: 60px;}&lt;/style&gt; App.vue 是一个vue组件，这个组件中包含三部分内容：1.页面模板（template）；2.页面脚本（script）；3.页面样式（style） 页面模板中，定义了页面的 HTML 元素，这里定义了两个，一个是一张图片，另一个则是一个 router-view 页面脚本主要用来实现当前页面数据初始化、事件处理等等操作 页面样式就是针对 template 中 HTML 元素的页面美化操作 需要额外解释的是，router-view，这个指展示路由页面的位置，可以简单理解为一个占位符，这个占位符展示的内容将根据当前具体的 URL 地址来定。具体展示的内容，要参考路由表，即 router/index.js 文件，该文件如下： 12345678910111213import Vue from 'vue'import Router from 'vue-router'import HelloWorld from '@/components/HelloWorld'Vue.use(Router)export default new Router({ routes: [ { path: '/', name: 'HelloWorld', component: HelloWorld } ]}) 这个文件中，首先导入了Vue对象、Router对象以及 HelloWorld 组件， 创建一个Router对象，并定义路由表 这里定义的路由表，path为 / ，对应的组件为 HelloWorld，即浏览器地址为 / 时，在router-view位置显示 HelloWorld 组件 WebStorm 中启动Vue也可以直接在 webstorm 中配置vue并启动，点击右上角进行配置： 然后配置一下脚本 ： 配置完成后，点击右上角启动按钮，就可以启动一个 Vue 项目，如下： 项目编译这么大一个前端项目，肯定没法直接发布运行，当开发者完成项目开发后，将 cmd 命令行定位到当前项目目录，然后执行如下命令对项目进行打包： 1npm run build 打包成功后，当前项目目录下会生成一个 dist 文件夹，这个文件夹中有两个文件，分别是 index.html 和 static ，index.html 页面就是我们 SPA 项目中唯一的 HTML 页面了，static 中则保存了编译后的 js、css等文件，项目发布时，可以使用 nginx 独立部署 dist 中的静态文件，也可以将静态文件拷贝到 Spring Boot 项目的 static 目录下，然后对 Spring Boot 项目进行编译打包发布。 总结因为松哥的读者以后端程序猿居多，也有少量前端程序猿，因此本文松哥想从一个后端程序猿的角度来带大家理解一下前后端分离以及 Vue 的一些基本用法，也欢迎专业的前端工程师出来拍砖。","link":"/2019/0419/springboot-vue.html"},{"title":"一个小小的里程碑！啥也不说了，签名书奉上！","text":"就在昨天，松哥的公众号【江南一点雨】迎来了第 20000 个读者，算是一个小小的里程碑吧。 距离公众号读者到达 10000 差不多过去三个月了，3 个月时间，用户又增加了 10000，很多小伙伴可能不知道，我在今年 3 月份的时候，公众号的读者才只有 5200+，到现在差不多半年时间，用户增长了 15000，今天我想和大家聊聊我的运营故事。 故事很简单，无非就是死撑二字。 漫不经心的开始我是在 2017 年 2 月 19 号申请的公众号，到 2019 年 1 月 31 号的时候，公众号的用户才终于突破 5000，这 5000+ 小伙伴基本上都是看了我的博客或者开源项目关注了我的公众号。我觉得早期关注我的小伙伴真的很不容易，因为那个时候，我公众号基本上都是连着更一两个月，然后又歇好几个月，歇几个月之后，又开始更，断断续续的，在这样的状态下还没有取关，真爱无疑。 这一段时间里，我没有专门去运营公众号，也是因为不懂公众号要怎么运营，公众号在我眼里就只是一个类似于 CSDN、sf 那样的博客发布平台而已，我甚至傻傻的想着通过公众号里边的阅读原文增加博客的访问量。不得不说，松哥在运营公众号这事上后知后觉没有一丁点的天赋。 基于这样不成熟的想法，这一段时间也谈不上坚持，就是顺其自然，心情好了发几篇，心情不好了就断更几个月。 下定决心今年 3 月份，华为云组织了云享专家的线下活动，活动中认识了微笑哥（公众号：纯洁的微笑），活动结束后和微笑哥一起吃了个饭，听他讲做公众号的事，感觉很有意思，也极大的吸引到我。于是当天晚上回到家，我就下定决心今年要好好做我的公众号。 小号的倔强下定决心后，真正要动手的时候，我却发现没有头绪。我只有 5000+ 读者，一篇原创技术干货只有 200 左右的阅读，怎么办？这时候是我最缺乏信心的时候，因为我也不知道我是否适合做这个事。虽然我从大学毕业那年就开始写博客，但是把公众号当成一件事认认真真去做，还是有些不够自信。 刚好那段时间，我写了一篇文章，回忆了一路坚持写博客，再到最后出书的历程(起早贪黑几个月，我写完了人生第一本书！)，在写这篇文章的时候，我就在想，当初决定要写博客，其实也不知道写了之后有没有用，就是觉得时间不能浪费，于是就开干了，写博客这事是典型的坚持了之后才看到希望。那么做公众号是否也一样？把当初写博客的那股劲用到写公众号上，先坚持几个月，看看到底怎么样，就这样，我慢慢的说服了自己，不要怂，就是干。 做公众号虽然有很多运营技巧，但其实最关键的就是内容，没有内容，再多技巧都是白搭，对于像松哥这样的小号而言，内容就更显得重要了，所以我从一开始就下定决心，要坚持做原创，实际上从 2017 年到 2019 年 3 月份，我的公众号上基本都是原创技术干货，而且我几乎很少写零散的问题解决方案，基本上都是成体系的教程，例如： Spring Cloud 系列 Redis 系列 Git 系列 MongoDB 系列 Elasticsearch 系列 Docker 系列 这些系列教程都是我一个字一个字码出来的。于是我就想继续延续我公众号原创的这一优良传统。 于是，从今年 3 月份到现在，一共发了 106 篇原创，这些原创主要围绕 Spring Boot 和前后端分离来写的(例如 Spring Boot 系列)，当然也有一个小小的 MyCat 系列，这些松哥以后都会给大家整理好。 由于一直坚持原创，公众号的阅读数也慢慢上来了，从 200 到 300、500、700、1000….直到现在的 2000 左右，不断增长的阅读给了我继续做下去的信心。 全靠死撑然而写原创并不容易，特别是写出一篇皆大欢喜的原创更难。今天的文章发了，就得琢磨明天发啥，明天的准备好了，又得琢磨后天发发啥，每天晚上写到 12 点如同家常便饭，因为一篇文章不是把几百个字码出来就完事了，你要写案例，写 Demo，然后运行 Demo，截图保存，然后在文章中使用，这个过程费时费力，一篇原创干货往往要好几个小时才能出炉。 松哥写原创技术文最疯狂的时候，差不多连续两个月，篇篇原创，大家可以翻一下 4 月到 7 月的文章。那个时候我经常一坐好几个小时，坐久了腰疼屁股疼，最近买了个升降桌，情况才得以慢慢缓解。 松哥并不是全职做公众号，白天和大部分小伙伴一样，松哥也要上班，晚上回家后，除了写公众号上的技术文章之外，还有很多其他的事情要做。3-6 月份期间，应慕课网邀请，我和微笑哥一起在慕课网上出了一个 Spring Cloud 微服务开发实践 的专栏，那段时间，晚上除了写公众号，还要写专栏，周末也是一样。专栏在六月份完工之后，七月份松哥又开始录制 Spring Boot + 微人事 的视频教程，这个教程目前还在录制中，没有完工，所以我基本上没有什么业余活动，周末也是正常作息，时间都是一分一分抠出来的，除了偶尔去游个泳。 七月底的时候，实在是有点扛不住了，那段时间状态很差，做事效率也很低，但看着每天增长的数据，实在不忍心放弃，最终还是坚持了下来。 不得不说的广告做公众号这段时间，松哥前前后后算起来一共就接过 4 个商业广告，老实说，这个数字在我目前这个同量级的号主中算是很少了，很少的原因不是因为我接不到广告，相反，随着号越来越大，找上门的广告主其实挺多的，每天公众号后台都有很多广告主留言，只不过基本上都被我拒绝掉了。 但是有的时候松哥推荐其他号主，这个还是希望大家能理解，因为做公众号，或者说写一篇技术干货，从松哥的角度来说，当然是希望文章被越多的人看到越好，毕竟几个小时产出一篇技术干货，结果就十来个人看了，自己心里都凉透了，写作的动力也会减弱，而公众号本身是一个封闭的平台，通过其他号主的推荐，显然可以有效的将自己的文章曝光给更多的小伙伴，因此这个还是希望小伙伴们能多多理解。 革命尚未成功，同志仍需努力松哥经历过公司倒闭，技术栈也从当年如日中天的 Android 切换到如今的 JavaEE，深知这个行业虽然薪水高，但是高处不胜寒，技术之路不进则退，那些火爆一时的技术，让人眼花缭乱的薪水，极有可能冲昏你的头脑，当潮水退去，日子最难过的就是技术菜鸟了。所以，Coding 与学习之路不能停下来，这也是我一直坚持写博客的原因之一，写完一篇博客就会感觉到充实，要是长时间没写，会有很强烈的不安全感，似乎要被技术同行抛弃了。 自从松哥开始认真做公众号，也认识了很多同行，我最近发现 4 月份认识的公众号主有好几个人都停更了，说明做公众号这个事确实不易。很多时候你都不用拼智商、拼资源、拼人脉，靠死撑，就能够超过很多同行了。 当然我也希望自己能把做公众号这件事认真坚持下去。小伙伴平时看到了合胃口的文章，顺手点个在看我就很开心啦，如果还能转发一下，那松哥就要膨胀了。 好了，和大家又叨叨了这么多，最后，5 本松哥自己的签名书《Spring Boot + Vue 全栈开发实战》送给经常来公众号打卡的小伙伴。 送书规则：大家留言说说你坚持做的最久的一件事，我会挑选出 5 位幸运读者，签名版《Spring Boot + Vue 全栈开发实战》包邮到家。也欢迎各位小伙伴多来公众号打卡学习哦，可以有效提高送书时的获奖概率哦。","link":"/2019/0917/official-accounts-20000.html"},{"title":"公司倒闭 1 年了，而我当年的项目上了 GitHub 热榜","text":"公司倒闭 1 年多了，而我在公司倒闭时候做的开源项目，最近却上了 GitHub Trending，看着这个数据，真是不胜唏嘘。 缘起2017 年 11 月份的时候，松哥所在的公司因为经营不善要关门了，关门的是深圳分公司，北京总部还在正常运转。 然后就是北京那边来人，和深圳的员工挨个谈话，谈裁员和赔偿，公司制度还算完善，都按照劳动合同法走，有的同事担心公司最后不按劳动合同法走，因此觉得先拿钱先走比价划算。我当时主要考虑到两个原因，并不着急走： 公司毕竟是香港上市公司，跑的了和尚跑不了庙，深圳关门了，北京那边还在运转，所以我不太担心公司赖账的事。 年底工作不好找，11 月拿赔偿走人，还有俩月才过年，这个时候不太容易拿到满意的 offer，很多公司年底都关闭 HC 了。 基于上面两点考虑，我当时并不急着走人，当公司说还需要有人留下来善后一直到 2018 年 1 月 31 号的时候，我就争取了下，然后就给留下来了。 留下来后并没有太多事情要做。划水划了一周，同事在楼下叫我：“老王下来聊天”，于是下楼跟他们吹吹牛，虽然吹牛，不过大多数时候还是在筹划来年找工作的事，不过我觉得这样没什么用，与其天天规划，不如来点实实在在的东西，为来年找工作积累一点筹码。 第一次尝试心里想着手上就开始行动了，技术栈就选择当时最流行的 Spring Boot + Vue 前后端分离，业务就打算先做一个简单的博客试试水，博客的业务比较简单，做起来快，于是，V部落项目就诞生了： V 部落 一个简单的博客后台管理，集成了博客编辑、发表、排版引入了 md 编辑器，博客的分类展示等，记得不到一周时间就弄完了，毕竟还是非常容易的。 V 部落项目发布后，我认认真真的写了一个介绍的 README，README 和我以前的开源项目一样，就是展示了一下项目的效果图，然后说了下要如何部署运行就完了。虽然自我感觉良好，但是并没有引起太多人关注。 在为数不多的几个关注中，我发现小伙伴在运行项目时候总是会遇到各种各样的问题，很多人多前后端分离的这种开发方式非常陌生，很多后端工程师甚至不懂，没听说过前端工程化，很多小伙伴在 GitHub 上提了很多非常简单的 issue，他们在部署V 部落项目时老是出错。 另一方面，由于博客项目比较简单，Vue 中很多高级功能没用上，例如状态管理，还有前后端分离时的动态权限管理，这些都没有体现出来。再加上当时才是 12 月，离过年还早着，我心想着再做一个业务复杂点的，然后把这些之前没用到的技能点都给用上。于是就有了微人事项目，这也是我们今天的主角，上了 6 月份 GitHub Trending。 微人事微人事项目，我就吸取 V 部落的经验，没有等项目完全发布后再上传到 GitHub 上，而是边做变更新，每做完一个功能，就写一个文档，把实现的思路，代码的原理等都记录下来，然后在打一个 tag ，发布到 GitHub 上，这样，即使是一些新手，跟着文档，也能完全做出来。 这是当时的一些提交记录： 基本上每隔一两天就能完成一个新功能，然后就提交一次，这样的更新频率一直持续到 2018 年 1 月 20 之前，1 月 21 号女票从昆士兰大学访学回来，陪她在深圳玩了几天，然后把女票送回家，耽搁了好几天没更新。 到了 1 月 31 号，公司正式关门，我也就回家了，先去了女票家，在她家里呆了十多天，顺便完成了用 WebSocket 实现在线聊天的功能，提交了两个版本。 下面这两个是在女票家里提交的： 这两次提交之后，差不多就回家过年了，我家在岭上没有网，因此过年期间就没再继续做这个项目了，年后从家里到深圳的当天就拿到 offer 了，上班后就比较忙了，这个项目也就更新的慢了，没有再提交比较大的版本了，主要是一些修修补补的操作。 ALL IN现在不是流行一个词叫做 all in ，用来形容我当时的状态再贴切不过了。 兴趣来了，谁都拦不住。那一段时间晚上经常在家里搞到一两点，第二天正常上班。记得那一年平安夜那天是周日，我早上依然按时起床去了公司，因为当天我的 V 部落项目就要收尾了，在公司里搞到晚上 9 点多，终于弄完了，第二天就可以发布了。然后收拾东西，骑着摩拜从科兴科学园那里出发，先走北环大道，然后再下到大沙河边上，沿着大沙河骑到西丽大学城，因为是平安夜，一路上都没什么行人，到家后还没吃饭，去楼下的餐厅随便吃点，店主一家人正在准备他们的平安夜饭，见有客人，抽出一个人给我弄了一碗面，然后我就独自边玩手机边吃饭，他们一家人围在旁边的桌子上吃他们的平安夜饭，这种感觉很奇怪，孤独又充实。 吃完回到宿舍，和女票视频，心理盘算着女票再过 20 多天就从昆士兰回来了，慢慢就睡着了，第二天到公司，我的 V 部落项目就正式上线了。 当你沉迷于一件事情的时候，效率非常高。 那段时间，我每天骑车上下班，一边骑着自行车，心里就在想着这个功能要怎么做更好，那个功能得怎么样实现，一路上就这样不断的规划着，到了公司，放下书包，就赶紧打开电脑挨个试验路上的各种想法，做出来了就很开心，在已经没有几个人的公司走走转转休息下，然后继续坐下 coding。 有一个周末去公司做这两个开源项目的时候，中午去吃饭，两只狗懒洋洋的躺在马路中间晒太阳，我甚至有点羡慕(下图拍摄于南山科兴科学园附近，我吃饭喜欢走远一点，多溜达溜达，不知不觉就溜达到工地了，见到这么惬意一幕，冬天的太阳，真的舒服)。 项目在 GitHub 上开源之后，有好几位小伙伴发邮件希望能在毕设中引用这个项目，记得有一位是国内高校 top20 的研究生，我当时又诧异又激动。后来也有好几个小伙伴加松哥微信表示想将这个项目作为脚手架用在公司的项目中，虽然没能亲眼见见小伙伴的项目，但是想到自己的项目帮到了这么多人，还是挺开心的。 这两个开源项目也带给我不少收获，技术上的提升+认识很多优秀的小伙伴，并且因此还出了一本书，也算是收获满满吧。 上榜前两天有个小伙伴发消息说是看到我的项目出现 GitHub Trending 上了，我点开看了下，果然是的。 我是在 2013 年 11 月份注册的 GitHub，但是很长一段时间都没啥活动，后来也断断续续做过几个开源项目，但是都没啥动静，我总结其中一个原因可能也是因为我没好好对待项目，README 只是随便写写，读者一般很难上手项目，从微人事开始，我懂得了，你想要让别人重视你的项目，你首先得自己重视自己的项目。 这里再向大家安利一波这个开源项目微人事，这是一个使用了 Spring Boot + Vue 开发的前后端分离的人力资源管理系统，有一个非常完整的文档： 项目地址：https://github.com/lenve/vhr 欢迎大家点击阅读原文查看本项目。 其他另外再给大家一个小小建议，如果你是初次接触前后端分离，可以先看 V 部落项目，这个项目不论从技术点还是业务上来说，都要比微人事简单， V 部落搞懂之后，再来学习微人事就会容易很多了。 如果你只是刚刚接触 Java，那么松哥也有一个前后端不分开源项目可以给你练手： https://github.com/lenve/CoolMeeting 这个项目做的比较早，功能做的相对较全，但是介绍文档没有微人事那么详细，大家可以参考。","link":"/2019/0710/vhr.html"},{"title":"创建一个 Spring Boot 项目，你会几种方法？","text":"我最早是 2016 年底开始写 Spring Boot 相关的博客，当时使用的版本还是 1.4.x ，文章发表在 CSDN 上，阅读量最大的一篇有 42W+，如下图： 2017 年由于种种原因，就没有再继续更新 Spring Boot 相关的博客了，2018年又去写书了，也没更新，现在 Spring Boot 最新稳定版是 2.1.4 ，松哥想针对此写一个系列教程，专门讲 Spring Boot2 中相关的知识点。这个系列，就从本篇开始吧。 Spring Boot 介绍我们刚开始学习 JavaWeb 的时候，使用 Servlet/JSP 做开发，一个接口搞一个 Servlet ，很头大，后来我们通过隐藏域或者反射等方式，可以减少 Servlet 的创建，但是依然不方便，再后来，我们引入 Struts2/SpringMVC 这一类的框架，来简化我们的开发 ，和 Servlet/JSP 相比，引入框架之后，生产力确实提高了不少，但是用久了，又发现了新的问题，即配置繁琐易出错，要做一个新项目，先搭建环境，环境搭建来搭建去，就是那几行配置，不同的项目，可能就是包不同，其他大部分的配置都是一样的，Java 总是被人诟病配置繁琐代码量巨大，这就是其中一个表现。那么怎么办？Spring Boot 应运而生，Spring Boot 主要提供了如下功能： 为所有基于 Spring 的 Java 开发提供方便快捷的入门体验。 开箱即用，有自己自定义的配置就是用自己的，没有就使用官方提供的默认的。 提供了一系列通用的非功能性的功能，例如嵌入式服务器、安全管理、健康检测等。 绝对没有代码生成，也不需要XML配置。 Spring Boot 的出现让 Java 开发又回归简单，因为确确实实解决了开发中的痛点，因此这个技术得到了非常广泛的使用，松哥很多朋友出去面试 Java 工程师，从2017年年初开始，Spring Boot基本就是必问，现在流行的 Spring Cloud 微服务也是基于 Spring Boot，因此，所有的 Java 工程师都有必要掌握好 Spring Boot。 系统要求截至本文写作（2019.04.11），Spring Boot 目前最新版本是 2.1.4，要求至少 JDK8，集成的 Spring 版本是 5.1.6 ，构建工具版本要求如下： Build Tool Version Maven 3.3+ Gradle 4.4+ 内置的容器版本分别如下： Name Version Tomcat 9.0 4.0 Jetty 9.4 3.1 Undertow 2.0 4.0 三种创建方式初学者看到 Spring Boot 工程创建成功后有那么多文件就会有点懵圈，其实 Spring Boot 工程本质上就是一个 Maven 工程，从这个角度出发，松哥在这里向大家介绍三种项目创建方式。 在线创建这是官方提供的一个创建方式，实际上，如果我们使用开发工具去创建 Spring Boot 项目的话（即第二种方案），也是从这个网站上创建的，只不过这个过程开发工具帮助我们完成了，我们只需要在开发工具中进行简单的配置即可。 首先打开 https://start.spring.io 这个网站，如下： 这里要配置的按顺序分别如下： 项目构建工具是 Maven 还是 Gradle ？松哥见到有人用 Gradle 做 Java 后端项目，但是整体感觉 Gradle 在 Java 后端中使用的还是比较少，Gradle 在 Android 中使用较多，Java 后端，目前来看还是 Maven 为主，因此这里选择第一项。 开发语言，这个当然是选择 Java 了。 Spring Boot 版本，可以看到，目前最新的稳定版是 2.1.4 ，这里我们就是用最新稳定版。 既然是 Maven 工程，当然要有项目坐标，项目描述等信息了，另外这里还让输入了包名，因为创建成功后会自动创建启动类。 Packing 表示项目要打包成 jar 包还是 war 包，Spring Boot 的一大优势就是内嵌了 Servlet 容器，打成 jar 包后可以直接运行，所以这里建议打包成 jar 包，当然，开发者根据实际情况也可以选择 war 包。 然后选选择构建的 JDK 版本。 最后是选择所需要的依赖，输入关键字如 web ，会有相关的提示，这里我就先加入 web 依赖。 所有的事情全部完成后，点击最下面的 Generate Project 按钮，或者点击 Alt+Enter 按键，此时会自动下载项目，将下载下来的项目解压，然后用 IntelliJ IDEA 或者 Eclipse 打开即可进行开发。 使用开发工具创建有人觉得上面的步骤太过于繁琐，那么也可以使用 IDE 来创建，松哥这里以 IntelliJ IDEA 和 STS 为例，需要注意的是，IntelliJ IDEA 只有 ultimate 版才有直接创建 Spring Boot 项目的功能，社区版是没有此项功能的。 IntelliJ IDEA首先在创建项目时选择 Spring Initializr，如下图： 然后点击 Next ，填入 Maven 项目的基本信息，如下： 再接下来选择需要添加的依赖，如下图： 勾选完成后，点击 Next 完成项目的创建。 STS这里我再介绍下 Eclipse 派系的 STS 给大家参考， STS 创建 Spring Boot 项目，实际上也是从上一小节的那个网站上来的，步骤如下： 首先右键单击，选择 New -&gt; Spring Starter Project ，如下图： 然后在打开的页面中填入项目的相关信息，如下图： 这里的信息和前面提到的都一样，不再赘述。最后一路点击 Next ，完成项目的创建。 Maven 创建上面提到的几种方式，实际上都借助了 https://start.spring.io/ 这个网站，松哥记得在 2017 年的时候，这个网站还不是很稳定，经常发生项目创建失败的情况，从2018年开始，项目创建失败就很少遇到了，不过有一些读者偶尔还是会遇到这个问题，他们会在微信上问松哥这个问题腰怎么处理？我一般给的建议就是直接使用 Maven 来创建项目。步骤如下： 首先创建一个普通的 Maven 项目，以 IntelliJ IDEA 为例，创建步骤如下： 注意这里不用选择项目骨架（如果大伙是做练习的话，也可以去尝试选择一下，这里大概有十来个 Spring Boot 相关的项目骨架），直接点击 Next ，下一步中填入一个 Maven 项目的基本信息，如下图： 然后点击 Next 完成项目的创建。 创建完成后，在 pom.xml 文件中，添加如下依赖： 1234567891011&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加成功后，再在 java 目录下创建包，包中创建一个名为 App 的启动类，如下： 1234567891011@EnableAutoConfiguration@RestControllerpublic class App { public static void main(String[] args) { SpringApplication.run(App.class, args); } @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} @EnableAutoConfiguration 注解表示开启自动化配置。 然后执行这里的 main 方法就可以启动一个 Spring Boot 工程了。 项目结构使用工具创建出来的项目结构大致如下图： 对于我们来说，src 是最熟悉的， Java 代码和配置文件写在这里，test 目录用来做测试，pom.xml 是 Maven 的坐标文件，就这几个。 总结本文主要向大家介绍了三种创建 Spring Boot 工程的方式，大家有更6的方法欢迎来讨论。","link":"/2019/0412/springboot-init.html"},{"title":"前后端分离，我怎么就选择了 Spring Boot + Vue 技术栈？","text":"前两天又有小伙伴私信松哥，问题还是职业规划，Java 技术栈路线这种，实际上对于这一类问题我经常不太敢回答，每个人的情况都不太一样，而小伙伴也很少详细介绍自己的情况，大都是一两句话就把问题抛出来了，啥情况都不了解，就要指出一个方向，这实在是太难了。 因此今天我想从我学习 Spring Boot + Vue 这套技术栈的角度，来和大家聊一聊没有人指导，我是如何一步一步建立起自己的技术体系的。 线上大家看我经常写文章，线下我其实比较宅，跟人交流比较少，我也很少问别人职业规划或者技术规划这些问题，因为这种学什么的问题，我喜欢自己把握，我不太喜欢被别人牵着走。 Rome was not built in a day，刚开始接触 Spring Boot + Vue 时，我甚至都没有一个明确的想法，只是觉得该学点什么，不能让时间浪费，没有告诉我 Spring Boot 要火了，也没有人告诉我 Vue 要超过 React 了，都是我自己一直在摸索摸索，一步一步，直到构建起这套技术大厦。 Spring Boot先说说 Spring Boot 吧，三年前差不多也是这个时候，是我第一次接触 Spring Boot ，那个时候我的正式身份还是一名 Android 工程师，那段时间在研究 Android7 的源码，还写了一些博客： 但是那个时候 Android 的行情在慢慢下滑，而我刚毕业 1 年多，未来还有更加丰富的技术人生，我不愿意这么早就把技术栈定死，而且还定在一个行情日渐下滑的技术栈上。所以我打算学一点新的东西。 Python、Go、前端 和 Java 都是备选的方向，但是最终还是选择继续做 Java，有三个原因： 做 Java 当时可以在公司内部转岗，做 Python 或者 Go 的话，可能就得换工作了，技术栈切换，一切从头开始，当时心里还是没底，于是就选择继续做 Java 刚好大学的时候也有 JavaEE 的底子，重新捡起来 JavaEE 相关的技术点倒也不是啥难事 第三点也是最重要的一点，我一直希望能够独立接点私活，这样有一天赚钱能够不受工作地点的限制，基于这样的初衷，我一直希望走全栈的路线，用 Python 和 Go 虽然也可以做企业级应用，但是在目前的技术环境下，这并不算是主流方案，主流方案依然是 Java ，虽然它被被多人吐槽 基于以上三点，我决定还是走 Java 的方向吧。 2016 年那会，CSDN 几乎每个月送我一本技术图书，10 月份的图书我就和梦鸽美女要了一本 Spring Boot 相关的书，书到了之后，一直在忙各种事情没时间看，到了当年 12 月份的时候，公司安排我去深圳出差，出差的话，每天下班后时间就比较充裕了，于是我就带上了书，每天下班回到酒店，就开始搞 Spring Boot。 一开始我就发现这玩意相比我大学时候搞得 XML 配置的 SSM 太好用了，还是 SSM 那套东西，但是有了自动化配置，不用再去写让人头大的 XML 配置了，可以基于 Spring Boot 快速搞一个 SSM 应用出来。不过刚开始学的时候我还不知道 Spring Boot 在 Java 领域如此火爆，当我写了几篇博客之后，我发现每篇博客的阅读量都暴涨，远远高于其他博客的阅读，我隐隐约约感觉到这次稀里糊涂的技术栈切换，算是没走错路。 不过老实说，Spring Boot 技术栈其实不算难，都是 SSM 那一套东西，只是多了自动化配置（当然，Spring Boot 也有不少自己的东西，不过整体上基于 SSM 这点应该没啥争议），我刚开始搞 Spring Boot 的时候，有时候会有一些东西看的云里雾里，后来发现问题出在 Spring + SpringMVC 上，好几年不写 JavaEE，这些东西有一点点生疏了，后来又花了一些时间把 SSM 这些东西系统过了一遍，然后再去看 Spring Boot 就顺畅多了。 所以有一些小伙伴问松哥能不能跳过 SSM 直接学习 Spring Boot，这个我不建议，大家在 Spring Boot 中见到的很多神奇的自动化配置大部分都是基于 Spring 现有功能实现的，要是不懂实现原理，你会发现 Spring Boot 用得时候虽然好用，但是出了问题，你就束手无策了。 就这样，跳入了 Spring Boot 的坑里了。Spring Boot 学完没多久，工作上，马上要从 Android 切换到 JavaEE 了，亟需一个项目练练手，当时我的上司给我介绍了一个西藏大学的项目，我使用 Spring Boot+EasyUI 的技术栈花了不到一个礼拜做完了，从此就算是叩开了 JavaEE 的大门，那会是 2017 年。 当时前端选择 EasyUI 也是没办法，甲方催得紧，而我来不及搞其他的前端框架，当时只有 EasyUI 熟悉一些，不用花时间学，直接就能用，于是就选择了 EasyUI，但是 EasyUI 太丑了，所以在做完西藏大学的项目后，我就一直思量着再整一个专业的前端框架，这样以后再有私活，我就可以独立做出来一个好看的后端管理系统了。 就这样，在综合对比了 Vue、React 以及 Angular 之后，决定跳入 Vue 的坑。 Vue前端其实还算接触的比较早，最早的 jQuery Mobile，PhoneGap 上大学的时候就玩过，我的第一本 NodeJS 的书是在 2013 年买的，那个时候 NodeJS 还算是一个比较新的事物。当我还是一名 Android 工程师的时候，我就玩过 React 和 ReactNative，RN 是当时比较流行的一个跨平台解决方案。但是在我比较这三个技术栈的时候，我发现 Vue 更加好用，生态也更加丰富，而且大有超过 React 的架势（当时 Vue 在 GitHub 上的 star 数还没超过 React），于是我就选择了 Vue。其实当时我心里想，大不了学完 Vue 再学 React，反正我才刚毕业两年多，没必要这么早就锁定技术栈停止学习。 Vue 的学习确实不费啥事，花了两三天时间刷了一遍官网，然后就开始做项目，但是要去深入学习，又是一个漫长的过程了。 Vue 有很多漂亮的 UI 库，像 ElementUI 等都算是做的比较好的，这些东西只要会用其中一个，其他的就可以手到擒来。 到 2018 年初，Spring Boot+Vue 技术栈基本上已经熟悉了，两个开源项目 V 部落(V 部落)和微人事(视频揭秘微人事项目实现过程)也受到小伙伴们的欢迎，常规的企业级应用可以一个人独立完成了，5 月份的时候，经朋友介绍，接了哈尔滨工程大学一位老师的项目，毫无疑问我就使用了最擅长的 Spring Boot+Vue 技术栈来做了，前后端都是自己做，没人扯皮，美滋滋。 再后来，就是写书（我的第一本书，被选作大学教材了！），业余继续搞点项目用 Spring Boot + Vue 来做，这些以前都和大家聊过我就不再多说了，业余接点项目来做这块倒是有一些经验，以后和小伙伴们细聊。 就这样，没有任何人的指引，我慢慢构建了 Spring Boot + Vue 这套技术体系，这个过程中，最大的学习经验就是要写博客，做笔记，写博客不仅仅是记录，也是总结提炼，在写的过程中，融入自己的思考，加深对技术的理解。 掌握了这套技术栈之后，我觉得我离全栈又更近了一步，离赚钱不受工作地点的限制这个目标也更近一步了。 结语有前辈大佬的指引，你可能走得快，自己摸索，走的踏实。其实从我第一天自学 Java 开始，基本上都是一直在摸索。大学时候一个 BUG 折腾两三天才解决，但是一旦自己想明白解决了，以后类似的错误不会再犯，这是我的感受。 好了，一点点学习经验，和小伙伴们分享，要是觉得有启发，欢迎转发哦。","link":"/2019/1119/springboot-vue-stack.html"},{"title":"另一种缓存，Spring Boot 整合 Ehcache","text":"用惯了 Redis ，很多人已经忘记了还有另一个缓存方案 Ehcache ，是的，在 Redis 一统江湖的时代，Ehcache 渐渐有点没落了，不过，我们还是有必要了解下 Ehcache ，在有的场景下，我们还是会用到 Ehcache。 今天松哥就来和大家聊聊 Spring Boot 中使用 Ehcache 的情况。相信看完本文，大家对于[Spring Boot 操作 Redis，三种方案全解析！]一文中的第二种方案会有更加深刻的理解。 Ehcache 也是 Java 领域比较优秀的缓存方案之一，Ehcache 这个缓存的名字很有意思，正着念反着念，都是 Ehcache，Spring Boot 中对此也提供了很好的支持，这个支持主要是通过 Spring Cache 来实现的。 Spring Cache 可以整合 Redis，当然也可以整合 Ehcache，两种缓存方案的整合还是比较相似，主要是配置的差异，具体的用法是一模一样的，就类似于 JDBC 和 数据库驱动的关系一样。前面配置完成后，后面具体使用的 API 都是一样的。 和 Spring Cache + Redis 相比，Spring Cache + Ehcache 主要是配置有所差异，具体的用法是一模一样的。我们来看下使用步骤。 项目创建首先，来创建一个 Spring Boot 项目，引入 Cache 依赖： 工程创建完成后，引入 Ehcache 的依赖，Ehcache 目前有两个版本： 这里采用第二个，在 pom.xml 文件中，引入 Ehcache 依赖： 123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt; &lt;version&gt;2.10.6&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加 Ehcache 配置在 resources 目录下，添加 ehcache 的配置文件 ehcache.xml ，文件内容如下： 123456789101112131415161718&lt;ehcache&gt; &lt;diskStore path=\"java.io.tmpdir/shiro-spring-sample\"/&gt; &lt;defaultCache maxElementsInMemory=\"10000\" eternal=\"false\" timeToIdleSeconds=\"120\" timeToLiveSeconds=\"120\" overflowToDisk=\"false\" diskPersistent=\"false\" diskExpiryThreadIntervalSeconds=\"120\" /&gt; &lt;cache name=\"user\" maxElementsInMemory=\"10000\" eternal=\"true\" overflowToDisk=\"true\" diskPersistent=\"true\" diskExpiryThreadIntervalSeconds=\"600\"/&gt;&lt;/ehcache&gt; 配置含义： name:缓存名称。 maxElementsInMemory：缓存最大个数。 eternal:对象是否永久有效，一但设置了，timeout将不起作用。 timeToIdleSeconds：设置对象在失效前的允许闲置时间（单位：秒）。仅当eternal=false对象不是永久有效时使用，可选属性，默认值是0，也就是可闲置时间无穷大。 timeToLiveSeconds：设置对象在失效前允许存活时间（单位：秒）。最大时间介于创建时间和失效时间之间。仅当eternal=false对象不是永久有效时使用，默认是0.，也就是对象存活时间无穷大。 overflowToDisk：当内存中对象数量达到maxElementsInMemory时，Ehcache将会对象写到磁盘中。 diskSpoolBufferSizeMB：这个参数设置DiskStore（磁盘缓存）的缓存区大小。默认是30MB。每个Cache都应该有自己的一个缓冲区。 maxElementsOnDisk：硬盘最大缓存个数。 diskPersistent：是否缓存虚拟机重启期数据。 diskExpiryThreadIntervalSeconds：磁盘失效线程运行时间间隔，默认是120秒。 memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。默认策略是LRU（最近最少使用）。你可以设置为FIFO（先进先出）或是LFU（较少使用）。 clearOnFlush：内存数量最大时是否清除。 diskStore 则表示临时缓存的硬盘目录。 注意 默认情况下，这个文件名是固定的，必须叫 ehcache.xml ，如果一定要换一个名字，那么需要在 application.properties 中明确指定配置文件名，配置方式如下： 1spring.cache.ehcache.config=classpath:aaa.xml 开启缓存开启缓存的方式，也和 Redis 中一样，如下添加 @EnableCaching 依赖即可： 1234567@SpringBootApplication@EnableCachingpublic class EhcacheApplication { public static void main(String[] args) { SpringApplication.run(EhcacheApplication.class, args); }} 其实到这一步，Ehcache 就算配置完成了，接下来的用法，和松哥之前讲 Redis 的文章一模一样。不过这里松哥还是带大家使用下。 使用缓存这里主要向小伙伴们介绍缓存中几个核心的注解使用。 @CacheConfig这个注解在类上使用，用来描述该类中所有方法使用的缓存名称，当然也可以不使用该注解，直接在具体的缓存注解上配置名称，示例代码如下： 1234@Service@CacheConfig(cacheNames = \"user\")public class UserService {} @Cacheable这个注解一般加在查询方法上，表示将一个方法的返回值缓存起来，默认情况下，缓存的 key 就是方法的参数，缓存的 value 就是方法的返回值。示例代码如下： 12345@Cacheable(key = \"#id\")public User getUserById(Integer id,String username) { System.out.println(\"getUserById\"); return getUserFromDBById(id);} 当有多个参数时，默认就使用多个参数来做 key ，如果只需要其中某一个参数做 key ，则可以在 @Cacheable 注解中，通过 key 属性来指定 key ，如上代码就表示只使用 id 作为缓存的 key ，如果对 key 有复杂的要求，可以自定义 keyGenerator 。当然，Spring Cache 中提供了root对象，可以在不定义 keyGenerator 的情况下实现一些复杂的效果，root 对象有如下属性： 也可以通过 keyGenerator 自定义 key ，方式如下： 1234567@Componentpublic class MyKeyGenerator implements KeyGenerator { @Override public Object generate(Object target, Method method, Object... params) { return method.getName()+Arrays.toString(params); }} 然后在方法上使用该 keyGenerator ： 12345678@Cacheable(keyGenerator = \"myKeyGenerator\")public User getUserById(Long id) { User user = new User(); user.setId(id); user.setUsername(\"lisi\"); System.out.println(user); return user;} @CachePut这个注解一般加在更新方法上，当数据库中的数据更新后，缓存中的数据也要跟着更新，使用该注解，可以将方法的返回值自动更新到已经存在的 key 上，示例代码如下： 1234@CachePut(key = \"#user.id\")public User updateUserById(User user) { return user;} @CacheEvict这个注解一般加在删除方法上，当数据库中的数据删除后，相关的缓存数据也要自动清除，该注解在使用的时候也可以配置按照某种条件删除（ condition 属性）或者或者配置清除所有缓存（ allEntries 属性），示例代码如下： 1234@CacheEvict()public void deleteUserById(Integer id) { //在这里执行删除操作， 删除是去数据库中删除} 总结本文主要向大家了 Spring Boot 整合 Ehcache 的用法，其实说白了还是 Spring Cache 的用法。相信读完本文，大家对于 Redis + Spring Cache 的用法会有更深的认识。 本文案例我已上传到 GitHub ，欢迎大家 star：https://github.com/lenve/javaboy-code-samples 关于本文，有问题欢迎留言讨论。","link":"/2019/0612/springboot-ehcache.html"},{"title":"干货|一个案例学会Spring Security 中使用 JWT","text":"在前后端分离的项目中，登录策略也有不少，不过 JWT 算是目前比较流行的一种解决方案了，本文就和大家来分享一下如何将 Spring Security 和 JWT 结合在一起使用，进而实现前后端分离时的登录解决方案。 1 无状态登录1.1 什么是有状态？有状态服务，即服务端需要记录每次会话的客户端信息，从而识别客户端身份，根据用户身份进行请求的处理，典型的设计如Tomcat中的Session。例如登录：用户登录后，我们把用户的信息保存在服务端session中，并且给用户一个cookie值，记录对应的session，然后下次请求，用户携带cookie值来（这一步有浏览器自动完成），我们就能识别到对应session，从而找到用户的信息。这种方式目前来看最方便，但是也有一些缺陷，如下： 服务端保存大量数据，增加服务端压力 服务端保存用户状态，不支持集群化部署 1.2 什么是无状态微服务集群中的每个服务，对外提供的都使用RESTful风格的接口。而RESTful风格的一个最重要的规范就是：服务的无状态性，即： 服务端不保存任何客户端请求者信息 客户端的每次请求必须具备自描述信息，通过这些信息识别客户端身份 那么这种无状态性有哪些好处呢？ 客户端请求不依赖服务端的信息，多次请求不需要必须访问到同一台服务器 服务端的集群和状态对客户端透明 服务端可以任意的迁移和伸缩（可以方便的进行集群化部署） 减小服务端存储压力 1.3.如何实现无状态无状态登录的流程： 首先客户端发送账户名/密码到服务端进行认证 认证通过后，服务端将用户信息加密并且编码成一个token，返回给客户端 以后客户端每次发送请求，都需要携带认证的token 服务端对客户端发送来的token进行解密，判断是否有效，并且获取用户登录信息 1.4 JWT1.4.1 简介JWT，全称是Json Web Token， 是一种JSON风格的轻量级的授权和身份认证规范，可实现无状态、分布式的Web应用授权： JWT 作为一种规范，并没有和某一种语言绑定在一起，常用的Java 实现是GitHub 上的开源项目 jjwt，地址如下：https://github.com/jwtk/jjwt 1.4.2 JWT数据格式JWT包含三部分数据： Header：头部，通常头部有两部分信息： 声明类型，这里是JWT 加密算法，自定义 我们会对头部进行Base64Url编码（可解码），得到第一部分数据。 Payload：载荷，就是有效数据，在官方文档中(RFC7519)，这里给了7个示例信息： iss (issuer)：表示签发人 exp (expiration time)：表示token过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 这部分也会采用Base64Url编码，得到第二部分数据。 Signature：签名，是整个数据的认证信息。一般根据前两步的数据，再加上服务的的密钥secret（密钥保存在服务端，不能泄露给客户端），通过Header中配置的加密算法生成。用于验证整个数据完整和可靠性。 生成的数据格式如下图： 注意，这里的数据通过 . 隔开成了三部分，分别对应前面提到的三部分，另外，这里数据是不换行的，图片换行只是为了展示方便而已。 1.4.3 JWT交互流程流程图： 步骤翻译： 应用程序或客户端向授权服务器请求授权 获取到授权后，授权服务器会向应用程序返回访问令牌 应用程序使用访问令牌来访问受保护资源（如API） 因为JWT签发的token中已经包含了用户的身份信息，并且每次请求都会携带，这样服务的就无需保存用户信息，甚至无需去数据库查询，这样就完全符合了RESTful的无状态规范。 1.5 JWT 存在的问题说了这么多，JWT 也不是天衣无缝，由客户端维护登录状态带来的一些问题在这里依然存在，举例如下： 续签问题，这是被很多人诟病的问题之一，传统的cookie+session的方案天然的支持续签，但是jwt由于服务端不保存用户状态，因此很难完美解决续签问题，如果引入redis，虽然可以解决问题，但是jwt也变得不伦不类了。 注销问题，由于服务端不再保存用户信息，所以一般可以通过修改secret来实现注销，服务端secret修改后，已经颁发的未过期的token就会认证失败，进而实现注销，不过毕竟没有传统的注销方便。 密码重置，密码重置后，原本的token依然可以访问系统，这时候也需要强制修改secret。 基于第2点和第3点，一般建议不同用户取不同secret。 2 实战说了这么久，接下来我们就来看看这个东西到底要怎么用？ 2.1 环境搭建首先我们来创建一个Spring Boot项目，创建时需要添加Spring Security依赖，创建完成后，添加 jjwt 依赖，完整的pom.xml文件如下： 12345678910111213&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.1&lt;/version&gt;&lt;/dependency&gt; 然后在项目中创建一个简单的 User 对象实现 UserDetails 接口，如下： 12345678910111213141516171819202122232425public class User implements UserDetails { private String username; private String password; private List&lt;GrantedAuthority&gt; authorities; public String getUsername() { return username; } @Override public boolean isAccountNonExpired() { return true; } @Override public boolean isAccountNonLocked() { return true; } @Override public boolean isCredentialsNonExpired() { return true; } @Override public boolean isEnabled() { return true; } //省略getter/setter} 这个就是我们的用户对象，先放着备用，再创建一个HelloController，内容如下：1234567891011@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello jwt !\"; } @GetMapping(\"/admin\") public String admin() { return \"hello admin !\"; }} HelloController 很简单，这里有两个接口，设计是 /hello 接口可以被具有 user 角色的用户访问，而 /admin 接口则可以被具有 admin 角色的用户访问。 2.2 JWT 过滤器配置接下来提供两个和 JWT 相关的过滤器配置： 一个是用户登录的过滤器，在用户的登录的过滤器中校验用户是否登录成功，如果登录成功，则生成一个token返回给客户端，登录失败则给前端一个登录失败的提示。 第二个过滤器则是当其他请求发送来，校验token的过滤器，如果校验成功，就让请求继续执行。 这两个过滤器，我们分别来看，先看第一个： 1234567891011121314151617181920212223242526272829303132333435363738public class JwtLoginFilter extends AbstractAuthenticationProcessingFilter { protected JwtLoginFilter(String defaultFilterProcessesUrl, AuthenticationManager authenticationManager) { super(new AntPathRequestMatcher(defaultFilterProcessesUrl)); setAuthenticationManager(authenticationManager); } @Override public Authentication attemptAuthentication(HttpServletRequest req, HttpServletResponse resp) throws AuthenticationException, IOException, ServletException { User user = new ObjectMapper().readValue(req.getInputStream(), User.class); return getAuthenticationManager().authenticate(new UsernamePasswordAuthenticationToken(user.getUsername(), user.getPassword())); } @Override protected void successfulAuthentication(HttpServletRequest req, HttpServletResponse resp, FilterChain chain, Authentication authResult) throws IOException, ServletException { Collection&lt;? extends GrantedAuthority&gt; authorities = authResult.getAuthorities(); StringBuffer as = new StringBuffer(); for (GrantedAuthority authority : authorities) { as.append(authority.getAuthority()) .append(\",\"); } String jwt = Jwts.builder() .claim(\"authorities\", as)//配置用户角色 .setSubject(authResult.getName()) .setExpiration(new Date(System.currentTimeMillis() + 10 * 60 * 1000)) .signWith(SignatureAlgorithm.HS512,\"sang@123\") .compact(); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(jwt)); out.flush(); out.close(); } protected void unsuccessfulAuthentication(HttpServletRequest req, HttpServletResponse resp, AuthenticationException failed) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"登录失败!\"); out.flush(); out.close(); }} 关于这个类，我说如下几点： 自定义 JwtLoginFilter 继承自 AbstractAuthenticationProcessingFilter，并实现其中的三个默认方法。 attemptAuthentication方法中，我们从登录参数中提取出用户名密码，然后调用AuthenticationManager.authenticate()方法去进行自动校验。 第二步如果校验成功，就会来到successfulAuthentication回调中，在successfulAuthentication方法中，将用户角色遍历然后用一个 , 连接起来，然后再利用Jwts去生成token，按照代码的顺序，生成过程一共配置了四个参数，分别是用户角色、主题、过期时间以及加密算法和密钥，然后将生成的token写出到客户端。 第二步如果校验失败就会来到unsuccessfulAuthentication方法中，在这个方法中返回一个错误提示给客户端即可。 再来看第二个token校验的过滤器： 123456789101112131415public class JwtFilter extends GenericFilterBean { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest req = (HttpServletRequest) servletRequest; String jwtToken = req.getHeader(\"authorization\"); System.out.println(jwtToken); Claims claims = Jwts.parser().setSigningKey(\"sang@123\").parseClaimsJws(jwtToken.replace(\"Bearer\",\"\")) .getBody(); String username = claims.getSubject();//获取当前登录用户名 List&lt;GrantedAuthority&gt; authorities = AuthorityUtils.commaSeparatedStringToAuthorityList((String) claims.get(\"authorities\")); UsernamePasswordAuthenticationToken token = new UsernamePasswordAuthenticationToken(username, null, authorities); SecurityContextHolder.getContext().setAuthentication(token); filterChain.doFilter(req,servletResponse); }} 关于这个过滤器，我说如下几点： 首先从请求头中提取出 authorization 字段，这个字段对应的value就是用户的token。 将提取出来的token字符串转换为一个Claims对象，再从Claims对象中提取出当前用户名和用户角色，创建一个UsernamePasswordAuthenticationToken放到当前的Context中，然后执行过滤链使请求继续执行下去。 如此之后，两个和JWT相关的过滤器就算配置好了。 2.3 Spring Security 配置接下来我们来配置 Spring Security,如下： 1234567891011121314151617181920212223242526272829@Configurationpublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Bean PasswordEncoder passwordEncoder() { return NoOpPasswordEncoder.getInstance(); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication().withUser(\"admin\") .password(\"123\").roles(\"admin\") .and() .withUser(\"sang\") .password(\"456\") .roles(\"user\"); } @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .antMatchers(\"/hello\").hasRole(\"user\") .antMatchers(\"/admin\").hasRole(\"admin\") .antMatchers(HttpMethod.POST, \"/login\").permitAll() .anyRequest().authenticated() .and() .addFilterBefore(new JwtLoginFilter(\"/login\",authenticationManager()),UsernamePasswordAuthenticationFilter.class) .addFilterBefore(new JwtFilter(),UsernamePasswordAuthenticationFilter.class) .csrf().disable(); }} 简单起见，这里我并未对密码进行加密，因此配置了NoOpPasswordEncoder的实例。 简单起见，这里并未连接数据库，我直接在内存中配置了两个用户，两个用户具备不同的角色。 配置路径规则时， /hello 接口必须要具备 user 角色才能访问， /admin 接口必须要具备 admin 角色才能访问，POST 请求并且是 /login 接口则可以直接通过，其他接口必须认证后才能访问。 最后配置上两个自定义的过滤器并且关闭掉csrf保护。 2.4 测试做完这些之后，我们的环境就算完全搭建起来了，接下来启动项目然后在 POSTMAN 中进行测试，如下： 登录成功后返回的字符串就是经过 base64url 转码的token，一共有三部分，通过一个 . 隔开，我们可以对第一个 . 之前的字符串进行解码，即Header，如下： 再对两个 . 之间的字符解码，即 payload： 可以看到，我们设置信息，由于base64并不是加密方案，只是一种编码方案，因此，不建议将敏感的用户信息放到token中。 接下来再去访问 /hello 接口，注意认证方式选择 Bearer Token，Token值为刚刚获取到的值，如下： 可以看到，访问成功。 总结这就是 JWT 结合 Spring Security 的一个简单用法，讲真，如果实例允许，类似的需求我还是推荐使用 OAuth2 中的 password 模式。 不知道大伙有没有看懂呢？如果没看懂，松哥还有一个关于这个知识点的视频教程，如下： 如何获取这个视频教程呢？很简单，将本文转发到一个超过100人的微信群中(QQ群不算，松哥是群主的微信群也不算，群要为Java方向)，或者多个微信群中，只要累计人数达到100人即可，然后加松哥微信，截图发给松哥即可获取资料。","link":"/2019/0408/springboot-jwt.html"},{"title":"徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！","text":"我们使用 Spring Boot，基本上都是沉醉在它 Stater 的方便之中。Starter 为我们带来了众多的自动化配置，有了这些自动化配置，我们可以不费吹灰之力就能搭建一个生产级开发环境，有的小伙伴会觉得这个 Starter 好神奇呀！其实 Starter 也都是 Spring + SpringMVC 中的基础知识点实现的，今天松哥就来带大家自己来撸一个 Starter ，慢慢揭开 Starter 的神秘面纱！ 核心知识其实 Starter 的核心就是条件注解 @Conditional ，当 classpath 下存在某一个 Class 时，某个配置才会生效，前面松哥已经带大家学习过不少 Spring Boot 中的知识点，有的也涉及到源码解读，大伙可能也发现了源码解读时总是会出现条件注解，其实这就是 Starter 配置的核心之一，大伙有兴趣可以翻翻历史记录，看看松哥之前写的关于 Spring Boot 的文章，这里我就不再重复介绍了。 定义自己的 Starter定义所谓的 Starter ，其实就是一个普通的 Maven 项目，因此我们自定义 Starter ，需要首先创建一个普通的 Maven 项目，创建完成后，添加 Starter 的自动化配置类即可，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 配置完成后，我们首先创建一个 HelloProperties 类，用来接受 application.properties 中注入的值，如下： 12345678910111213141516171819@ConfigurationProperties(prefix = \"javaboy\")public class HelloProperties { private static final String DEFAULT_NAME = \"江南一点雨\"; private static final String DEFAULT_MSG = \"牧码小子\"; private String name = DEFAULT_NAME; private String msg = DEFAULT_MSG; public String getName() { return name; } public void setName(String name) { this.name = name; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; }} 这个配置类很好理解，将 application.properties 中配置的属性值直接注入到这个实例中， @ConfigurationProperties 类型安全的属性注入，即将 application.properties 文件中前缀为 javaboy 的属性注入到这个类对应的属性上， 最后使用时候，application.properties 中的配置文件，大概如下： 12javaboy.name=zhangsanjavaboy.msg=java 关注类型安全的属性注入，读者可以参考松哥之前的这篇文章：Spring Boot中的yaml配置简介，这篇文章虽然是讲 yaml 配置，但是关于类型安全的属性注入和 properties 是一样的。 配置完成 HelloProperties 后，接下来我们来定义一个 HelloService ，然后定义一个简单的 say 方法， HelloService 的定义如下： 12345678910111213141516171819public class HelloService { private String msg; private String name; public String sayHello() { return name + \" say \" + msg + \" !\"; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public String getName() { return name; } public void setName(String name) { this.name = name; }} 这个很简单，没啥好说的。 接下来就是我们的重轴戏，自动配置类的定义，用了很多别人定义的自定义类之后，我们也来自己定义一个自定义类。先来看代码吧，一会松哥再慢慢解释： 123456789101112131415@Configuration@EnableConfigurationProperties(HelloProperties.class)@ConditionalOnClass(HelloService.class)public class HelloServiceAutoConfiguration { @Autowired HelloProperties helloProperties; @Bean HelloService helloService() { HelloService helloService = new HelloService(); helloService.setName(helloProperties.getName()); helloService.setMsg(helloProperties.getMsg()); return helloService; }} 关于这一段自动配置，解释如下： 首先 @Configuration 注解表明这是一个配置类。 @EnableConfigurationProperties 注解是使我们之前配置的 @ConfigurationProperties 生效，让配置的属性成功的进入 Bean 中。 @ConditionalOnClass 表示当项目当前 classpath 下存在 HelloService 时，后面的配置才生效。 自动配置类中首先注入 HelloProperties ，这个实例中含有我们在 application.properties 中配置的相关数据。 提供一个 HelloService 的实例，将 HelloProperties 中的值注入进去。 做完这一步之后，我们的自动化配置类就算是完成了，接下来还需要一个 spring.factories 文件，那么这个文件是干嘛的呢？大家知道我们的 Spring Boot 项目的启动类都有一个 @SpringBootApplication 注解，这个注解的定义如下： 12345678@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })public @interface SpringBootApplication {} 大家看到这是一个组合注解，其中的一个组合项就是 @EnableAutoConfiguration ，这个注解是干嘛的呢？ @EnableAutoConfiguration 表示启用 Spring 应用程序上下文的自动配置，该注解会自动导入一个名为 AutoConfigurationImportSelector 的类,而这个类会去读取一个名为 spring.factories 的文件, spring.factories 中则定义需要加载的自动化配置类，我们打开任意一个框架的 Starter ，都能看到它有一个 spring.factories 文件，例如 MyBatis 的 Starter 如下： 那么我们自定义 Starter 当然也需要这样一个文件，我们首先在 Maven 项目的 resources 目录下创建一个名为 META-INF 的文件夹，然后在文件夹中创建一个名为 spring.factories 的文件，文件内容如下： 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=org.javaboy.mystarter.HelloServiceAutoConfiguration 在这里指定我们的自动化配置类的路径即可。 如此之后我们的自动化配置类就算完成了。 本地安装如果在公司里，大伙可能需要将刚刚写好的自动化配置类打包，然后上传到 Maven 私服上，供其他同事下载使用，我这里就简单一些，我就不上传私服了，我将这个自动化配置类安装到本地仓库，然后在其他项目中使用即可。安装方式很简单，在 IntelliJ IDEA 中，点击右边的 Maven Project ，然后选择 Lifecycle 中的 install ，双击即可，如下： 双击完成后，这个 Starter 就安装到我们本地仓库了，当然小伙伴也可以使用 Maven 命令去安装。 使用 Starter接下来，我们来新建一个普通的 Spring Boot 工程，这个 Spring Boot 创建成功之后，加入我们自定义 Starter 的依赖，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.javaboy&lt;/groupId&gt; &lt;artifactId&gt;mystarter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 此时我们引入了上面自定义的 Starter ，也即我们项目中现在有一个默认的 HelloService 实例可以使用，而且关于这个实例的数据，我们还可以在 application.properties 中进行配置，如下： 12javaboy.name=牧码小子javaboy.msg=java 配置完成后，方便起见，我这里直接在单元测试方法中注入 HelloSerivce 实例来使用，代码如下： 1234567891011@RunWith(SpringRunner.class)@SpringBootTestpublic class UsemystarterApplicationTests { @Autowired HelloService helloService; @Test public void contextLoads() { System.out.println(helloService.sayHello()); }} 执行单元测试方法，打印日志如下： 好了，一个简单的自动化配置类我们就算完成了，是不是很简单！ 总结本文主要带领小伙伴自己徒手撸一个 Starter ，使用这种方式帮助大家揭开 Starter 的神秘面纱！大伙有问题可以留言讨论。 本文的案例，松哥已经上传到 GitHub上了，地址：https://github.com/lenve/javaboy-code-samples 。","link":"/2019/0520/springboot-starter.html"},{"title":"手把手带你入门 Spring Security！","text":"Spring Security 是 Spring 家族中的一个安全管理框架，实际上，在 Spring Boot 出现之前，Spring Security 就已经发展了多年了，但是使用的并不多，安全管理这个领域，一直是 Shiro 的天下。 相对于 Shiro，在 SSM/SSH 中整合 Spring Security 都是比较麻烦的操作，所以，Spring Security 虽然功能比 Shiro 强大，但是使用反而没有 Shiro 多（Shiro 虽然功能没有 Spring Security 多，但是对于大部分项目而言，Shiro 也够用了）。 自从有了 Spring Boot 之后，Spring Boot 对于 Spring Security 提供了 自动化配置方案，可以零配置使用 Spring Security。 因此，一般来说，常见的安全管理技术栈的组合是这样的： SSM + Shiro Spring Boot/Spring Cloud + Spring Security 注意，这只是一个推荐的组合而已，如果单纯从技术上来说，无论怎么组合，都是可以运行的。 我们来看下具体使用。 1.项目创建在 Spring Boot 中使用 Spring Security 非常容易，引入依赖即可： pom.xml 中的 Spring Security 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 只要加入依赖，项目的所有接口都会被自动保护起来。 2.初次体验我们创建一个 HelloController: 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} 访问 /hello ，需要登录之后才能访问。 当用户从浏览器发送请求访问 /hello 接口时，服务端会返回 302 响应码，让客户端重定向到 /login 页面，用户在 /login 页面登录，登陆成功之后，就会自动跳转到 /hello 接口。 另外，也可以使用 POSTMAN 来发送请求，使用 POSTMAN 发送请求时，可以将用户信息放在请求头中（这样可以避免重定向到登录页面）： 通过以上两种不同的登录方式，可以看出，Spring Security 支持两种不同的认证方式： 可以通过 form 表单来认证 可以通过 HttpBasic 来认证 3.用户名配置默认情况下，登录的用户名是 user ，密码则是项目启动时随机生成的字符串，可以从启动的控制台日志中看到默认密码： 这个随机生成的密码，每次启动时都会变。对登录的用户名/密码进行配置，有三种不同的方式： 在 application.properties 中进行配置 通过 Java 代码配置在内存中 通过 Java 从数据库中加载 前两种比较简单，第三种代码量略大，本文就先来看看前两种，第三种后面再单独写文章介绍，也可以参考我的微人事项目。 3.1 配置文件配置用户名/密码可以直接在 application.properties 文件中配置用户的基本信息： 12spring.security.user.name=javaboyspring.security.user.password=123 配置完成后，重启项目，就可以使用这里配置的用户名/密码登录了。 3.2 Java 配置用户名/密码也可以在 Java 代码中配置用户名密码，首先需要我们创建一个 Spring Security 的配置类，集成自 WebSecurityConfigurerAdapter 类，如下： 123456789101112131415@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { //下面这两行配置表示在内存中配置了两个用户 auth.inMemoryAuthentication() .withUser(\"javaboy\").roles(\"admin\").password(\"$2a$10$OR3VSksVAmCzc.7WeaRPR.t0wyCsIj24k0Bne8iKWV1o.V9wsP8Xe\") .and() .withUser(\"lisi\").roles(\"user\").password(\"$2a$10$p1H8iWa8I4.CA.7Z8bwLjes91ZpY.rYREGHQEInNtAp4NzL6PLKxi\"); } @Bean PasswordEncoder passwordEncoder() { return new BCryptPasswordEncoder(); }} 这里我们在 configure 方法中配置了两个用户，用户的密码都是加密之后的字符串(明文是 123)，从 Spring5 开始，强制要求密码要加密，如果非不想加密，可以使用一个过期的 PasswordEncoder 的实例 NoOpPasswordEncoder，但是不建议这么做，毕竟不安全。 Spring Security 中提供了 BCryptPasswordEncoder 密码编码工具，可以非常方便的实现密码的加密加盐，相同明文加密出来的结果总是不同，这样就不需要用户去额外保存盐的字段了，这一点比 Shiro 要方便很多。 4.登录配置对于登录接口，登录成功后的响应，登录失败后的响应，我们都可以在 WebSecurityConfigurerAdapter 的实现类中进行配置。例如下面这样： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Autowired VerifyCodeFilter verifyCodeFilter; @Override protected void configure(HttpSecurity http) throws Exception { http.addFilterBefore(verifyCodeFilter, UsernamePasswordAuthenticationFilter.class); http .authorizeRequests()//开启登录配置 .antMatchers(\"/hello\").hasRole(\"admin\")//表示访问 /hello 这个接口，需要具备 admin 这个角色 .anyRequest().authenticated()//表示剩余的其他接口，登录之后就能访问 .and() .formLogin() //定义登录页面，未登录时，访问一个需要登录之后才能访问的接口，会自动跳转到该页面 .loginPage(\"/login_p\") //登录处理接口 .loginProcessingUrl(\"/doLogin\") //定义登录时，用户名的 key，默认为 username .usernameParameter(\"uname\") //定义登录时，用户密码的 key，默认为 password .passwordParameter(\"passwd\") //登录成功的处理器 .successHandler(new AuthenticationSuccessHandler() { @Override public void onAuthenticationSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"success\"); out.flush(); } }) .failureHandler(new AuthenticationFailureHandler() { @Override public void onAuthenticationFailure(HttpServletRequest req, HttpServletResponse resp, AuthenticationException exception) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"fail\"); out.flush(); } }) .permitAll()//和表单登录相关的接口统统都直接通过 .and() .logout() .logoutUrl(\"/logout\") .logoutSuccessHandler(new LogoutSuccessHandler() { @Override public void onLogoutSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"logout success\"); out.flush(); } }) .permitAll() .and() .httpBasic() .and() .csrf().disable(); }} 我们可以在 successHandler 方法中，配置登录成功的回调，如果是前后端分离开发的话，登录成功后返回 JSON 即可，同理，failureHandler 方法中配置登录失败的回调，logoutSuccessHandler 中则配置注销成功的回调。 5.忽略拦截如果某一个请求地址不需要拦截的话，有两种方式实现： 设置该地址匿名访问 直接过滤掉该地址，即该地址不走 Spring Security 过滤器链 推荐使用第二种方案，配置如下： 1234567@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Override public void configure(WebSecurity web) throws Exception { web.ignoring().antMatchers(\"/vercode\"); }} Spring Security 另外一个强大之处就是它可以结合 OAuth2 ，玩出更多的花样出来，这些我们在后面的文章中再和大家细细介绍。 本文就先说到这里，有问题欢迎留言讨论。","link":"/2019/0725/springboot-springsecurity.html"},{"title":"数据库分库分表，都有哪些分片规则？","text":"上次和大伙聊了 MyCat 的安装，今天来说一个新的话题，就是数据库的分片。 当我们把 MyCat + MySQL 的架构搭建完成之后，接下来面临的一个问题就是，数据库的分片规则：有那么多 MySQL ，一条记录通过 MyCat 到底要插入到哪个 MySQL 中？这就是我们今天要讨论的问题。 关于数据库分库分表的问题，我们前面还有几篇铺垫的文章，阅读前面的文章有助于更好的理解本文： 提高性能，MySQL 读写分离环境搭建(一) 提高性能，MySQL 读写分离环境搭建(二) MySQL 只能做小项目？松哥要说几句公道话！ 北冥有 Data，其名为鲲，鲲之大，一个 MySQL 放不下！ What？Tomcat 竟然也算中间件？ 分布式数据库中间件 MyCat 搞起来！ 基本概念逻辑库一般来说，对于应用而言，数据库中间件是透明的，应用并不需要去了解中间件复杂的运作过程，中间件对应用来说就是透明的，我们操作中间件就像操作一个普通的 MySQL 一样，这就是 MyCat 的优势之一。 但是我们毕竟操作的不是 MySQL ，而是 MyCat ，MyCat 中的数据库并不真正存储数据，数据还是存储在 MySQL 中，因此，我们可以将 MyCat 看作是一个或者多个数据库集群构成的逻辑库。 逻辑表逻辑表又有几种不同的划分： 逻辑表 既然有逻辑库，那么就会有逻辑表。 因为数据库分片之后，本来存储在一张表中的数据现在被分散到 N 张表中去了，但是在应用程序眼里，还是只有一张表，它也只操作这一张表，这张表并不真正存储数据，数据存储在 N 张物理表中，这个并不真正存储数据的表称之为逻辑表。 分片表 分片表，是指那些原有的很大数据的表，需要切分到多个数据库的表，这样，每个分片都有一部分数据，所有分片构成了完整的数据。 非分片表 一个数据库中并不是所有的表都很大，某些表是可以不用进行切分的，非分片是相对分片表来说的，就是那些不需要进行数据切分的表。 ER 表 关系型数据库是基于实体关系模型之上，通过其描述了真实世界中事物与关系，Mycat 中的 ER 表即是来源于此。根据这一思路，提出了基于 E-R 关系的数据分片策略，子表的记录与所关联的父表记录存放在同一个数据分片上，即子表依赖于父表，通过表分组保证数据 join 不会跨库操作。 表分组是解决跨分片数据 join 的一种很好的思路，也是数据切分规划的重要一条规则。 全局表 一个真实的业务系统中，往往存在大量的类似字典表的表，这些表基本上很少变动，字典表具有以下几个特性： 变动不频繁 数据量总体变化不大 数据规模不大，很少有超过数十万条记录 对于这类的表，在分片的情况下，当业务表因为规模而进行分片以后，业务表与这些附属的字典表之间的关联，就成了比较棘手的问题，所以 MyCat 中通过数据冗余来解决这类表的 join ，即所有的分片都有一份数据的拷贝，所有将字典表或者符合字典表特性的一些表定义为全局表。 数据冗余是解决跨分片数据 join 的一种很好的思路，也是数据切分规划的另外一条重要规则。 分片节点数据切分后，一个大表被分到不同的分片数据库上面，每个表分片所在的数据库就是分片节点（dataNode）。 节点主机数据切分后，每个分片节点（dataNode）不一定都会独占一台机器，同一机器上面可以有多个分片数据库，这样一个或多个分片节点（dataNode）所在的机器就是节点主机（dataHost）,为了规避单节点主机并发数限制，尽量将读写压力高的分片节点（dataNode）均衡的放在不同的节点主机（dataHost）。 分片规则前面讲了数据切分，一个大表被分成若干个分片表，就需要一定的规则，这样按照某种业务规则把数据分到某个分片的规则就是分片规则，数据切分选择合适的分片规则非常重要，将极大的避免后续数据处理的难度。 MyCat 提供的分片规则有如下几种： 分片枚举 固定分片 hash 算法 范围约定 取模 按日期（天）分片 取模范围约束 截取数字做 hash 求模范围约束 应用指定 截取数字 hash 解析 一致性 hash 按单月小时拆分 范围求模分片 日期范围 hash 分片 冷热数据分片 自然月分片 实践这里向大家简单介绍 5 种规则。 global有一些表，数据量不大，也不怎么修改，主要是查询操作，例如系统配置表，这一类表我们可以使用 global 这种分片规则。global 的特点是，该表会在所有的库中都创建，而且每一个库中都保存了该表的完整数据。具体配置方式，就是在 schema.xml 的 table 节点中添加一个 type 属性，值为 global： 配置完成后，重启 mycat 1./bin/mycat restart 重启完成后，要删除之前已经创建的 t_user 表，然后重新创建表，创建完成后，向表中插入数据，可以看到，db1、db2 以及 db3 中都有数据了。 这里 虽然查询出来的记录只有一条，实际上 db1、db2 以及 db3 中都有该条记录。 总结：global 适合于 数据量不大、以查询为主、增删改较少的表。 sharding-by-intfilesharding-by-intfile 这个是枚举分片，就是在数据表中专门设计一个字段，以后根据这个字段的值来决定数据插入到哪个 dataNode 上。 注意，在配置 sharding-by-intfile 规则时，一定要删除 type=”global” ，否则配置不会生效。具体配置如下： 配置完成后，还需要指定枚举的数据。枚举的数据可以在 rule.xml 中查看。 在 rule.xml 文件中，首先找到 tableRule 的名字为 sharding-by-intfile 的节点，这个节点中定义了两个属性，一个是 columns 表示一会在数据表中定义的枚举列的名字（数据表中一会需要创建一个名为 sharding_id 的列，这个列的值决定了该条数据保存在哪个数据库实例中），这个名字可以自定义；另外一个属性叫做 algorithm ，这是指 sharding-by-intfile 所对应的算法名称。根据这个名称，可以找到具体的算法： 还是在 rule.xml 文件中，我们找到了 hash-int ，class 表示这个算法对应的 Java 类的路径。第一个属性 mapFile 表示相关的配置文件，从这个文件名可以看出，这个文件 就在 conf 目录下。 打开 conf 目录下的 partition-hash-int.txt 文件，内容如下： 前面的数字表示枚举的值 ，后面的数字表示 dataNode 的下标，所以前面的数字可以自定义，后面的数字不能随意定义。 配置完成后，重启 MyCat ，然后进行测试： 123456drop table if exists t_user;create table t_user (id integer primary key,username varchar(255),sharding_id integer);insert into t_user(id,username,sharding_id) values(1,&apos;www.javaboy.org&apos;,0);insert into t_user(id,username,sharding_id) values(1,&apos;www.javaboy.org&apos;,1);insert into t_user(id,username,sharding_id) values(1,&apos;www.javaboy.org&apos;,2);select * from t_user; 执行完后，sharding_id 对应值分别为 0 、1 、2 的记录分别插入到 db1 、db2 以及 db3 中。 auto-sharding-longauto-sharding-long 表示按照既定的范围去存储数据。就是提前规划好某个字段的值在某个范围时，相应的记录存到某个 dataNode 中。 配置方式，首先修改路由规则： 然后去 rule.xml 中查看对应的算法了规则相关的配置： 可以看到，默认是按照 id 的范围来划分数据的存储位置的，对应的算法就是 rang-long 。 继续查看，可以找到算法对应的类，以及相关的配置文件，这个配置文件也在 conf 目录下，打开该文件： 如上配置，表示 当 id 的取值在 0-5之间时，将数据存储到 db1 中，当 id 在 5-10 之间时，存储到 db2 中，当 id 的取值在 10-1500W 之间时，存储到 db3 中。 配置完成后，重启 MyCat ，测试： mod-long取模：根据表中的某一个字段，做取模操作。根据取模的结果将记录存放在不同的 dataNode 上。这种方式不需要再添加额外字段。 然后去 rule.xml 中配置一下 dataNode 的个数。 可以看到，取模的字段是 id ，取模的算法名称是 mod-long ，再看具体的算法： 在具体的算法中，配置了 dataNode 的个数为 3。 然后保存退出，重启 MyCat，进行测试： sharding-by-murmur前面介绍的几种方式，都存在一个问题，如果数据库要扩容，之前配置会失效，可能会出现数据库查询紊乱。因此我们要引入一致性 hash 这样一种分片规则，可以解决这个问题。具体配置和前面一样： 另外需要注意，在 rule.xml 中修改默认 dataNode 的数量： 修改完后，重启 MyCat ，进行测试。 好了，本文主要向大家介绍了 MyCat 的五种不同的切片规则。有问题欢迎留言讨论。 参考资料： MyCat 官网","link":"/2019/0712/mycat-rule.html"},{"title":"是时候了解下Spring Boot整合 Jpa啦","text":"Spring Boot中的数据持久化方案前面给大伙介绍了两种了，一个是JdbcTemplate，还有一个MyBatis，JdbcTemplate配置简单，使用也简单，但是功能也非常有限，MyBatis则比较灵活，功能也很强大，据我所知，公司采用MyBatis做数据持久化的相当多，但是MyBatis并不是唯一的解决方案，除了MyBatis之外，还有另外一个东西，那就是Jpa，松哥也有一些朋友在公司里使用Jpa来做数据持久化，本文就和大伙来说说Jpa如何实现数据持久化。 Jpa介绍首先需要向大伙介绍一下Jpa，Jpa（Java Persistence API）Java持久化API，它是一套ORM规范，而不是具体的实现，Jpa的江湖地位类似于JDBC，只提供规范，所有的数据库厂商提供实现（即具体的数据库驱动），Java领域，小伙伴们熟知的ORM框架可能主要是Hibernate，实际上，除了Hibernate之外，还有很多其他的ORM框架，例如： Batoo JPA DataNucleus (formerly JPOX) EclipseLink (formerly Oracle TopLink) IBM, for WebSphere Application Server JBoss with Hibernate Kundera ObjectDB OpenJPA OrientDB from Orient Technologies Versant Corporation JPA (not relational, object database) Hibernate只是ORM框架的一种，上面列出来的ORM框架都是支持JPA2.0规范的ORM框架。既然它是一个规范，不是具体的实现，那么必然就不能直接使用（类似于JDBC不能直接使用，必须要加了驱动才能用），我们使用的是具体的实现，在这里我们采用的实现实际上还是Hibernate。 Spring Boot中使用的Jpa实际上是Spring Data Jpa，Spring Data是Spring家族的一个子项目，用于简化SQL和NoSQL的访问，在Spring Data中，只要你的方法名称符合规范，它就知道你想干嘛，不需要自己再去写SQL。 工程创建创建Spring Boot工程，添加Web、Jpa以及MySQL驱动依赖，如下： 工程创建好之后，添加Druid依赖，完整的依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 如此，工程就算创建成功了。 基本配置工程创建完成后，只需要在application.properties中进行数据库基本信息配置以及Jpa基本配置，如下： 123456789101112131415# 数据库的基本配置spring.datasource.username=rootspring.datasource.password=rootspring.datasource.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=UTF-8spring.datasource.type=com.alibaba.druid.pool.DruidDataSource# JPA配置spring.jpa.database=mysql# 在控制台打印SQLspring.jpa.show-sql=true# 数据库平台spring.jpa.database-platform=mysql# 每次启动项目时，数据库初始化策略spring.jpa.hibernate.ddl-auto=update# 指定默认的存储引擎为InnoDBspring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL57Dialect 注意这里和JdbcTemplate以及MyBatis比起来，多了Jpa配置，Jpa配置含义我都注释在代码中了，这里不再赘述，需要强调的是，最后一行配置，默认情况下，自动创建表的时候会使用MyISAM做表的引擎，如果配置了数据库方言为MySQL57Dialect，则使用InnoDB做表的引擎。 好了，配置完成后，我们的Jpa差不多就可以开始用了。 基本用法ORM(Object Relational Mapping)框架表示对象关系映射，使用ORM框架我们不必再去创建表，框架会自动根据当前项目中的实体类创建相应的数据表。因此，我这里首先创建一个User对象，如下： 12345678910@Entity(name = \"t_user\")public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; @Column(name = \"name\") private String username; private String address; //省略getter/setter} 首先@Entity注解表示这是一个实体类，那么在项目启动时会自动针对该类生成一张表，默认的表名为类名，@Entity注解的name属性表示自定义生成的表名。@Id注解表示这个字段是一个id，@GeneratedValue注解表示主键的自增长策略，对于类中的其他属性，默认都会根据属性名在表中生成相应的字段，字段名和属性名相同，如果开发者想要对字段进行定制，可以使用@Column注解，去配置字段的名称，长度，是否为空等等。 做完这一切之后，启动Spring Boot项目，就会发现数据库中多了一个名为t_user的表了。 针对该表的操作，则需要我们提供一个Repository，如下： 123456public interface UserDao extends JpaRepository&lt;User,Integer&gt; { List&lt;User&gt; getUserByAddressEqualsAndIdLessThanEqual(String address, Integer id); @Query(value = \"select * from t_user where id=(select max(id) from t_user)\",nativeQuery = true) User maxIdUser();} 这里，自定义UserDao接口继承自JpaRepository，JpaRepository提供了一些基本的数据操作方法，例如保存，更新，删除，分页查询等，开发者也可以在接口中自己声明相关的方法，只需要方法名称符合规范即可，在Spring Data中，只要按照既定的规范命名方法，Spring Data Jpa就知道你想干嘛，这样就不用写SQL了，那么规范是什么呢？参考下图： 当然，这种方法命名主要是针对查询，但是一些特殊需求，可能并不能通过这种方式解决，例如想要查询id最大的用户，这时就需要开发者自定义查询SQL了，如上代码所示，自定义查询SQL，使用@Query注解，在注解中写自己的SQL，默认使用的查询语言不是SQL，而是JPQL，这是一种数据库平台无关的面向对象的查询语言，有点定位类似于Hibernate中的HQL，在@Query注解中设置nativeQuery属性为true则表示使用原生查询，即大伙所熟悉的SQL。上面代码中的只是一个很简单的例子，还有其他一些点，例如如果这个方法中的SQL涉及到数据操作，则需要使用@Modifying注解。 好了，定义完Dao之后，接下来就可以将UserDao注入到Controller中进行测试了(这里为了省事，就没有提供Service了，直接将UserDao注入到Controller中)。 1234567891011121314151617181920212223242526272829303132333435363738394041@RestControllerpublic class UserController { @Autowired UserDao userDao; @PostMapping(\"/\") public void addUser() { User user = new User(); user.setId(1); user.setUsername(\"张三\"); user.setAddress(\"深圳\"); userDao.save(user); } @DeleteMapping(\"/\") public void deleteById() { userDao.deleteById(1); } @PutMapping(\"/\") public void updateUser() { User user = userDao.getOne(1); user.setUsername(\"李四\"); userDao.flush(); } @GetMapping(\"/test1\") public void test1() { List&lt;User&gt; all = userDao.findAll(); System.out.println(all); } @GetMapping(\"/test2\") public void test2() { List&lt;User&gt; list = userDao.getUserByAddressEqualsAndIdLessThanEqual(\"广州\", 2); System.out.println(list); } @GetMapping(\"/test3\") public void test3() { User user = userDao.maxIdUser(); System.out.println(user); }} 如此之后，即可查询到需要的数据。 好了，本文的重点是Spring Boot和Jpa的整合，这个话题就先说到这里。 多说两句在和Spring框架整合时，如果用到ORM框架，大部分人可能都是首选Hibernate，实际上，在和Spring+SpringMVC整合时，也可以选择Spring Data Jpa做数据持久化方案，用法和本文所述基本是一样的，Spring Boot只是将Spring Data Jpa的配置简化了，因此，很多初学者对Spring Data Jpa觉得很神奇，但是又觉得无从下手，其实，此时可以回到Spring框架，先去学习Jpa，再去学习Spring Data Jpa，这是给初学者的一点建议。","link":"/2019/0407/springboot-jpa.html"},{"title":"一个野生程序员的自我修养","text":"6 月 25 对我来说是一个特殊的日子，2011 年 6 月 25 ，高考分数出来，我去了遥远的南方读书。2015 年 6 月 25 正式步入社会！每年到了这个日子，总不免唏嘘感慨！(今年 6 月 25 我的公众号读者突破了 1w,也算是一个特殊日子吧！) 关于高中高中非常幸运，也非常不幸！这得从两方面说起。 我的母亲虽然生活在农村，但是她在教育我时，非常有眼光。我们镇上的初中，大部分学生的最高目标就是读我们县城最好的高中，但是从我上初一开始，我母亲就希望我能努力学习，以后能去西安读更好的高中上更好的大学。中考结束后，我的成绩还算可以（县里排第二）。那几年，西安的高中喜欢到周边县里去挖人，我有幸被选中，去了西安读高中。高中三年不仅免去了学费，也免去了生活费（住宿+吃饭），相当于高中没有花费家里一分钱，对一个农村孩子来说，这省下了相当大一笔钱，因此我说非常幸运！ 到了西安后，我才发现，我中考的分数，在我高中那个火箭班里，排在中间的位置，我前面大概有 20-30 人左右，真是不敢想象城乡教育差距这么大！ 同学们穿着耐克、阿迪（很多牌子我也不认识，只是知道这很贵），我母亲在我们镇上打工一个月才 600 块钱，两个月的薪水估计才能买人家一件衣服，巨大的落差让我感到自卑，自卑又变得敏感孤僻，这种状态下，很难融入到同学中，经常独来独往，最后这种状态就影响到学习了，最后的自信也崩塌了。 放假回到家里，看到父母对我的期望，亲戚朋友都觉得我很厉害，只有我自己知道我是一只菜鸟。这种家人殷切期望叠加上自己糟糕的成绩，真是非常痛苦，我刚开始时候两周回一次家，后来高二高三基本一个学期才回去一次（虽然从学校到家里也就 1 个多小时），平时都呆在学校里，这样感觉会稍微好一点。 高中三年，就是这样浑浑噩噩，孤独、自卑、敏感 ，进而变得冷漠、自私，真是艰难的三年。这也是我说不幸的原因。 虽然一直状态很差，但是我并没有放弃自己，还是艰难的向前走，从来没有自暴自弃。高考虽然没有达到预期分数，可是也没有太差劲。 不过话说回来，从农村到城市，这个过程总是要经历，高中经历这么大落差，大学就会好很多了，落差不会这么大。 关于大学2011 年 6 月 25 ，高考分数出来，我基本上没有考虑西安的学校，高中三年，西安真的呆够了。 从小到大，去过最远的地方就是高一军训的时候从西安去了渭南，坐了 4 个小时大巴车，激动。这次读大学，我想趁机出趟远门，想离西安远一点，甚至不太想再回来。 大学学费一年 4600 ，家里有压力，后来经过县上红十字会牵线，一个陕北的叔叔资助了我的大学学费，非常感动。 这几年一直在外奔波，其实我很想有一天也能做类似的事情，4600 一个程序员可能不到一个礼拜就赚到了，资助一个大学生读四年书也没什么压力，但是可能改变的是一个人的一生，大部分程序员可能都生活在一线城市，可能很难想象西北地区或者西南地区一些农村的贫困状态。 大学离家很远，也是我第一次出远门，但是一走就是一年半。大二过年时候才回家，也是我大学四年唯一一次在家里过年。 大一寒假没回家，呆在学校盘算着赚点生活费，思来想去只有做家教能赚到钱，也相对轻松一些。然后期末考试结束之后，我就印刷了几十张自我介绍，附上电话，趁着夜色，去附近的公交车站、小区门口都给张贴上，我心想只要有一个电话来，这个东西就算没有白贴。回到宿舍等电话，焦急又期待，要是寒假没有找到合适的工作，白白在学校呆一个月啥事没干，那就太浪费了，而且生活费还有点捉襟见肘，当时想着，如果没人来电话咨询，那我第二天就去更远的小区张贴广告。 还好后面有两个家长来电话咨询，一个是初中生，另一个是小学生，那个初中生后来足足被我教了 1 年半，直到我后来决定 all in Java 才没有继续做家教了。 两个家教，一个是下午，另一个是晚上。我放假之前从学校图书馆借了很多书，每天早上起床后，先在宿舍看书到 12 点左右，然后去吃午饭，吃完就从学校出发，先去第一个初中生那里，上完课大概到下午 5 点左右，然后出来吃个饭，再坐公交车在 7 点之前到另外一家。 有一天晚上九点上完课，外面下大雨，南方冬天的雨很冷，又夹杂着大风，孩子家长给我了 20 块钱让我打车回去，不过我最终还是选择坐公交车回学校了，打车这种高级玩意还是不太舍得，到学校后衣服都湿透了，就是冷。印象中那天好像还是南方的小年夜。 我从家里去学校的时候，以为海南的冬天很暖和，没有带冬天的衣服，实际海口的冬天也冷的要命，那个时候京东淘宝也还不太会玩，不懂网上购物，也就没买衣服，就那样哆哆嗦嗦的过完了冬天。 大学做家教，赚了一点点钱，够当时花了，那个时候觉得做家教赚钱简直太容易了。现在回头看看，一个学期辛辛苦苦赚个千把块，做程序员一下就赚回来了。所以我觉得大学还是多多学习专业知识，多多提高专业技能，兼职偶尔做一下也能接受，要是本末倒置了，毕业之后可能就会后悔了。 其实我不太喜欢跟别人讲这些事，因为我觉得没有谁是容易的，每个人都有自己的心酸，有时间唠叨这个还不如去做点实在的事情提高自己。不过偶尔回忆一下还是可以的，可以鞭策自己继续奋斗，不要放弃。 关于 Java大学学习基本上中规中矩，成绩没有名列前茅，但是也没挂过科，虽然我后面侧重于搞开发，但是专业课也没有荒废，大四拿了优秀毕业论文，算是给大学画上一个完美句号。 大一的时候一心想转到中文系，没能成功。 大二时候，校公选课无意间选修了 Java语言程序设计 ，从此打开了一个新世界，后来就走上了自学 Java 的道路。 刚开始的时候是最艰难的，关于计算机的很多概念都是空白的，学习也没有方向，JavaSE 学完之后都不知道干啥，jsp、html 啥的全都不懂。 后来在大二第二学期遇到了张老师，非常专业也非常敬业的一位老师，美中不足的是老师做 .NET ，而我当时对 .NET 实在提不起兴趣。但是老师给我指明了很多学习的方向，前端、跨平台、Java 的方向等等，那个时候 HTML5 标准还没公布，老师已经让我关注这方面的信息，在老师的指导下，我感觉慢慢打开了局面，进入到软件开发的世界。 很多人觉得我幸运，遇到了一位好老师，是的，毕业多年我一直心怀感激。2016 年的时候，老师去广工大出差，我当时刚好在广州上班，学弟告诉我老师去广州了，我赶紧趁老师有时间约出来吃个饭。去年新书出版后，到手的第一本书恭恭敬敬写上祝福，送给了大学老师。 我大一的时候参加了学生会，但是因为办事不力，被自动剔除了（实际上是因为进去之后我就后悔了，所以消极怠工），后来也没参加其他社团，所以我的时间一直都是大把的，但是我从来没有宅在宿舍睡大觉。 大一的时候有课上课，没课就去图书馆。有的时候早上一二节没课，三四节有课，我也都会按时起床，先去图书馆，到时间了就去上课。大二的时候，开始搞 Java 之后，我就很少去图书馆了，老师给提供了一个安静的，可以放电脑的学习场地，基本上每天就宅在那里学习。 这张照片是我上大三的时候拍的，那个时候晚上十一点之后回寝室基本是家常便饭。印象深刻的是有一年过年没回家（实际上只有大二过年回家了），除夕晚上在搞数据库行转列，因为第二天打算搞一个 GIS 应用，所以前一天要把数据库搞好，除夕晚上弄到 10 点左右回寝室。 没有人逼你，但是我们自己总得给自己立一点规矩。 还好，大二到大四，没有白费。大四春招找工作时候是异常顺利，一周之内拿了三个不同方向的 offer ：Android ，Java Swing 以及 JavaEE。 学 Java 之后，还有另外一个好处，就是整个人变得自信了，我发现我能做这些事，而且能做的很好，自信之后，精神状态 OK 了，很多事情做起来就很顺了。 关于爱情和女票在一起今年也是第八个年头了。 我们大一时候在一起，女票一直是学霸，毕业之后保送到西北工业大学读研，期间受学校资助被派去昆士兰大学访学，回来后现在做大数据开发。 大学四年，毕业之后又异地了三年（她在西安，我在广州），去年 4 月份女票硕士毕业从西安来深圳，终于又团聚了。 估计松哥很快会结婚了吧~ 关于买房买房是我工作至今最为骄傲的事情之一，我觉得这是对我的一个证明。 我刚毕业的时候，薪水是 5000 ，转正之后是 6250 ，包住。我和女票说我每个月攒 4000 块钱，每年攒 5W，等你研究生毕业的时候，我差不多就能在西安买房了。想法还是太天真，女票去年研究生毕业的时候，西安的房价已经翻了一翻。 刚工作头半年没攒下钱，第一家公司干了 5 个月就跳槽了，第二年薪水就飞了，不到一年时间翻了三倍。于是在 2016 年 11 月的时候，我用打工攒下来的十几万在西安上车了。买房的时候，家里才只有两万块钱，那几年供我和妹妹读书，家里没有什么积蓄，我妈东拼西凑又借了两万，家里一共出了四万块钱，剩下的十几万首付都是我出的。 记得刚买完房那会，元气大伤。回到广州都没钱交房租了，和同事合租的房子，交房租的时候让同事先帮忙垫着，等了十多天工资下来了，才缓了口气。 买房也是一件非常幸运的事情的，西安那个时候只要 20% 的首付，我 11 月交完首付，王永康 12 月当上了西安市委书记，然后西安房价就一路狂飙刹不住车，首付也提高到 30%-40% ，买房还要摇号。真是惊险万分。 这是去年 12 月底交房的时候拍的，房子顺利交付，但是我已经不太想回西安了，虽然留在深圳可能会很累，但是以我的性格，回去了，估计也不会过得轻松。所以，先争取吧，或许以后去广州也说不定。 关于写书只要坚持写博客，出书的机会其实有很多。2016 年的时候就有出版社的编辑老师找我，但是当时刚刚工作，实在没啥好写的，于是就婉拒了。后来还是有很多编辑老师找来，我都没有答应。 2018 年刚过完年，那时候我搞 Spring Boot + Vue 也有一段时间了，自我感觉积累了一点点料，有种想和大伙分享的欲望，另一方面也觉得该为自己的职业生涯留下一点东西，不能就这么默默无闻的搬一辈子砖，在认真考虑后，决定写一本 Spring Boot 相关的书，刚好清华社的夏老师没过几天就加了我微信，邀请我写一本 Spring Boot 相关的书，于是一拍即合，这件事就愉快的定下来了。 写书期间，每天早上 7 点起床，写到 8 点半然后去上班，晚上 6 点下班后，差不多 7 点开始写，写到 11 点半，周末写两天，拒掉了大部分的社交活动，差不多就这样连续了几个月，交稿的时候有种高考结束的感觉，有的小伙伴可能觉得我是个假程序员，竟然不加班，老实说，敝司确实不怎么加班。 稿子交到出版社之后，还要经过 排版-&gt;编辑-&gt;改错-&gt;初审-&gt;复审-&gt;终审-&gt;发稿-&gt;申请书号、CIP-&gt;封面设计-&gt;出片-&gt;下厂印制-&gt;发样书-&gt;入库-&gt;上市销售 ，整个过程大约持续了三个多月。到今年年初的时候，《Spring Boot + Vue 全栈开发实战》终于出版了，迄今为止，书已经加印了 5 次了。 关于公众号公众号是我在 2017 年申请的，但是一直之前一直没有认真做，今年微笑哥（公众号：纯洁的微笑）给了我一些很实在的建议，也帮了我很多，现在打算用心去做公众号，尽量保持原创，希望有一天能够像微笑哥一样把公众号做好。 这张照片是华为云云享专家线下活动时候和微笑哥拍的，正是微笑哥的建议促使我认真做公众号，也非常感谢微笑哥的提携与帮助。 从小学到初中到高中到大学，读书的时候每个阶段都有对我帮助很大影响很大的老师，直到现在一些做事风格都受老师影响。工作后也有给过我很多帮助的领导同事，现在做公众号也有大佬指导，虽然感觉一直都很辛苦，但是一直也很幸运。 关于未来刚毕业的时候，想着以后要回西安发展。现在广深两地呆久了，有点喜欢上这里了，现在很希望以后能留在这里，眼前紧迫的事情就是能早点在这边买房安家，深圳的房子太贵了，广州还可以考虑下，目前也在积极准备中。 头发依旧茂盛，所以程序员这个工作还能继续干。写代码写文章都是让人舒适惬意的事情，在可预见的未来，应该会一直写下去。 公众号也是未来坚持的一个方向，也希望能把公众号做好。 不知不觉码了三千多字，感谢大家看我唠叨了这么多！","link":"/2019/0812/life.html"},{"title":"最简单的SpringBoot整合MyBatis教程","text":"前面两篇文章和读者聊了Spring Boot中最简单的数据持久化方案JdbcTemplate，JdbcTemplate虽然简单，但是用的并不多，因为它没有MyBatis方便，在Spring+SpringMVC中整合MyBatis步骤还是有点复杂的，要配置多个Bean，Spring Boot中对此做了进一步的简化，使MyBatis基本上可以做到开箱即用，本文就来看看在Spring Boot中MyBatis要如何使用。 工程创建首先创建一个基本的Spring Boot工程，添加Web依赖，MyBatis依赖以及MySQL驱动依赖，如下： 创建成功后，添加Druid依赖，并且锁定MySQL驱动版本，完整的依赖如下： 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 如此，工程就算是创建成功了。读者注意，MyBatis和Druid依赖的命名和其他库的命名不太一样，是属于xxx-spring-boot-stater模式的，这表示该starter是由第三方提供的。 基本用法MyBatis的使用和JdbcTemplate基本一致，首先也是在application.properties中配置数据库的基本信息： 1234spring.datasource.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=utf-8spring.datasource.username=rootspring.datasource.password=rootspring.datasource.type=com.alibaba.druid.pool.DruidDataSource 配置完成后，MyBatis就可以创建Mapper来使用了，例如我这里直接创建一个UserMapper2，如下： 12345678910111213141516171819202122232425public interface UserMapper2 { @Select(\"select * from user\") List&lt;User&gt; getAllUsers(); @Results({ @Result(property = \"id\", column = \"id\"), @Result(property = \"username\", column = \"u\"), @Result(property = \"address\", column = \"a\") }) @Select(\"select username as u,address as a,id as id from user where id=#{id}\") User getUserById(Long id); @Select(\"select * from user where username like concat('%',#{name},'%')\") List&lt;User&gt; getUsersByName(String name); @Insert({\"insert into user(username,address) values(#{username},#{address})\"}) @SelectKey(statement = \"select last_insert_id()\", keyProperty = \"id\", before = false, resultType = Integer.class) Integer addUser(User user); @Update(\"update user set username=#{username},address=#{address} where id=#{id}\") Integer updateUserById(User user); @Delete(\"delete from user where id=#{id}\") Integer deleteUserById(Integer id);} 这里是通过全注解的方式来写SQL，不写XML文件，@Select、@Insert、@Update以及@Delete四个注解分别对应XML中的select、insert、update以及delete标签，@Results注解类似于XML中的ResultMap映射文件（getUserById方法给查询结果的字段取别名主要是向小伙伴们演示下@Results注解的用法），另外使用@SelectKey注解可以实现主键回填的功能，即当数据插入成功后，插入成功的数据id会赋值到user对象的id属性上。 UserMapper2创建好之后，还要配置mapper扫描，有两种方式，一种是直接在UserMapper2上面添加@Mapper注解，这种方式有一个弊端就是所有的Mapper都要手动添加，要是落下一个就会报错，还有一个一劳永逸的办法就是直接在启动类上添加Mapper扫描，如下： 1234567@SpringBootApplication@MapperScan(basePackages = \"org.sang.mybatis.mapper\")public class MybatisApplication { public static void main(String[] args) { SpringApplication.run(MybatisApplication.class, args); }} 好了，做完这些工作就可以去测试Mapper的使用了。 mapper映射当然，开发者也可以在XML中写SQL，例如创建一个UserMapper，如下： 123456789public interface UserMapper { List&lt;User&gt; getAllUser(); Integer addUser(User user); Integer updateUserById(User user); Integer deleteUserById(Integer id);} 然后创建UserMapper.xml文件，如下： 123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"org.sang.mybatis.mapper.UserMapper\"&gt; &lt;select id=\"getAllUser\" resultType=\"org.sang.mybatis.model.User\"&gt; select * from t_user; &lt;/select&gt; &lt;insert id=\"addUser\" parameterType=\"org.sang.mybatis.model.User\"&gt; insert into user (username,address) values (#{username},#{address}); &lt;/insert&gt; &lt;update id=\"updateUserById\" parameterType=\"org.sang.mybatis.model.User\"&gt; update user set username=#{username},address=#{address} where id=#{id} &lt;/update&gt; &lt;delete id=\"deleteUserById\"&gt; delete from user where id=#{id} &lt;/delete&gt;&lt;/mapper&gt; 将接口中方法对应的SQL直接写在XML文件中。 那么这个UserMapper.xml到底放在哪里呢？有两个位置可以放，第一个是直接放在UserMapper所在的包下面： 放在这里的UserMapper.xml会被自动扫描到，但是有另外一个Maven带来的问题，就是java目录下的xml资源在项目打包时会被忽略掉，所以，如果UserMapper.xml放在包下，需要在pom.xml文件中再添加如下配置，避免打包时java目录下的XML文件被自动忽略掉： 12345678910111213&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt; 当然，UserMapper.xml也可以直接放在resources目录下，这样就不用担心打包时被忽略了，但是放在resources目录下，又不能自动被扫描到，需要添加额外配置。例如我在resources目录下创建mapper目录用来放mapper文件，如下： 此时在application.properties中告诉mybatis去哪里扫描mapper： 1mybatis.mapper-locations=classpath:mapper/*.xml 如此配置之后，mapper就可以正常使用了。注意第二种方式不需要在pom.xml文件中配置文件过滤。 原理分析在SSM整合中，开发者需要自己提供两个Bean，一个SqlSessionFactoryBean，还有一个是MapperScannerConfigurer，在Spring Boot中，这两个东西虽然不用开发者自己提供了，但是并不意味着这两个Bean不需要了，在org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration类中，我们可以看到Spring Boot提供了这两个Bean，部分源码如下： 1234567891011121314151617181920212223242526272829303132333435@org.springframework.context.annotation.Configuration@ConditionalOnClass({ SqlSessionFactory.class, SqlSessionFactoryBean.class })@ConditionalOnSingleCandidate(DataSource.class)@EnableConfigurationProperties(MybatisProperties.class)@AutoConfigureAfter(DataSourceAutoConfiguration.class)public class MybatisAutoConfiguration implements InitializingBean { @Bean @ConditionalOnMissingBean public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception { SqlSessionFactoryBean factory = new SqlSessionFactoryBean(); factory.setDataSource(dataSource); return factory.getObject(); } @Bean @ConditionalOnMissingBean public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) { ExecutorType executorType = this.properties.getExecutorType(); if (executorType != null) { return new SqlSessionTemplate(sqlSessionFactory, executorType); } else { return new SqlSessionTemplate(sqlSessionFactory); } } @org.springframework.context.annotation.Configuration @Import({ AutoConfiguredMapperScannerRegistrar.class }) @ConditionalOnMissingBean(MapperFactoryBean.class) public static class MapperScannerRegistrarNotFoundConfiguration implements InitializingBean { @Override public void afterPropertiesSet() { logger.debug(\"No {} found.\", MapperFactoryBean.class.getName()); } }} 从类上的注解可以看出，当当前类路径下存在SqlSessionFactory、 SqlSessionFactoryBean以及DataSource时，这里的配置才会生效，SqlSessionFactory和SqlTemplate都被提供了。为什么要看这段代码呢？下篇文章，松哥和大伙分享Spring Boot中MyBatis多数据源的配置时，这里将是一个重要的参考。 好了，欢迎加入我的星球，关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0407/springboot-mybatis.html"},{"title":"松哥整理了 15 道 Spring Boot 高频面试题，看完当面霸","text":"什么是面霸？就是在面试中，神挡杀神佛挡杀佛，见招拆招，面到面试官自惭形秽自叹不如！松哥希望本文能成为你面霸路上的垫脚石！ 做 Java 开发，没有人敢小觑 Spring Boot 的重要性，现在出去面试，无论多小的公司 or 项目，都要跟你扯一扯 Spring Boot，扯一扯微服务，不会？没用过？ Sorry ，我们不合适！ 今天松哥就给大家整理了 15 道高频 Spring Boot 面试题，希望能够帮助到刚刚走出校门的小伙伴以及准备寻找新的工作机会的小伙伴。 1.什么是 Spring Boot ? 传统的 SSM/SSH 框架组合配置繁琐臃肿，不同项目有很多重复、模板化的配置，严重降低了 Java 工程师的开发效率，而 Spring Boot 可以轻松创建基于 Spring 的、可以独立运行的、生产级的应用程序。通过对 Spring 家族和一些第三方库提供一系列自动化配置的 Starter，来使得开发快速搭建一个基于 Spring 的应用程序。 Spring Boot 让日益臃肿的 Java 代码又重回简洁。在配合 Spring Cloud 使用时，还可以发挥更大的威力。 2.Spring Boot 有哪些特点 ? Spring Boot 主要有如下特点： 为 Spring 开发提供一个更快、更广泛的入门体验。 开箱即用，远离繁琐的配置。 提供了一系列大型项目通用的非业务性功能，例如：内嵌服务器、安全管理、运行数据监控、运行状况检查和外部化配置等。 绝对没有代码生成，也不需要XML配置。 3.Spring Boot 中的 starter 到底是什么 ? 首先，这个 Starter 并非什么新的技术点，基本上还是基于 Spring 已有功能来实现的。首先它提供了一个自动化配置类，一般命名为 XXXAutoConfiguration ，在这个配置类中通过条件注解来决定一个配置是否生效（条件注解就是 Spring 中原本就有的），然后它还会提供一系列的默认配置，也允许开发者根据实际情况自定义相关配置，然后通过类型安全的属性注入将这些配置属性注入进来，新注入的属性会代替掉默认属性。正因为如此，很多第三方框架，我们只需要引入依赖就可以直接使用了。 当然，开发者也可以自定义 Starter，自定义 Starter 可以参考：徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！。 4.spring-boot-starter-parent 有什么用 ? 我们都知道，新创建一个 Spring Boot 项目，默认都是有 parent 的，这个 parent 就是 spring-boot-starter-parent ，spring-boot-starter-parent 主要有如下作用： 定义了 Java 编译版本为 1.8 。 使用 UTF-8 格式编码。 继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。 执行打包操作的配置。 自动化的资源过滤。 自动化的插件配置。 针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。 关于这个问题，读者可以参考：你真的理解 Spring Boot 项目中的 parent 吗？ 5.YAML 配置的优势在哪里 ? YAML 现在可以算是非常流行的一种配置文件格式了，无论是前端还是后端，都可以见到 YAML 配置。那么 YAML 配置和传统的 properties 配置相比到底有哪些优势呢？ 配置有序，在一些特殊的场景下，配置有序很关键 支持数组，数组中的元素可以是基本数据类型也可以是对象 简洁 相比 properties 配置文件，YAML 还有一个缺点，就是不支持 @PropertySource 注解导入自定义的 YAML 配置。 关于 YAML 配置，要是大家还不熟悉，可以参考: Spring Boot 中的 yaml 配置简介 6.Spring Boot 中如何解决跨域问题 ? 跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 （CORS，Cross-origin resource sharing） 来解决跨域问题。这种解决方案并非 Spring Boot 特有的，在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在则是通过 @CrossOrigin 注解来解决跨域问题。关于 CORS ，小伙伴们可以参考：Spring Boot 中通过 CORS 解决跨域问题 7.比较一下 Spring Security 和 Shiro 各自的优缺点 ? 由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，包括 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。当然这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。Shiro 和 Spring Security 相比，主要有如下一些特点： Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架 Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单 Spring Security 功能强大；Shiro 功能简单 8.微服务中如何实现 session 共享 ? 在微服务中，一个完整的项目被拆分成多个不相同的独立的服务，各个服务独立部署在不同的服务器上，各自的 session 被从物理空间上隔离开了，但是经常，我们需要在不同微服务之间共享 session ，常见的方案就是 Spring Session + Redis 来实现 session 共享。将所有微服务的 session 统一保存在 Redis 上，当各个微服务对 session 有相关的读写操作时，都去操作 Redis 上的 session 。这样就实现了 session 共享，Spring Session 基于 Spring 中的代理过滤器实现，使得 session 的同步操作对开发人员而言是透明的，非常简便。 session 共享大家可以参考：Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！ 9.Spring Boot 如何实现热部署 ? Spring Boot 实现热部署其实很容易，引入 devtools 依赖即可，这样当编译文件发生变化时，Spring Boot 就会自动重启。在 Eclipse 中，用户按下保存按键，就会自动编译进而重启 Spring Boot，IDEA 中由于是自动保存的，自动保存时并未编译，所以需要开发者按下 Ctrl+F9 进行编译，编译完成后，项目就自动重启了。 如果仅仅只是页面模板发生变化，Java 类并未发生变化，此时可以不用重启 Spring Boot，使用 LiveReload 插件就可以轻松实现热部署。 10.Spring Boot 中如何实现定时任务 ? 定时任务也是一个常见的需求，Spring Boot 中对于定时任务的支持主要还是来自 Spring 框架。 在 Spring Boot 中使用定时任务主要有两种不同的方式，一个就是使用 Spring 中的 @Scheduled 注解，另一个则是使用第三方框架 Quartz。 使用 Spring 中的 @Scheduled 的方式主要通过 @Scheduled 注解来实现。 使用 Quartz ，则按照 Quartz 的方式，定义 Job 和 Trigger 即可。 关于定时任务这一块，大家可以参考：Spring Boot 中实现定时任务的两种方式! 11.前后端分离，如何维护接口文档 ? 前后端分离开发日益流行，大部分情况下，我们都是通过 Spring Boot 做前后端分离开发，前后端分离一定会有接口文档，不然会前后端会深深陷入到扯皮中。一个比较笨的方法就是使用 word 或者 md 来维护接口文档，但是效率太低，接口一变，所有人手上的文档都得变。在 Spring Boot 中，这个问题常见的解决方案是 Swagger ，使用 Swagger 我们可以快速生成一个接口文档网站，接口一旦发生变化，文档就会自动更新，所有开发工程师访问这一个在线网站就可以获取到最新的接口文档，非常方便。关于 Swagger 的用法，大家可以参考：SpringBoot整合Swagger2，再也不用维护接口文档了！ 12.什么是 Spring Data ? Spring Data 是 Spring 的一个子项目。用于简化数据库访问，支持NoSQL 和 关系数据存储。其主要目标是使数据库的访问变得方便快捷。Spring Data 具有如下特点： SpringData 项目支持 NoSQL 存储： MongoDB （文档数据库） Neo4j（图形数据库） Redis（键/值存储） Hbase（列族数据库） SpringData 项目所支持的关系数据存储技术： JDBC JPA Spring Data Jpa 致力于减少数据访问层 (DAO) 的开发量. 开发者唯一要做的，就是声明持久层的接口，其他都交给 Spring Data JPA 来帮你完成！Spring Data JPA 通过规范方法的名字，根据符合规范的名字来确定方法需要实现什么样的逻辑。 13.Spring Boot 是否可以使用 XML 配置 ? Spring Boot 推荐使用 Java 配置而非 XML 配置，但是 Spring Boot 中也可以使用 XML 配置，通过 @ImportResource 注解可以引入一个 XML 配置。 14.Spring Boot 打成的 jar 和普通的 jar 有什么区别 ? Spring Boot 项目最终打包成的 jar 是可执行 jar ，这种 jar 可以直接通过 java -jar xxx.jar 命令来运行，这种 jar 不可以作为普通的 jar 被其他项目依赖，即使依赖了也无法使用其中的类。 Spring Boot 的 jar 无法被其他项目依赖，主要还是他和普通 jar 的结构不同。普通的 jar 包，解压后直接就是包名，包里就是我们的代码，而 Spring Boot 打包成的可执行 jar 解压后，在 \\BOOT-INF\\classes 目录下才是我们的代码，因此无法被直接引用。如果非要引用，可以在 pom.xml 文件中增加配置，将 Spring Boot 项目打包成两个 jar ，一个可执行，一个可引用。 15.bootstrap.properties 和 application.properties 有何区别 ? 单纯做 Spring Boot 开发，可能不太容易遇到 bootstrap.properties 配置文件，但是在结合 Spring Cloud 时，这个配置就会经常遇到了，特别是在需要加载一些远程配置文件的时侯。 bootstrap.properties 在 application.properties 之前加载，配置在应用程序上下文的引导阶段生效。一般来说我们在 Spring Cloud Config 或者 Nacos 中会用到它。bootstrap.properties 被 Spring ApplicationContext 的父类加载，这个类先于加载 application.properties 的 ApplicatonContext 启动。 当然，前面叙述中的 properties 也可以修改为 yaml 。 好了，本文就说到这里，欢迎小伙伴留言说说你曾经遇到过的 Spring Boot 面试题！","link":"/2019/0619/interview.html"},{"title":"极简 Spring Boot 整合 Thymeleaf 页面模板","text":"虽然现在慢慢在流行前后端分离开发，但是据松哥所了解到的，还是有一些公司在做前后端不分的开发，而在前后端不分的开发中，我们就会需要后端页面模板（实际上，即使前后端分离，也会在一些场景下需要使用页面模板，例如邮件发送模板）。 早期的 Spring Boot 中还支持使用 Velocity 作为页面模板，现在的 Spring Boot 中已经不支持 Velocity 了，页面模板主要支持 Thymeleaf 和 Freemarker ，当然，作为 Java 最最基本的页面模板 Jsp ，Spring Boot 也是支持的，只是使用比较麻烦。 松哥打算用三篇文章分别向大家介绍一下这三种页面模板技术。 今天我们主要来看看 Thymeleaf 在 Spring Boot 中的整合！ Thymeleaf 简介Thymeleaf 是新一代 Java 模板引擎，它类似于 Velocity、FreeMarker 等传统 Java 模板引擎，但是与传统 Java 模板引擎不同的是，Thymeleaf 支持 HTML 原型。 它既可以让前端工程师在浏览器中直接打开查看样式，也可以让后端工程师结合真实数据查看显示效果，同时，SpringBoot 提供了 Thymeleaf 自动化配置解决方案，因此在 SpringBoot 中使用 Thymeleaf 非常方便。 事实上， Thymeleaf 除了展示基本的 HTML ，进行页面渲染之外，也可以作为一个 HTML 片段进行渲染，例如我们在做邮件发送时，可以使用 Thymeleaf 作为邮件发送模板。 另外，由于 Thymeleaf 模板后缀为 .html，可以直接被浏览器打开，因此，预览时非常方便。 整合 创建项目 Spring Boot 中整合 Thymeleaf 非常容易，只需要创建项目时添加 Thymeleaf 即可： 创建完成后，pom.xml 依赖如下： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 当然，Thymeleaf 不仅仅能在 Spring Boot 中使用，也可以使用在其他地方，只不过 Spring Boot 针对 Thymeleaf 提供了一整套的自动化配置方案，这一套配置类的属性在 org.springframework.boot.autoconfigure.thymeleaf.ThymeleafProperties 中，部分源码如下： 1234567891011121314@ConfigurationProperties(prefix = \"spring.thymeleaf\")public class ThymeleafProperties { private static final Charset DEFAULT_ENCODING = StandardCharsets.UTF_8; public static final String DEFAULT_PREFIX = \"classpath:/templates/\"; public static final String DEFAULT_SUFFIX = \".html\"; private boolean checkTemplate = true; private boolean checkTemplateLocation = true; private String prefix = DEFAULT_PREFIX; private String suffix = DEFAULT_SUFFIX; private String mode = \"HTML\"; private Charset encoding = DEFAULT_ENCODING; private boolean cache = true; //...} 首先通过 @ConfigurationProperties 注解，将 application.properties 前缀为 spring.thymeleaf 的配置和这个类中的属性绑定。 前三个 static 变量定义了默认的编码格式、视图解析器的前缀、后缀等。 从前三行配置中，可以看出来，Thymeleaf 模板的默认位置在 resources/templates 目录下，默认的后缀是 html 。 这些配置，如果开发者不自己提供，则使用 默认的，如果自己提供，则在 application.properties 中以 spring.thymeleaf 开始相关的配置。 而我们刚刚提到的，Spring Boot 为 Thymeleaf 提供的自动化配置类，则是 org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration ，部分源码如下： 123456@Configuration@EnableConfigurationProperties(ThymeleafProperties.class)@ConditionalOnClass({ TemplateMode.class, SpringTemplateEngine.class })@AutoConfigureAfter({ WebMvcAutoConfiguration.class, WebFluxAutoConfiguration.class })public class ThymeleafAutoConfiguration {} 可以看到，在这个自动化配置类中，首先导入 ThymeleafProperties ，然后 @ConditionalOnClass 注解表示当当前系统中存在 TemplateMode 和 SpringTemplateEngine 类时，当前的自动化配置类才会生效，即只要项目中引入了 Thymeleaf 相关的依赖，这个配置就会生效。 这些默认的配置我们几乎不需要做任何更改就可以直接使用了。如果开发者有特殊需求，则可以在 application.properties 中配置以 spring.thymeleaf 开头的属性即可。 创建 Controller 接下来我们就可以创建 Controller 了，实际上引入 Thymeleaf 依赖之后，我们可以不做任何配置。新建的 IndexController 如下： 12345678910111213141516171819202122@Controllerpublic class IndexController { @GetMapping(\"/index\") public String index(Model model) { List&lt;User&gt; users = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { User u = new User(); u.setId((long) i); u.setName(\"javaboy:\" + i); u.setAddress(\"深圳:\" + i); users.add(u); } model.addAttribute(\"users\", users); return \"index\"; }}public class User { private Long id; private String name; private String address; //省略 getter/setter} 在 IndexController 中返回逻辑视图名+数据，逻辑视图名为 index ，意思我们需要在 resources/templates 目录下提供一个名为 index.html 的 Thymeleaf 模板文件。 创建 Thymeleaf 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;编号&lt;/td&gt; &lt;td&gt;用户名&lt;/td&gt; &lt;td&gt;地址&lt;/td&gt; &lt;/tr&gt; &lt;tr th:each=\"user : ${users}\"&gt; &lt;td th:text=\"${user.id}\"&gt;&lt;/td&gt; &lt;td th:text=\"${user.name}\"&gt;&lt;/td&gt; &lt;td th:text=\"${user.address}\"&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 在 Thymeleaf 中，通过 th:each 指令来遍历一个集合，数据的展示通过 th:text 指令来实现， 注意 index.html 最上面要引入 thymeleaf 名称空间。 配置完成后，就可以启动项目了，访问 /index 接口，就能看到集合中的数据了： 另外，Thymeleaf 支持在 js 中直接获取 Model 中的变量。例如，在 IndexController 中有一个变量 username ： 12345678@Controllerpublic class IndexController { @GetMapping(\"/index\") public String index(Model model) { model.addAttribute(\"username\", \"李四\"); return \"index\"; }} 在页面模板中，可以直接在 js 中获取到这个变量： 1234&lt;script th:inline=\"javascript\"&gt; var username = [[${username}]]; console.log(username)&lt;/script&gt; 这个功能算是 Thymeleaf 的特色之一吧。 手动渲染前面我们说的是返回一个 Thymeleaf 模板，我们也可以手动渲染 Thymeleaf 模板，这个一般在邮件发送时候有用，例如我在 resources/templates 目录下新建一个邮件模板，如下： 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;hello 欢迎 &lt;span th:text=\"${username}\"&gt;&lt;/span&gt;加入 XXX 集团，您的入职信息如下：&lt;/p&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;职位&lt;/td&gt; &lt;td th:text=\"${position}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;薪水&lt;/td&gt; &lt;td th:text=\"${salary}\"&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;img src=\"http://www.javaboy.org/images/sb/javaboy.jpg\" alt=\"\"&gt;&lt;/body&gt;&lt;/html&gt; 这一个 HTML 模板中，有几个变量，我们要将这个 HTML 模板渲染成一个 String 字符串，再把这个字符串通过邮件发送出去，那么如何手动渲染呢？ 1234567891011@AutowiredTemplateEngine templateEngine;@Testpublic void test1() throws MessagingException { Context context = new Context(); context.setVariable(\"username\", \"javaboy\"); context.setVariable(\"position\", \"Java工程师\"); context.setVariable(\"salary\", 99999); String mail = templateEngine.process(\"mail\", context); //省略邮件发送} 渲染时，我们需要首先注入一个 TemplateEngine 对象，这个对象就是在 Thymeleaf 的自动化配置类中配置的（即当我们引入 Thymeleaf 的依赖之后，这个实例就有了）。 然后构造一个 Context 对象用来存放变量。 调用 process 方法进行渲染，该方法的返回值就是渲染后的 HTML 字符串，然后我们将这个字符串发送出去。 这是 Spring Boot 整合 Thymeleaf 的几个关键点，关于 Thymeleaf 这个页面模板本身更多的用法，大家可以参考 Thymeleaf 的文档：https://www.thymeleaf.org。 总结本文主要向大家简单介绍了 Spring Boot 和 Thymeleaf 整合时的几个问题，还是比较简单的，大家可以阅读 Thymeleaf 官方文档学习 Thymeleaf 的更多用法。本文案例我已上传到 GitHub ，欢迎大家 star :https://github.com/lenve/javaboy-code-samples 关于本文，有问题欢迎留言讨论。","link":"/2019/0613/springboot-thymeleaf.html"},{"title":"起早贪黑几个月，我写完了人生第一本书！","text":"今天有小伙伴在网上问了我一个问题：写书的整个过程是什么感受？想想我好像还没各位小伙伴聊过我写书的故事，只是在书出版后做过一次送书活动，其他的好像就没分享啥了，今天我想借这个机会和大伙聊聊我写书的故事，也希望我的经验能帮助到各位小伙伴。 1.缘起故事得从我大学毕业时候说起啦。大四第一学期忙着准备考研，错过了秋招，然而研究生也没考上，过完年研究生考试成绩出来后，一看不行就赶紧出来找工作，西北农村娃，不敢耗也耗不起。大学所在的城市高校不多，所以春招的时候我回到了老家西安参加春招，花了一个多礼拜，拿了三个offer，感觉差不多了又急匆匆返回学校，返校后，回忆找工作的过程，有得意也有失落，得意的是没想到找工作这么顺利，失落的是想去的公司没去成，我的学校还被一些中等公司鄙视了。 我本科学位是管理学学士，计算机是我从大二开始自学的，自学了JavaEE和Android，当时找工作时候，招Android的，招Java的我都去面，3个offer有两个是Java，一个Android。虽然找工作整体上感觉不错，不过还是有一些不如意的地方，有一个超级大的大厂，过了笔试，也过了两轮技术面，止步于最后一轮人事面，这算是找工作期间最遗憾的一件事了。也有一些不怎么大的厂，却歧视我的学校（某末流211），这让我忿忿不平，但是学校也没法改变，思来想去，决定写博客，提高自己的技术影响力，弥补专业和学校的不足，就这样，在CSDN上注册了博客账号，当年4月15号发表了第一篇博客，从此打开了一扇新大门。 博客写了一段时间，CSDN的运营梦鸽美女邀请我申请博客专家，有了title写的就更有劲了。写博客的过程中，感觉自身的技术也在不断的提高，因为刚开始学一个新技术点的时候，很多东西没太关注，只会用，没细究，写博客则是一个整理的过程，是自己思维一个锻炼的过程，博客写完了，感觉对相关知识点的认知又上升了。 刚开始写的时候，博客的访问量并不高，好在我当时也是刚毕业，不着急，就慢慢的写着，就这样，第二年刚过完年就开始有人找我写书，被我婉拒了，我的理由是刚刚毕业半年，实在没啥好写的，也不知道该写啥。不过我却发现写书好像没那么难，好像很容易，因为竟然有人找我写书。再之后，隔三差五就会有出版社的编辑找来了，电子工业出版社、人民邮电出版社、清华大学出版社等等，不过我自己从来没下定决心，虽然心里也有想法，但是总觉得还差点火候。 2018年刚过完年，那时候我搞Spring Boot+Vue也有一段时间了，自我感觉积累了一点点料，有种想和大伙分享的欲望，另一方面也觉得该为自己的职业生涯留下一点东西，不能就这么默默无闻的搬一辈子砖，在认真考虑后，决定写一本Spring Boot相关的书，刚好清华社的夏老师没过几天就加了我微信，于是一拍即合。 这是写书的第一步，先有技术积累，有博客或者公众号，圈子里有一点点名气，就会有出版社的编辑找来，因为出版社编辑比较喜欢那种在某一领域深耕多年，对相关技术有自己的看法和认识，有原创的博客，并且博客写作思路清晰，文章脉络清楚的作者。在这个阶段我觉得最难的还是坚持，写博客积累技术和名气并非一朝一夕的事，有一些超级大牛，抓住了技术的风口一下就积累了很多的关注，刚入行的小辈看到这些大牛的博客，感觉达到这样的高度太难了，所以放弃了。其实很多时候，你不用成为执牛耳的大牛，成为一个小小的小牛，就够了。 这一阶段，总结两个字：坚持。 2.写作在答应了出版社的邀请之后，就着手开始准备了。在刚开始答应的时候，需要提交一个图书选题单给出版社的老师，选题单中会列出书名，章节，作者等信息。 选题定下来之后，先和出版社签订出版合同，合同中会约定图书字数、作者、稿费计算方式等，签好合同后，和出版社的事情暂时就先告一段落了。 接下来就开始写了，细化每个章节的目录，每章大概写多少，准备写哪些内容，提纲细化之后，后面基本就不动了，主要是填内容进去。写书和写博客不一样，博客，我只需要介绍某一个知识点，解决某一个问题就行了，写书，不仅要介绍知识点解决问题，还要讲究知识点的全面，不能有遗漏，很多东西，我们可能经常用某一种方式实现，但实际上换一种方式也能实现，但是你可能就不知道，关键是你并不知道他还有另一种实现方式，这就很累了，为了不遗漏知识点，只能把官方文档反复看。有的时候卡在某一个技术点上，上班时候脑子里都是相关问题，一有解决思路就赶紧先记下来，回家后赶紧尝试。在写书之前，我在公众号上已经陆续发了Redis系列教程、MongoDB系列教程、Spring Cloud系列教程以及Git系列教程等，因此在写Spring Boot时，遇到这几方面的问题基本上都能得心应手，也算是前期准备比较充分吧（其实写这些教程的时候压根就没想到写书的事，但是掌握了，写出来的技术，总会在某一天发挥作用的）。 写书期间最大的挑战还不是来自技术，而是自信，有的时候写着写着甚至会怀疑自己，这书写出来有人看吗？但是合同签了，没人看也得硬着头皮写下去，而且得认真写。有时候一些出版的问题要和编辑老师沟通，沟通完后，又会信心满满，这一点，还是要感谢出版社编辑老师的鼓励。我自己因为不爱交流，很多问题喜欢自己瞎琢磨，其实很多出版方面的问题都可以和编辑及时沟通，避免给自己徒增烦恼（这个建议送给想要写书的小伙伴）。 那一段时间，我每天早上7点起床，写到8点半然后去上班，晚上6点下班后，差不多7点开始写，写到11点半，周末写两天，拒掉了大部分的社交活动，差不多就这样连续了几个月，交稿的时候有种高考考完的感觉，有的小伙伴可能觉得我是个假程序员，竟然不加班，老实说，敝司确实不怎么加班。稿子交到出版社之后，还要经过排版-&gt;编辑-&gt;改错-&gt;初审-&gt;复审-&gt;终审-&gt;发稿-&gt;申请书号、CIP-&gt;封面设计-&gt;出片-&gt;下厂印制-&gt;发样书-&gt;入库-&gt;上市销售，整个过程大约持续了三个多月。封面设计时候，出版社给了两个参考的封面，纠结了半天，后来选择了绿色的（可能有小伙伴要吐槽我的审美了）： 关于封面这里，也可以自己提一些设计思路给出版社去做，不过我最终还是选择了出版社的方案，想想民国时那些自己给自己设计图书封面的大佬，真是佩服的五体投地。关于书的定价，也是出版社给一个参考范围，作者自己选，现在技术图书的定价基本都是按照印张来的，作者选择的范围不大，除非是超级超级大牛，可能会额外照顾（我瞎猜的）。 到了2019年1月份的时候，有一天午休醒来，有个人加我微信，备注说是读者，我才发现书已经上市销售了，至此，2018年的工程，总算告一段落了，几个月起早贪黑，甚至打了退堂鼓，还好最终没有放弃，总算有了收货。 这一阶段的总结：不要怂，就是干。 3.收获图书出版后，感觉收获还是蛮大的。从以下三个方面来跟大伙聊聊： 技术首先就是技术了，写书是一个非常非常系统化的工程，虽然我以前也写过多个成体系的教程，但是感觉和写书还是有很大的不同，写书的过程，也是重新梳理自己知识体系的过程，对于以前不求甚解的东西都去认真研究了，还要想办法将一些复杂的东西写的浅显易懂，让读者容易上手。在不断的锤炼中，自己的技术也得到了极大的提高。 信心由于我并非科班出身，有幸在这个行业混口饭吃其实已经很满足了，计算机理论捉襟见肘，虽然我一直在努力弥补，但总是不够自信。这本书一定程度上让我更有信心在这个圈子里混下去。 圈子我自己平时不怎么出去玩，比较宅，线下的圈子不多，线上的圈子倒不少，但是很多人都是听其名，不知其人。书出版之后，加入的第一个圈子就是华为云享专家，在华为云组织的openday中，认识了很多大佬，很多人名字和人终于对上了，自己也收获了很多。还有一些由于时间原因被我推掉的活动，但总体感觉就是活动多了。其实这就是我自己一向所说的，提高自己才是最重要的，与其削尖了脑袋挤进某一个圈子，不如修炼内功，时间到了，该有的就有了。 4.一点建议其实经常会有一些读者在后台联系我，有刚毕业的大学生，也有在读的研究生，他们想知道在技术的道路上要如何选择，C\\C++\\Java\\前端，都会，但是却不精通，这里我给的建议就是苍蝇模式，因为我一开始也是自学的，我相信我曾经遇到的困惑也有后来者会遇到，那么什么是苍蝇模式呢？ 美国密歇根大学教授卡尔·韦克做过这样一个实验：把一群蜜蜂和一群苍蝇同时装进一个玻璃瓶里，将瓶子横着放平，让瓶底朝着光，小蜜蜂们会一刻不停地在瓶底附近飞舞，因为蜜蜂的复眼有更强的向光性，对阳光的敏感和偏执决定它们不肯接近黑暗的地方，哪怕是出口，蜜蜂一次次撞到瓶底，直到力竭而死，而苍蝇则在瓶子里乱撞，不一会儿，就能从瓶口逃之夭夭。 刚入行可以多了解、多打听、多去尝试慢慢找到适合自己的，自己喜欢的，选定了方向之后，就可以开始做技术积累了，积累可以从写博客开始，初期建议选个大平台，例如博客园、CSDN或者慕课网之类的，有了名气之后，可以考虑独立建站或者写公众号，慢慢打造个人品牌，个人品牌建立了，写书就是愿不愿意的事了。其实，事儿不难，难在坚持！","link":"/2019/0407/javaboy-book.html"},{"title":"快来，松哥的免费资源库更新啦","text":"今年 5 月份的时候，松哥发了一个视频资源库，当时和大家说，这个资源库会定期更新，后来却迟迟未更新，其实不是我没资源了，是因为当时的关键字是我一个一个在微信后台配置的，配置到后面发现，后台配置关键字有数量上限，没法继续配置了，所以这事就搁置下来了。 九月份松哥上线了自己的服务，和微信的后台对接起来，具体实现大家可以参考这两篇文章： Spring Boot 开发微信公众号后台 Spring Boot 如何给微信公众号返回消息 现在再配置关键字就没有限制了。于是最近抽空把资源更新了一波，废话不多说，大家在公众号【江南一点雨】后台回复相应的口令，就可以获取相应的视频下载地址。 Java 基础 资源名称 口令 Java 基础语法 javaboy4096 Java 面向对象 javaboy6148 JavaSE 飞机大战项目 javaboy2053 深入面向对象和数组 javaboy8200 Java 常用类详解 javaboy4105 Java 异常机制解析 javaboy6157 Java 集合与数据结构 javaboy2062 JavaIO 流全解析 javaboy8209 深入理解 Java 多线程 javaboy4114 Java 网络编程 javaboy6166 手动开发一个 Web 服务器 javaboy2071 深入理解 Java 注解+反射 javaboy8218 Java23 种设计模式 javaboy4123 学会 Java 正则表达式 javaboy6175 JDBC 详解 javaboy2080 独立开发 SORM 框架 javaboy8227 快人一步，Java10 新特性全解析 javaboy4132 Java 数据结构和算法 javaboy6184 深入理解 Java 虚拟机 javaboy2089 Java 解析XML文件 javaboy8236 数据库 资源名称 口令 Oracle 数据库安装及简单 SQL javaboy4141 Oracle 账户管理及查询语句 javaboy6193 Oracle 中的函数 javaboy2098 Oracle 中的子查询 javaboy8245 Oracle 中常见的表操作 javaboy4150 Oracle 中的数据备份 javaboy6202 MySQL 基础 javaboy2107 PowerDesigner 教程 javaboy8254 JDBC 操作数据库 javaboy4159 MySQL 优化 javaboy6211 Oracle 高级课程 javaboy2116 数据库与 SQL 优化 javaboy6283 数据库集群与高并发 javaboy2188 Web 基础 资源名称 口令 HTML 入门教程 javaboy8263 CSS 教程 javaboy4168 JavaScript 视频教程 javaboy6220 jQuery 视频教程 javaboy2125 EasyUI 视频教程 javaboy8272 Servlet 基础 javaboy4177 Servlet 中的 Request 和 Response javaboy6229 Servlet 请求转发与重定向 javaboy2134 Session 和 Cookie javaboy8281 JSP 详解 javaboy4186 用户管理系统实战 javaboy6238 Ajax 详解 javaboy2143 EL 和 JSTL javaboy8290 过滤器详解 javaboy4195 监听器详解 javaboy6247 KnockoutJS 实战视频 javaboy2152 Java 高级 资源名称 口令 IntelliJIDEA 视频教程 javaboy4285 Java 高并发秒杀方案 javaboy8299 Activiti 工作流实战解析 javaboy4204 Java 并发编程与高并发实战 javaboy6256 Linux 快速入门 javaboy2161 Maven 详解 javaboy8308 Git 应用详解 javaboy4213 Svn 入门教程 javaboy6265 高并发编程与线程池 javaboy2170 系统优化与 JVM 调优 javaboy8317 Java 编程规范 javaboy4222 AIO、BIO、NIO 详解 javaboy6274 Netty 高级视频教程 javaboy2179 ActiveMQ 消息中间详解 javaboy8326 单点登录视频教程 javaboy4231 Dubbo 详解 javaboy8335 Redis 全解析 javaboy4240 VSFTPD+NGINX 视频教程 javaboy6292 MyBatis 视频教程 javaboy2197 Spring4 视频教程 javaboy8344 SpringMVC 视频教程 javaboy4249 SSM 框架整合视频教程 javaboy6301 RBAC 权限控制视频教程 javaboy2206 Hibernate4 视频教程 javaboy8353 Jfinal 视频教程 javaboy4258 Shiro 视频教程 javaboy6310 Solr 视频教程 javaboy2215 Struts2 视频教程 javaboy8362 Nginx 视频教程 javaboy4267 Redis 缓存详解 javaboy6319 JVM 虚拟机优化 javaboy2224 Zookeeper 详解视频 javaboy8371 Linux 基本操作 javaboy6328 架构师面试攻略（文档） javaboy2233 架构师面试攻略（视频） javaboy8380 JUC 视频教程 javaboy6400 MySQL 高级教程 javaboy2305 Java 邮件开发教程 javaboy8452 Maven 实战视频 javaboy8443 自己 DIY 一个 Tomcat javaboy4339 大前端 资源名称 口令 HTML5 新特性 javaboy4276 AngularJS 视频教程 javaboy6337 Grunt 视频教程 javaboy2242 Gulp 视频教程 javaboy8389 Webpack 视频教程 javaboy4294 Bootstrap 视频教程 javaboy6346 CSS3 视频教程 javaboy2251 ES6 视频教程 javaboy8398 HTML5 核心技术 javaboy4303 HTML5 实战 javaboy6355 HTML5 项目实战 javaboy2260 JS 模块化视频教程 javaboy8407 less 视频教程 javaboy4312 NodeJS 视频教程 javaboy6364 React 视频教程 javaboy2269 Zepto 视频教程 javaboy8416 HTML+CSS 实战视频 javaboy4321 JavaScript140 集 javaboy6373 jQuery 视频教程 javaboy2278 JavaScript 高级语法视频教程 javaboy8425 Vue 项目实战视频 javaboy4330 CSS3 特效实战 javaboy6382 HTML5 特效实战 javaboy2287 HTML5+Canvas 实现刮刮卡 javaboy8434 Gradle 从入门到精通 javaboy6391 mpvue 项目实战 javaboy2296 Vue 最新最全视频教程 javaboy4348 大数据 资源名称 口令 Linux 操作系统 javaboy4357 Linux 基本命令 javaboy6409 Linux 文件安装 javaboy2314 Shell 编程 javaboy8461 网络基础知识 javaboy4366 LVS 集群与高并发 javaboy6418 Nginx 和高并发 javaboy2323 keepalive 和单点故障 javaboy8470 HDFS 分布式文件系统 javaboy4375 mapreduce 分布式计算 javaboy6427 YARN 资源管理与任务调度 javaboy2332 mapreduce 计算案例 javaboy8479 HIVE 视频教程 javaboy4384 Hbase 数据库详解 javaboy6436 zookeeper 协同处理 javaboy2341 CDH 使用 javaboy8488 HUE 使用 javaboy4393 IMPALA 详解 javaboy6445 oozie 详解 javaboy2350 elasticsearch 详解 javaboy8497 Redis 内存数据 javaboy4402 Scala 入门 javaboy6454 Spark 详解 javaboy2359 Spark 高级 javaboy8506 Spark-Stream 流式计算 javaboy4411 Kafka 分布式消息队列 javaboy6463 STORM 流式计算框架 javaboy2368 Python 语言基础 javaboy8515 回归算法 javaboy4420 分类算法、决策树 javaboy6472 聚类算法、微博案例 javaboy2377 推荐算法 javaboy8524 大型电商日志分析（项目实战） javaboy4429 智慧交通（项目实战） javaboy6481 智能 App（项目实战） javaboy2386 人工智能 资源名称 口令 人工智能入门 javaboy8533 线性回归深入与代码实现 javaboy4438 梯度下降算发实现 javaboy6490 逻辑回归详解和应用 javaboy2395 分类项目案例与神经网络算法 javaboy8542 多分类、决策树分类与随机森林分类 javaboy4447 分类评估与聚类 javaboy6499 密度聚类与谱聚类 javaboy2404 Tensorflow 安装并实现线性回归 javaboy8551 TensorFlow 深入、TensorFlow可视化 javaboy4456 DNN 深度神经网络手写图片识别 javaboy6508 TensorBoard 可视化 javaboy2413 卷积神经网络、CNN 识别图片 javaboy8560 卷积神经网络深入，AlexNet 模型实现 javaboy4465 Keras 深度学习框架 javaboy6517 分布式相关 资源名称 口令 ZooKeeper 简介 javaboy2422 ZooKeeper 安装 javaboy8569 ZooKeeper 基本数据模型 javaboy4474 基于 Linux 的 ZK 客户端命令 javaboy6526 选举模式和 ZK 集群安装 javaboy2431 JavaAPI 操作 ZK javaboy8578 ApacheCurator 客户端 javaboy4483 Dubbo 入门到重构服务 javaboy6535 分布式锁 javaboy2440 Zookeeper 总结 javaboy8587 项目实战 资源名称 口令 OA 办公自动化项目1 javaboy4492 OA 办公自动化项目2 javaboy6544 OA 办公自动化项目3 javaboy2449 OA 办公自动化项目4 javaboy8596 备锋客户关系管理(CRM)系统 javaboy4501 百战客户关系管理系统 javaboy6553 宅急送项目 javaboy2458 高仿人人网项目 javaboy8605 Java 邮件开发项目 javaboy4510 在线支付实战视频 javaboy6562 俄罗斯方块游戏实战 javaboy2467 贪吃蛇视频教程 javaboy8614 交通灯管理系统 javaboy4519 银行业务调度系统实战 javaboy6571 供应链系统实战视频 javaboy2476 网上商城项目实战 javaboy8623 医药采购平台管理系统 javaboy4528 点餐系统实战 javaboy6580 杰信商贸 SSM 版 javaboy2485 国家税务协同平台项目 javaboy8632 javaWeb 聊天室 javaboy4537 网上书店 javaboy6589 手机进销存系统 javaboy2494 QQ 聊天器 javaboy8641 ERP 项目 javaboy4546 坦克大战 javaboy6598 五子棋游戏 javaboy2503 报名系统 Activity javaboy8650 OA 供应链系统 javaboy4555 用户管理系统 javaboy6607 JavaWeb 图书商城 javaboy2512 VIP 商场 javaboy8659 企业招聘系统 javaboy4564 博客系统项目 javaboy6616 超级玛丽 javaboy2521 成绩管理系统 javaboy8668 个人理财系统 javaboy4573 人事管理系统 javaboy6625 JBPM 采购申请系统 javaboy2530 电子商务网站 javaboy8677 跨平台 App 开发 javaboy4582 文档资源 资源名称 口令 Docker 教程 docker Redis 教程 redis RocketMQ 教程 rocketmq Java8 新特性文档 java8 设计模式教程 设计模式 网络协议教程 网络 netty 教程 netty web 全栈指南 web全栈 好了，本次就先更新这么多，如果这里没有你想要的，也也可以留言说说你需要的资料，松哥会及时更新资源哦。 另外，大家在公众号后台回复 资源 ，也可以获取本文电子版。 如果这些资源帮助到你了，欢迎转发给更多小伙伴哦。","link":"/2019/1112/java-video.html"},{"title":"这一次，我连 web.xml 都不要了，纯 Java 搭建 SSM 环境","text":"在 Spring Boot 项目中，正常来说是不存在 XML 配置，这是因为 Spring Boot 不推荐使用 XML ，注意，并非不支持，Spring Boot 推荐开发者使用 Java 配置来搭建框架，Spring Boot 中，大量的自动化配置都是通过 Java 配置来实现的，这一套实现方案，我们也可以自己做，即自己也可以使用纯 Java 来搭建一个 SSM 环境，即在项目中，不存在任何 XML 配置，包括 web.xml 。 环境要求： 使用纯 Java 来搭建 SSM 环境，要求 Tomcat 的版本必须在 7 以上。 快速体验1 创建工程创建一个普通的 Maven 工程（注意，这里可以不必创建 Web 工程），并添加 SpringMVC 的依赖，同时，这里环境的搭建需要用到 Servlet ，所以我们还需要引入 Servlet 的依赖（一定不能使用低版本的 Servlet），最终的 pom.xml 文件如下： 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 2 添加 Spring 配置工程创建成功之后，首先添加 Spring 的配置文件，如下： 1234@Configuration@ComponentScan(basePackages = \"org.javaboy\", useDefaultFilters = true, excludeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION, classes = Controller.class)})public class SpringConfig {} 关于这个配置，我说如下几点： @Configuration 注解表示这是一个配置类，在我们这里，这个配置的作用类似于 applicationContext.xml @ComponentScan 注解表示配置包扫描，里边的属性和 xml 配置中的属性都是一一对应的，useDefaultFilters 表示使用默认的过滤器，然后又除去 Controller 注解，即在 Spring 容器中扫描除了 Controller 之外的其他所有 Bean 。 3 添加 SpringMVC 配置接下来再来创建 springmvc 的配置文件： 1234@Configuration@ComponentScan(basePackages = \"org.javaboy\",useDefaultFilters = false,includeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)})public class SpringMVCConfig {} 注意，如果不需要在 SpringMVC 中添加其他的额外配置，这样就可以了。即 视图解析器、JSON 解析、文件上传……等等，如果都不需要配置的话，这样就可以了。 4 配置 web.xml此时，我们并没有 web.xml 文件，这时，我们可以使用 Java 代码去代替 web.xml 文件，这里会用到 WebApplicationInitializer ，具体定义如下： 12345678910111213public class WebInit implements WebApplicationInitializer { public void onStartup(ServletContext servletContext) throws ServletException { //首先来加载 SpringMVC 的配置文件 AnnotationConfigWebApplicationContext ctx = new AnnotationConfigWebApplicationContext(); ctx.register(SpringMVCConfig.class); // 添加 DispatcherServlet ServletRegistration.Dynamic springmvc = servletContext.addServlet(\"springmvc\", new DispatcherServlet(ctx)); // 给 DispatcherServlet 添加路径映射 springmvc.addMapping(\"/\"); // 给 DispatcherServlet 添加启动时机 springmvc.setLoadOnStartup(1); }} WebInit 的作用类似于 web.xml，这个类需要实现 WebApplicationInitializer 接口，并实现接口中的方法，当项目启动时，onStartup 方法会被自动执行，我们可以在这个方法中做一些项目初始化操作，例如加载 SpringMVC 容器，添加过滤器，添加 Listener、添加 Servlet 等。 注意： 由于我们在 WebInit 中只是添加了 SpringMVC 的配置，这样项目在启动时只会去加载 SpringMVC 容器，而不会去加载 Spring 容器，如果一定要加载 Spring 容器，需要我们修改 SpringMVC 的配置，在 SpringMVC 配置的包扫描中也去扫描 @Configuration 注解，进而加载 Spring 容器，还有一种方案可以解决这个问题，就是直接在项目中舍弃 Spring 配置，直接将所有配置放到 SpringMVC 的配置中来完成，这个在 SSM 整合时是没有问题的，在实际开发中，较多采用第二种方案，第二种方案，SpringMVC 的配置如下： 1234@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig {} 这种方案中，所有的注解都在 SpringMVC 中扫描，采用这种方案的话，则 Spring 的配置文件就可以删除了。 5 测试最后，添加一个 HelloController ，然后启动项目进行测试： 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} 启动项目，访问接口，结果如下： 6 其他配置6.1 静态资源过滤静态资源过滤在 SpringMVC 的 XML 中的配置如下： 1&lt;mvc:resources mapping=\"/**\" location=\"/\"/&gt; 在 Java 配置的 SSM 环境中，如果要配置静态资源过滤，需要让 SpringMVC 的配置继承 WebMvcConfigurationSupport ，进而重写 WebMvcConfigurationSupport 中的方法，如下： 12345678@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/js/**\").addResourceLocations(\"classpath:/\"); }} 重写 addResourceHandlers 方法，在这个方法中配置静态资源过滤，这里我将静态资源放在 resources 目录下，所以资源位置是 classpath:/ ，当然，资源也可以放在 webapp 目录下，此时只需要修改配置中的资源位置即可。如果采用 Java 来配置 SSM 环境，一般来说，可以不必使用 webapp 目录，除非要使用 JSP 做页面模板，否则可以忽略 webapp 目录。 6.2 视图解析器在 XML 文件中，通过如下方式配置视图解析器： 1234&lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt;&lt;/bean&gt; 如果通过 Java 类，一样也可以实现类似功能。 首先为我们的项目添加 webapp 目录，webapp 目录中添加一个 jsp 目录，jsp 目录中添加 jsp 文件： 然后引入 JSP 的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet.jsp-api&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt;&lt;/dependency&gt; 然后，在配置类中，继续重写方法： 12345678@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void configureViewResolvers(ViewResolverRegistry registry) { registry.jsp(\"/jsp/\", \".jsp\"); }} 接下来，在 Controller 中添加控制器即可访问 JSP 页面： 1234567@Controllerpublic class HelloController2 { @GetMapping(\"/hello2\") public String hello() { return \"hello\"; }} 6.3 路径映射有的时候，我们的控制器的作用仅仅只是一个跳转，就像上面小节中的控制器，里边没有任何业务逻辑，像这种情况，可以不用定义方法，可以直接通过路径映射来实现页面访问。如果在 XML 中配置路径映射，如下： 1&lt;mvc:view-controller path=\"/hello\" view-name=\"hello\" status-code=\"200\"/&gt; 这行配置，表示如果用户访问 /hello 这个路径，则直接将名为 hello 的视图返回给用户，并且响应码为 200，这个配置就可以替代 Controller 中的方法。 相同的需求，如果在 Java 代码中，写法如下： 12345678@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(\"/hello3\").setViewName(\"hello\"); }} 此时，用户访问 /hello3 接口，就能看到名为 hello 的视图文件。 6.4 JSON 配置SpringMVC 可以接收JSON 参数，也可以返回 JSON 参数，这一切依赖于 HttpMessageConverter。 HttpMessageConverter 可以将一个 JSON 字符串转为 对象，也可以将一个对象转为 JSON 字符串，实际上它的底层还是依赖于具体的 JSON 库。 所有的 JSON 库要在 SpringMVC 中自动返回或者接收 JSON，都必须提供和自己相关的 HttpMessageConverter 。 SpringMVC 中，默认提供了 Jackson 和 gson 的 HttpMessageConverter ，分别是：MappingJackson2HttpMessageConverter 和 GsonHttpMessageConverter 。 正因为如此，我们在 SpringMVC 中，如果要使用 JSON ，对于 jackson 和 gson 我们只需要添加依赖，加完依赖就可以直接使用了。具体的配置是在 AllEncompassingFormHttpMessageConverter 类中完成的。 如果开发者使用了 fastjson，那么默认情况下，SpringMVC 并没有提供 fastjson 的 HttpMessageConverter ，这个需要我们自己提供，如果是在 XML 配置中，fastjson 除了加依赖，还要显式配置 HttpMessageConverter，如下： 123456&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=\"com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter\"&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; 在 Java 配置的 SSM 中，我们一样也可以添加这样的配置： 12345678910111213@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); converter.setDefaultCharset(Charset.forName(\"UTF-8\")); FastJsonConfig fastJsonConfig = new FastJsonConfig(); fastJsonConfig.setCharset(Charset.forName(\"UTF-8\")); converter.setFastJsonConfig(fastJsonConfig); converters.add(converter); }} 然后，就可以在接口中直接返回 JSON 了，此时的 JSON 数据将通过 fastjson 生成。 总结好了，本文通过一个简单的例子向读者展示了使用 Java 来配置 Spring+SpringMVC 环境，事实上，只要这两个配置 OK ，再加入 MyBatis 就是非常容易的事了，本文相关的案例松哥已经上传到 GitHub 上了：https://github.com/lenve/javaboy-code-samples。 关于本文，有问题欢迎留言讨论。","link":"/2019/0528/javassm.html"},{"title":"1024，20个入行故事，浮世中见证着程序员的奋斗","text":"上周，在 1024 来临之际，松哥向大家征集程序员的入行故事，收到了许多小伙伴的留言。 有杭州某宝出来的大佬，也有华中某 985 毕业的程序员，也有自学转行的牛人，还有一些通过培训转行的程序员，每个人都有自己的精彩故事。 我一直觉得一个优秀的程序员必然是不停的学习，这些学习的故事见证了一个人技术之路上的成长。小伙伴们的留言很多都很精彩，无法一一列举，松哥这里就只遴选了 22 个入行故事和大家分享，哪位小伙伴的故事令你印象深刻，可以在文末给他投上一票，那么他就有可能获取松哥准备的大奖。 由于有的小伙伴需要匿名，所以我这里就用开发语言来来给每个故事进行编号，令你印象深刻的故事可以记着对应的开发语言哦，然后在文末进行投票。 C 18 岁前属于基耕道文青，破三本，18岁前没想过做工程师相关的职业，18那年在学校遇到了一帮有志青年，加入某学校技术组织，遇到了头儿和狮虎，19岁稀里糊涂从学校生活娱乐类网站某网编编程了某java后台开发，此后二年接触jsp servlet 自写小框架，struts1 struts2 springMVC…ajax…java 虚拟机…而后一年际遇高人恩师，去了杭州某宝，且与恩人合伙，又二年，家中召唤，遂携资回蓉，去了某成研院，不敢言衣锦二次，已饱腹无虑，路遇佳人，成缘，遂稀里糊涂买买买，买了好些水泥，而后至今，无变动，其间某司上市，亦获其蝇头小利，与水泥获利相比，不够塞牙，但看某司浪潮浪起，身边人来人去，终觉无趣，正欲归隐。 Python 两周面试了 7 家公司，最终在今年 3 月 20 在合肥入职，这六个月一路走来，真的很不容易！这也是我第一次从学校出来，正式步入社会，每天过着 12 点睡觉，6 点起床的日子，一刻都不敢多睡，因为我知道，新人只有多学习，才不会落伍，我每天争分夺秒的与时间竞争，我不敢睡觉，我甚至认为睡觉就是在浪费时间，哪天如果多睡几个小时，就会感觉到内疚，在陌生的城市，一个人，真的很不容易！加油。 Scala 我来讲讲我去华为实习的坑，实习刚开始被一个项目组老大联系，说的做的项目很高端，到了之后才发现项目组去年刚成立，大家都是刚从另一个方向转过来的，做的东西也和说的项目差别很大。进入项目组后，刚开始让花很大精力标注数据，时常会加班，有时候会在 6:30 之后开会。实习答辩，实习生的通过率大概在 1/3–1/2，周围很多参加秋招的同学都收到 offer 签约了。从 8 月中下旬答辩通过，等到现在都没有一点消息。听周围同学说，当实习岗位和正式工作内容不一致时（算法干开发，开发干测试）。这样就算通过答辩，还需要再重新面试一次，甚至重新参加秋招。（有个同学就是 8 月答辩，10 月中旬才通知让重新参加秋招，黄瓜菜早都凉了） C++ 谁能想到，我一个日语专业毕业的，会走上 JAVA 码农的道路。因为机遇，日本公司想试着在中国招聘没 JAVA 基础的日语系毕业生简单培训/自学四个月，即将踏上日本国土，唉，现在是日语天天被批评 C 17 岁某马培训出来的，找了大半年的工作，因为学历被拒绝了很多家，后来终于有一家录取我了，公司地方在河北沧州，过去以后被骗了，依稀还记得是 6 月 18 号到的，被关到了 6 月 23 号晚上 10 点放出来的，还被贷款了一万块钱财（我只想在这说 不是可怜啥的 只想提醒各位在找工作的和要出去找工作的一个提醒 一定要去查一下该公司 公司包住的千万一定一定要查 特别是外地让你过去的，直接去公司报道 不要相信人家会来接你），回到家后我还是在找工作 后来在找工作的 q 群里面找到了（当时我在那几个工作群很活跃 很多人知道我 我退群了，后来我加群后 继续找工作 然后说出来这个悲伤的故事）有几个好心人就收留了我，开的 3k 我现在还在公司做，已经有快3个多月了，加油吧，相信自己，万事开头难，没有什么过不去的坎 Java 我是大学修的双学位，主冶金(打铁)，另外一个是计算机，入了个门。也是在大学认识的女友，到现在七年了，今年领的证。大学毕业了没想着去干计算机相关的，就跑去钢铁厂了。后面女友快研究生毕业了，感觉来钢铁厂太亏待她了，就让她找工作往大城市找，自己自学 Java 也去同样的城市找工作。在钢铁厂上班的最后一年，一下班就自学，然后后面辞职了三个月自学再去找工作。那时候也是菜，找了半个多月才入的坑。1024了，keep study，day day up .NET 第一次听到 1024 还是去年这个时候吧。那时候还在培训班培训，转眼出来工作快一年了。身为非科班的我，在帝都找了两个月找到一份勉强能保障自己生活的工作。这一年来，每天有时间就会不断的学习，因为我清楚我与别人的差距(基础很不扎实)。可能只有类似我这种的，才会清楚压力有多吧。这里不说培训班的好坏。至少它带你入了门。幸好努力还是有回报的，比起之前确实成长太多了。薪资也成长了蛮多。但是我惊喜的是我好像对编程真的感兴趣。 JavaScript 因为家里发生了一些事情，所以离开了之前的工厂，来的北京，由于自己原来的专业属于重工业，在北京几乎找不到工作，所以就一边尝试新工作，一边发现北京这个地，什么适合自己，后来发现 Java 比较适合我，在咨询了一个同学后，决定学 Java。自学了差不多一个月吧，由于是闭门造车，结果没方向，不知道学些什么，后来报班学了半年，开始工作了 SQL 17 年电气工程及其自动化毕业，毕业做了大半年发电厂工人，然后本来想去培训机构学 Python，当时还没开班，就先学了 java，从此爱上 java，感谢李刚老师，他的书写的详细易懂，给了我很多帮助，后来培训完，开始找工作，换工作，工作不到一年踩了很多坑，很艰难，但从来没后悔过 PHP 2013 年底从山东滨州回到西安大本营（魏桥铝业相信山东的人并不陌生），头也不回的把抡大锤的工作辞掉了，回来也不知道干啥，陪朋友在西安送了三个月的快递，（对于我这种路痴，地方都找不到，更何况让我送快递），偶尔一次用朋友电脑上网，刚开始想学平面设计（一看要有画画的基础，立马觉得不合适，连个鸟都不会画），页面上显示出了编程的培训机构，达内（说有一周试听课），听了一周，就被老师连”拐带忽悠 ”的入了”坑“， 2014 年底终于出山，到现在 2019 年了， 5 年虽有过挫折困苦，但我们必须碾破困苦，我是程序员，我骄傲。 Objective-C 18 年决心学编程，为什么选择 Java 呢？曾经用塞班玩游戏的时候那个一杯咖啡启动画面深深印在心中，当在选择什么编程语言的时候，搜了下语言排行榜，接着各种百度排行第一的 Java，当搜索结果出现那杯咖啡，我滴个亲娘，好亲切的 log,那是一杯我的青春啊。然后在我要自学网自学了 Java 基础，感觉 Java 可以搞，后来就报了个培训班，现在是馅在那杯咖啡里无法自拔。 Groovy 大四学生，之前一年一直在刚 cv，本来打算出国继续读，幸好看清了行情(可能是自己设水平不够），刚转学开发 10 天，幸好有一点基础，一开始就看了松哥视频，感觉完全就是我想要的！太棒了！昨天刚开始看微人事，加油大家！！！ Perl 百度：喵喵喵，这也能黑我？都怪我年少不懂事，经不住诱惑一时脚滑，进了一个 1024 社区，不然纯洁的我哪里能成为纵横X矧?燉鵡羖\u0015は繵脺的老司机呢？至于为什么走向开发，还不是因为要饿死了，2016 年华中某 985 劝退专业毕业，去广州某国企拿 3000 工资，要不是地方偏僻，钱不太好用出去，讲不定真饿死咯。给我点赞的估计都是老司机了，也别问我要资源了，我已经从良了，被杀了一网盘也就剩个硬盘了。 Ruby 15 年机械专业毕业。农村娃，高考完选专业的时候也是啥都不了解，就想学个工科，当时把专业里面带自动化的全选了一遍，结果就入了机械的坑。大学生活过得平淡无奇，虽然对专业没有多大兴趣，但是成绩还可以，毕业之后，进入了一家国企，一干就是三年。在国企，低微的收入，压抑的工作氛围，还有一些官僚主义让我迷茫彷徨，借鉴周围朋友的成功经历，让我下定决心转行学 IT。去年这个时候辞职，直接去找了一家 JAVA 培训机构，学费一万八，培训了 5 个月吧。培训学习的那段时间很辛苦但是也很充实，每天都是新的知识，白天上一天，晚上还有自习，甚至回到家里还要学习一会儿，每天一两点睡觉再正常不过了。周末基本上没有出去过，都是待在房子里面看视频，做笔记。中间也一度迷茫过，感觉没自信，学得不好，不过还好，第一次做项目的时候我负责了大部分的工作，最后得到了班里唯一的一个最佳个人奖，感觉自信了好多。培训结束之后，也是包装了工作经验去找的，虽然没有真正的工作经验，但是基础理论知识还是比较扎实的，也是顺利入职。现在工作已经快半年了，虽然也一度也很艰难，但是我很庆幸自己选对了路，因为对这个感兴趣，而且收入几乎是之前的两倍。现在每天也都在努力地提高着自己。总之一句话，相信自己，脚踏实地，一定会有个好结果的！＾０＾~ Go 填志愿偶然选中软件专业。高考分不到二本线，也算是意料之中。考虑过了，没打算复习，没打算上三本，我希望减轻父亲的压力，早点读完大学出去工作，所以我选择了大专。在填报志愿时，随便选了一个大专，软件开发（两年制），这个专业也跟我喜欢玩游戏有关系，其他志愿随便填，填完志愿就上工地打工了。大学的我活成了废物。在大学，白天上课随便听，想睡就睡，作业随便抄；晚上在寝室，沉迷于游戏，疯狂熬夜。大学一年半，游戏占满了我的课余和课上时间；大学没有参加过一个课外活动，图书馆没去过。求职莫名进入培训的坑。我海投简历，各种职位；收到一个软件教育公司的面试，笔试没过；但他们说可以参加内部培训提高自己，免费一个月；我去了，后来才知道这种招聘叫‘招转培’。我参加的是前端培训，遇到了各样的学员。这期间，我接到过学校老师的电话，问我工作情况，她听了我的经历，说这是招转培，希望我能去达内培训，毕竟我们是校企合作，有优惠的补贴，我说考虑一下。培训20天左右，这边让我们签合同：全付或分期付，我决定离开。当时的我也不知道要做些什么，不知道能干什么工作，就这样又迷迷糊糊去参加了培训。两天后，我来到达内，开始了 5 个月的 Java 培训，很辛苦，但也收获满满。现在我工作了大概7个月了，很感谢经理能给我这个机会，他没有多问我的技术，我也实话实说，没有实际经验（培训的都包装），他说他是过来人，想拉我一把，我现在想想都很感动。 Swift 松哥好，我是来自苏州的一名菜鸟程序员，第一次留言，最近空闲时候在看你的文章，收益匪浅，我虽然毕业三年，但是今年初才转行做开发(去年开始自学 Java 和数据库这类开发知识，本来打算报班的，但是公司同意我转开发岗位)，一路磕磕碰碰，但是越战越勇，公司做 mes 业务的，Java 开发主要是移动端和看板系统，业务方面是罗克韦尔的 mes 开发平台。但是我依然坚持空闲时间写写 Java 练习，熟悉主流的框架(最近在写一个前后端分离的后台管理案例)，可以的话，希望松哥能抽到我，送我一本签名版springboot+vue全栈的书，最后衷心祝愿松哥和广大其他的程序员节日快乐！ VB 要说为什么要入这行，其实也要从我高考填志愿说起，我跟松哥一样都是农村的，因此也对大学专业这些是真的一点都不懂，我当时是怎么选志愿的呢？我当时选的是跟汽车，电脑还有化学制药方面有关的。我当时觉得我们社会车挺多的，感觉跟车有关的专业可能以后比较好找工作，至于电脑我当时比较喜欢玩游戏，化学制药则是我家里人推荐的。录取通知下来后我是有点后悔的，为什么？因为学费贵，计算机专业几乎是我们学校最贵的几个二本专业了，其他大部分专业包括土木以及我们学校的王牌专业车辆都是 4500 一学年，而我的则是 7900，讲真我也搞不懂为什么就我们这个专业最贵，但后悔也没办法只能去了啊，大学四年我就没有松哥那么勤奋了，直到大三下半年我的专业知识依然只停留在那本 Java 教材里，甚至连 eclipse 都不会装，所幸在后面去了培训机构打基础，我很庆幸在里面学习到了很多的基础知识，也知道在这个行业要学习的路线，基础差不是最致命的，最致命的是不知道学习的方向，到现在我终于成为了 IT 行业的新人。 Kotlin 2013,14 年的时候，拥有了一台 Windows Phone 手机，系统是微软的 WP 系统，很喜欢，但是生态太差了，很多软件都没有，甚至想到，既然没有，那就自己做吧，找了 C# 的视频，后面去了培训，发现没有教 WP 软件开发的了 ，最终学了安卓开发，现在已经做了 3 年半的安卓仔 MATLAB 18 年毕业，现在边工作边学习。在校期间也稍微学习点 Java 基础。真正学习估计是工作吧，其实一直对这个行业充满了好奇，就是不知道怎么学习。从刚开始的看学习资料再到学习视频，每一个小白都是这么过来的吧。加油 SAS 听说程序员每天九点上班，而且大牛可以在家里上班。我就转行了，我喜欢睡懒觉。后来…发现我想多了。 Scratch 当初选计算机专业，是可能是电影看多了，看到黑客、红客，各种耍帅，入了坑发现了很大的问题，黑客一定懂编程，但是懂编程不一定是黑客。 F 软件工程专业，入行差不多两年吧！可惜工作一直不是开发，一直在锻炼开发技能，同样很迷茫，想考个事业单位，也想在企业打拼，也想独立出来创业，最近也在考虑结婚生子哈哈哈，最近才开始坚持写博客，准备软考，96年的我呀！加油！ 很多小伙伴的故事让人动容，真是应了那句话，成年人的世界没有容易二字。那么哪位小伙伴的故事让你印象深刻呢？可以多选哦。 投票结果排名前五名的小伙伴可以从以下三个奖品中任选一个，投票时间截止到今晚(10月24) 23:59。 签名版 《Spring Boot + Vue 全栈开发实战》 一本 松哥录制的 Spring Boot2 系列视频教程一套（1-15章） 京东上 80 元以内的图书任选一本 没有谁是容易的，我每天也是早起晚睡。早上起来录视频，晚上写博客，剪视频。技术之路，松哥和各位小伙伴一起进步。","link":"/2019/1024/1024.html"},{"title":"一文读懂 Spring Data Jpa！","text":"有很多读者留言希望松哥能好好聊聊 Spring Data Jpa!其实这个话题松哥以前零零散散的介绍过，在我的书里也有介绍过，但是在公众号中还没和大伙聊过，因此本文就和大家来仔细聊聊 Spring Data 和 Jpa! 故事的主角Jpa1. JPA是什么 Java Persistence API：用于对象持久化的 API Java EE 5.0 平台标准的 ORM 规范，使得应用程序以统一的方式访问持久层 2. JPA和Hibernate的关系 JPA 是 Hibernate 的一个抽象（就像JDBC和JDBC驱动的关系）； JPA 是规范：JPA 本质上就是一种 ORM 规范，不是ORM 框架，这是因为 JPA 并未提供 ORM 实现，它只是制订了一些规范，提供了一些编程的 API 接口，但具体实现则由 ORM 厂商提供实现； Hibernate 是实现：Hibernate 除了作为 ORM 框架之外，它也是一种 JPA 实现 从功能上来说， JPA 是 Hibernate 功能的一个子集 3. JPA的供应商JPA 的目标之一是制定一个可以由很多供应商实现的 API，Hibernate 3.2+、TopLink 10.1+ 以及 OpenJPA 都提供了 JPA 的实现，Jpa 供应商有很多，常见的有如下四种： HibernateJPA 的始作俑者就是 Hibernate 的作者，Hibernate 从 3.2 开始兼容 JPA。 OpenJPAOpenJPA 是 Apache 组织提供的开源项目。 TopLinkTopLink 以前需要收费，如今开源了。 EclipseLink 4. JPA的优势 标准化: 提供相同的 API，这保证了基于JPA 开发的企业应用能够经过少量的修改就能够在不同的 JPA 框架下运行。 简单易用，集成方便: JPA 的主要目标之一就是提供更加简单的编程模型，在 JPA 框架下创建实体和创建 Java 类一样简单，只需要使用 javax.persistence.Entity 进行注解；JPA 的框架和接口也都非常简单。 可媲美JDBC的查询能力: JPA的查询语言是面向对象的，JPA定义了独特的JPQL，而且能够支持批量更新和修改、JOIN、GROUP BY、HAVING 等通常只有 SQL 才能够提供的高级查询特性，甚至还能够支持子查询。 支持面向对象的高级特性: JPA 中能够支持面向对象的高级特性，如类之间的继承、多态和类之间的复杂关系，最大限度的使用面向对象的模型 5. JPA包含的技术 ORM 映射元数据：JPA 支持 XML 和 JDK 5.0 注解两种元数据的形式，元数据描述对象和表之间的映射关系，框架据此将实体对象持久化到数据库表中。 JPA 的 API：用来操作实体对象，执行CRUD操作，框架在后台完成所有的事情，开发者从繁琐的 JDBC 和 SQL 代码中解脱出来。 查询语言（JPQL）：这是持久化操作中很重要的一个方面，通过面向对象而非面向数据库的查询语言查询数据，避免程序和具体的 SQL 紧密耦合。 Spring DataSpring Data 是 Spring 的一个子项目。用于简化数据库访问，支持NoSQL 和 关系数据存储。其主要目标是使数据库的访问变得方便快捷。Spring Data 具有如下特点： SpringData 项目支持 NoSQL 存储：MongoDB （文档数据库）Neo4j（图形数据库）Redis（键/值存储）Hbase（列族数据库） SpringData 项目所支持的关系数据存储技术：JDBCJPA Spring Data Jpa 致力于减少数据访问层 (DAO) 的开发量. 开发者唯一要做的，就是声明持久层的接口，其他都交给 Spring Data JPA 来帮你完成！ 框架怎么可能代替开发者实现业务逻辑呢？比如：当有一个 UserDao.findUserById() 这样一个方法声明，大致应该能判断出这是根据给定条件的 ID 查询出满足条件的 User 对象。Spring Data JPA 做的便是规范方法的名字，根据符合规范的名字来确定方法需要实现什么样的逻辑。 主角的故事Jpa 的故事为了让大伙彻底把这两个东西学会，这里我就先来介绍单纯的Jpa使用，然后我们再结合 Spring Data 来看 Jpa如何使用。 整体步骤如下： 1.使用 IntelliJ IDEA 创建项目，创建时选择 JavaEE Persistence ，如下： 2.创建成功后，添加依赖jar，由于 Jpa 只是一个规范，因此我们说用Jpa实际上必然是用Jpa的某一种实现，那么是哪一种实现呢？当然就是Hibernate了，所以添加的jar，实际上来自 Hibernate，如下： 3.添加实体类 接下来在项目中添加实体类，如下： 12345678910111213@Entity(name = \"t_book\")public class Book { private Long id; private String name; private String author; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) public Long getId() { return id; } // 省略其他getter/setter} 首先@Entity注解表示这是一个实体类，那么在项目启动时会自动针对该类生成一张表，默认的表名为类名，@Entity注解的name属性表示自定义生成的表名。@Id注解表示这个字段是一个id，@GeneratedValue注解表示主键的自增长策略，对于类中的其他属性，默认都会根据属性名在表中生成相应的字段，字段名和属性名相同，如果开发者想要对字段进行定制，可以使用@Column注解，去配置字段的名称，长度，是否为空等等。 4.创建 persistence.xml 文件 JPA 规范要求在类路径的 META-INF 目录下放置persistence.xml，文件的名称是固定的 123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;persistence xmlns=\"http://java.sun.com/xml/ns/persistence\" version=\"2.0\"&gt; &lt;persistence-unit name=\"NewPersistenceUnit\" transaction-type=\"RESOURCE_LOCAL\"&gt; &lt;provider&gt;org.hibernate.jpa.HibernatePersistenceProvider&lt;/provider&gt; &lt;class&gt;org.sang.Book&lt;/class&gt; &lt;properties&gt; &lt;property name=\"hibernate.connection.url\" value=\"jdbc:mysql:///jpa01?useUnicode=true&amp;amp;characterEncoding=UTF-8\"/&gt; &lt;property name=\"hibernate.connection.driver_class\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"hibernate.connection.username\" value=\"root\"/&gt; &lt;property name=\"hibernate.connection.password\" value=\"123\"/&gt; &lt;property name=\"hibernate.archive.autodetection\" value=\"class\"/&gt; &lt;property name=\"hibernate.show_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.format_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.hbm2ddl.auto\" value=\"update\"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 注意： persistence-unit 的name 属性用于定义持久化单元的名字, 必填。 transaction-type：指定 JPA 的事务处理策略。RESOURCE_LOCAL：默认值，数据库级别的事务，只能针对一种数据库，不支持分布式事务。如果需要支持分布式事务，使用JTA：transaction-type=”JTA” class节点表示显式的列出实体类 properties中的配置分为两部分：数据库连接信息以及Hibernate信息 执行持久化操作 1234567891011EntityManagerFactory entityManagerFactory = Persistence.createEntityManagerFactory(\"NewPersistenceUnit\");EntityManager manager = entityManagerFactory.createEntityManager();EntityTransaction transaction = manager.getTransaction();transaction.begin();Book book = new Book();book.setAuthor(\"罗贯中\");book.setName(\"三国演义\");manager.persist(book);transaction.commit();manager.close();entityManagerFactory.close(); 这里首先根据配置文件创建出来一个 EntityManagerFactory ，然后再根据 EntityManagerFactory 的实例创建出来一个 EntityManager ，然后再开启事务，调用 EntityManager 中的 persist 方法执行一次持久化操作，最后提交事务，执行完这些操作后，数据库中旧多出来一个 t_book 表，并且表中有一条数据。 关于 JPQL JPQL语言，即 Java Persistence Query Language 的简称。JPQL 是一种和 SQL 非常类似的中间性和对象化查询语言，它最终会被编译成针对不同底层数据库的 SQL 查询，从而屏蔽不同数据库的差异。JPQL语言的语句可以是 select 语句、update 语句或delete语句，它们都通过 Query 接口封装执行。 Query接口封装了执行数据库查询的相关方法。调用 EntityManager 的 createQuery、create NamedQuery 及 createNativeQuery 方法可以获得查询对象，进而可调用 Query 接口的相关方法来执行查询操作。 Query接口的主要方法如下： int executeUpdate(); | 用于执行update或delete语句。 List getResultList(); | 用于执行select语句并返回结果集实体列表。 Object getSingleResult(); | 用于执行只返回单个结果实体的select语句。 Query setFirstResult(int startPosition); | 用于设置从哪个实体记录开始返回查询结果。 Query setMaxResults(int maxResult); | 用于设置返回结果实体的最大数。与setFirstResult结合使用可实现分页查询。 Query setFlushMode(FlushModeType flushMode); | 设置查询对象的Flush模式。参数可以取2个枚举值：FlushModeType.AUTO 为自动更新数据库记录，FlushMode Type.COMMIT 为直到提交事务时才更新数据库记录。 setHint(String hintName, Object value); | 设置与查询对象相关的特定供应商参数或提示信息。参数名及其取值需要参考特定 JPA 实现库提供商的文档。如果第二个参数无效将抛出IllegalArgumentException异常。 setParameter(int position, Object value); | 为查询语句的指定位置参数赋值。Position 指定参数序号，value 为赋给参数的值。 setParameter(int position, Date d, TemporalType type); | 为查询语句的指定位置参数赋 Date 值。Position 指定参数序号，value 为赋给参数的值，temporalType 取 TemporalType 的枚举常量，包括 DATE、TIME 及 TIMESTAMP 三个，，用于将 Java 的 Date 型值临时转换为数据库支持的日期时间类型（java.sql.Date、java.sql.Time及java.sql.Timestamp）。 setParameter(int position, Calendar c, TemporalType type); | 为查询语句的指定位置参数赋 Calenda r值。position 指定参数序号，value 为赋给参数的值，temporalType 的含义及取舍同前。 setParameter(String name, Object value); | 为查询语句的指定名称参数赋值。 setParameter(String name, Date d, TemporalType type); | 为查询语句的指定名称参数赋 Date 值,用法同前。 setParameter(String name, Calendar c, TemporalType type); | 为查询语句的指定名称参数设置Calendar值。name为参数名，其它同前。该方法调用时如果参数位置或参数名不正确，或者所赋的参数值类型不匹配，将抛出 IllegalArgumentException 异常。 JPQL 举例和在 SQL 中一样，JPQL 中的 select 语句用于执行查询。其语法可表示为：select_clause form_clause [where_clause] [groupby_clause] [having_clause] [orderby_clause] 其中： from 子句是查询语句的必选子句。 select 用来指定查询返回的结果实体或实体的某些属性。 from 子句声明查询源实体类，并指定标识符变量（相当于SQL表的别名）。 如果不希望返回重复实体，可使用关键字 distinct 修饰。select、from 都是 JPQL 的关键字，通常全大写或全小写，建议不要大小写混用。 在 JPQL 中，查询所有实体的 JPQL 查询语句很简单，如下：select o from Order o 或 select o from Order as o这里关键字 as 可以省去，标识符变量的命名规范与 Java 标识符相同，且区分大小写,调用 EntityManager 的 createQuery() 方法可创建查询对象，接着调用 Query 接口的 getResultList() 方法就可获得查询结果集，如下： 123456Query query = entityManager.createQuery( \"select o from Order o\"); List orders = query.getResultList();Iterator iterator = orders.iterator();while(iterator.hasNext() ) { // 处理Order} 其他方法的与此类似，这里不再赘述。 Spring Data 的故事在 Spring Boot 中，Spring Data Jpa 官方封装了太多东西了，导致很多人用的时候不知道底层到底是怎么配置的，本文就和大伙来看看在手工的Spring环境下，Spring Data Jpa要怎么配置，配置完成后，用法和 Spring Boot 中的用法是一致的。 基本环境搭建首先创建一个普通的Maven工程，并添加如下依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-oxm&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;5.2.12.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-jpamodelgen&lt;/artifactId&gt; &lt;version&gt;5.2.12.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.29&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;1.11.3.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 这里除了 Jpa 的依赖之外，就是Spring Data Jpa 的依赖了。 接下来创建一个 User 实体类，创建方式参考 Jpa中实体类的创建方式，这里不再赘述。 接下来在resources目录下创建一个applicationContext.xml文件，并配置Spring和Jpa，如下： 123456789101112131415161718192021222324252627282930&lt;context:property-placeholder location=\"classpath:db.properties\"/&gt;&lt;context:component-scan base-package=\"org.sang\"/&gt;&lt;bean class=\"com.alibaba.druid.pool.DruidDataSource\" id=\"dataSource\"&gt; &lt;property name=\"driverClassName\" value=\"${db.driver}\"/&gt; &lt;property name=\"url\" value=\"${db.url}\"/&gt; &lt;property name=\"username\" value=\"${db.username}\"/&gt; &lt;property name=\"password\" value=\"${db.password}\"/&gt;&lt;/bean&gt;&lt;bean class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\" id=\"entityManagerFactory\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"jpaVendorAdapter\"&gt; &lt;bean class=\"org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter\"/&gt; &lt;/property&gt; &lt;property name=\"packagesToScan\" value=\"org.sang.model\"/&gt; &lt;property name=\"jpaProperties\"&gt; &lt;props&gt; &lt;prop key=\"hibernate.show_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQL57Dialect&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean class=\"org.springframework.orm.jpa.JpaTransactionManager\" id=\"transactionManager\"&gt; &lt;property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt;&lt;!-- 配置jpa --&gt;&lt;jpa:repositories base-package=\"org.sang.dao\" entity-manager-factory-ref=\"entityManagerFactory\"/&gt; 这里和 Jpa 相关的配置主要是三个，一个是entityManagerFactory，一个是Jpa的事务，还有一个是配置dao的位置，配置完成后，就可以在 org.sang.dao 包下创建相应的 Repository 了，如下： 123public interface UserDao extends Repository&lt;User, Long&gt; { User getUserById(Long id);} getUserById表示根据id去查询User对象，只要我们的方法名称符合类似的规范，就不需要写SQL，具体的规范一会来说。好了，接下来，创建 Service 和 Controller 来调用这个方法，如下： 12345678910111213141516@Service@Transactionalpublic class UserService { @Resource UserDao userDao; public User getUserById(Long id) { return userDao.getUserById(id); }}public void test1() { ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); UserService userService = ctx.getBean(UserService.class); User user = userService.getUserById(1L); System.out.println(user);} 这样，就可以查询到id为1的用户了。 Repository上文我们自定义的 UserDao 实现了 Repository 接口，这个 Repository 接口是什么来头呢？ 首先来看 Repository 的一个继承关系图： 可以看到，实现类不少。那么到底如何理解 Repository 呢？ Repository 接口是 Spring Data 的一个核心接口，它不提供任何方法，开发者需要在自己定义的接口中声明需要的方法 public interface Repository&lt;T, ID extends Serializable&gt; { } 若我们定义的接口继承了 Repository, 则该接口会被 IOC 容器识别为一个 Repository Bean，进而纳入到 IOC 容器中，进而可以在该接口中定义满足一定规范的方法。 Spring Data可以让我们只定义接口，只要遵循 Spring Data 的规范，就无需写实现类。 与继承 Repository 等价的一种方式，就是在持久层接口上使用 @RepositoryDefinition 注解，并为其指定 domainClass 和 idClass 属性。像下面这样： 123456@RepositoryDefinition(domainClass = User.class, idClass = Long.class)public interface UserDao{ User findById(Long id); List&lt;User&gt; findAll();} 基础的 Repository 提供了最基本的数据访问功能，其几个子接口则扩展了一些功能，它的几个常用的实现类如下： CrudRepository： 继承 Repository，实现了一组 CRUD 相关的方法 PagingAndSortingRepository： 继承 CrudRepository，实现了一组分页排序相关的方法 JpaRepository： 继承 PagingAndSortingRepository，实现一组 JPA 规范相关的方法 自定义的 XxxxRepository 需要继承 JpaRepository，这样的 XxxxRepository 接口就具备了通用的数据访问控制层的能力。 JpaSpecificationExecutor： 不属于Repository体系，实现一组 JPA Criteria 查询相关的方法 方法定义规范1.简单条件查询 按照 Spring Data 的规范，查询方法以 find | read | get 开头 涉及条件查询时，条件的属性用条件关键字连接，要注意的是：条件属性以首字母大写 例如：定义一个 Entity 实体类： 1234class User｛ private String firstName; private String lastName; ｝ 使用And条件连接时，条件的属性名称与个数要与参数的位置与个数一一对应，如下： 1findByLastNameAndFirstName(String lastName,String firstName); 支持属性的级联查询. 若当前类有符合条件的属性, 则优先使用, 而不使用级联属性. 若需要使用级联属性, 则属性之间使用 _ 进行连接. 查询举例：1.按照id查询 12User getUserById(Long id);User getById(Long id); 2.查询所有年龄小于90岁的人 1List&lt;User&gt; findByAgeLessThan(Long age); 3.查询所有姓赵的人 1List&lt;User&gt; findByUsernameStartingWith(String u); 4.查询所有姓赵的、并且id大于50的人 1List&lt;User&gt; findByUsernameStartingWithAndIdGreaterThan(String name, Long id); 5.查询所有姓名中包含”上”字的人 1List&lt;User&gt; findByUsernameContaining(String name); 6.查询所有姓赵的或者年龄大于90岁的 1List&lt;User&gt; findByUsernameStartingWithOrAgeGreaterThan(String name, Long age); 7.查询所有角色为1的用户 1List&lt;User&gt; findByRole_Id(Long id); 2.支持的关键字支持的查询关键字如下图： 3.查询方法流程解析为什么写上方法名，JPA就知道你想干嘛了呢？假如创建如下的查询：findByUserDepUuid()，框架在解析该方法时，首先剔除 findBy，然后对剩下的属性进行解析，假设查询实体为Doc： 先判断 userDepUuid （根据 POJO 规范，首字母变为小写）是否为查询实体的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，继续第二步； 从右往左截取第一个大写字母开头的字符串(此处为Uuid)，然后检查剩下的字符串是否为查询实体的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，则重复第二步，继续从右往左截取；最后假设 user 为查询实体的一个属性； 接着处理剩下部分（DepUuid），先判断 user 所对应的类型是否有depUuid属性，如果有，则表示该方法最终是根据 “ Doc.user.depUuid” 的取值进行查询；否则继续按照步骤 2 的规则从右往左截取，最终表示根据 “Doc.user.dep.uuid” 的值进行查询。 可能会存在一种特殊情况，比如 Doc包含一个 user 的属性，也有一个 userDep 属性，此时会存在混淆。可以明确在属性之间加上 “_” 以显式表达意图，比如 “findByUser_DepUuid()” 或者 “findByUserDep_uuid()” 还有一些特殊的参数：例如分页或排序的参数： 12Page&lt;UserModel&gt; findByName(String name, Pageable pageable); List&lt;UserModel&gt; findByName(String name, Sort sort); @Query注解有的时候，这里提供的查询关键字并不能满足我们的查询需求，这个时候就可以使用 @Query 关键字，来自定义查询 SQL，例如查询Id最大的User： 12@Query(\"select u from t_user u where id=(select max(id) from t_user)\")User getMaxIdUser(); 如果查询有参数的话，参数有两种不同的传递方式: 1.利用下标索引传参，索引参数如下所示，索引值从1开始，查询中 ”?X” 个数需要与方法定义的参数个数相一致，并且顺序也要一致： 12@Query(\"select u from t_user u where id&gt;?1 and username like ?2\")List&lt;User&gt; selectUserByParam(Long id, String name); 2.命名参数（推荐）：这种方式可以定义好参数名，赋值时采用@Param(“参数名”)，而不用管顺序： 12@Query(\"select u from t_user u where id&gt;:id and username like :name\")List&lt;User&gt; selectUserByParam2(@Param(\"name\") String name, @Param(\"id\") Long id); 查询时候，也可以是使用原生的SQL查询，如下： 12@Query(value = \"select * from t_user\",nativeQuery = true)List&lt;User&gt; selectAll(); @Modifying注解涉及到数据修改操作，可以使用 @Modifying 注解，@Query 与 @Modifying 这两个 annotation一起声明，可定义个性化更新操作，例如涉及某些字段更新时最为常用，示例如下： 123@Modifying@Query(\"update t_user set age=:age where id&gt;:id\")int updateUserById(@Param(\"age\") Long age, @Param(\"id\") Long id); 注意： 可以通过自定义的 JPQL 完成 UPDATE 和 DELETE 操作. 注意: JPQL 不支持使用 INSERT 方法的返回值应该是 int，表示更新语句所影响的行数 在调用的地方必须加事务，没有事务不能正常执行 默认情况下, Spring Data 的每个方法上有事务, 但都是一个只读事务. 他们不能完成修改操作 说到这里，再来顺便说说Spring Data 中的事务问题： Spring Data 提供了默认的事务处理方式，即所有的查询均声明为只读事务。 对于自定义的方法，如需改变 Spring Data 提供的事务默认方式，可以在方法上添加 @Transactional 注解。 进行多个 Repository 操作时，也应该使它们在同一个事务中处理，按照分层架构的思想，这部分属于业务逻辑层，因此，需要在Service 层实现对多个 Repository 的调用，并在相应的方法上声明事务。 好了，关于Spring Data Jpa 本文就先说这么多。","link":"/2019/0412/springdata-jpa.html"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/tags/Spring-Boot/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Jedis","slug":"Jedis","link":"/tags/Jedis/"},{"name":"MyBatis","slug":"MyBatis","link":"/tags/MyBatis/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"SpringMVC","slug":"SpringMVC","link":"/tags/SpringMVC/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"LiveReload","slug":"LiveReload","link":"/tags/LiveReload/"},{"name":"Https","slug":"Https","link":"/tags/Https/"},{"name":"yaml","slug":"yaml","link":"/tags/yaml/"},{"name":"CORS","slug":"CORS","link":"/tags/CORS/"},{"name":"JdbcTemplate","slug":"JdbcTemplate","link":"/tags/JdbcTemplate/"},{"name":"Spring Data","slug":"Spring-Data","link":"/tags/Spring-Data/"},{"name":"SpringSecurity","slug":"SpringSecurity","link":"/tags/SpringSecurity/"},{"name":"前后端分离","slug":"前后端分离","link":"/tags/前后端分离/"},{"name":"Swagger2","slug":"Swagger2","link":"/tags/Swagger2/"},{"name":"Spring Security","slug":"Spring-Security","link":"/tags/Spring-Security/"},{"name":"中间件","slug":"中间件","link":"/tags/中间件/"},{"name":"MyCat","slug":"MyCat","link":"/tags/MyCat/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"杂谈","slug":"杂谈","link":"/tags/杂谈/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"router","slug":"router","link":"/tags/router/"},{"name":"vue","slug":"vue","link":"/tags/vue/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"微服务","slug":"微服务","link":"/tags/微服务/"},{"name":"单体应用","slug":"单体应用","link":"/tags/单体应用/"},{"name":"Video","slug":"Video","link":"/tags/Video/"},{"name":"微人事","slug":"微人事","link":"/tags/微人事/"},{"name":"vhr","slug":"vhr","link":"/tags/vhr/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Chrome","slug":"Chrome","link":"/tags/Chrome/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"mail","slug":"mail","link":"/tags/mail/"},{"name":"学习资源","slug":"学习资源","link":"/tags/学习资源/"},{"name":"条件注解","slug":"条件注解","link":"/tags/条件注解/"},{"name":"life","slug":"life","link":"/tags/life/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"IDEA","slug":"IDEA","link":"/tags/IDEA/"},{"name":"JavaWeb","slug":"JavaWeb","link":"/tags/JavaWeb/"},{"name":"weixin","slug":"weixin","link":"/tags/weixin/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/tags/SpringBoot/"},{"name":"Freemarker","slug":"Freemarker","link":"/tags/Freemarker/"},{"name":"Shiro","slug":"Shiro","link":"/tags/Shiro/"},{"name":"Mail","slug":"Mail","link":"/tags/Mail/"},{"name":"Jpa","slug":"Jpa","link":"/tags/Jpa/"},{"name":"生活","slug":"生活","link":"/tags/生活/"},{"name":"Ehcache","slug":"Ehcache","link":"/tags/Ehcache/"},{"name":"JWT","slug":"JWT","link":"/tags/JWT/"},{"name":"面试","slug":"面试","link":"/tags/面试/"},{"name":"Thymeleaf","slug":"Thymeleaf","link":"/tags/Thymeleaf/"}],"categories":[{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/categories/Spring-Boot/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"Redis","slug":"Redis","link":"/categories/Redis/"},{"name":"MyBatis","slug":"MyBatis","link":"/categories/MyBatis/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"Nginx","slug":"Nginx","link":"/categories/Nginx/"},{"name":"前后端分离","slug":"前后端分离","link":"/categories/前后端分离/"},{"name":"SpringSecurity","slug":"SpringSecurity","link":"/categories/SpringSecurity/"},{"name":"SpringMVC","slug":"SpringMVC","link":"/categories/SpringMVC/"},{"name":"MongoDB","slug":"MongoDB","link":"/categories/MongoDB/"},{"name":"杂谈","slug":"杂谈","link":"/categories/杂谈/"},{"name":"vue","slug":"vue","link":"/categories/vue/"},{"name":"redis","slug":"redis","link":"/categories/redis/"},{"name":"微服务","slug":"微服务","link":"/categories/微服务/"},{"name":"微人事","slug":"微人事","link":"/categories/微人事/"},{"name":"Chrome","slug":"Chrome","link":"/categories/Chrome/"},{"name":"mail","slug":"mail","link":"/categories/mail/"},{"name":"学习资源","slug":"学习资源","link":"/categories/学习资源/"},{"name":"life","slug":"life","link":"/categories/life/"},{"name":"vhr","slug":"vhr","link":"/categories/vhr/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"GitHub","slug":"GitHub","link":"/categories/GitHub/"},{"name":"hexo","slug":"hexo","link":"/categories/hexo/"},{"name":"IDEA","slug":"IDEA","link":"/categories/IDEA/"},{"name":"JavaWeb","slug":"JavaWeb","link":"/categories/JavaWeb/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"生活","slug":"生活","link":"/categories/生活/"},{"name":"面试","slug":"面试","link":"/categories/面试/"}]}