{"pages":[{"title":"分类","text":"","link":"/categories/index.html"},{"title":"","text":"Docker 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } Docker 教程合集(顺序已经整理好) 1 Docker 入门及安装[Docker 系列-1] docker 如日中天，这不是单纯的炒概念，docker 确确实实解决了开发与运维的痛点，因此在企业开发中得到了非常广泛的使用，本文对于 docker 的这些基本知识点再做一些简单回顾。 1 Docker 容器基本操作[Docker 系列-2] docker 中的容器就是一个轻量级的虚拟机，是镜像运行起来的一个状态，本文就先来看看容器的基本操作。 1 Docker 容器高级操作[Docker 系列-3] 上篇文章向\b读者介绍了一个 Nginx 的例子，对于 Nginx 这样一个容器而言，当它启动成功后，我们不可避免的需要对 Nginx 进行的配置进行修改，\b那么这个修改要如何完成呢\b？且看下文。 1 Docker 镜像基本操作[Docker 系列-4] 镜像也是 docker 的核心组件之一，镜像时容器运行的基础，容器是镜像运行后的形态。前面我们介绍了容器的用法，今天来和大家聊聊镜像的问题。 1 DockerHub 与容器网络[Docker 系列-5] DockerHub 类似于 GitHub 提供的代码托管服务，Docker Hub 提供了镜像托管服务，Docker Hub 地址为 https://hub.docker.com/。 1 Docker 数据卷操作[Docker 系列-6] 在前面的案例中，如果我们需要将数据从宿主机拷贝到容器中，一般都是使用 Docker 的拷贝命令，这样性能还是稍微有点差，没有办法能够达到让这种拷贝达到本地磁盘 I/O 性能呢？有！ 1 Docker 容器连接[Docker 系列-7] 数据卷容器以及和大家聊过了，本文我们再来看看使用数据卷容器实现数据的备份与恢复，然后再来看看容器的连接操作。 1 Docker 容器编排入门[Docker 系列-8] 在实际的开发环境或者生产环境，容器往往都不是独立运行的，经常需要多个容器一起运行，此时，如果继续使用 run 命令启动容器，就会非常不便，在这种情况下，docker-compose 是一个不错的选择，使用 docker-compose 可以实现简单的容器编排,本文就来看看 docker-compose 的使用。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【牧码小子】，牧码小子专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【牧码小子】，【牧码小子】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/docker/index.html"},{"title":"我是谁","text":"大家好，我是江南一点雨，管理学学士，大学自学 Java 编程，从移动端到前端到后端均有涉猎，现在专注于 Java 微服务，我是 CSDN 博客专家、华为云云享专家、《Spring Boot + Vue全栈开发实战》 作者、运营了一个公众号 牧码小子，专注于 Spring Boot + 微服务技术分享，欢迎大家关注！关注公众号回复 Java，领取松哥为大家精心准备的 Java 干货！ 我的公众号 我的站点 独立站点： https://www.javaboy.org GitHub： https://github.com/lenve CSDN： http://wangsong.blog.csdn.net 思否： https://segmentfault.com/u/lenve 博客园： https://www.cnblogs.com/lenve 掘金： https://juejin.im/user/57d679af0bd1d000585012a7","link":"/about/index.html"},{"title":"","text":"Git 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } Git 教程合集(顺序已经整理好) 1 Git 概述 一直以来想出一个 Git 的教程，去年写过一篇，后来没了下文，烂尾了。最近忙里偷闲，还是想把这个 Git 系列写一遍，这次争取写完。 1 Git 基本操作 上篇文章我们简单的介绍了 Git 的诞生和发展，然后也说了 Windows 环境下 Git 的安装和一些基本的配置，本文我们就来说一说 Git 中的一些基本概念和基本操作。 1 Git 中的各种后悔药 Git 强大的撤销、版本回退功能，让我们在开发的过程中能够随意的回到任何一个时间点的状态，本文我们就来看看 Git 中的各种后悔药! 1 Git 分支管理 Svn 中也有分支管理，但是很 low，Git 的分支管理非常强大，本文先不去说分支管理内部到底怎么做的，我们先来看看 Git 中最基本的分支管理操作。 1 Git 关联远程仓库 前面我们介绍的所有操作都是在本地仓库完成的，本文我们主要来看看如何和远程仓库进行交互，为了方便起见，这里远程仓库我们选择 GitHub。 1 Git 工作区储藏 这是一篇计划之外的文章，之所以有这篇文章，是因为有一个小伙伴在阅读Git 分支管理一文时遇到了一个问题，而这个问题又比较典型，因此我想专门来谈谈 Git 中工作区的储藏问题。 1 Git 标签管理 我们可以针对某一次的提交打上一个标签，有点类似于给某次提交取个别名，比如 1.0 版本发布时打个标签叫 v1.0,2.0 版本发布时打个标签叫 v2.0 ，因为每次版本提交的结果都是一连串的哈希码，不容易记忆，打上 v1.0,v2.0 这些具有某种含义的标签后，可以方便我们进行版本管理。 1 Git 学习资料 关于Git的用法我们已经写七篇文章，介绍了Git的不少用法，这些足以应付工作中90%的需求了，剩下的10%就需要小伙伴们在工作中自己慢慢总结了，我这里再给小伙伴们推荐一点Git学习资料，为我们的Git系列画上一个句号。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【牧码小子】，牧码小子专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【牧码小子】，【牧码小子】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/git/index.html"},{"title":"","text":"最新版 Spring Boot2 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } 最新版 Spring Boot2 教程合集(顺序已经整理好) 1 创建一个 Spring Boot 项目，你会几种方法？ 现在 Spring Boot 最新稳定版是 2.1.5 ，松哥想针对此写一个系列教程，专门讲 Spring Boot2 中相关的知识点。这个系列，就从本篇开始吧。 1 这一次，我连 web.xml 都不要了，纯 Java 搭建 SSM 环境 在 Spring Boot 项目中，正常来说是不存在 XML 配置，这是因为 Spring Boot 不推荐使用 XML ，注意，并非不支持，Spring Boot 推荐开发者使用 Java 配置来搭建框架 1 你真的理解 Spring Boot 项目中的 parent 吗？ 前面和大伙聊了 Spring Boot 项目的三种创建方式，这三种创建方式，无论是哪一种，创建成功后，pom.xml 坐标文件中都有如下一段引用： 1 是时候彻底搞清楚 Spring Boot 的配置文件 application.properties 了！ 在 Spring Boot 中，配置文件有两种不同的格式，一个是 properties ，另一个是 yaml 。 1 Spring Boot中的 yaml 配置简介 搞Spring Boot的小伙伴都知道，Spring Boot中的配置文件有两种格式，properties或者yaml，一般情况下，两者可以随意使用，选择自己顺手的就行了，那么这两者完全一样吗？ 1 Spring Boot 中的静态资源到底要放在哪里？ 当我们使用 SpringMVC 框架时，静态资源会被拦截，需要添加额外配置，之前老有小伙伴在微信上问松哥 Spring Boot 中的静态资源加载问题：“松哥，我的HTML页面好像没有样式？” 1 极简 Spring Boot 整合 Thymeleaf 页面模板 虽然现在慢慢在流行前后端分离开发，但是据松哥所了解到的，还是有一些公司在做前后端不分的开发，而在前后端不分的开发中，我们就会需要后端页面模板 1 Spring Boot 中关于自定义异常处理的套路！ 在 Spring Boot 项目中 ，异常统一处理，可以使用 Spring 中 @ControllerAdvice 来统一处理，也可以自己来定义异常处理方案。Spring Boot 中，对异常的处理有一些默认的策略 1 Spring Boot 中通过 CORS 解决跨域问题 今天和小伙伴们来聊一聊通过 CORS 解决跨域问题。 1 SpringMVC 中 @ControllerAdvice 注解的三种使用场景！ @ControllerAdvice ，很多初学者可能都没有听说过这个注解，实际上，这是一个非常有用的注解，顾名思义，这是一个增强的 Controller。 1 Spring Boot 中，Redis 缓存还能这么用！ 经过 Spring Boot 的整合封装与自动化配置，在 Spring Boot 中整合 Redis 已经变得非常容易了，开发者只需要引入 Spring Data Redis 依赖，然后简单配下 redis 的基本信息，系统就会提供一个 RedisTemplate 供开发者使用 1 Spring Boot 操作 Redis，三种方案全解析！ 在 Redis 出现之前，我们的缓存框架各种各样，有了 Redis ，缓存方案基本上都统一了，关于 Redis，松哥之前有一个系列教程，尚不了解 Redis 的小伙伴可以参考这个教程： 1 Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！ 有的人可能会觉得题目有点夸张，其实不夸张，题目没有使用任何修辞手法！认真读完本文，你就知道松哥说的是对的了！ 1 另一种缓存，Spring Boot 整合 Ehcache 用惯了 Redis ，很多人已经忘记了还有另一个缓存方案 Ehcache ，是的，在 Redis 一统江湖的时代，Ehcache 渐渐有点没落了，不过，我们还是有必要了解下 Ehcache ，在有的场景下，我们还是会用到 Ehcache。 1 徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！ 我们使用 Spring Boot，基本上都是沉醉在它 Stater 的方便之中。Starter 为我们带来了众多的自动化配置，有了这些自动化配置，我们可以不费吹灰之力就能搭建一个生产级开发环境 1 Spring Boot 定义系统启动任务，你会几种方式？ 在 Servlet/Jsp 项目中，如果涉及到系统任务，例如在项目启动阶段要做一些数据初始化操作，这些操作有一个共同的特点，只在项目启动时进行，以后都不再执行，这里，容易想到 web 基础中的三大组件 1 一文读懂 Spring Data Jpa！ 有很多读者留言希望松哥能好好聊聊 Spring Data Jpa! 其实这个话题松哥以前零零散散的介绍过，在我的书里也有介绍过 1 Spring Boot 数据持久化之 JdbcTemplate 在 Java 领域，数据持久化有几个常见的方案，有Spring自带的 JdbcTemplate、有 MyBatis，还有 JPA，在这些方案中，最简单的就是 Spring 自带的 JdbcTemplate 了 1 Spring Boot 多数据源配置之 JdbcTemplate 多数据源配置也算是一个常见的开发需求，Spring 和 SpringBoot 中，对此都有相应的解决方案，不过一般来说，如果有多数据源的需求，我还是建议首选分布式数据库中间 MyCat 去解决相关问题 1 最简单的 SpringBoot 整合 MyBatis 教程 前面两篇文章和读者聊了 Spring Boot 中最简单的数据持久化方案 JdbcTemplate，JdbcTemplate 虽然简单，但是用的并不多，因为它没有 MyBatis 方便 1 极简 Spring Boot 整合 MyBatis 多数据源 关于多数据源的配置，前面和大伙介绍过 JdbcTemplate 多数据源配置，那个比较简单，本文来和大伙说说 MyBatis 多数据源的配置。 其实关于多数据源 1 是时候了解下 Spring Boot 整合 Jpa 啦 Spring Boot 中的数据持久化方案前面给大伙介绍了两种了，一个是 JdbcTemplate ，还有一个 MyBatis，JdbcTemplate 配置简单，使用也简单，但是功能也非常有限 1 Spring Boot 整合 Jpa 多数据源 本文是 Spring Boot 整合数据持久化方案的最后一篇，主要和大伙来聊聊 Spring Boot 整合 Jpa 多数据源问题。在 Spring Boot 整合 JbdcTemplate 多数据源、Spring Boot 整合 MyBatis 多数据源以及 Spring Boot 整合 Jpa 多数据源这三个知识点中 1 Spring Boot 中 10 行代码构建 RESTful 风格应用 RESTful ，到现在相信已经没人不知道这个东西了吧！关于 RESTful 的概念，我这里就不做过多介绍了，传统的 Struts 对 RESTful 支持不够友好 ，但是 SpringMVC 对于 RESTful 提供了很好的支持，常见的相关注解有： 1 Spring Boot 整合 Shiro ，两种方式全总结！ 在 Spring Boot 中做权限管理，一般来说，主流的方案是 Spring Security ，但是，仅仅从技术角度来说，也可以使用 Shiro。 1 干货|一个案例学会 Spring Security 中使用 JWT 在前后端分离的项目中，登录策略也有不少，不过 JWT 算是目前比较流行的一种解决方案了，本文就和大家来分享一下如何将 Spring Security 和 JWT 结合在一起使用，进而实现前后端分离时的登录解决方案。 1 SpringSecurity 中的角色继承问题 今天想和小伙伴们来聊一聊 Spring Security 中的角色继承问题。 1 SpringSecurity登录添加验证码 登录添加验证码是一个非常常见的需求，网上也有非常成熟的解决方案，其实，要是自己自定义登录实现这个并不难，但是如果需要在 Spring Security 框架中实现这个功能，还得稍费一点功夫 1 SpringSecurity登录使用JSON格式数据 在使用 SpringSecurity 中，大伙都知道默认的登录数据是通过 key/value 的形式来传递的，默认情况下不支持 JSON格式的登录数据 1 Spring Boot 中实现定时任务的两种方式 在 Spring + SpringMVC 环境中，一般来说，要实现定时任务，我们有两中方案，一种是使用 Spring 自带的定时任务处理器 @Scheduled 注解，另一种就是使用第三方框架 Quartz 1 SpringBoot 整合 Swagger2 ，再也不用维护接口文档了！ 前后端分离后，维护接口文档基本上是必不可少的工作。一个理想的状态是设计好后，接口文档发给前端和后端，大伙按照既定的规则各自开发，开发好了对接上了就可以上线了。 1 整理了八个开源的 Spring Boot 学习资源 今天松哥整理了几个优质 Spring Boot 开源项目给大家参考，希望能够帮助到正在学习 Spring Boot 的小伙伴！ 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【牧码小子】，牧码小子专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【牧码小子】，【牧码小子】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/springboot/index.html"},{"title":"标签","text":"","link":"/tags/index.html"},{"title":"","text":"MySQL 非基础教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } MySQL 非基础教程合集(持续更新) 1 提高性能，MySQL 读写分离环境搭建(一) MySQL 读写分离在互联网项目中应该算是一个非常常见的需求了。受困于 Linux 和 MySQL 版本问题，很多人经常会搭建失败，今天松哥就给大伙举一个成功的例子 1 提高性能，MySQL 读写分离环境搭建(二) 上篇文章和大家聊了 CentOS7 安装 MySQL5.7 ，这个大家一般装在虚拟机里边，装好了，把虚拟拷贝一份，这样我们就有两个 MySQL ，就可以开始今天的主从搭建了。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【牧码小子】，牧码小子专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【牧码小子】，【牧码小子】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/mysql/index.html"},{"title":"","text":"前后端分离教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } 前后端分离教程合集(持续更新) 1 一个Java程序猿眼中的前后端分离以及Vue.js入门 松哥的书里边，其实有涉及到 Vue，但是并没有详细说过，原因很简单，Vue 的资料都是中文的，把 Vue.js 官网的资料从头到尾浏览一遍该懂的基本就懂了，个人感觉这个是最好的 Vue.js 学习资料 1 Spring Boot + Vue 前后端分离，两种文件上传方式总结 在Vue.js 中，如果网络请求使用 axios ，并且使用了 ElementUI 库，那么一般来说，文件上传有两种不同的实现方案： 通过 Ajax 实现文件上传 通过 ElementUI 里边的 Upload 组件实现文件上传 1 Spring Boot + Vue 前后端分离开发，前端网络请求封装与配置 前端网络访问，主流方案就是 Ajax，Vue 也不例外，在 Vue2.0 之前，网络访问较多的采用 vue-resources，Vue2.0 之后，官方不再建议使用 vue-resources ，这个项目本身也停止维护，目前建议使用的方案是 axios。 1 Spring Boot + Vue 前后端分离开发，权限管理的一点思路 在传统的前后端不分的开发中，权限管理主要通过过滤器或者拦截器来进行（权限管理框架本身也是通过过滤器来实现功能），如果用户不具备某一个角色或者某一个权限，则无法访问某一个页面。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【牧码小子】，牧码小子专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【牧码小子】，【牧码小子】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/fronted-backend/index.html"},{"title":"","text":"Redis 教程合集 .demo { background: transparent; padding: 2em 0; display: flex; justify-content: center; } .main-timeline { width: 80%; margin: 0px auto; margin-top: 20px !important; position: relative; } .main-timeline:before { content: \"\"; display: block; width: 2px; height: 100%; background: rgba(37, 48, 59, 0.7); margin: 0 0 0 -1px; position: absolute; top: 0; left: 50%; } .content h2:not(:first-child) { margin-top: 0px; } .main-timeline .timeline { width: 100%; margin-bottom: 20px; position: relative; border-left: 0px; margin-left: 0px; padding-left: 0px; } .main-timeline .timeline:after { content: \"\"; display: block; clear: both; } .main-timeline .timeline-content { width: 40%; float: left; margin: -10px 0 0 0; border-radius: 6px; } .timeline-content .description { border: 1px solid #dedede; } .main-timeline .date { display: block; width: 70px; height: 70px; line-height: 70px; border-radius: 50%; background: #25303b; padding: 0px 0; margin: 0 0 0 -36px; position: absolute; top: 0; left: 50%; font-size: 12px; font-weight: 900; text-transform: uppercase; color: rgba(255, 255, 255, 0.5); border: 2px solid rgba(255, 255, 255, 0.2); box-shadow: 0 0 0 7px #25303b; } .main-timeline .date span { display: block; text-align: center; } .main-timeline .day, .main-timeline .year { font-size: 10px; } .main-timeline .month { font-size: 18px; } .main-timeline .title { padding: 15px; margin: 0; font-size: 20px; color: #fff; text-transform: none; letter-spacing: -1px; border-radius: 6px 6px 0 0; position: relative; } .main-timeline .title:after { content: \"\"; width: 10px; height: 10px; position: absolute; top: 20px; right: -5px; transform: rotate(-45deg); } .main-timeline .description { padding: 15px; margin: 0; font-size: 14px; color: #656565; background: #fff; border-radius: 0 0 6px 6px; } .main-timeline .timeline:nth-child(2n+2) .timeline-content { float: right; } .main-timeline .timeline:nth-child(2n+2) .title:after { left: -5px; } .main-timeline .timeline:nth-child(4n+1) .title, .main-timeline .timeline:nth-child(4n+1) .title:after { background: #9f84c4; } .main-timeline .timeline:nth-child(4n+2) .title, .main-timeline .timeline:nth-child(4n+2) .title:after { background: #02a2dd; } .main-timeline .timeline:nth-child(4n+3) .title, .main-timeline .timeline:nth-child(4n+3) .title:after { background: #58b25e; } .main-timeline .timeline:nth-child(4n+4) .title, .main-timeline .timeline:nth-child(4n+4) .title:after { background: #eab715; } @media only screen and (max-width: 990px) { .main-timeline { width: 100%; } } @media only screen and (max-width: 767px) { .main-timeline:before, .main-timeline .date { left: 6%; } .main-timeline .timeline-content { width: 85%; float: right; } .main-timeline .title:after { left: -5px; } } @media only screen and (max-width: 480px) { .main-timeline:before, .main-timeline .date { left: 12%; } .main-timeline .timeline-content { width: 75%; } .main-timeline .date { width: 60px; height: 60px; margin-left: -30px; } .main-timeline .month { font-size: 14px; } } Redis 教程合集(顺序已经整理好) 1 Linux 上安装 Redis hello，各位小伙伴们好久不见！那么从今天开始，我想和各位小伙伴分享下 Redis 的用法，本文我们就先来看看什么是 Redis 以及如何安装 Redis。 1 Redis 中的五种数据类型简介 上篇文章我们介绍了如何在 Linux 中安装 Redis，本文我们来了解下 Redis 中的五种数据类型。 1 Redis 字符串 STRING 介绍 上篇文章我们介绍了五种数据类型中一些通用的命令，本文我们来看看 STRING 数据类型独有的操作命令。 1 Redis 字符串 STRING 中 BIT 相关命令 上篇文章我们对 STRING 数据类型中一些基本的命令进行了介绍，但是没有涉及到 BIT 相关的命令，本文我们就来看看几个和 BIT 相关的命令。 1 Redis 列表与集合 前面文章我们介绍了 STRING 的基本命令，本文我们来看看 Redis 中的列表与集合。 1 Redis 散列与有序集合 前面文章我们介绍了列表与集合中的基本命令，本文我们来看看Redis中的散列与有序集合。 1 Redis 中的发布订阅和事务 hello，小伙伴们好久不见！前面我们说了 redis 中的基本数据类型，本文我们来看看 redis 中的发布订阅和事务，因为这两个都比较简单，因此我放在一篇文章中来讲。 1 Redis 快照持久化 redis 的基础知识我们已经准备的差不多了，接下来两篇文章，我想和大家聊聊 redis 持久化这个话题。 1 Redis 之 AOF 持久化 上篇文章和小伙伴们聊了使用快照的方式实现 redis 数据的持久化，这只是持久化的一种方式，本文我们就来看看另一种持久化方式， AOF(append-only file)。 1 Redis 主从复制(一) 前面两篇文章和小伙伴们聊了 redis 中的数据备份问题，也对快照备份和 AOF 备份做了对比，本文我们来聊聊 redis 中的主从复制问题，算是数据备份的第三种解决方案。 1 Redis 主从复制(二) 上篇文章和小伙伴们一起搭建了 redis 主从复制环境，但是还不完善，本文我想再和小伙伴们聊聊主从复制环境搭建的一些细节。 1 Redis 集群搭建 主从的搭建差不多说完了，本文我们来看看集群如何搭建。 1 Jedis 使用 Redis 的知识我们已经介绍的差不多了，本文我们来看看如何使用 Java 操作 redis。 1 Spring Data Redis 使用 上文我们介绍了 Redis，在开发环境中，我们还有另外一个解决方案，那就是 Spring Data Redis 。本文我们就来看看这个东西。 完 谢谢浏览！ 文档会持续更新，欢迎大家关注公众号【牧码小子】，牧码小子专注于 Spring Boot + 微服务以及前后端分离技术点分享，都是原创干货！ 喜欢这篇文章吗？扫码关注公众号【牧码小子】，【牧码小子】专注于 SPRING BOOT+微服务以及前后端分离技术，每天推送原创技术干货，关注后回复 JAVA，领取松哥为你精心准备的 JAVA 干货! let els = document.getElementsByClassName(\"month\"); for (let i = 0; i < els.length; i++) { els[i].innerHTML = i + 1; }","link":"/redis/index.html"}],"posts":[{"title":"Docker 容器基本操作[Docker 系列-2]","text":"docker 中的容器就是一个轻量级的虚拟机，是镜像运行起来的一个状态，本文就先来看看容器的基本操作。 镜像就像是一个安装程序，而容器则是程序运行时的一个状态。 查看容器查看容器启动 docker 后，使用 docker ps 命令可以查看当前正在运行的容器： 查看所有容器\b上面这条命令是查看当前正在运行的容器，如果需要查看所有容器，则可以通过 docker ps -a 命令查看： 在查看容器时，涉及到几个查看参数，含义分别如下： CONTAINER ID:CONTAINER ID是指容器的id，是一个唯一标识符,这是一个64位的十六进制整数，在不会混淆的情况下可以只采用id的前几位进行\b标识一个容器。 IMAGE:IMAGE表示创建容器时使用的镜像。 COMMAND:COMMAND表示容器最后运行的命令。 CREATED:创建容器的时间。 STATUS:容器的状态，这里可能显示一个容器启动时间，也能显示容器关闭时间。具体显示哪个要看容器当前的状态。 PORTS:容器对外开放的端口。 NAMES:容器的名字，如果不设置，会有一个默认的名字。 查看最新创建的容器使用 docker ps -l 可以查看最近创建的容器，如下： 查看最新创建的n个容器可以使用 docker ps -n=XXX 来查看最新创建的n个容器，如下： 创建容器创建容器整体上来说有两种不同的方式，可以先创建，再启动，也可以连创建带启动一步到位，无论是那种方式，流程都是相似的，当执行一个创建命令之后，docker 首先会去本地路径下查找是否有相应的镜像，如果没有，就去 docker hub 上\b搜索\b，如果搜索到了，则下载下来，然后利用该镜像\b创建一个容器并启动。容器的文件系统是\b在只读的镜像文件上添加一层可读写的文件层，这样可以使在不改变镜像的情况下，只记录改变的数据。下面对这两种方式分别予以介绍。 容器创建开发者可以首先使用\b docker create 命令\b创建一个容器，这个时候创建出来的容器是处于停止状态，\b没有运行，\b例如要创建一个 nginx 容器，\b创建命令如下： 1docker create nginx 创建成功后，可以查看容器是否创建成功： 此时创建的容器并未运行，处于停止状态，\b容器的 name 是随机生成的，开发者也可以在创建容器时指定 name ，如下： 1docker create --name=nginx nginx 运行结果如下： 此时的 name 属性就不是随机生成的，而是用户指定的 name。 这种方式只是单纯的创建了一个用户，并未启动。 容器创建+启动如果开发者需要既创建又启动容器，则可以使用 docker run 命令。 docker run 命令又可以启动两种不同模式的容器：\b后台型容器和交互型容器，\b顾名思义，\b后台型容器就是一个在后台运行的容器，默默的在后台执行计算就行了，不需要和开发者进行交互，而交互型容器则\b需要接收开发者的输入进行处理给出反馈。对于开发者而言，大部分情况下创建的都是后台型容器，不过在很多时候，即使是后台型容器也不可避免的需要进行交互，\b下面分别来看。 \b后台型容器后台型容器以 nginx 为例，一般 nginx 在后台运行即可： 1docker run --name nginx1 -d -p 8080:80 nginx --name 含义和上文一样，表示创建的容器的名字，-d 表示容器在后台运行，-p 表示将容器的 80 端口映射到宿主机的 8080 端口，\b创建过程如下图： 首先依然会去本地检查，本地\b没有相应的容器，则会去 Docker Hub 上查找，\b查找到了下载并运行，并且生成了一个容器 id。运行成功后，在浏览器中输入 http://localhost:8080 就能看到 Nginx 的默认页面了，如下： 这是一个后台型容器的基本创建方式。 交互型\b容器也可以创建交互型容器，例如创建一个 ubuntu 容器，开发者可能需要在 ubuntu 上面输入\b命令执行相关操作，\b交互型\b容器创建方式如下： 1docker run --name ubuntu -it ubuntu /bin/bash 参数含义都和上文一致，除了 -it，-it\b 参数，i 表示开发容器的标准输入（STDIN），t 则表示告诉 \bdocker，为容器创建一个\b命令行终端。执行结果如下： 该命令执行完后，会打开一个输入终端，读者就可以在这个终端里愉快的操作 ubuntu 了。 想要退出该终端，只需要输入 exit 命令即可。 容器启动启动如果开发者使用了 docker run 命令创建了容器，则创建完成后容器就已经启动了，如果使用了 docker create 命令创建了容器，则需要再执行 docker start 命令来启动容器，使用 docker start 命令结合容器 id 或者容器 name 可以启动一个容器，如下： docker start 启动的是一个已经存在的容器，要使用该命令启动一个容器，必须要先知道容器的 id 或者 name ，\b开发者可以通过这两个属性启动一个容器（案例中，nginx 是通过 name 启动，而 ubuntu 则是通过 id 启动）。一般来说，第一次可以使用 docker run 启动一个容器，以后直接使用 docker start 即可。 重启容器在\b\b运行过程中，会不可避免的出问题，出了问题时，需要能够自动重启，在容器启动时使用\b –restart 参数可以实现这一需求。根据 docker 官网的解释，docker 的重启策略可以分为 4 种，如下： 四种的含义分别如下： no表示不自动重启容器，默认即此。 on:failure:[max-retries]表示在退出\b状态为非0时才会重启（非正常退出），有一个可选择参数：最大重启次数，可以设置\b最大重启次数，重启次数达到上限后就会放弃重启。 always表示始终重启容器，当docker守护进程启动时，也会\b无论容器当时的状态为何，都会尝试重启\b容器。 unless-stopped表示始终重启容器，但是当docker守护进程启动时，如果\b容器已经停止运行，则不会去重启它。 容器停止通过\b docker stop 命令可以终止一个容器，如下： 可以通过 name 或者 id 终止一个容器。 容器删除单个删除容器停止\b后还依然存在，如果需要\b，还可以通过 docker start 命令再次重启一个容器，\b\b如果不需要一个容器，则可以通过 docker rm 命令删除一个容器。删除容器时，只能删除已经停止运行的容器，不能删除正在运行的容器。如下： 可以通过 name 或者 id 删除一个容器。如果非要删除一个\b正在运行的容器，可以通过 -f 参数实现，如下： 批量删除容器也可以批量删除，命令如下： 1docker rm $(docker ps -a -q) docker ps -a -q 会列出所有容器的 id ，供 rm 命令删除。 如下命令也\b\b支持删除已退出的孤立的容器： 1docker container prune 总结本文主要向大家介绍了 Docker 容器的基本操作，更多高级操作我们将在下篇文章中介绍。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0524/docker-container-basic.html"},{"title":"Docker 容器编排入门[Docker 系列-8]","text":"在实际的开发环境或者生产环境，容器往往都不是独立运行的，经常需要多个容器一起运行，此时，如果继续使用 run 命令启动容器，就会非常不便，在这种情况下，docker-compose 是一个不错的选择，使用 docker-compose 可以实现简单的容器编排,本文就来看看 docker-compose 的使用。 本文以 jpress 这样一个开源网站的部署为例，向读者介绍 docker-compose 的使用。jpress 是 Java 版的 wordPress ，不过我们不必关注 jpress 的实现，在这里我们只需要将之当作一个普通的应用即可，完成该项目的部署工作。 准备工作这里我们一共需要两个容器： Tomcat MySQL 然后需要 jpress 的 war 包，war 包地址：jpress 当然，这里的 jpress 并不是必须的，读者也可以结合自身情况，选择其他的 Java 项目或者自己写一个简单的 Java 项目部署都行。 编写 DockerfileTomcat 容器中，要下载相关的 war 等，因此我这里编写一个 Dockerfile 来做这个事。在一个空的文件夹下创建 Dockerfile ，内容如下： 1234FROM tomcatADD https://github.com/JpressProjects/jpress/raw/alpha/wars/jpress-web-newest.war /usr/local/tomcat/webapps/RUN cd /usr/local/tomcat/webapps/ \\ &amp;&amp; mv jpress-web-newest.war jpress.war 解释： 容器基于 Tomcat 创建。 下载 jpress 项目的 war 包到 tomcat 的 webapps 目录下。 给 jpress 项目重命名。 编写 docker-compose.yml在相同的目录下编写 docker-compose.yml ，内容如下（关于 yml 的基础知识，这里不做介绍，读者可以自行查找了解）： 123456789101112131415161718192021version: \"3.1\"services: web: build: . container_name: jpress ports: - \"8080:8080\" volumes: - /usr/local/tomcat/ depends_on: - db db: image: mysql container_name: mysql command: --default-authentication-plugin=mysql_native_password restart: always ports: - \"3306:3306\" environment: MYSQL_ROOT_PASSWORD: 123 MYSQL_DATABASE: jpress 解释： 首先声明了 web 容器，然后声明db容器。 build . 表示 web 容器项目构建上下文为 . ，即，将在当前目录下查找 Dockerfile 构建 web 容器。 container_name 表示容器的名字。 ports 是指容器的端口映射。 volumes 表示配置容器的数据卷。 depends_on 表示该容器依赖于 db 容器，在启动时，db 容器将先启动，web 容器后启动，这只是启动时机的先后问题，并不是说 web 容器会等 db 容器完全启动了才会启动。 对于 db 容器，则使用 image 来构建，没有使用 Dockerfile 。 restart 描述了容器的重启策略。 environment 则是启动容器时的环境变量，这里配置了数据库 root 用户的密码以及在启动时创建一个名为 jpress 的库，environment 的配置可以使用字典和数组两种形式。 OK，经过如上步骤，docker-compose.yml 就算配置成功了 运行运行的方式有好几种，但是建议使用 up 这个终极命令，up 命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。对于大部分应用都可以直接通过该命令来启动。默认情况下， docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试，通过 Ctrl-C 停止命令时，所有容器将会停止，而如果使用 docker-compose up -d 命令，则将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 因此，这里进入到 docker-compose.yml 所在目录下，执行如下命令： 1docker-compose up -d 执行结果如下： 执行后，通过 docker-compose ps 命令可以看到容器已经启动了。 初始化配置接下来，浏览器中输入 http://localhost:8080/jpress ，就可以看到 jpress 的配置页面，如下： 根据引导页面配置数据库的连接信息以及网站的基本信息： 注意：由于 mysql 和 web 都运行在容器中，因此在配置数据库地址时，不能写回环地址，否则就去 web 所在的容器里找数据库了。 配置完成后，运行如下命令，重启 web 容器： 1docker restart jpress 测试浏览器中分别查看博客首页以及后台管理页，如下图： 其他如果想要停止容器的运行，可以执行如下命令： 1docker-compose down 总结本文主要向大家介绍了简单的容器编排，专业的或者大型项目的容器编排需要结合 K8s 来做，我们后面有机会再向大家介绍。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0602/docker-container-arrangement.html"},{"title":"Docker 容器连接[Docker 系列-7]","text":"数据卷容器以及和大家聊过了，本文我们再来看看使用数据卷容器实现数据的备份与恢复，然后再来看看容器的连接操作。 利用数据卷容器可以实现实现数据的备份和恢复。 数据备份与恢复备份数据的备份操作很容易，执行如下命令： 1docker run --volumes-from mydata --name backupcontainer -v $(pwd):/backup/ ubuntu tar cvf /backup/backup.tar /usr/share/nginx/html/ 命令解释： 首先使用 --volumes-from 连接待备份容器。 -v 参数用来将当前目录挂载到容器的 /backup 目录下。 接下来，将容器中 /usr/share/nginx/html 目录下的内容备份到 /backup 目录下的 backup.tar 文件中，由于已经设置将当前目录映射到容器的 /backup 目录，因为备份在容器 /backup 目录下的压缩文件在当前目录下可以立马看到。 执行结果如下： 备份完成后，在当前目录下就可以看到 /backup 文件，打开压缩文件，发现就是 /usr/share/nginx/html 目录及内容。 恢复数据的恢复则稍微麻烦一些，操作步骤如下： 创建容器首先创建一个容器，这个容器就是要使用恢复的数据的容器，我这里创建一个 nginx 容器，如下： 1docker run -itd -p 80:80 -v /usr/share/nginx/html/ --name nginx3 nginx 创建一个名为 nginx3 的容器，并且挂载一个数据卷。 恢复数据恢复需要一个临时容器，如下：1docker run --volumes-from nginx3 -v $(pwd):/backup nginx tar xvf /backup/backup.tar 命令解释： 首先还是使用 --volumes-from 参数连接上备份容器，即第一步创建出来的 nginx3 。 然后将当前目录映射到容器的 /backup 目录下。 然后执行解压操作，将 backup.tar 文件解压。解压文件位置描述是一个容器内的地址，但是该地址已经映射到宿主机中的当前目录了，因此这里要解压缩的文件实际上就是宿主机当前目录下的文件。 容器连接一般来说，容器启动后，我们都是通过端口映射来使用容器提供的服务，实际上，端口映射只是使用容器服务的一种方式，除了这种方式外，还可以使用容器连接的方式来使用容器服务。 例如，有两个容器，一个容器运行一个 SpringBoot 项目，另一个容器运行着 mysql 数据库，可以通过容器连接使 SpringBoot 直接访问到 Mysql 数据库，而不必通过端口映射来访问 mysql 服务。 为了案例简单，我这里举另外一个例子： 有两个容器，一个 nginx 容器，另一个 ubuntu ，我启动 nginx 容器，但是并不分配端口映射，然后再启动 ubuntu ，通过容器连接，在 ubuntu 中访问 nginx 。 具体操作步骤如下： 首先启动一个 nginx 容器，但是不分配端口，命令如下： 1docker run -d --name nginx1 nginx 命令执行结果如下： 容器启动成功后，在宿主机中是无法访问的。 启动ubuntu 接下来，启动一个 ubuntu ，并且和 nginx 建立连接，如下： 1docker run -dit --name ubuntu --link nginx1:mylink ubuntu bash 这里使用 –link 建立连接，nginx1 是要建立连接的容器，后面的 mylink 则是连接的别名。 运行成功后，进入到 ubuntu 命令行： 1docker exec -it ubuntu bash 然后，有两种方式查看 nginx 的信息： 第一种 在 ubuntu 控制台直接输入 env ，查看环境变量信息： 可以看到 docker 为 nginx 创建了一系列环境变量。每个前缀变量是 MYLINK ，这就是刚刚给连接取得别名。开发者可以使用这些环境变量来配置应用程序连接到 nginx 。该连接是安全、私有的。 访问结果如下： 第二种 另一种方式则是查看 ubuntu 的 hosts 文件，如下： 可以看到，在 ubuntu 的 hosts 文件中已经给 nginx1 取了几个别名，可以直接使用这些别名来访问 nginx1 。 小贴士： 默认情况下，ubuntu 容器中没有安装 curl 命令，需要手动安装下，安装命令如下：apt-get updateapt-get install curl 总结本文主要向大家介绍了 Docker 容器中的数据备份与恢复操作，同时也向大家介绍了容器连接相关的操作，不知道你学会了吗？ 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0601/docker-link.html"},{"title":"Docker 镜像基本操作[Docker 系列-4]","text":"镜像也是 docker 的核心组件之一，镜像时容器运行的基础，容器是镜像运行后的形态。前面我们介绍了容器的用法，今天来和大家聊聊镜像的问题。 总体来说，镜像是一个包含程序运行必要以来环境和代码的只读文件，它采用分层的文件系统，将每一层的改变以读写层的形式增加到原来的只读文件上。这有点像洋葱，一层一层的，当我们后面学习了 Dockerfile ，相信大家对于这样的架构理解将更为准确。 镜像与容器的关系前文已经向读者介绍过容器的使用了，细心的读者可能已经发现，容器在启动或者创建时，必须指定一个镜像的名称或者 id ，其实，这时镜像所扮演的角色就是容器的模版，不同的镜像可以构造出不同的容器，同一个镜像，我们也可以通过配置不同参数来构造出不通的容器。如下命令： 1docker run -itd --name nginx nginx 命令中的最后一个 nginx 即表示创建该容器所需要的镜像（模版），当然这里还省略了一些信息，例如版本号等，这些我们后文会详细介绍。 镜像的体系结构镜像的最底层是一个启动文件系统（bootfs）镜像，bootfs 的上层镜像叫做根镜像，一般来说，根镜像是一个操作系统，例如 Ubuntu、CentOS 等，用户的镜像必须构建于根镜像之上，在根镜像之上，用户可以构建出各种各样的其他镜像。从上面的介绍读者可以看出，镜像的本质其实就是一系列文件的集合，一层套一层的结构有点类似于 Git ，也有点类似于生活中的洋葱。 镜像的写时复制机制通过 docker run 命令指定一个容器创建镜像时，实际上是在该镜像之上创建一个空的可读写的文件系统层级，可以将这个文件系统层级当成一个临时的镜像来对待，而命令中所指的模版镜像则可以称之为父镜像。父镜像的内容都是以只读的方式挂载进来的，容器会读取共享父镜像的内容，用户所做的所有修改都是在文件系统中，不会对父镜像造成任何影响。当然用户可以通过其他一些手段使修改持久化到父镜像中，这个我们后面会详细介绍到。 简而言之，镜像就是一个固定的不会变化的模版文件，容器是根据这个模版创建出来的，容器会在模版的基础上做一些修改，这些修改本身并不会影响到模版，我们还可以根据模版（镜像）创建出来更多的容器。 如果有必要，我们是可以修改模版（镜像）的。 镜像查看用户可以通过 docker images 命令查看本地所有镜像，如下： 这里一共有五个参数，含义分别如下： TAG: TAG用于区分同一仓库中的不同镜像，默认为latest。 IMAGE ID: IMAGE ID是镜像的一个唯一标识符。 CREATED: CREATED表示镜像的创建时间。 SIZE: SIZE表示镜像的大小。 REPOSITORY:仓库名称，仓库一般用来\b存放同一类型的镜像。仓库的名称由其创建者指定。如果没有指定则为 &lt;none&gt; 。一般来说，仓库名称有如下几种不同的形式: [namespace\\ubuntu]:这种仓库名称由命名空间和实际的仓库名组成，中间通过 \\ 隔开。当开发者在 Docker Hub 上创建一个用户时，用户名就是默认的命名空间，这个命令空间是用来区分 Docker Hub 上注册的不同用户或者组织（类似于 GitHub 上用户名的作用），如果读者想将自己的\b镜像上传到 Docker Hub 上供别人使用，则必须指定命名空间，否则上传会失败。 [ubuntu]:这种只有仓库名，对于这种没有命名空间的仓库名，可以认为其属于顶级命名空间，该空间的仓库只用于官方的镜像，由 Docker 官方进行管理，但一般会授权给第三方进行开发维护。当然用户自己创建的镜像也可以使用这种命名方式，但是将无法\b上传到 Docker Hub 上共享。 [hub.c.163.com/library/nginx]:这种指定 url 路径的方式，一般用于非\b Docker Hub 上的镜像命名，例如一个第三方服务商提供的镜像或者开发者\b自己搭建的镜像中心，都可以使用这种命名方式命名。 使用 docker images 命令可以查看本地所有的镜像，如果镜像过多，可以通过通配符进行匹配，如下： 如果需要查看镜像的详细信息，也可以通过上文提到的 docker inspect 命令来查看。 镜像下载当用户执行 docker run 命令时，就会自动去 Docker Hub 上下载相关的镜像，这个就不再重复演示，开发者也可以通过 search 命令去 \bDocker Hub 上搜索符合要求的镜像，如下： 其中： NAME：表示镜像的名称。 DESCRIPTION：表示镜像的简要描述。 STARS：表示用户对镜像的评分，评分越高越可以放心使用。 OFFICIAL：是否为\b官方镜像。 AUTOMATED：是否使用了自动构建。 在执行 docker run 命令时再去下载，速度会有点慢，如果希望该命令能够快速执行，可以在执行之前，先利用 docker pull 命令\b将\b镜像先下载下来，然后再\b运行。 运行命令如下： 镜像删除镜像可以\b通过 docker rmi 命令进行删除，参数为镜像的id或者镜像名，参数可以有多个，多个参数之间用空格隔开。如下： 有的时候，无法删除一个镜像，大部分原因是因为该镜像被一个容器所依赖，此时需要先删除容器，然后就可以删除镜像了，删除容器的命令可以参考本系列前面的文章。 通过前面文章的阅读，读者已经了解到所谓的容器实际上是在父镜像的基础上创建了一个可读写的文件层级，所有的修改操作都在这个文件层级上进行，而父镜像并未受影响，如果读者需要根据这种修改创建一个新的本地镜像，有两种不同的方式，先来看第一种方式：commit。 创建容器首先，根据本地镜像运行一个容器，如下： 命令解释： 首先执行 docker images 命令，查看本地镜像。 根据本地镜像中\b的 nginx 镜像，创建一个名为 nginx 的容器，并启动。 将宿主机中一个名为 index.html 的文件\b拷贝到容器中。 访问容器，发现改变已经生效。 接下来再重新创建一个容器，名为 nginx2. 访问 nginx2 ，发现 nginx2 中默认的页面还是 nginx 的默认页面，并未发生改变。 commint 创建本地镜像接下来，根据\b\b刚刚创建的第一个容器，创建一个本地镜像，如下： 命令解释： 参数 -m 是对创建的该镜像的一个简单描述。 –author 表示该镜像的作者。 ce1fe32739402 表示创建镜像所依据的容器的 id。 sang/nginx 则表示仓库名，sang 是名称空间，nginx 是镜像名。 v1 表示仓库的 tag。 创建完成后，通过 docker images 命令就可以查看到刚刚创建的镜像。 通过刚刚创建的镜像运行一个容器，访问该容器，发现 nginx 默认的首页已经发生改变。 这是我们通过 commint 方式创建本地镜像的方式，但是 commit 方式存在一些问题，比如不够透明化，无法重复，体积较大，为了解决这些问题，可以考虑使用 Dockerfile ，实际上，主流方案也是 Dockerfile。 DockerfileDockerfile 就是一个普通的文本文件，其内包含了一条条的指令，每一条指令都会构建一层。先来看一个简单的例子。 首先在一个空白目录下创建一个名为 Dockerfile 的文件，内容如下： 命令解释： FROM nginx 表示该镜像的构建，以已有的 nginx 镜像为基础，在该镜像的基础上构建。 MAINTAINER 指令用来声明创建镜像的作者信息以及邮箱信息，这个命令不是必须的。 RUN 指令用来修改镜像，算是使用比较频繁的一个指令了，\b该指令可以用来安装程序、安装库以及配置应用程序等，一个 RUN 指令执行会在当前镜像的基础上创建一个新的镜像层，接下来的指令将在这个新的镜像层上执行，RUN 语句有两种不同的形式：shell 格式和 exec 格式。本案例采用的 shell 格式，shell 格式就像 linux 命令一样，exec 格式则是一个 JSON 数组，将命令放到数组中即可。在使用 RUN 命令时，适当的时候可以将多个 RUN 命令合并成一个，这样可以避免在创建镜像时创建过多的层。 COPY 语句则是将镜像上下文中的 hello.html 文件拷贝到镜像中。 文件创建完成后，执行如下命令进行构建： 命令解释： -t 参数用来指定镜像的命名空间，仓库名以及 TAG 等信息。 最后面的 . 是指镜像构建上下文。 注意： Docker 采用了 C/S 架构，分为 Docker 客户端（Docker 可执行程序）与 Docker 守护进程，Docker 客户端通过命令行和 API 的形式与 Docker 守护进程进行通信，Docker 守护进程则提供 Docker 服务。因此，我们操作的各种 docker 命令实际上都是由 docker\b 客户端发送到 docker 守护进程上去执行。我们在构建一个镜像时，不可避免的需要将一些本地文件拷贝到镜像中，例如上文提到的 COPY 命令，用户在构建镜像时，需要指定构建镜像的上下文路径（即前文的 . ）, docker build 在获得这个路径之后，会将路径下的所有内容打包，然后上传给 Docker 引擎。 镜像\b构建成功后，可以通过 docker images 命令查看，如下： 然后创建\b容器并启动，就可以看到之前的内容都生效了。 总结本文主要向大家介绍了 Docker 中镜像的基本操作，操作其实并不难，关键是理解好镜像和容器的关系，以及镜像洋葱式的文件结构。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0526/docker-images.html"},{"title":"Docker 容器高级操作[Docker 系列-3]","text":"上篇文章向\b读者介绍了一个 Nginx 的例子，对于 Nginx 这样一个容器而言，当它启动成功后，我们不可避免的需要对 Nginx 进行的配置进行修改，\b那么这个修改要如何完成呢\b？且看下文。 依附容器docker attach 依附容器这个主要是针对交互型容器而言的，该命令有一定的局限性，可以作为了解即可，真正工作中使用较少。 要是用 docker attach 命令，首先要\b确保容器已经启动，然后使用该命令才能\b进入到容器中。具体操作步骤如下： 创建一个容器，然后启动： 不要关闭当前窗口，再打开一个新的终端，执行 docker attach ubuntu : 此时就能进入到容器的命令行进行操作了。 如果容器已经关闭或者容器是一个后台容器，则该命令就无用武之地了。 由上面的操作大家可以看到，这个命令的局限性很大，使用场景也不多，因此大家作为一个了解即可。 容器内执行命令如果容器在后台启动，则可以使用 docker exec 在容器内执行命令。不同于 docker attach ，使用 docker exec 即使用户从终端退出，容器也不会停止运行，而使用 docker attach 时，如果用户从终端退出，则容器会停止运行。如下图： 基于这样的特性， 我们以后在操作容器内部时，基本上都是通过 docker exec 命令来实现。 查看容器信息容器创建成功后，用户可以\b通过 docker inspect 命令查看容器的详细信息，这些详细信息\b包括容器的\b id 、容器名、环境变量、运行命令、主机配置、网络配置以及数据卷配置等信息。执行部分结果如下图： 使用 format 参数可以只查看用户关心的数据，例如： 查看容器运行状态 查看容器ip地址 查看容器名、容器id 查看容器主机信息 容器的详细信息，在我们后边配置容器共享目录、容器网络时候非常有用，这个我们到后面再来详细介绍。 查看容器进程使用 docker top 命令可以查看容器中正在运行的进程，首先确保容器已经启动，然后执行 docker top 命令，如下： 查看容器日志交互型容器查看日志很方便，因为日志就直接在控制台打印出来了，但是对于后台型容器，如果要查看日志，则可以使用docker提供的 docker logs 命令来查看，\b如下： 如下图，首先启动一个不停打日志的\b容器，然后利用 docker logs 命令查看日志，但是\b默认情况下只能查看到历史日志，无法查看实时日志，使用 -f 参数后，就可以查看实时日志了。 使用 --tail 参数可以\b精确控制日志的输出行数， -t 参数则可以显示日志的输出时间。 该命令在执行的过程中，首先输出最近的三行日志，同时由于添加了 -f 参数，因此，还会有其他日志持续输出。同时，因为添加了 -t 参数，时间随同日志一起打印出来了。 docker 的一大优势就是可移植性，容器因此 docker 容器可以随意的进行导入导出操作。 容器导出既然是容器，我们当然希望 Docker 也能够像 VMWare 那样方便的在不同系统之间拷贝，不过 Docker 并不像 VMWare 导出容器那样方便（事实上，VMWare 中不存在容器导出操作，直接拷贝安装目录即可），在 Docker 中，使用 export 命令可以导出容器，具体操作如下： 创建一个容器，进行基本的配置操作 本案例中我首先创建一个 nginx 容器，然后启动，启动成功后，将本地一个 index.html 文件上传到容器中，修改 nginx 首页的显示内容。具体操作步骤如下： docker run -itd --name nginx -p 80:80 nginx vi ./blog/docker/index.html docker cp ./blog/docker/index.html nginx:/usr/share/nginx/html/ 首先运行一个名为 nginx 的容器，然后在宿主机中编辑一个 index.html 文件，编辑完成后，将该文件上传到容器中。然后在浏览器中输入 http://localhost:80 可以看到如下结果： 容器已经修改成功了。 接下来通过 export 命令将容器导出，如下： 该命令将容器导入到 docker\b 目录下。导出成功之后，我们就可以随意传播这个导出文件了，可以发送给其他小伙伴去使用了，相对于 VMWare 中庞大的文件，这个导出文件非常小。一般可能只有几百兆，当然也看具体情况。 容器导入其他小伙伴拿到这个文件，通过执行如下命令可以导入容器（如果自己重新导入，需要记得将 docker 中和 nginx 相关的容器和镜像删除）： 容器导入成功后，就可以使用 docker run 命令运行了。运行成功之后，我们发现自己定制的 index.html 页面依然存在，说明这是我们自己的那个容器。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0525/docker-container-advanced.html"},{"title":"Docker 入门及安装[Docker 系列-1]","text":"docker 如日中天，这不是单纯的炒概念，docker 确确实实解决了开发与运维的痛点，因此在企业开发中得到了非常广泛的使用，本文对于 docker 的这些基本知识点再做一些简单回顾。 什么是 docker根据 wikipedia 中的介绍： Docker 是一个开放源代码软件项目，让应用程序布署在软件容器下的工作可以自动化进行，借此在 Linux 操作系统上，提供一个额外的软件抽象层，以及操作系统层虚拟化的自动管理机制。Docker 利用 Linux 核心中的资源分脱机制，例如 cgroups ，以及 Linux 核心名字空间（name space），来创建独立的软件容器（containers）。这可以在单一 Linux 实体下运作，避免启动一个虚拟机造成的额外负担。Linux 核心对名字空间的支持完全隔离了工作环境中应用程序的视野，包括进程树、网络、用户 ID 与挂载文件系统，而核心的 cgroup 提供资源隔离，包括 CPU 、存储器、block I/O 与网络。从 0.9 版本起，Dockers 在使用抽象虚拟是经由 libvirt 的 LXC 与 systemd - nspawn 提供界面的基础上，开始包括 libcontainer 库做为以自己的方式开始直接使用由 Linux 核心提供的虚拟化的设施。依据行业分析公司“451研究”：“Dockers 是有能力打包应用程序及其虚拟容器，可以在任何 Linux 服务器上运行的依赖性工具，这有助于实现灵活性和便携性，应用程序在任何地方都可以运行，无论是公有云、私有云、单机等。” 。 这里的介绍有点绕口，让我来介绍下 docker 解决了哪些痛点： 简化\b环境管理 传统的软件开发与发布环境复杂，配置繁琐，经常有读者在微信上问：我的代码开发环境可以运行，一旦部署到服务器上就运行不了了。这个问题\b很常见，也确实很烦人，但是问题总要解决，开发环境、测试环境、生产环境，每个环节都有可能出现这样那样的问题，如果能够在各个环境中实现一键部署，就会方便很多，例如一键安装 linux 、一键安装 mysql、一键安装 nginx 等，docker 彻底解决了这个问题。 虚拟化更加轻量级 说到容器，说到虚拟化，很多人总会想到虚拟机，想到 VMware、VirtualBox 等工具，不同于这些虚拟技术，docker 虚拟化更加轻量级，传统的虚拟机都是先虚拟出一个操作系统，然后在操作系统上完成各种各样的配置，这样并不能充分的利用物理机的性能，docker 则是一种操作系统级别的虚拟技术，它运行在操作系统之上的用户空间，所有的容器都共用一个系统内核甚至公共库，容器引擎提供了进程级别的隔离，让每个容器都像运行在单独的系统之上，但是又能够共享很多底层资源。因此 docker 更为轻量、快速和易于管理。 程序可移植 有了前面介绍的两个特点，程序可移植就是顺理成章的事情了。 docker 和虚拟机前面介绍了 docker 与传统虚拟机的差异，通过下表再来详细了解下这种差异： docker 虚拟机 相同点 1. 都可在不同的主机之间迁移2. 都具备 root 权限3. 都可以远程控制4. 都有备份、回滚操作 操作系统 在性能上有优势，可以轻易的运行多个操作系统 可以安装任何系统，但是性能不及容器 原理 和宿主机共享内核，所有容器运行在容器引擎之上，容器并非一个完整的操作系统，\b所有容器共享操作系统，在进程级进行隔离 每一个虚拟机都建立在虚拟的硬件之上，提供指令级的虚拟，具备一个完整的操作系统 优点 高效、集中。一个硬件\b节点可以运行数以百计的的容器，非常节省资源，QoS 会尽量满足，但不保证一定满足。内核\b由提供者升级，服务由服务提供者管理 对操作系统具有绝对权限，对系统版本和系统升级具有完全的管理权限。具有一整套的的资源：CPU、RAM 和磁盘。QoS 是有保证的，每一个虚拟机就像一个真实的物理机一样，可以实现不同的操作系统运行在同一物理节点上。 资源管理 弹性资源分配：资源可以在没有关闭容器的情况下添加，数据卷也无需重新分配大小 虚拟机需要重启，虚拟机里边的操作系统需要处理新加入的资源，如磁盘等，都需要重新分区。 远程管理 根据操作系统的不同，可以通过 shell 或者远程桌面进行 远程控制由虚拟化平台提供，可以在\b虚拟机启动之前连接 缺点 对内核没有控制权限，只有容器的提供者具备升级权限。只有一个内核运行在物理节点上，几乎不能实现不同的操作系统混合。容器提供者一般仅提供少数的几个操作系统 每一台虚拟机都具有更大的负载，耗费更多的资源，用户需要全权维护和管理。一台物理机上能够运行的虚拟机非常有限 配置 快速，基本上是一键配置 配置时间长 启动时间 秒级 分钟级 硬盘使用 MB GB 性能 接近原生态 弱于原生态 系统支持数量 \b单机支持上千个 一般不多于几十个 docker 与传统容器不同与传统容器，docker 早起基于 LXC，\b后来基于自研的 libContainer，docker 对于传统容器做了许多优化，如下： 跨平台的可移植性 面向应用 版本控制 组件复用 共享性 工具生态系统 docker 应用场景 加速本地开发 自动打包和部署应用 创建轻量、私有的PaaS环境 自动化测试和持续集成/部署 部署并扩展Web应用、数据库和后端服务器 创建安全沙盒 轻量级的桌面虚拟化 docker 核心组件docker 中有三大核心组件： 镜像 镜像是一个只读的静态模版，它保存了容器需要的环境和应用的执行代码，可以将镜像看成是容器的代码，当代码运行起来之后，就成了容器，\b镜像和容器的关系也类似于程序和进程的关系。 容器 容器是一个运行时环境，是镜像的一个运行状态，它是镜像执行的动态表现。 库 库是一个特定的用户存储镜像的目录，一个用户可以建立多个库来保存自己的镜像。 docker相关技术 隔离性 可度量性 移植性 安全性 docker 安装相对而言，Linux 上安装 Docker 是最容易的，其次是 Mac ，最后是 Windows ，Windows 因此要装的东西比较多，官方也提供了两个不同的安装包，支持不同的 Windows 的不同版本，一个是针对 Win10 的安装引导程序，还有一个是兼容性较好的 Toolbox ，但是在 Windows 上运行 Docker ，后期在虚拟目录等方面还会遇到各种问题，所以这里松哥是非常不建议大家在 Windows 中安装 Docker ，有 Mac 的上 Mac （Mac 上安装 Docker 就像安装普通软件一样），没有 Mac 的装 Linux 虚拟机，再装 Docker 即可，这里我就先以 CentOS 上安装 Docker 为例，来说说 Docker 安装。 分别执行如下安装命令： 12345678# 首先安装 Dockeryum -y install docker# 然后启动 Docker 服务service docker start# 测试安装是否成功docker -v 安装完成后，看到如下页面，表示安装成功： 总结本文主要向大家介绍了 Docker 的基本概念以及 Docker 的安装 ，下篇文章我们向大家介绍 Docker 中基本的容器操作。有问题欢迎留言讨论。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0522/docker-install.html"},{"title":"Docker 数据卷操作[Docker 系列-6]","text":"数据卷入门在前面的案例中，如果我们需要将数据从宿主机拷贝到容器中，一般都是使用 Docker 的拷贝命令，这样性能还是稍微有点差，没有办法能够达到让这种拷贝达到本地磁盘 I/O 性能呢？有！ 数据卷可以绕过拷贝系统，在多个容器之间、容器和宿主机之间共享目录或者文件，数据卷绕过了拷贝系统，可以达到本地磁盘 I/O 性能。 本文先通过一个简单的案例向读者展示数据卷的基本用法。 以前面使用的 nginx 镜像为例，在运行该容器时，可以指定一个数据卷，命令如下： 1docker run -itd --name nginx -v /usr/share/nginx/html/ -p 80:80 bc26f1ed35cf 运行效果如下： 此时，我们创建了一个数据卷并且挂载到容器的 /usr/share/nginx/html/ 目录下，小伙伴们知道，该目录实际上是 nginx 保存 html 目录，在这里挂载数据卷，一会我们只需要修改本地的映射位置，就能实现页面的修改了。 接下来使用 docker inspect 命令查看刚刚创建的容器的具体情况，找到数据卷映射目录，如下： 可以看到，Docker默认将宿主机的 /var/lib/docker/volumes/0746bdcfc045b237a6fe2288a3af9d7b80136cacb3e965db65a212627e217d75/_data 目录作为source目录，接下来，进入到该目录中，如下： 此时发现该目录下的文件内容与容器中 /usr/share/nginx/html/ 目录下的文件内容一致，这是因为挂载一个空的数据卷到容器中的一个非空目录中，那么这个目录下的文件会被复制到数据卷中（如果挂载一个非空的数据卷到容器中的一个目录中，那么容器中的目录中会显示数据卷中的数据。如果原来容器中的目录中有数据，那么这些原始数据会被隐藏掉）。 小贴士： 由于 Mac 中的 Docker 有点特殊，上文提到的 /var/lib/xxxx 目录，如果是在 linux 环境下，则直接进入即可，如果是在 mac 中，需要首先执行如下命令，在新进入的命令行中进入到 /var/lib/xxx 目录下：screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty 接下来修改改文件中的index.html文件内容，如下： 1echo &quot;hello volumes&quot;&gt;index.html 修改完成后，再回到浏览器中，输入 http://localhost查看nginx中index.html 页面中的数据，发现已经发生改变。说明宿主机中的文件共享到容器中去了。 结合宿主机目录上文中对于数据卷的用法还不是最佳方案，一般来说，我们可能需要明确指定将宿主机中的一个目录挂载到容器中，这种指定方式如下： 1docker run -itd --name nginx -v /Users/sang/blog/docker/docker/:/usr/share/nginx/html/ -p 80:80 bc26f1ed35cf 这样便是将宿主机中的 /Users/sang/blog/docker/docker/ 目录挂载到容器的 /usr/share/nginx/html/ 目录下。接下来读者只需要在 /Users/sang/blog/docker/docker/ 目录下添加 html 文件，或者修改 html 文件，都能在 nginx 访问中立马看到效果。 这种用法对于开发测试非常方便，不用重新部署，重启容器等。 注意：宿主机地址是一个绝对路径 更多操作Dockerfile中的数据卷如果开发者使用了 Dockerfile 去构建镜像，也可以在构建镜像时声明数据卷，例如下面这样： 1234FROM nginxADD https://www.baidu.com/img/bd_logo1.png /usr/share/nginx/html/RUN echo &quot;hello docker volume!&quot;&gt;/usr/share/nginx/html/index.htmlVOLUME /usr/share/nginx/html/ 这样就配置了一个匿名数据卷，运行过程中，将数据写入到 /usr/share/nginx/html/ 目录中，就可以实现容器存储层的无状态变化。 查看所有数据卷使用如下命令可以查看所有数据卷： 1docker volume ls 如图： 查看数据卷详情根据 volume name 可以查看数据详情，如下： 1docker volume inspect 执行结果如下图： 删除数据卷可以使用 docker volume rm 命令删除一个数据卷，也可以使用 docker volume prune 批量删除数据卷，如下： 批量删除时，未能删除掉所有的数据卷，还剩一个，这是因为该数据卷还在使用中，将相关的容器停止并移除，再次删除数据卷就可以成功删除了，如图： 数据卷容器数据卷容器是一个专门用来挂载数据卷的容器，该容器主要是供其他容器引用和使用。所谓的数据卷容器，实际上就是一个普通的容器，举例如下： 创建数据卷容器 使用如下方式创建数据卷容器： 1docker run -itd -v /usr/share/nginx/html/ --name mydata ubuntu 命令执行效果如下图： 引用容器 使用如下命令引用数据卷容器： 12docker run -itd --volumes-from mydata -p 80:80 --name nginx1 nginxdocker run -itd --volumes-from mydata -p 81:80 --name nginx2 nginx 此时， nginx1 和 nginx2 都挂载了同一个数据卷到 /usr/share/nginx/html/ 目录下，三个容器中，任意一个修改了该目录下的文件，其他两个都能看到变化。 此时，使用 docker inspect 命令查看容器的详情，发现三个容器关于数据卷的描述都是一致的，如下图： 总结本文主要向大家介绍了数据卷中的容器操作，整体来说还是非常简单的，小伙伴们，你学会了吗？ 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0531/docker-data.html"},{"title":"Git 关联远程仓库","text":"前面我们介绍的所有操作都是在本地仓库完成的，本文我们主要来看看如何和远程仓库进行交互，为了方便起见，这里远程仓库我们选择 GitHub。 本文是 Git 系列的第五篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药4.Git 分支管理 配置 SSH KEYSSH KEY 的配置不是必须的，不配置的话我们就只能使用 HTTPS 协议，这样每次提交时要输入用户名密码，略麻烦，所以还是配置一下。配置 SSH KEY 的原理很简单，采用非对称加密方式生成公钥和私钥，公钥告诉 GitHub ，私钥留在自己电脑上(私钥不可泄露)，当我们向 GitHub 上提交数据时，GitHub 会用我们留给它的公钥加密一段消息返回给我们的电脑，如果我们能够用私钥解密成功，说明是合法的用户，这样就避免我们输入用户名密码了。大致的原理就是这样，现在很多免登录的系统都采用了这种方式，比如 Hadoop 免登录配置也是这样。那我们就来看看这个 SSH KEY 要怎么生成。 1.查看本地是否已有 SSHKEY查看当前用户目录下是否有 .ssh 文件，如下： 如果查看之后有结果，则直接跳转到第四步，什么都没有就继续生成。 2.生成 SSH 指纹生成 SSH 指纹的命令很简单，如下： 1ssh-keygen -t rsa -b 4096 -C &quot;你的邮箱地址&quot; 注意邮箱地址要替换。 3.添加 ssh 到 ssh-agent 中执行如下命令即可： 1eval &quot;$(ssh-agent -s)&quot; OK，做好这一切之后，我们当前用户目录下已经有了一个名为 .ssh 的隐藏文件夹了，打开这个目录，会发现有一个名为 id_rsa.pub 的文件，这就是我们一会要使用的公钥文件。 4.将公钥告诉 GitHub登录 GitHub ，点击右上角的向下的箭头，选择 Settings ，在新打开的页面中左边侧栏选择 SSH and GPG keys ，如下： 完了之后点击最下面的 Add SSH key 按钮即可，如此之后，我们的 SSH KEY 就配置成功了。 创建远程仓库接下来我们在 GitHub 上创建一个仓库，登录成功之后，直接点击右上角绿色的 New repository 按钮，如下： 其实这里我们只需要填一个版本仓库的名字，我填了 test，填好之后，点击 Create repository 就 OK 了。 关联远程仓库创建成功之后，我们会看到仓库的地址，如下： `git@github.com:lenve/test.git` ，然后我需要将我们之前的本地仓库和这个远程仓库进行关联，使用 git remote add 命令，如下： 1$ git remote add origin git@github.com:lenve/test.git 在这条命令中，git 会自动将远程仓库的名字设置为 origin ，方便我们的后续操作。 推送到远程仓库推送到master分支假设我想将本地 master 分支上的内容推送到远程 master 分支上，方式如下： 1$ git push -u origin master -u参数可以在推送的同时，将 origin 仓库的 master 分支设置为本地仓库当前分支的 upstream（上游）。添加了这个参数，将来运行 git pull 命令从远程仓库获取内容时，本地仓库的这个分支就可以直接从 origin 的 master 分支获取内容，省去了另外添加参数的麻烦。这个参数也只用在第一次 push 时加上，以后直接运行 git push 命令即可。 推送到其他分支如果想推送到其他分支，还是这条命令，修改一下分支的名字即可，比如我也想把我的 fa 分支推送到远程仓库中，执行如下命令： 12$ git checkout fa$ git push -u origin fa 先切换到 fa 分支，然后执行 git push 命令，参数含义和之前的一样，这里我们创建的远程仓库的分支名也为 fa（当然我们可以取任何名字，但是为了不混淆，最好取一致的名字）。这两条命令执行成功之后，此时在网页中我们就可以看到已经有多个分支了，如下： 从远程仓库获取首次获取刚刚是我们向远程仓库提交数据，有提交当然就有获取，我们可以通过 git clone 命令克隆一个远程仓库到本地，方式也简单，在本地创建一个空文件夹，执行如下命令： 1$ git clone git@github.com:lenve/test.git 表示克隆文件到本地仓库。此时克隆的远程仓库的 master 分支到本地仓库，我们可以通过 git branch -a 来查看本地仓库和远程仓库的信息，-a 参数可以同时显示本地仓库和远程仓库的信息，如下： 我们看到远程仓库中已经有了 fa 分支了，如果我们想把 fa 分支也克隆下来，执行如下命令： 1$ git checkout -b fa origin/fa 表示根据远程仓库的 fa 分支创建一个本地仓库的 fa 分支，创建完成之后进行切换，也可以通过如下命令只创建不切换： 1$ git branch fa origin/fa 此时我在 fa 分支下修改 git01.txt 文件并提交，如下： 注意由于 fa 分支就是从远程仓库克隆下来的，所以这里可以不添加 -u 参数。 从远程仓库更新此时我们回到第一次最早的那个 test 本地仓库中，那个 test 仓库的 fa 分支现在和远程仓库不一致了，我们可以通过 git pull 命令来更新，如下： Ok，关联远程仓库我们先说这么多。有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-remote.html"},{"title":"Git 中的各种后悔药","text":"Git 强大的撤销、版本回退功能，让我们在开发的过程中能够随意的回到任何一个时间点的状态，本文我们就来看看 Git 中的各种后悔药! 本文是Git系列的第三篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作 本文将从如下三个方面介绍 Git 中的后悔药： 工作区的代码想撤销 add 到暂存区的代码想撤销 提交到本地仓库的代码想撤销 提交到远程仓库的后悔药我们统一都在关联远程仓库一文中讲解，敬请期待。 工作区的代码想撤销可能有一天我正在写代码，写了很久发现写错了，想恢复到一开始的状态，一个笨办法就是把刚刚写的代码一行一行的删除，不过这种方式成本太高，我们可以通过 git checkout -- &lt;file&gt; 命令来撤销工作区的代码修改。如下图： 首先我们执行了 git status 命令，发现工作区是干净的，然后执行了 cat 命令，发现文件只有两行内容，然后通过 vi 编辑器向文件中添加一行，保存并退出，退出来之后又执行了 git status 命令，此时工作区的状态已经发生变化，然后我们执行了 git checkout – git01.txt 命令，表示撤销之前的操作，让 git01.txt 恢复到之前的状态，该命令执行成功之后，我们再执行 cat 命令发现文件内容已经恢复了，此时再执行 git status ，状态也恢复了。 add 到暂存区的代码想撤销如果想要撤销，但是代码已经提交到暂存区了，不用担心，也能撤销，分两个步骤： 1.将暂存区的代码撤销到工作区2.将工作区的代码撤销(具体操作和’工作区的代码想撤销’一致) 将暂存区的代码撤销，我们可以使用 git reset HEAD 命令来实现。如下图： 这里的代码都比较简单，核心的过程就是先执行 git reset HEAD 命令，从暂存区撤销，剩下的操作参考’工作区的代码想撤销’一节。 提交到本地仓库的代码想撤销同样的，提交到本地仓库的代码一样也可以撤销，我们可以利用 git reset --hard &lt;版本号&gt; 命令来实现版本回退，该命令中的版本号有几种不同的写法： 1.可以使用 HEAD^ 来描述版本，一个 ^ 表示前一个版本，两个 ^^ 表示前两个版本，以此类推。2.也可以使用数字来代替 ^ ，比如说前 100 个版本可以写作 HEAD~100 。3.也可以直接写版本号，表示跳转到某一个版本处。我们每次提交成功后，都会生成一个哈希码作为版本号，所以这里我们也可以直接填版本号，哈希码很长，但是我们不用全部输入，只需要输入前面几个字符即可，就能识别出来。 看下面一系列的操作： 1.通过 git log 查看当前提交日志： 2.通过 git reset HEAD^^ 向前回退两个版本： 3.查看日志，发现最后一次提交的版本号是 695ce1fe ,利用 git reset –hard 695ce1fe 命令回到回退之前的状态： 4.通过 git reset –hard HEAD~1 回到上一个版本： 好了，Git 中的后悔药我们就先介绍到这里，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-delete.html"},{"title":"Git 分支管理","text":"Svn 中也有分支管理，但是很 low，Git 的分支管理非常强大，本文先不去说分支管理内部到底怎么做的，我们先来看看 Git 中最基本的分支管理操作。 本文是 Git 系列的第四篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药 分支的必要性小伙伴们都知道，我们在完成一个项目时，不可能是“单线程”开发的，很多时候任务是并行的，举个栗子：项目 2.0 版本上线了，现在要着手开发 3.0 版本，同时 2.0 版本可能还有一些 bug 需要修复，这些 bug 修复之后我们可能还会发 2.1，2.2，2.3 这些版本，我们不可能等所有 bug 都修复完了再去开发 3.0 版本，修复 2.0 的 bug 和开发 3.0 的新功能是两个并行的任务，这个时候我们 3.0 的功能开发直接在 master 分支上进行肯定不合适，我们要保证有一个稳定，可以随时发版本的分支存在（一般情况下这个角色由 master 分支来扮演），此时我们就可以灵活的使用 Git 中的分支管理功能： 创建一个长期分支用来开发 3.0 功能，假设这个分支的名字就叫 v3，我们在 v3 上添加新功能，并不断测试，当 v3 稳定后，将 v3 合并到 master 分支上。 创建一个特性分支用来修复 2.0 的 bug ，一旦 bug 修复成功，就将该分支合并到 master 上，一旦发现新 bug ，就立马再创建分支进行修复，修复成功之后再合并。 以上两个步骤同步进行，这在 Svn 中简直是不可想象的，因为 Svn 的分支管理太 low，而 Git 能够让我们做到随心所欲的创建、合并和删除分支。 查看分支我们可以通过 git branch 命令来查看当前仓库有哪些分支，而我们处于哪一个分支中，如下： 这里显示当前仓库只有一个 master 分支，这是 git 默认创建出来的，master 前面的 * 表示我们当前处于这一个分支中。 分支创建和切换我们可以利用 git branch &lt;分支名&gt; 命令来创建一个分支，然后利用 git checkout &lt;分支名&gt; 来切换分支，如下： 如果小伙伴觉得这样太麻烦，可以通过 git checkout -b &lt;分支名&gt; 来一步到位，创建并切换分支，如下： 也可以通过 git checkout - 命令来切换回上一个分支，如下： 分支合并现在我切换到 fa 分支中，由于 fa 分支是从 master 分支中创建出来的，所以此时 fa 分支的内容和 master 分支的内容是一致的，然后我在 fa 分支中向 git01.txt 文件添加一行内容并提交，此时 fa 分支中的 git01.txt 和 master 分支中 git01.txt 的内容就不相同了，具体操作如下： 上图展示了此时 master 分支和 fa 分支的不同，现在我通过 git merge --no-ff &lt;分支名&gt; 命令将 fa 分支合并到 master 分支上。其中 –no-ff 表示强行关闭 fast-forward 方式， fast-forward 方式表示当条件允许时， git 直接把 HEAD 指针指向合并分支的头，完成合并，这种方式合并速度快，但是在整个过程中没有创建 commit，所以如果当我们删除掉这个分支时就再也找不回来了，因此在这里我们将之关闭。 想要合并分支，我们先切换到 master 分支上，然后执行 git merge --no-ff fa 命令即可完成分支合并，如下图： 合并成功后，我们看到 master 分支上的 git01.txt 上已经有了 fa 分支中的内容了。 以图表方式查看分支我们可以通过 git log --graph 命令来直观的查看分支的创建和合并等操作，如下图： 分支衍合所谓的分支衍合其实也是分支合并的一种方式，下面我们就来看看这个分支衍合到底是什么样的。现在我的 master 分支的内容和 fa 分支的内容是保持一致的，fa 是从 master 中创建出来的，如下图： 现在我向 fa 和 master 中各自做一次提交，如下图： 此时我们执行如下两条命令将两个分支合并： 12$ git checkout fa$ git rebase master rebase 命令在执行的过程中会首先把 fa 中的每个 commit 取消，并且将之保存为临时 patch ，再将 fa 分支更新为最新的 master 分支，然后再把那些临时的 patch 应用到 fa 上，此时 fa 分支将指向新创建的 commit 上，那些老的 commit 将会被丢弃，这些被丢弃的 commit 在执行 git gc 命令时会被删除。合并后的分支如下图： 上面的 git rebase master 命令在执行的过程中有可能会发生冲突，发生冲突时我们有两种方案，一种直接退回到之前的状态，另一种就是解决冲突继续提交。 退回到之前的状态我们可以通过如下命令来回到之前的状态： 1$ git rebase --abort 解决冲突不过大多数情况下我们都是要解决冲突的，解决之后继续提交。此时我们用编辑器打开冲突的文件，看到的内容可能是这样的： ======上面的是 HEAD 中的内容，下面的是要合并的内容，根据自己的需求编辑文件，编辑完成之后，通过如下两条命令继续完成合并： 12$ git add git01.txt$ git rebase --continue 如下图： 冲突解决我们前面提到了在分支衍合时出现冲突的解决方案，其实普通的合并也有可能出冲突，出现冲突很正常，解决就是了，git merge 合并分支时如果出现冲突还是先重新编辑冲突文件，编辑完成之后，再执行 git add 和 git commit 即可。 好了，分支管理我们就先说这么多，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-branch.html"},{"title":"DockerHub 与容器网络[Docker 系列-5]","text":"DockerHubDockerHub 类似于 GitHub 提供的代码托管服务，Docker Hub 提供了镜像托管服务，Docker Hub 地址为 https://hub.docker.com/。 利用 Docker Hub 读者可以搜索、创建、分享和管理镜像。Docker Hub 上的镜像分为两大类，一类是官方镜像，例如我们之前用到的 nginx、mysql 等，还有一类是普通的用户镜像，普通用户镜像由用户自己上传。对于国内用户，如果觉得 Docker Hub 访问速度过慢，可以使用国内一些公司提供的镜像，例如网易： https://c.163yun.com/hub#/m/home/ 本文使用官方的 Docker Hub 来演示，读者有兴趣可以尝试网易的镜像站。 首先读者打开 Docker Hub ，注册一个账号，这个比较简单，我就不赘述了。 账号注册成功之后，在客户端命令行可以登录我们刚刚注册的账号，如下： 看到 Login Succeeded 表示登录成功！ 登录成功之后，接下来就可以使用 push 命令上传我们自制的镜像了。注意，自制的镜像要能够上传，命名必须满足规范，即 namespace/name 格式，其中 namespace 必须是用户名，以前文我们创建的 Dockerfile 为例，这里重新构建一个本地镜像并上传到 Docker Hub ，如下： 首先调用 docker build 命令重新构建一个本地镜像，构建成功后，通过 docker images 命令可以看到本地已经有一个名为 wongsung/nginx 的镜像，接下来通过 docker push 命令将该镜像上传至服务端。上传成功后，用户登录Docker Hub ，就可以看到刚刚的镜像已经上传成功了，如下： 看到这个表示镜像已经上传成功了，接下来，别人就可以通过如下命令下载我刚刚上传的镜像： 1docker pull wongsung/nginx pull下来之后，就可以直接根据该镜像创建容器了。具体的创建过程读者可以参考我们本系列前面的文章。 自动化构建自动化构建，就是使用 Docker Hub 连接一个包含 Dockerfile 文件的 GitHub 仓库或者 BitBucket 仓库， Docker Hub 则会自动构建镜像，通过这种方式构建出来的镜像会被标记为 Automated Build ，也称之为受信构建 （Trusted Build） ，这种构建方式构建出来的镜像，其他人在使用时可以自由的查看 Dockerfile 内容，知道该镜像是怎么来的，同时，由于构建过程是自动的，所以能够确保仓库中的镜像都是最新的。具体构建步骤如下： 添加仓库 首先登录到 Docker Hub，点击右上角的 Create，然后选择 Create Automated Build ，如下图： 则新进入到的页面，选择 Link Account 按钮，然后，选择连接 GitHub ，在连接方式选择页面，我们选择第一种连接方式，如下： 选择完成后，按照引导登录 GitHub ，完成授权操作，授权完成后的页面如下： 构建镜像 授权完成后，再次点击右上角的 Create 按钮，选择 Create Automated Build ，在打开的页面中选择 GitHub ，如下两张图： 这里展示了刚刚关联的 GitHub 上的仓库，只有一个 docker ，然后点击进去，如下： 填入镜像的名字以及描述，然后点击 Create 按钮，创建结果如下： 如此之后，我们的镜像就算构建成功了，一旦 GitHub 仓库中的 Dockerfile 文件有更新， Docker Hub 上的镜像构建就会被自动触发，不用人工干预，从而保证镜像始终都是最新的。 接下来，用户可以通过如下命令获取镜像： 1docker pull wongsung/nginx2 获取到镜像之后，再运行即可。 有没有觉得很神奇！镜像更新只要更新自己的 GitHub 即可。镜像就会自动更新！事实上，我们使用的大部分 镜像都是这样生成的。 构建自己的 DockerHub前面我们使用的 Docker Hub 是由 Docker 官方提供的，我们也可以搭建自己的 Docker Hub ，搭建方式也很容器，因为 Docker 官方已经将 Docker 注册服务器做成镜像了，我们直接 pull 下来运行即可，没有没很酷！。具体步骤如下： 拉取镜像 运行如下命令拉取registry官方镜像： 1docker pull registry 运行 接下来运行如下命令将registry运行起来，如下： 1docker run -itd --name registry -p 5000:5000 2e2f252f3c88 运行成功后，我们就可以将自己的镜像提交到registry上了，如下： 这里需要注意的是，本地镜像的命名按照 registryHost:registryPort/imageName:tag 的格式命名。 容器运行在宿主机上，如果外网能够访问容器，才能够使用它提供的服务。本文就来了解下容器中的网络知识。 暴露网络端口在前面的文章中，我们已经有用过暴露网络端口相关的命令了，即 -p 参数，实际上，Docker 中涉及暴露网络端口的参数有两个，分别是 -p 和 -P 。下面分别来介绍。 -P 使用 -P，Docker 会在宿主机上随机为应用分配一个未被使用的端口，并将其映射到容器开放的端口，以 Nginx 为例，如下： 可以看到，Docker 为应用分配了一个随机端口 32768 ，使用该端口即可访问容器中的 nginx（http://lcalhost:32768）。 -p -p 参数则有几种不同的用法： hostPort:containerPort 这种用法是将宿主机端口和容器端口绑定起来，如下用法： 如上命令表示将宿主机的80端口映射到容器的80上，第一个 80 是宿主机的 80 ，第二个 80 是容器的 80 。 ip:hostPort:containerPort 这种是将指定的 ip 地址的端口和容器的端口进行映射。如下： 将 192.168.0.195 地址的80端口映射到容器的80端口上。 ip::containerPort 这种是将指定 ip 地址的随机端口映射到容器的开放端口上，如下： 总结本文主要向大家介绍了 DockerHub 和容器网络，DockerHub 是我们容器的集散中心，网络则使我们的容器有办法对外提供服务。 参考资料： [1] 曾金龙，肖新华，刘清.Docker开发实践[M].北京：人民邮电出版社，2015.","link":"/2019/0529/dockerhub-newwork.html"},{"title":"Git 基本操作","text":"上篇文章我们简单的介绍了 Git 的诞生和发展，然后也说了 Windows 环境下 Git 的安装和一些基本的配置，本文我们就来说一说 Git 中的一些基本概念和基本操作。 本文是 Git 系列的第二篇，了解前面的文章有助于更好的理解本文： 1.Git 概述 工作区和暂存区和 Svn 有很大的不同，Git 中引入了暂存区/缓存区 (Stage/Index) 的概念，如下图： 工作区很好理解，就是我们能看到的工作目录，就是本地的文件夹。 这些本地的文件夹我们要通过 git add 命令先将他们添加到暂存区中。 git commit 命令则可以将暂存区中的文件提交到本地仓库中去。 在 Svn 中我们都是直接将文件提交到版本仓库中去，而在 Git 中，则多了一层关卡。 基本操作下面我主要介绍一下 Git 中的常见操作。 初始化仓库仓库的初始化有两种方式：一种是直接从远程仓库克隆，另一种则是直接从当前目录初始化，这里我们主要介绍当前目录初始化，远程仓库克隆我们在后面的文章中会说到。从当前目录初始化的方式很简单，直接执行如下命令: 1$ git init 执行完成后当前目录下会多出一个 .git 的隐藏文件夹，所有 git 需要的数据和资源都存放在该目录中。 查看仓库状态我们可以通过 git status 命令来查看仓库中文件的状态，比如，在我们仓库刚刚初始化完成之后，我们执行 git status 命令，执行效果如下： 执行结果首先展示了我们当前处于 master 分支下，然后又说暂时没有东西可以提交，因为当前仓库中还没有记录任何文件的任何状态。此时，我在当前目录下创建一个名为 git01.txt 的文件，然后再执行 git status 命令，如下： 此时执行结果中显示有一个未被追踪的文件就是我们刚刚添加的 git01.txt ，这个表示该文件目前并未被 git 仓库所管理，所以接下来我们要将这个文件添加到暂存区。 添加文件到暂存区git add 命令可以将一个文件添加到暂存区，我们现在已经有一个 git01.txt 文件了，接下来，执行如下命令将文件添加到暂存区中： 1$ git add git01.txt 文件添加到暂存区之后，我们再执行 git status 命令，可以看到如下结果： 文件提交到暂存区之后，我们看到此时的状态已经发生了变化。 提交到本地仓库当文件提交到暂存区之后，此时我们可以通过 git commit 命令将当前暂存区的文件提交到本地仓库，如下： 注意，执行 commit 命令时，我们需要加上提交备注，即 -m 参数，提交成功之后，我们再执行 git status 命令，结果如下： 此时一切又恢复宁静了，没有需要 add 的东西，也没有需要 commit 的东西。 如果我们要写的备注非常多，我们可以直接执行 git commit 命令，此时会自动打开一个 vi 编辑器，我们直接在编辑器中输入备注信息即可。假设我在 git01.txt 中随意添加一行内容，然后依次执行 git add、git commit 命令，此时系统会自动打开一个 vi 编辑器，如下： 如图所示，我们在vi编辑器中按照既定的格式编辑内容，编辑完成之后保存退出，此时文件就 commit 成功了。如果在备注信息编辑的过程中我们不想提交了，则直接删除备注信息，保存退出，此时提交就终止了，如下： 提交成功之后，我们可以通过如下命令修改提交备注： 1git commit --amend 运行该命令，会自动打开vi编辑器，此时我们可以重新编辑上次提交的备注信息。 查看提交日志通过 git log 命令我们可以查看以往仓库中提交的日志，比如提交的版本号、提交者、提交者邮箱、提交时间、提交备注等信息，如下： 有的时候我们要查看的命令并不用这么详细，可以在 git log 后面加上 --pretty=short ，这样显示出来的就只是简略信息了： 此时显示出来的是我们这个仓库中的所有日志信息，如果我只想查看某一个文件的提交日志，在 git log 后面加上文件名即可。如下： 如果我还想查看提交时文件的变化，加上 -p 参数即可，如下： 绿色的 + 表示新增的行，红色的 - 表示删除的行（当然这里没有删除的行）。 但是 git log 有一个局限性，就是不能查看已经删除的 commit 的日志，举个例子：下班了，我发现今天下午提交的代码全都是有问题的，于是做了一个版本回退，回退到今天早上的状态，然后关机回家，第二天来了后我发现搞错了，其实那些代码都是 OK 的，于是我又想让仓库版本前进到昨天下午的状态，却发现 git log 命令查看不到昨天下午提交的版本号。此时，我们可以使用 git reflog 命令来实现这一个请求， git reflog 命令可以显示整个本地仓库的 commit , 包括所有 branch 的 commit , 甚至包括已经撤销的 commit , 只要 HEAD 发生了变化, 就会在 reflog 里面看得到，而 git log 只显示当前分支的 commit ，并且不显示删除掉的 commit。如下图： 查看更改前后的差异使用 git diff 命令我们可以查看工作区和暂存区的区别以及工作区和最新提交的差别。我往 git01.txt 文件中再添加一行 hello world ，此时执行 git diff 命令，结果如下： 此时这里显示我们新增了一行。此时我们执行 git add 命令，将文件提交到暂存区，然后再执行 git diff ，如下： 此时没有任何信息输出，因此此时工作区的内容和暂存区的内容已经保持一致了。但是此时工作区和本地仓库中最新提交的内容还是不一致，我们可以通过 git diff HEAD 命令来查看，如下： 此时我们需要执行 git commit 命令将暂存区中的文件提交，提交成功之后，再执行 git diff HEAD 命令，则又恢复宁静了。如下： 压缩提交历史 git rebase -i 命令可以实现提交历史的压缩。比如我们在开发某一个功能时，提交了很多次，当所有功能都写完时，想将这些提交压缩为一个，就可以使用该命令，如下： 如上图，该命令执行之后，会自动打开一个vi编辑器，在vi编辑器中将最新提交的日志的 pick 改为 fixup 即可。压缩之后，最新一次的提交日志就没了，但是数据还在。 OK，Git 基本操作我们就先说这么多，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-basic.html"},{"title":"Git 工作区储藏","text":"这是一篇计划之外的文章，之所以有这篇文章，是因为有一个小伙伴在阅读Git 分支管理一文时遇到了一个问题，而这个问题又比较典型，因此我想专门来谈谈 Git 中工作区的储藏问题。 本文是 Git 系列的第六篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药4.Git 分支管理5.Git 关联远程仓库 问题回顾小伙伴遇到的问题是这样的： 现在有一个 master 分支，master 分支中有一个文件叫 01.txt ，该文件中只有一行数据，然后对 01.txt 执行 add 和 commit ，然后再从 master 分支中创建出一个新的分支 fa ，切换到 fa 分支上，然后向 01.txt 中再添加一行数据，添加成功之后，不做任何事情，再切换回 master 分支，此时用 cat 命令查看 01.txt 文件，发现竟然有两行数据，按理说 master 中的 01.txt 只有一行数据，而 fa 中的 01.txt 有两行数据，整个过程如下图： 要搞清楚这个问题，得先明白下面这个问题： cat 命令和 git 无关，就是用来查看文件的，我为了演示方便使用了 cat 命令，这和直接用记事本打开文件查看效果是一样的。 可能眼尖的小伙伴已经发现端倪了，我们上面这个操作少了两个步骤，那就是 add/commit ，fa 分支中的数据修改之后直接切换回了 master ，而没有 add/commit 。正常情况下（修改数据后 add/commit），如果 master 和 fa 分支中的数据不一致，我们执行了 git checkout - 进行分支的切换，这个时候工作区中的文件内容也是会跟着变化的（大家可以通过 cat 命令或者直接在记事本中打开工作区的文件来查看这种变化），但是如果我在 fa 分支中修改了文件却没有 add/commit 就切换回 master ，此时如果工作区的文件变化了，可能会导致我在 fa 分支中的修改丢失，因此，这个时候工作区的文件就没有变化，即工作区的文件内容还是 fa 分支中修改的内容。 解决这个问题，我们有两种方案，请小伙伴们往下看。 解决方案方案一第一种解决方案就是在某一个分支修改文件之后，先 add 并且 commit 之后再去切换分支，这个操作就比较简单了，我这里就不再演示了。 方案二(储藏)第二种解决方案就是储藏 (Stashing)，储藏适用在如下场景中： 当我在一个分支 fa 中修改了文件，但是还没有完全改好，此时我并不想 add/commit ，但是这个时候有一个更急迫的事情在另外一个分支 fb 上需要我去做，我必须要切换分支。 在这样一个场景中，如果我直接切换分支，会出现如下两个问题： 1.从 fa 切换到 fb 之后，工作区的代码还是 fa 的代码，不符合我的工作要求。2.假设我不在乎问题 1，在 fb 中直接修改工作区的代码，等我在 fb 中修改完后提交后再回到 fa ，会发现我之前的代码丢失了。 为了解决这个问题，Git 给我们提供了储藏 (Stashing)。 现在假设一开始 master 和 fa 分支中的文件内容都是一致的，而且两个分支的工作区都是干净的，即没有东西需要 add/commit ，此时，我在 master 中修改了文件，修改完成之后，执行 git status 命令我们看到 master 中有东西需要 add/commit ，此时我想切换到 fa 分支中去，但是并不想对 master 分支执行 add/commit ，这个时候我们可以执行如下命令，先将当前分支中的文件储藏起来： 1$ git stash OK，执行完 git stash 命令之后，再执行 git status ，我们发现此时 master 分支已经是干净的了，此时我们可以愉快的切换到 fa 分支中去了，切换到 fa 分支之后，我们发现 master 中的修改并没有干扰到 fa 分支，当我们完成了 fa 分支中的工作之后，再回到 master 分支，此时执行如下命令可以恢复刚刚储藏的数据： 1$ git stash apply 上面这个命令执行完之后，master 分支中的工作区中的文件就恢复了，此时执行 git status 就可以看到又有数据需要 add/commit 了。 我们也可将工作区储藏多次，这个时候我们可以执行如下命令来查看储藏： 1$ git stash list 执行效果如下： git stash apply 表示恢复最近一次储藏，如果我们想恢复到之前的某一次储藏，可以加上储藏的名字，如下： 1$ git stash apply stash@{1} 还有一些其他的关于储藏的命令： 1.恢复储藏并出栈1$ git stash pop 执行效果和 git stash apply 一样，不同的是，这里执行完之后，会将栈顶的储藏移除。 2.删除某一个储藏1$ git stash drop stash@{4} 最后一个参数是指储藏的名字。 Ok，储藏问题我们先说这么多。有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-stash.html"},{"title":"Git 标签管理","text":"我们可以针对某一次的提交打上一个标签，有点类似于给某次提交取个别名，比如 1.0 版本发布时打个标签叫 v1.0,2.0 版本发布时打个标签叫 v2.0 ，因为每次版本提交的结果都是一连串的哈希码，不容易记忆，打上 v1.0,v2.0 这些具有某种含义的标签后，可以方便我们进行版本管理。 本文是 Git 系列的第七篇，了解前面的文章有助于更好的理解本文： 1.Git 概述2.Git 基本操作3.Git 中的各种后悔药4.Git 分支管理5.Git 关联远程仓库6.Git 工作区储藏兼谈分支管理中的一个小问题 轻量级标签轻量级标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。 首先我们可以通过如下命令来查看当前仓库中的所有标签： 1$ git tag 打标签的方式很简单，直接通过 git tag &lt;tagname&gt; 来完成即可，如下命令： 1$ git tag v1 表示创建了一个名为 v1 的 tag ，这个 tag 默认是创建在最新一次的 commit 上的，如下： 我们可以利用 git show &lt;tagname&gt; 来查看标签对应的版本信息，如下： 我们可以通过 $ git tag -d &lt;tagname&gt; 命令删除一个标签： 1$ git tag -d v1 如下图： 如果我想给历史上的某次 commit 打一个标签呢?我们可以通过如下命令 git tag &lt;tagname&gt; &lt;commitversion&gt; ,如下： 1$ git tag v0.0 7d519 表示给 commit 的哈希码为 7d519 的那一次 commit 打上一个标签，如下图： 含附注的标签而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。 打一个含附注的标签很简单，使用 git tag -a &lt;tagname&gt; -m &lt;msg&gt; 命令,如下： 1$ git tag -a v0.0 -m &quot;文件初次建立&quot; 7d519 如下： 如果不加最后的版本号参数，表示给最新的一次 commit 打标签。 签署标签说到签署标签我们得先介绍一下 GPG ： GPG 是加密软件，可以使用 GPG 生成的公钥在网上安全的传播你的文件、代码。为什么说安全的？以 Google 所开发的 repo 为例，repo 即采用 GPG 验证的方式，每个里程碑 tag 都带有 GPG 加密验证，假如在里程碑 v1.12.3 处你想要做修改，修改完后将这个 tag 删除，然后又创建同名 tag 指向你的修改点，这必然是可以的。但是，在你再次 clone 你修改后的项目时，你会发现，你对此里程碑 tag 的改变不被认可，验证失败，导致你的修改在这里无法正常实现。这就是 GPG 验证的作用，这样就能够保证项目作者（私钥持有者）所制定的里程碑别人将无法修改。那么，就可以说，作者的代码是安全传播的。为什么会有这种需求？一个项目从开发到发布，再到后期的更新迭代，一定会存在若干的稳定版本与开发版本（存在不稳定因素）。作为项目发起者、持有者，有权定义他（们）所认可的稳定版本，这个稳定版本，将不允许其他开发者进行改动。还以 Google 的 repo 项目为例，项目所有者定义项目开发过程中的点 A 为稳定版 v1.12.3，那么用户在下载 v1.12.3 版本后，使用的肯定是 A 点所生成的项目、产品，就算其他开发者能够在本地对 v1.12.3 进行重新指定，指定到他们修改后的B点，但是最终修改后的版本给用户用的时候，会出现 GPG 签名验证不通过的问题，也就是说这样的修改是不生效的。 —-摘自&lt;带 GPG 签名的 Git tag&gt;一文 使用签署标签我们先要生成 GPG Key，生成命令如下： 1$ gpg --gen-key 能默认的就直接按回车默认，不能默认的就根据提示输入相应的值，这里的都很简单，不再赘述。完了之后，就可以通过如下命令来打标签了： 1$ git tag -s v0.0 -u &quot;laowang&quot; -m &quot;文件初次建立&quot; 7d519 就把上面的-a换成-s，然后添加-u参数，-u参数的值是我们在生成 GPG Key 的时候配置的 name 属性的值，注意-u参数不可以写错，否则标签会创建失败，如下： 如上图，-u 参数写错时，标签创建失败。 标签推送到远程仓库git push 命令并不会把tag提交到远程仓库中去，需要我们手动提交，如下： 1$ git push origin v0.0 表示将 v0.0 标签提交到远程仓库，也可以通过 $ git push origin --tags 提交所有的 tag 到远程仓库，如下： 此时别人调用 git pull 更新代码之后，就能看到我们的 tag。如下： Ok，Git 标签管理我们先说这么多。有问题欢迎留言讨论。 参考资料： 《GitHub入门与实践》 《Pro Git》","link":"/2019/0612/git-tag.html"},{"title":"Git 概述","text":"一直以来想出一个 Git 的教程，去年写过一篇，后来没了下文，烂尾了。最近忙里偷闲，还是想把这个 Git 系列写一遍，这次争取写完。 本文我主要想先简单介绍下Git，然后介绍下 Git 的安装。 毫无疑问， Git 是目前最优秀的分布式版本控制工具，木有之一，可是我见到的很多人还是不会用，我的老东家每天忍受着 SVN 带来的痛苦，却迟迟不愿切换到 Git 上，个人感觉，许多中小公司不用 Git ，不是因为 Git 不好，而是他们的项目经理不会用（逃。 OK，那么今天我们就先来简单介绍下 Git 的发展史以及 Git 的优点，然后再来看看 Git 的安装。 Git诞生记Git 诞生于 2005 年 4 月，由 Linux 的作者 Linus Torvalds 花费了两周的时间用 C 写了一个分布式版本控制系统，这就是 Git1.0 ，大牛写代码就是这么 666666 。 其实早在 Git 之前，这个世界就已经有一些非常流行的版本控制工具 （VCS） ，比如 CVS ，SVN，ClearCase 等，然而这些工具要么运行慢，要么要收费，都不是 Linus Torvalds 的菜。 当时，有一个公司叫做 BitMover ，该公司有一款产品叫做 BitKeeper ，这是一个分布式版本控制工具，但是这是收费的，不过这是一个有情怀的公司，老板 Larry 说服 Linus Torvalds 使用 BitKeeper 来管理 Linux 内核源码，于是，在 2002 到 2005 年之间，Linux 内核开发团队一直使用 BitKeeper 来管理 Linux 源码。 可是在开发的过程中， Linus Torvalds 有一个叫做安德鲁·垂鸠的小伙伴不安分了，他写了一个简单程序，可以连接 BitKeeper 的存储库，BitKeeper 著作权拥有者拉里·麦沃伊认为安德鲁·垂鸠对 BitKeeper 内部使用的协议进行逆向工程，决定收回无偿使用 BitKeeper 的授权。 合作无法继续，于是 Linus Torvalds 决定自己开发一套分布式版本控制工具，就是 Git ，至于这个分布式版本控制工具为什么叫 Git，Linus Torvalds 并没有给出一个让所有人满意的解释，倒是很多开发者一直尝试去给 Git 一个合理的解释，常见的一个解释就是 Global Information Tracker （中文译作全局信息追踪器）。 如果当年不是 BitMover 公司的逼迫，估计我们还不一定见到这么优秀的分布式版本控制工具。值得一说的是， 2016 年 5 月 11 日 BitKeeper 宣布以 Apache 2.0 许可证开源（新闻链接http://www.solidot.org/story?sid=48171），我很好奇 BitKeeper 宣布开源的时候他的老板心中是何感受？ Git的优势Git 一出世立马就成为最流行的分布式版本控制工具，2008 年 4 月，GitHub 正式上线，GitHub 是一个利用 Git 进行版本控制，专门用于存放代码与内容的共享虚拟主机服务，GitHub 上线之后，许多开源项目都移植到 GitHub 上了，不管你从事那门语言的研发，都会在 GitHub 上找到你需要的项目吧！OK，说了这么多，接下来我们也该说说 Git 这个分布式版本控制工具的优势了。Git 主要有以下几个优势： 与传统的集中式版本控制工具不同，分布式版本控制工具不需要联网就可以工作，每台电脑都是一个完整的版本仓库。 Git 可以胜任上万人的开发规模，这个大家看看 GitHub 中的开源项目就知道了，不需要我多说。 性能优异。我们前面说过 Linus Torvalds 之所以不愿意使用 CVS、SVN 等版本控制工具就是因为这些工具的性能太差。所以 Linus Torvalds 在开发 Git 时就决定要革除积弊，确保 Git 的运行效率。笔者在上家公司做开发时，深受 SVN 的毒害，但是公司还是不愿意迁移到 Git 上，我猜测是由于项目经理不会用。 保证项目的安全。我们知道，在 SVN 之前还有一个集中式版本控制工具叫做 CVS ，这个 CVS 有一个问题，就是你的文件有的时候会莫名其妙的丢失，做开发的各位筒子都知道，如果你的项目中突然有一个文件不见了，你不出一身汗才怪。所以，Git 使用 SHA1 这种通用的加密散列函数来对数据库中的对象进行命名，从而来确保文件的安全。 好用的分支。用过 Git 的人都知道 Git 中的分支用起来有多么爽，分支在我们的项目中用的非常普遍，可是 SVN 虽然也有分支，但是却不能像 Git 用的这么爽。这个做过项目的筒子都知道。我们到后文会给大家详细介绍分支的用法。 OK，Git 的优势还有很多种，这里我就不再一一列举了，有兴趣的大家自行搜索。 Git 的安装作为一个屌丝码农，我的本子还是 windows ,不过我的本本装了双系统，所以我这里就只给大家演示一下 Windows 下如何安装 Git 以及 Ubuntu 下如何安装 Git。 windows7 安装 Gitwindows 安装 Git 整体上来说有两种解决方案 安装Cygwin（下载地址http://cygwin.com/）用来模拟Linux运行环境，但是Cygwin大配置非常麻烦，容易出错，所以一般不推荐这种方式。 安装独立的Git，也就是msysGit（下载地址https://git-for-windows.github.io/），这就是一个简单的exe文件，一路next就安装成功了。安装成功后，在你的开始菜单中找到Git Bash，点击Git Bash，输入git –version查看git版本号，运行界面如下： Ubuntu 安装 Gitubuntu 安装 Git 就是一句话: 1sudo apt-get install git 早期的 Linux 版本直接运行下面的代码即可： 1sudo apt-get install git-core 安装成功之后，输入 git –version 查看 git 版本号。 基本配置不管是 Windows 安装还是 Linux 安装，安装好之后，我们都先通过如下两行命令做一个基本配置,配置的信息将展示在我们每一次提交的后面，所以不要使用不方便公开的信息，如果不配置以后每次提交的时候都会让你输入用户名和密码，配置方式如下： 12$ git config --global user.name &quot;zhangsan&quot; $ git config --global user.email &quot;111@qq.com&quot; 这个配置会保存在当前用户目录下的 .gitconfig 文件中，如下： OK，本文我们就先说到这里，有问题欢迎留言讨论。 参考资料： 1.《GitHub入门与实践》2.《Pro Git》","link":"/2019/0612/git-install.html"},{"title":"JavaWeb 乱码问题终极解决方案！","text":"经常有读者在公众号上问 JavaWeb 乱码的问题，昨天又有一个小伙伴问及此事，其实这个问题很简单，但是想要说清楚却并不容易，因为每个人乱码的原因都不一样，给每位小伙伴都把乱码的原因讲一遍也挺费时间的，因此，松哥今天决定写一篇文章，和大伙好好捋捋 JavaWeb 中的乱码问题。 对于一些老司机而言，其实并不太容易遇到乱码问题，但是对于一些新手来说，乱码几乎是家常便饭，而且每当乱码时，网上搜了一大堆解决方案，发现自己的问题还是没能解决，其实这就是平时研究代码不求甚解导致的，乱码问题，也要去分析，然后才能对症下药，才能药到病除。 整体思路首先出现乱码之后，要先去确认乱码的地方，当一个网页上出现乱码，有可能是浏览器显示问题，也有可能是 Java 编码问题，也有可能数据库中的数据本身就是乱码的，所以我们要做的第一件事就是确认乱码发生的位置，缩小 bug 范围，通过打印日志或者 debug 首先去确认乱码发生的位置，然后再去进一步解决，一般来说，乱码的原因大致上可以分为两类： 请求乱码 响应乱码 请求乱码，可能是因为参数放在 URL 地址中乱码，也有可能是参数放在请求体中乱码，不同传参方案也对应了不同的乱码解决方案。如果是响应乱码，那么原因就会比较多了，一般来说，有如下几种可能的原因： 数据库本身乱码 数据在 Java 代码中乱码 数据在浏览器显示的时候乱码 数据在从 Java 应用传到数据库的过程中乱码 对于不同的乱码原因，会有不同的解决方案，对症下药，才能药到病除，所以当出现乱码时，大家要做的第一件事就是分析乱码发生的原因，找到原因了，才能找到解决方案。 基本原则发生乱码是因为各自编码不同导致的，所以，大家首先要有一个良好的开发习惯，项目编码，文件编码都要统一起来，松哥有个同事就因为 Freemarker 乱码，找了半天没找到原因，后来在松哥建议下修改了项目编码，乱码问题才解决了，一般来说，公司制度稍微成熟一些，都会对项目编码，文件编码有硬性规定的。在Eclipse 中，设置项目编码方式如下（工程的编码要提前设置，如果项目已经开发一半再去设置，已有的中文就会乱码）： Window-&gt;Preferences-&gt;General 然后对于 JSP 文件也需要提前设置好编码方式，如下： 这是在 Eclipse 中设置文件编码，如果是在 IntelliJ IDEA中，则不需要设置JSP文件编码，因为默认就是 UTF-8，只需要提前设置下工程编码即可： 除了开发工具的编码，数据库的编码也要统一，一般来说，主要是设置一下数据库的编码和数据表的编码，如下： 设置数据库编码： 1CREATE DATABASE `vhr` DEFAULT CHARACTER SET utf8; 设置数据表编码： 123456DROP TABLE IF EXISTS `adjustsalary`;CREATE TABLE `adjustsalary` ( `id` int(11) NOT NULL AUTO_INCREMENT, `eid` int(11) DEFAULT NULL, PRIMARY KEY (`id`),) ENGINE=InnoDB DEFAULT CHARSET=utf8; 这些是准备工作，这些工作做好了，还是有可能会遇到乱码问题，接下来我们就具体问题具体分析。 请求乱码请求乱码，就是说数据在浏览器中显示是正常的，但是传到 Java 后端之后，就乱码了，这种乱码一般来说，分为两种： 参数放在 URL 地址中导致的乱码 参数放在请求体中导致的乱码 两种乱码原因，对应了两种不同的解决方案。分别来看。 URL 地址中的参数乱码这种乱码主要发生在 GET 请求中，因为在 GET 请求中我们一般通过 URL 来传递参数，这个问题可以在代码中解决，但是太过于麻烦，因此一般我们直接在Tomcat配置中解决，修改 Tomcat的conf/server.xml 文件，修改 URL 编码格式，如下： 这样就可以搞定 URL 地址中的参数乱码。 请求体中的参数乱码请求体中的参数乱码，我们可以在解析参数之前通过设置 HttpServletRequest 的编码来解决，如下： 1request.setCharacterEncoding(\"UTF-8\"); 但是一样也太过于麻烦，所以如果是普通的 Servlet/JSP 项目，我们就可以直接定义一个过滤器来处理，如下： 1234567public class EncodingFilter implements Filter { @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { request.setCharacterEncoding(\"UTF-8\"); chain.doFilter(request, response); }} 过滤器配置： 12345678&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.sang.filter.EncodingFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 在工程编码和JSP/HTML编码都没问题的情况下，请求乱码基本上就是这两种情况。 响应乱码如果在浏览器上加载页面看到了乱码，大家首先要确认在从服务端往浏览器写数据的前一刻，这个数据还没有乱码（即数据库中查询出来的数据是OK的，没有发生乱码的问题），那么对于这种乱码，我们只需要设置响应数据的 ContentType 就可以了，如下： 1response.setContentType(\"text/html;charset=UTF-8\"); 如果从数据库中查询出来的数据就是乱码的，那么就需要去确认数据库中的编码是否 OK 。 框架处理前面提到的方案，都是在 Servlet/JSP 项目中我们可以采用的方案，在 SSM 框架中当然也可以使用，但是，SpringMVC 框架本身也提供了一个过滤器，我们可以借用这个过滤器更加高效的解决响应乱码问题，如下： 1234567891011121314151617181920&lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceRequestEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceResponseEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 当然，上面这段配置并不能代替 Tomcat 中 conf/server.xml 中的编码配置，如果是在 Spring Boot 中，配置可以更加简单，只需要在 application.properties 中添加如下配置即可： 123server.tomcat.uri-encoding=UTF-8spring.http.encoding.force-request=truespring.http.encoding.force-response=true 其他乱码其他乱码主要是指使用一些第三方框架导致的乱码，例如使用 Alibaba 的 fastjson，开发者就需要在配置 HttpMessageConverter 时指定编码格式，否则就有可能出现乱码，这种第三方框架的乱码松哥没法穷举，大伙在使用时需要注意看官方文档，fastjson 的 HttpMessageConverter 配置如下： 123456789@BeanFastJsonHttpMessageConverter fastJsonHttpMessageConverter() { FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); FastJsonConfig config = new FastJsonConfig(); config.setCharset(Charset.forName(\"UTF-8\")); converter.setFastJsonConfig(config); converter.setDefaultCharset(Charset.forName(\"UTF-8\")); return converter;} 一个隐蔽的乱码除了前面介绍的这几种乱码之外，还有一个比较隐蔽的乱码，容易被很多初学者忽略的地方，就是数据在从 Java 应用传递到 MySQL 的过程中，发生了乱码，这种问题一般在 Windows 上不易发生，如果数据库装在 Linux 上，则这个问题就很容易发生，数据在代码中命名没有乱码，存到 MySQL 上就乱码了，但是如果直接使用 Navicat 等工具往 MySQL 上存储数据，又不会乱码，或者 MySQL 中数据没有乱码，但是用 Java 查询出来就乱码了，这种都是数据在 应用 和 数据库 之间传递时发生了乱码，解决方式很简单，在数据库连接地址上指定编码即可，如下： 1db.url=jdbc:mysql:///yuetong?useUnicode=true&amp;characterEncoding=UTF-8 大致就这些，还有一些非常偶尔的情况可能会用到 @RequestMapping 注解中的 produces 属性，在这里指定数据类型即可。 好了，差不多就这些，下次有人问你为啥我的又乱码了，直接把这篇文章甩给他。大伙有什么解决乱码的独门密器也可以一起来讨论。","link":"/2019/0409/javaweb-encoding.html"},{"title":"Linux 上安装 Redis","text":"hello，各位小伙伴们好久不见！那么从今天开始，我想和各位小伙伴分享下 Redis 的用法，本文我们就先来看看什么是 Redis 以及如何安装 Redis。 什么是 RedisRedis 是一个使用 ANSI C 编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。从 2015 年 6 月开始，Redis 的开发由 Redis Labs 赞助，而 2013 年 5 月至 2015 年 6 月期间，其开发由 Pivotal 赞助。在 2013 年 5 月之前，其开发由 VMware 赞助。根据月度排行网站 DB-Engines.com 的数据显示，Redis是 最流行的键值对存储数据库。 Redis 具有如下特点： Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用，不会造成数据丢失 Redis 支持五种不同的数据结构类型之间的映射，包括简单的 key/value 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储 Redis 支持 master-slave 模式的数据备份 Redis 具有如下功能： 内存存储和持久化：redis 支持异步将内存中的数据写到硬盘上，在持久化的同时不影响继续服务 取最新N个数据的操作，如：可以将最新的 10 条评论的 ID 放在 Redis 的 List 集合里面 数据可以设置过期时间 自带发布、订阅消息系统 定时器、计数器 Redis 安装Windows 版 Redis 的安装，整体来说还是非常简单的，网上也有很多教程，考虑到 Redis 的大部分使用场景都是在 Linux 上，因此这里我对 Windows 上的安装不做介绍，小伙伴们有兴趣可以自行搜索，下面我们主要来看下 Linux 上怎么安装 Redis 。 环境： CentOS7 redis4.0.8 1.首先下载 Redis，下载地址https://redis.io/，下载获得 redis-4.0.8.tar.gz 后将它放入我们的 Linux 目录 /opt 2./opt 目录下，对文件进行解压，解压命令: tar -zxvf redis-4.0.8.tar.gz ，如下： 3.解压完成后出现文件夹：redis-4.0.8，进入到该目录中: cd redis-4.0.8 4.在 redis-4.0.8 目录下执行 make 命令进行编译 5.如果 make 完成后继续执行 make install 进行安装 OK，至此，我们的 redis 就算安装成功了。 6.在我们启动之前，需要先做一个简单的配置：修改 redis.conf 文件，将里面的 daemonize no 改成 yes，让服务在后台启动，如下： 7.启动，通过redis-server redis.conf命令启动redis，如下： 8.测试 首先我们可以通过 redis-cli 命令进入到控制台，然后通过 ping 命令进行连通性测试，如果看到 pong ，表示连接成功了，如下： 9.关闭，通过 shutdown 命令我们可以关闭实例，如下： OK，至此，我们的 Redis 就安装成功了，整体来说还是非常简单的，有问题欢迎留言讨论。","link":"/2019/0615/linux-redis.html"},{"title":"Git 学习资料","text":"关于Git的用法我们已经写七篇文章，介绍了Git的不少用法，这些足以应付工作中90%的需求了，剩下的10%就需要小伙伴们在工作中自己慢慢总结了，我这里再给小伙伴们推荐一点Git学习资料，为我们的Git系列画上一个句号。 书推荐两本个人觉得很不错的书： 《GitHub入门与实践》 《Pro Git》 《GitHub 入门与实践》秉承了日系技术书刊一贯的“手把手教学”风格，作者用亲切的语言，简明扼要的介绍，配以生动详实的示例一步步讲解GitHub和Git的使用方法。《Pro Git》作为 Git 官方推荐书籍，《Pro Git》 值得 Git 初学者和爱好者认真阅读一遍。 网站1.https://learngitbranching.js.org 链接是一个 git 学习网站，我们可以直接在上面练习 git 命令。 博客推荐本公号前面的几篇教程: 1.Git概述2.Git基本操作3.Git中的各种后悔药4.Git分支管理5.Git关联远程仓库6.Git工作区储藏兼谈分支管理中的一个小问题7.Git标签管理","link":"/2019/0612/git-resources.html"},{"title":"MyBatis中主键回填的两种实现方式","text":"主键回填其实是一个非常常见的需求，特别是在数据添加的过程中，我们经常需要添加完数据之后，需要获取刚刚添加的数据 id，无论是 Jdbc 还是各种各样的数据库框架都对此提供了相关的支持，本文我就来和和大家分享下数据库主键回填在 MyBatis 中的两种实现思路。 原生写法框架来源于我们学过的基础知识，主键回填实际上是一个在 JDBC 中就被支持的写法，有的小伙伴可能不知道这一点，因此这里我先来说说在 JDBC 中如何实现主键回填。 JDBC 中实现主键回填其实非常容易，主要是在构造 PreparedStatement 时指定需要主键回填，然后在插入成功后，查询刚刚插入数据的 id ，示例代码如下： 1234567891011121314151617public int insert(Person person) { Connection con = null; PreparedStatement ps = null; ResultSet rs = null; con = DBUtils.getConnection(); ps = con.prepareStatement(\"INSERT INTO person(username,password,money) VALUES(?,?,?)\", PreparedStatement.RETURN_GENERATED_KEYS); ps.setObject(1, person.getUsername()); ps.setObject(2, person.getPassword()); ps.setObject(3, person.getMoney()); int i = ps.executeUpdate(); rs = ps.getGeneratedKeys(); int id = -1; if (rs.next()) { id = rs.getInt(1); } return id;} 和普通的插入 SQL 不同之处主要体现在两个地方： 第一个是构造 PreparedStatement 时，多了一个参数，指定了需要主键回填。 在更新操作执行完成之后，调用 getGeneratedKeys ，然后又会获取到一个 ResultSet 对象，从这个游标集中就可以获取到刚刚插入数据的id。 这个是原生的写法，在 MyBatis 中，对此需求提供了两种不同的实现方案，下面分别来看。 框架写法一般情况下，主键有两种生成方式： 主键自增长 自定义主键（一般可以使用UUID，或者类UUID） 如果是第二种，主键一般是在Java代码中生成，然后传入数据库执行插入操作，如果是第一个主键自增长，此时，Java 可能需要知道数据添加成功后的主键。 MyBatis 的基本用法就无需多说了，这也不是本文的重点，我们还是来看看 MyBatis 中主键回填的两种不同实现方式吧！ 方式一第一种方式比较简单，也是松哥推荐的一种实现方式： 123&lt;insert id=\"insertBook\" useGeneratedKeys=\"true\" keyProperty=\"id\"&gt; insert into t_book (b_name,author) values (#{name},#{author});&lt;/insert&gt; 这种方式比较简单，就是在插入节点上添加 useGeneratedKeys 属性，同时设置接收回传主键的属性。配置完成后，我们执行一个插入操作，插入时传入一个对象，插入完成后，这个对象的 id 就会被自动赋值，值就是刚刚插入成功的id。 松哥推荐大家使用这种方式，原因很简单，这种方式实现简便省事。 方式二第二种方式则是利用MySQL自带的 last_insert_id() 函数查询刚刚插入的id，示例代码如下： 123456&lt;insert id=\"insertBook\"&gt; &lt;selectKey keyProperty=\"id\" resultType=\"java.lang.Integer\"&gt; SELECT LAST_INSERT_ID() &lt;/selectKey&gt; insert into t_book (b_name,author) values (#{name},#{author});&lt;/insert&gt; 这种方式是在 insert 节点中添加 selectKey 来实现主键回填，实际上这种方式的功能更加丰富，因为 selectKey 节点中的 SQL 我们既可以在插入之前执行，也可以在插入之后执行（通过设置节点的 Order 属性为 AFTER 或者 BEFORE 可以实现），具体什么时候执行，还是要看具体的需求，如果是做主键回填，我们当然需要在插入 SQL 执行之后执行 selectKey 节点中的 SQL。 注意第二种方式一样也要通过设置 keyProperty 来指定将查询到的数据绑定到哪个属性上。 总结好了，本文向大家介绍了 MyBatis 中主键回填的两种方式，大家有没有 get 到呢？有问题欢迎留言讨论。","link":"/2019/0424/mybatis-key-generated.html"},{"title":"Redis 中的五种数据类型简介","text":"上篇文章我们介绍了如何在 Linux 中安装 Redis，本文我们来了解下 Redis 中的五种数据类型。 本文是 Redis 系列的第二篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis 五大数据类型介绍redis 中的数据都是以 key/value 的形式存储的，五大数据类型主要是指 value 的数据类型，包含如下五种： STRINGSTRING 是 redis 中最基本的数据类型，redis 中的 STRING 类型是二进制安全的，即它可以包含任何数据，比如一个序列化的对象甚至一个 jpg 图片，要注意的是 redis 中的字符串大小上限是 512M 。 LISTLIST 是一个简单的字符串列表，按照插入顺序进行排序，我们可以从 LIST 的头部 (LEFT) 或者尾部 (RIGHT) 插入一个元素，也可以从 LIST 的头部(LEFT)或者尾部 (RIGHT) 弹出一个元素。 HASHHASH 类似于 Java 中的 Map ，是一个键值对集合，在 redis 中可以用来存储对象。 SETSET 是 STRING 类型的无序集合，不同于 LIST ，SET 中的元素不可以重复。 ZSETZSET 和 SET 一样，也是 STRING 类型的元素的集合，不同的是 ZSET 中的每个元素都会关联一个 double 类型的分数，ZSET 中的成员都是唯一的，但是所关联的分数可以重复。 OK，通过上面的介绍，相信小伙伴们对五大数据类型都有一个大致的认识了，接下来我们就来看看这五种数据类型要怎么操作。 key 相关的命令由于五大数据类型的数据结构本身有差异，因此对应的命令也会不同，但是有一些命令不管对于哪种数据类型都是存在的，我们今天就先来看看这样一些特殊的命令。 首先通过 redis-server redis.conf 命令启动 redi s，再通过 redis-cli 命令进入到控制台中，如下： 首先我们可以通过 set 命令插入一条记录： 12127.0.0.1:6379&gt; set k1 v1OK DEL 命令看到 OK 表示插入成功。通过 DEL 命令我们可以删除一个已经存在的 key，如下： 12127.0.0.1:6379&gt; DEL k1(integer) 1 看到 (integer) 1 表示数据已经删除成功。 DUMP 命令DUMP 命令可以序列化给定的 key，并返回序列化之后的值： 12127.0.0.1:6379&gt; DUMP k1&quot;\\x00\\x02v1\\b\\x00\\xe6\\xc8\\\\\\xe1bI\\xf3c&quot; EXISTS 命令EXISTS 命令用来检测一个给定的 key 是否存在，如下： 12345127.0.0.1:6379&gt; EXISTS k1(integer) 1127.0.0.1:6379&gt; EXISTS k2(integer) 0127.0.0.1:6379&gt; 上面的运行结果表示 k1 存在而 k2 不存在。 TTL 命令TTL 命令可以查看一个给定 key 的有效时间： 1234127.0.0.1:6379&gt; TTL k1(integer) -1127.0.0.1:6379&gt; TTL k2(integer) -2 -2 表示 key 不存在或者已过期；-1 表示 key 存在并且没有设置过期时间（永久有效）。当然，我们可以通过下面的命令给 key 设置一个过期时间： EXPIRE 命令EXPIRE 命令可以给 key 设置有效期，在有效期过后，key 会被销毁。 12345127.0.0.1:6379&gt; EXPIRE k1 30(integer) 1127.0.0.1:6379&gt; TTL k1(integer) 25127.0.0.1:6379&gt; 30 表示 30 秒，TTL k1 返回 25 表示这个 key 的有效期还剩 25 秒。 PERSIST 命令PERSIST 命令表示移除一个 key 的过期时间，这样该 key 就永远不会过期： 12345678127.0.0.1:6379&gt; EXPIRE k1 60(integer) 1127.0.0.1:6379&gt; ttl k1(integer) 57127.0.0.1:6379&gt; PERSIST k1(integer) 1127.0.0.1:6379&gt; ttl k1(integer) -1 PEXPIRE 命令PEXPIRE 命令的功能和 EXPIRE 命令的功能基本一致，只不过这里设置的参数是毫秒： 12127.0.0.1:6379&gt; PEXPIRE k1 60000(integer) 1 PTTL 命令PTTL 命令和 TTL 命令基本一致，只不过 PTTL 返回的是毫秒数： 12127.0.0.1:6379&gt; PTTL k1(integer) 25421 KEYS 命令KEYS 命令可以获取满足给定模式的所有 key，比如： 1234127.0.0.1:6379&gt; KEYS *1) &quot;k3&quot;2) &quot;k2&quot;3) &quot;k1&quot; KEYS * 表示获取所有的 KEY， * 也可以是一个正则表达式。 OK,key 相关的命令我们就介绍这么多，当然还有很多其他的，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-datatype.html"},{"title":"Redis 主从复制(一)","text":"前面两篇文章和小伙伴们聊了 redis 中的数据备份问题，也对快照备份和 AOF 备份做了对比，本文我们来聊聊 redis 中的主从复制问题，算是数据备份的第三种解决方案。 本文是 Redis 系列的第十篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化 主从复制主从复制可以在一定程度上扩展 redis 性能，redis 的主从复制和关系型数据库的主从复制类似，从机能够精确的复制主机上的内容。实现了主从复制之后，一方面能够实现数据的读写分离，降低 master 的压力，另一方面也能实现数据的备份。 配置方式假设我有三个 redis 实例，地址分别如下： 123192.168.248.128:6379 192.168.248.128:6380 192.168.248.128:6381 即同一台服务器上三个实例，配置方式如下： 1.将 redis.conf 文件更名为 redis6379.conf ，方便我们区分，然后把 redis6379.conf 再复制两份，分别为 redis6380.conf 和 redis6381.conf 。如下： 2.打开 redis6379.conf ，将如下配置均加上 6379,(默认是 6379 的不用修改)，如下： 12345port 6379pidfile /var/run/redis_6379.pidlogfile &quot;6379.log&quot;dbfilename dump6379.rdbappendfilename &quot;appendonly6379.aof&quot; 3.同理，分别打开 redis6380.conf和redis6381.conf 两个配置文件，将第二步涉及到 6379 的分别改为 6380 和 6381 。4.输入如下命令，启动三个 redis 实例： 123[root@localhost redis-4.0.8]# redis-server redis6379.conf[root@localhost redis-4.0.8]# redis-server redis6380.conf[root@localhost redis-4.0.8]# redis-server redis6381.conf 5.输入如下命令，分别进入三个实例的控制台： 123[root@localhost redis-4.0.8]# redis-cli -p 6379[root@localhost redis-4.0.8]# redis-cli -p 6380[root@localhost redis-4.0.8]# redis-cli -p 6381 此时我就成功配置了三个 redis 实例了。 6.假设在这三个实例中，6379 是主机，即 master，6380 和 6381 是从机，即 slave，那么如何配置这种实例关系呢，很简单，分别在 6380 和 6381 上执行如下命令： 12127.0.0.1:6381&gt; SLAVEOF 127.0.0.1 6379OK 这一步也可以通过在两个从机的 redis.conf 中添加如下配置来解决： 1slaveof 127.0.0.1 6379 OK，主从关系搭建好后，我们可以通过如下命令可以查看每个实例当前的状态，如下: 1234567891011121314127.0.0.1:6379&gt; INFO replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=6380,state=online,offset=56,lag=1slave1:ip=127.0.0.1,port=6381,state=online,offset=56,lag=0master_replid:26ca818360d6510b717e471f3f0a6f5985b6225dmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:56second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:56 我们可以看到 6379 是一个主机，上面挂了两个从机，两个从机的地址、端口等信息都展现出来了。如果我们在 6380 上执行 INFO replication ，显示信息如下: 1234567891011121314151617181920127.0.0.1:6380&gt; INFO replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:6master_sync_in_progress:0slave_repl_offset:630slave_priority:100slave_read_only:1connected_slaves:0master_replid:26ca818360d6510b717e471f3f0a6f5985b6225dmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:630second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:630 我们可以看到 6380 是一个从机，从机的信息以及它的主机的信息都展示出来了。 7.此时，我们在主机中存储一条数据，在从机中就可以 get 到这条数据了。 主从复制注意点 如果主机已经运行了一段时间了，并且了已经存储了一些数据了，此时从机连上来，那么从机会将主机上所有的数据进行备份，而不是从连接的那个时间点开始备份。 配置了主从复制之后，主机上可读可写，但是从机只能读取不能写入（可以通过修改 redis.conf 中 slave-read-only 的值让从机也可以执行写操作）。 在整个主从结构运行过程中，如果主机不幸挂掉，重启之后，他依然是主机，主从复制操作也能够继续进行。 复制原理每一个 master 都有一个 replication ID ，这是一个较大的伪随机字符串，标记了一个给定的数据集。每个 master 也持有一个偏移量，master 将自己产生的复制流发送给 slave 时，发送多少个字节的数据，自身的偏移量就会增加多少，目的是当有新的操作修改自己的数据集时，它可以以此更新 slave 的状态。复制偏移量即使在没有一个 slave 连接到 master 时，也会自增，所以基本上每一对给定的 Replication ID, offset 都会标识一个 master 数据集的确切版本。当 slave 连接到 master 时，它们使用 PSYNC 命令来发送它们记录的旧的 master replication ID 和它们至今为止处理的偏移量。通过这种方式，master 能够仅发送 slave 所需的增量部分。但是如果 master 的缓冲区中没有足够的命令积压缓冲记录，或者如果 slave 引用了不再知道的历史记录 （replication ID） ，则会转而进行一个全量重同步：在这种情况下，slave 会得到一个完整的数据集副本，从头开始(参考 redis 官网)。 简单来说，就是以下几个步骤： slave 启动成功连接到 master 后会发送一个 sync 命令。 Master 接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令。 在后台进程执行完毕之后，master 将传送整个数据文件到 slave ,以完成一次完全同步。 全量复制：而 slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave ,完成同步。 但是只要是重新连接 master ,一次完全同步（全量复制)将被自动执行。 OK,redis 主从复制我们先介绍这么多，更多资料小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-master-slave-1.html"},{"title":"Jedis 使用","text":"Redis 的知识我们已经介绍的差不多了，本文我们来看看如何使用 Java 操作 redis。 本文是Redis系列的第十三篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一)11.Redis 主从复制(二)12.Redis 集群搭建 有哪些解决方案查看 redis 官网，我们发现用 Java 操作 redis，我们有多种解决方案，如下图： 这里的解决方案有多种，我们采用 Jedis，其他的框架也都大同小异，我这里权当抛砖引玉，小伙伴也可以研究研究其他的方案，欢迎投稿。 配置客户端要能够成功连接上 redis 服务器，需要检查如下三个配置： 1.远程 Linux 防火墙已经关闭，以我这里的 CentOS7 为例，关闭防火墙命令 systemctl stop firewalld.service ，同时还可以再补一刀 systemctl disable firewalld.service 表示禁止防火墙开机启动。 2.关闭 redis 保护模式，在 redis.conf 文件中，修改 protected 为 no，如下： 1protected-mode no 3.注释掉 redis 的 ip 地址绑定，还是在 redis.conf 中，将 bind:127.0.0.1 注释掉，如下： 1# bind:127.0.0.1 确认了这三步之后，就可以远程连接 redis 了。 Java 端配置上面的配置完成后，我们可以创建一个普通的 JavaSE 工程来测试下了，Java 工程创建成功后，添加 Jedis 依赖，如下： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 然后我们可以通过如下一个简单的程序测试一下连接是否成功： 12345public static void main(String[] args) { Jedis jedis = new Jedis(\"192.168.248.128\", 6379); String ping = jedis.ping(); System.out.println(ping);} 运行之后，看到如下结果表示连接成功了： 连接成功之后，剩下的事情就比较简单了，Jedis 类中方法名称和 redis 中的命令基本是一致的，看到方法名小伙伴就知道是干什么的，因此这些我这里不再重复叙述。 频繁的创建和销毁连接会影响性能，我们可以采用连接池来部分的解决这个问题： 12345678public static void main(String[] args) { GenericObjectPoolConfig config = new GenericObjectPoolConfig(); config.setMaxTotal(100); config.setMaxIdle(20); JedisPool jedisPool = new JedisPool(config, \"192.168.248.128\", 6379); Jedis jedis = jedisPool.getResource(); System.out.println(jedis.ping());} 这样就不会频繁创建和销毁连接了，在 JavaSE 环境中可以把连接池配置成一个单例模式，如果用了 Spring 容器的话，可以把连接池交给 Spring 容器管理。 上面这种连接都是连接单节点的 Redis，如果是一个 Redis 集群，要怎么连接呢？很简单，如下： 1234567891011Set&lt;HostAndPort&gt; clusterNodes = new HashSet&lt;HostAndPort&gt;();clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7001));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7002));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7003));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7004));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7005));clusterNodes.add(new HostAndPort(\"192.168.248.128\", 7006));JedisCluster jc = new JedisCluster(clusterNodes);jc.set(\"address\", \"深圳\");String address = jc.get(\"address\");System.out.println(address); JedisCluster 中的方法与 Redis 命令也是基本一致，我就不再重复介绍了。 好了，jedis 就说这么多，有问题欢迎留言讨论。","link":"/2019/0615/redis-jedis.html"},{"title":"Redis 主从复制(二)","text":"上篇文章和小伙伴们一起搭建了 redis 主从复制环境，但是还不完善，本文我想再和小伙伴们聊聊主从复制环境搭建的一些细节。 本文是 Redis 系列的第十一篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一) 本文接上文，所用三个 redis 实例和上文一致，这里就不再赘述三个实例搭建方式。 一场接力赛在上篇文章中，我们搭建的主从复制模式是下面这样的： 实际上，一主二仆的主从复制，我们可以搭建成下面这种结构： 搭建方式很简单，在前文基础上，我们只需要修改 6381 的 master 即可，在 6381 实例上执行如下命令，让 6381 从 6380 实例上复制数据，如下： 12127.0.0.1:6381&gt; SLAVEOF 127.0.0.1 6380OK 此时，我们再看 6379 的 slave ，如下： 12345678910111213127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=6380,state=online,offset=0,lag=1master_replid:4a38bbfa37586c29139b4ca1e04e8a9c88793651master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:0 只有一个 slave，就 6380 ，我们再看 6380 的信息，如下： 123456789101112131415161718192021127.0.0.1:6380&gt; info replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:1master_sync_in_progress:0slave_repl_offset:70slave_priority:100slave_read_only:1connected_slaves:1slave0:ip=127.0.0.1,port=6381,state=online,offset=70,lag=0master_replid:4a38bbfa37586c29139b4ca1e04e8a9c88793651master_replid2:0000000000000000000000000000000000000000master_repl_offset:70second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:70 6380 此时的角色是一个从机，它的主机是 6379，但是 6380 自己也有一个从机，那就是 6381 .此时我们的主从结构如下图： 哨兵模式结合上篇文章，我们一共介绍了两种主从模式了，但是这两种，不管是哪一种，都会存在这样一个问题，那就是当主机宕机时，就会发生群龙无首的情况，如果在主机宕机时，能够从从机中选出一个来充当主机，那么就不用我们每次去手动重启主机了，这就涉及到一个新的话题，那就是哨兵模式。 所谓的哨兵模式，其实并不复杂，我们还是在我们前面的基础上来搭建哨兵模式。假设现在我的 master 是 6379 ，两个从机分别是 6380 和 6381 ，两个从机都是从 6379 上复制数据。先按照上文的步骤，我们配置好一主二仆，然后在 redis 目录下打开 sentinel.conf 文件，做如下配置： 1sentinel monitor mymaster 127.0.0.1 6379 1 其中 mymaster 是给要监控的主机取的名字，随意取，后面是主机地址，最后面的 2 表示有多少个 sentinel 认为主机挂掉了，就进行切换（我这里只有一个，因此设置为1）。好了，配置完成后，输入如下命令启动哨兵： 1redis-sentinel sentinel.conf 然后启动我们的一主二仆架构，启动成功后，关闭 master，观察哨兵窗口输出的日志，如下： 小伙伴们可以看到，6379 挂掉之后，redis 内部重新举行了选举，6380 重新上位。此时，如果 6379 重启，也不再是扛把子了，只能屈身做一个 slave 了。 注意问题由于所有的写操作都是先在 Master 上操作，然后同步更新到 Slave 上，所以从 Master 同步到 Slave 机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave 机器数量的增加也会使这个问题更加严重。因此我们还需要集群来进一步提升 redis 性能，这个问题我们将在后面说到。 OK,redis 主从复制问题我们就介绍这么多，更多资料小伙伴们可以参考官方文档http://www.redis.net.cn/tutorial/3501.html。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-master-slave-2.html"},{"title":"Redis 之 AOF 持久化","text":"上篇文章和小伙伴们聊了使用快照的方式实现 redis 数据的持久化，这只是持久化的一种方式，本文我们就来看看另一种持久化方式， AOF(append-only file)。 本文是 Redis 系列的第九篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化 AOF 持久化与快照持久化不同，AOF 持久化是将被执行的命令写到 aof 文件末尾，在恢复时只需要从头到尾执行一遍写命令即可恢复数据，AOF 在 redis 中默认也是没有开启的，需要我们手动开启，开启方式如下： 打开 redis.conf 配置文件，修改 appendonly 属性值为 yes ，如下： 1appendonly yes 另外几个和 AOF 相关的属性如下： 1234567appendfilename &quot;appendonly.aof&quot;# appendfsync alwaysappendfsync everysec# appendfsync nono-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 这几个属性的含义分别如下： 1.appendfilename 表示生成的 AOF 备份文件的文件名。2.appendfsync 表示备份的时机，always 表示每执行一个命令就备份一次，everysec 表示每秒备份一次，no 表示将备份时机交给操作系统。3.no-appendfsync-on-rewrite 表示在对 aof 文件进行压缩时，是否执行同步操作。4.最后两行配置表示 AOF 文件的压缩时机，这个我们一会再细说。 同时为了避免快照备份的影响，我们将快照备份关闭，关闭方式如下： 1234save &quot;&quot;# save 900 1# save 300 10# save 60 10000 此时，当我们在 redis 中进行数据操作时，就会自动生成 AOF 的配置文件 appendonly.aof ，如下： 注意此时没有 dump.rdb 文件，这时我们将 redis 关闭并重启，会发现之前的数据都还在，这就是 AOF 备份的结果。 AOF 备份的几个关键点1.通过上面的介绍，小伙伴们了解到 appendfsync 的取值一共有三种，我们在项目中首选 everysec，always 选项会严重降低 redis 性能。2.使用 everysec ，最坏的情况下我们可能丢失1秒的数据。 AOF 文件的重写与压缩AOF 备份有很多明显的优势，当然也有劣势，那就是文件大小。随着系统的运行，AOF 的文件会越来越大，甚至把整个电脑的硬盘填满，AOF 文件的重写与压缩机制可以在一定程度上缓解这个问题。当 AOF 的备份文件过大时，我们可以向 redis 发送一条 bgrewriteaof 命令进行文件重写，如下： 123127.0.0.1:6379&gt; BGREWRITEAOFBackground append only file rewriting started(0.71s) bgrewriteaof 的执行原理和我们上文说的 bgsave 的原理一致，这里我就不再赘述，因此 bgsave 执行过程中存在的问题在这里也一样存在。 bgrewriteaof 也可以自动执行，自动执行时间则依赖于 auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size 配置，auto-aof-rewrite-percentage 100 表示当目前 aof 文件大小超过上一次重写时的 aof 文件大小的百分之多少时会再次进行重写，如果之前没有重写，则以启动时的 aof 文件大小为依据，同时还要求 AOF 文件的大小至少要大于 64M(auto-aof-rewrite-min-size 64mb)。 最佳实践 如果 redis 只做缓存服务器，那么可以不使用任何持久化方式。 同时开启两种持久化方式，在这种情况下,当 redis 重启的时候会优先载入 AOF 文件来恢复原始的数据, 因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整；RDB 的数据不完整时，同时使用两者时服务器重启也只会找 AOF 文件。那要不要只使用 AOF 呢？ 作者建议不要，因为 RDB 更适合用于备份数据库( AOF 在不断变化不好备份)， 快速重启，而且不会有 AOF 可能潜在的 bug ，留着作为一个万一的手段。 因为 RDB 文件只用作后备用途，建议只在 slave 上持久化 RDB 文件，而且只要 15 分钟备份一次就够了，只保留 save 900 1 这条规则。 如果 Enalbe AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只 load 自己的 AOF 文件就可以了。代价一是带来了持续的 IO，二是 AOF rewrite 的最后将 rewrite 过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少 AOF rewrite 的频率，AOF 重写的基础大小默认值 64M 太小了，可以设到 5G 以上。默认超过原大小 100% 大小时重写可以改到适当的数值。 如果不 Enable AOF ，仅靠 Master-Slave Replication 实现高可用性也可以。能省掉一大笔 IO 也减少了 rewrite 时带来的系统波动。代价是如果 Master/Slave 同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个 Master/Slave 中的 RDB 文件，载入较新的那个。 OK,redis 数据持久化我们就介绍这么多，更多资料，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-aof.html"},{"title":"Nginx 极简入门教程！","text":"上篇文章和大家聊了 Spring Session 实现 Session 共享的问题，有的小伙伴看了后表示对 Nginx 还是很懵，因此有了这篇文章，算是一个 Nginx 扫盲入门吧！ 基本介绍Nginx 是一个高性能的 HTTP 和反向代理 web 服务器，同时也提供了 IMAP/POP3/SMTP 服务。 Nginx 是由伊戈尔·赛索耶夫为俄罗斯访问量第二的 Rambler.ru 站点开发的，第一个公开版本 0.1.0 发布于 2004 年 10 月 4 日。 Nginx 特点是占有内存少，并发能力强。 事实上 nginx 的并发能力确实在同类型的网页服务器中表现较好，一般来说，如果我们在项目中引入了 Nginx ，我们的项目架构可能是这样： 在这样的架构中 ， Nginx 所代表的角色叫做负载均衡服务器或者反向代理服务器，所有请求首先到达 Nginx 上，再由 Nginx 根据提前配置好的转发规则，将客户端发来的请求转发到某一个 Tomcat 上去。 那么这里涉及到两个概念： 负载均衡服务器 就是进行请求转发，降低某一个服务器的压力。负载均衡策略很多，也有很多层，对于一些大型网站基本上从 DNS 就开始负载均衡，负载均衡有硬件和软件之分，各自代表分别是 F5 和 Nginx （目前 Nginx 已经被 F5 收购），早些年，也可以使用 Apache 来做负载均衡，但是效率不如 Nginx ，所以现在主流方案是 Nginx 。 反向代理服务器： 另一个概念是反向代理服务器，得先说正向代理，看下面一张图： 在这个过程中，Google 并不知道真正访问它的客户端是谁，它只知道这个中间服务器在访问它。因此，这里的代理，实际上是中间服务器代理了客户端，这种代理叫做正向代理。 那么什么是反向代理呢？看下面一张图： 在这个过程中，10086 这个号码相当于是一个代理，真正提供服务的，是话务员，但是对于客户来说，他不关心到底是哪一个话务员提供的服务，他只需要记得 10086 这个号码就行了。 所有的请求打到 10086 上，再由 10086 将请求转发给某一个话务员去处理。因此，在这里，10086 就相当于是一个代理，只不过它代理的是话务员而不是客户端，这种代理称之为反向代理。 Nginx 的优势在 Java 开发中，Nginx 有着非常广泛的使用，随便举几点： 使用 Nginx 做静态资源服务器：Java 中的资源可以分为动态和静态，动态需要经过 Tomcat 解析之后，才能返回给浏览器，例如 JSP 页面、Freemarker 页面、控制器返回的 JSON 数据等，都算作动态资源，动态资源经过了 Tomcat 处理，速度必然降低。对于静态资源，例如图片、HTML、JS、CSS 等资源，这种资源可以不必经过 Tomcat 解析，当客户端请求这些资源时，之间将资源返回给客户端就行了。此时，可以使用 Nginx 搭建静态资源服务器，将静态资源直接返回给客户端。 使用 Nginx 做负载均衡服务器，无论是使用 Dubbo 还是 Spirng Cloud ，除了使用各自自带的负载均衡策略之外，也都可以使用 Nginx 做负载均衡服务器。 支持高并发、内存消耗少、成本低廉、配置简单、运行稳定等。 Nginx 安装：由于基本上都是在 Linux 上使用 Nginx，因此松哥这里主要向大家展示 CentOS 7 安装 Nginx： 首先下载 Nginx 1wget http://nginx.org/download/nginx-1.17.0.tar.gz 然后解压下载的目录，进入解压目录中，在编译安装之前，需要安装两个依赖： 12yum -y install pcre-develyum -y install openssl openssl-devel 然后开始编译安装： 123./configuremakemake install 装好之后，默认安装位置在 ： 1/usr/local/nginx/sbin/nginx 进入到该目录的 sbin 目录下，执行 nginx 即可启动 Nginx ： Nginx 启动成功之后，在浏览器中直接访问 Nginx 地址： 看到如上页面，表示 Nginx 已经安装成功了。 如果修改了 Nginx 配置，则可以通过如下命令重新加载 Nginx 配置文件： 1./nginx -s reload 总结本文算是一个简单的 Nginx 扫盲文，希望大家看完后对 Nginx 有一个基本的认知。本文先说到这里，有问题欢迎留言讨论。","link":"/2019/0605/nginx-guide.html"},{"title":"Redis 列表与集合","text":"前面文章我们介绍了 STRING 的基本命令，本文我们来看看 Redis 中的列表与集合。 本文是 Redis 系列的第五篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令 列表列表是 Redis 中另外一种数据类型。下面我们来看看列表中一些基本的操作命令。 LPUSH将一个或多个值 value 插入到列表 key 的表头，如果有多个 value 值，那么各个 value 值按从左到右的顺序依次插入到表头，如下： 12127.0.0.1:6379&gt; LPUSH k1 v1 v2 v3(integer) 3 LRANGE返回列表 key 中指定区间内的元素，区间以偏移量 start 和 stop 指定，下标 (index) 参数 start 和 stop 都以 0 为底，即 0 表示列表的第一个元素，1 表示列表的第二个元素，以此类推。我们也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。如下： 1234127.0.0.1:6379&gt; LRANGE k1 0 -11) &quot;v3&quot;2) &quot;v2&quot;3) &quot;v1&quot; RPUSHRPUSH 与 LPUSH 的功能基本一致，不同的是 RPUSH 的中的 value 值是按照从右到左的顺序依次插入，如下： 12345678127.0.0.1:6379&gt; RPUSH k2 1 2 3 4 5(integer) 5127.0.0.1:6379&gt; LRANGE k2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;5) &quot;5&quot; RPOPRPOP 命令可以移除并返回列表 key 的尾元素。如下： 1234567127.0.0.1:6379&gt; RPOP k2&quot;5&quot;127.0.0.1:6379&gt; LRANGE k2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot; LPOPLPOP 和 RPOP 类似，不同的是 LPOP 移除并返回列表 key 的头元素，如下： 123456127.0.0.1:6379&gt; LPOP k2&quot;1&quot;127.0.0.1:6379&gt; LRANGE k2 0 -11) &quot;2&quot;2) &quot;3&quot;3) &quot;4&quot; LINDEXLINDEX 命令可以返回列表 key 中，下标为 index 的元素，正数下标 0 表示第一个元素，也可以使用负数下标，-1 表示倒数第一个元素，如下： 1234127.0.0.1:6379&gt; LINDEX k2 0&quot;2&quot;127.0.0.1:6379&gt; LINDEX k2 -1&quot;4&quot; LTRIMLTRIM 命令可以对一个列表进行修剪，即让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。下标与之前介绍的写法都一致，这里不赘述。如下： 123456789127.0.0.1:6379&gt; LRANGE k1 0 -11) &quot;v3&quot;2) &quot;v2&quot;3) &quot;v1&quot;127.0.0.1:6379&gt; LTRIM k1 0 1OK127.0.0.1:6379&gt; LRANGE k1 0 -11) &quot;v3&quot;2) &quot;v2&quot; BLPOPBLPOP 是阻塞式列表的弹出原语。它是命令 LPOP 的阻塞版本，当给定列表内没有任何元素可供弹出的时候，连接将被 BLPOP 命令阻塞。当给定多个 key 参数时，按参数 key 的先后顺序依次检查各个列表，弹出第一个非空列表的头元素。同时，在使用该命令时也需要指定阻塞的时长，时长单位为秒，在该时长内如果没有元素可供弹出，则阻塞结束。返回的结果是 key 和 value 的组合，如下： 123456127.0.0.1:6379&gt; BLPOP k1 101) &quot;k1&quot;2) &quot;v2&quot;127.0.0.1:6379&gt; BLPOP k1 10(nil)(10.03s) 最后，BRPOP、BPOPLPUSH、BRPOPLPUSH 都是相应命令的阻塞版本，这里就不赘述了。 集合接下来我们来看看集合中一些常见的操作命令： SADDSADD 命令可以添加一个或多个指定的 member 元素到集合的 key 中，指定的一个或者多个元素 member 如果已经在集合 key 中存在则忽略，如果集合 key 不存在，则新建集合 key ,并添加 member 元素到集合 key 中。如下： 12127.0.0.1:6379&gt; SADD k1 v1 v2 v3 v4(integer) 4 SREMSREM 命令可以在 key 集合中移除指定的元素，如果指定的元素不是 key 集合中的元素则忽略。如果 key 集合不存在则被视为一个空的集合，该命令返回 0 。如下： 1234127.0.0.1:6379&gt; SREM k1 v2(integer) 1127.0.0.1:6379&gt; SREM k1 v10(integer) 0 SISMEMBERSISMEMBER 命令可以返回成员 member 是否是存储的集合 key 的成员。如下： 12127.0.0.1:6379&gt; SISMEMBER k1 v3(integer) 1 SCARDSCARD 命令可以返回集合存储的 key 的基数(集合元素的数量)，如下： 12127.0.0.1:6379&gt; SCARD k1(integer) 3 SMEMBERSSMEMBERS 命令可以返回 key 集合所有的元素，如下： 1234127.0.0.1:6379&gt; SMEMBERS k11) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot; SRANDMEMBERSRANDMEMBER 仅需我们提供 key 参数,它就会随机返回 key 集合中的一个元素，从 Redis2.6 开始,该命令也可以接受一个可选的 count 参数,如果 count 是整数且小于元素的个数，则返回 count 个随机元素,如果 count 是整数且大于集合中元素的个数时,则返回集合中的所有元素,当 count 是负数,则会返回一个包含 count 的绝对值的个数元素的数组，如果 count 的绝对值大于元素的个数,则返回的结果集里会出现一个元素出现多次的情况。如下： 1234567891011121314151617127.0.0.1:6379&gt; SRANDMEMBER k1&quot;v4&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 21) &quot;v4&quot;2) &quot;v1&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 51) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 -11) &quot;v4&quot;127.0.0.1:6379&gt; SRANDMEMBER k1 -51) &quot;v3&quot;2) &quot;v1&quot;3) &quot;v1&quot;4) &quot;v3&quot;5) &quot;v3&quot; SPOPSPOP 命令的用法和 SRANDMEMBER 类似，不同的是，SPOP 每次选择一个随机的元素之后，该元素会出栈，而 SRANDMEMBER 则不会出栈，只是将该元素展示出来。 SMOVESMOVE 命令可以将 member 从 source 集合移动到 destination 集合中，如下： 1234567127.0.0.1:6379&gt; SMOVE k1 k2 v1(integer) 1127.0.0.1:6379&gt; SMEMBERS k11) &quot;v4&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; SMEMBERS k21) &quot;v1&quot; SDIFFSDIFF 可以用来返回一个集合与给定集合的差集的元素，如下： 123127.0.0.1:6379&gt; SDIFF k1 k21) &quot;v4&quot;2) &quot;v3&quot; k1 中的元素是 v3、v4，k2 中的元素是 v1，差集就是 v3、v4. SDIFFSTORESDIFFSTORE 命令与 SDIFF 命令基本一致，不同的是 SDIFFSTORE 命令会将结果保存在一个集合中，如下： 12345127.0.0.1:6379&gt; SDIFFSTORE key k1 k2(integer) 2127.0.0.1:6379&gt; SMEMBERS key1) &quot;v4&quot;2) &quot;v3&quot; SINTERSINTER 命令可以用来计算指定 key 之间元素的交集，如下： 12345678127.0.0.1:6379&gt; SMEMBERS k11) &quot;v4&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; SMEMBERS k21) &quot;v1&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; SINTER k1 k21) &quot;v3&quot; SINTERSTORESINTERSTORE 命令和 SINTER 命令类似，不同的是它会将结果保存到一个新的集合中，如下： 1234127.0.0.1:6379&gt; SINTERSTORE k3 k1 k2(integer) 1127.0.0.1:6379&gt; SMEMBERS k31) &quot;v3&quot; SUNIONSUNION 可以用来计算两个集合的并集，如下： 1234127.0.0.1:6379&gt; SUNION k1 k21) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot; SUNIONSTORESUNIONSTORE 和 SUNION 命令类似，不同的是它会将结果保存到一个新的集合中，如下： 123456127.0.0.1:6379&gt; SUNIONSTORE k4 k1 k2(integer) 3127.0.0.1:6379&gt; SMEMBERS k41) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot; OK,列表和集合的命令我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-list-set.html"},{"title":"Redis 中的发布订阅和事务","text":"hello，小伙伴们好久不见！前面我们说了 redis 中的基本数据类型，本文我们来看看 redis 中的发布订阅和事务，因为这两个都比较简单，因此我放在一篇文章中来讲。 本文是 Redis 系列的第七篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中BIT相关命令5.Redis 列表与集合6.Redis 散列与有序集合 发布订阅redis 的发布订阅系统有点类似于我们生活中的电台，电台可以在某一个频率上发送广播，而我们可以接收任何一个频率的广播，Android 中的 broadcast 也和这类似。 订阅消息的方式如下: 1234567891011127.0.0.1:6379&gt; SUBSCRIBE c1 c2 c3Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;c1&quot;3) (integer) 11) &quot;subscribe&quot;2) &quot;c2&quot;3) (integer) 21) &quot;subscribe&quot;2) &quot;c3&quot;3) (integer) 3 这个表示接收 c1，c2，c3 三个频道传来的消息，发送消息的方式如下： 12127.0.0.1:6379&gt; PUBLISH c1 &quot;hello redis!&quot;(integer) 1 当 c1 这个频道上有消息发出时，此时在消息订阅控制台可以看到如下输出： 1231) &quot;message&quot;2) &quot;c1&quot;3) &quot;hello redis!&quot; 在 redis 中，我们也可以使用模式匹配订阅，如下： 12345127.0.0.1:6379&gt; PSUBSCRIBE c*Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot;2) &quot;c*&quot;3) (integer) 1 此时可以接收到所有以 c 开头的频道发来的消息。 tipsredis 中的发布订阅系统在某些场景下还是非常好用的，但是也有一些问题需要注意：由于网络在传输过程中可能会遭遇断线等意外情况，断线后需要进行重连，然而这会导致断线期间的数据丢失。 事务既然 redis 是一种 NoSQL 数据库，那它当然也有事务的功能，不过这里的事务和我们关系型数据库中的事务有一点点差异。 redis 中事务的用法非常简单，我们通过 MULTI 命令开启一个事务，如下： 12127.0.0.1:6379&gt; MULTIOK 在 MULTI 命令执行之后，我们可以继续发送命令去执行，此时的命令不会被立马执行，而是放在一个队列中，如下： 123456127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3QUEUED 当所有的命令都输入完成后，我们可以通过 EXEC 命令发起执行，也可以通过 DISCARD 命令清空队列，如下： 1234127.0.0.1:6379&gt; EXEC1) OK2) OK3) OK 事务中的异常情况redis 中事务的异常情况总的来说分为两类： 进入队列之前就能发现的错误，比如命令输错； 执行 EXEC 之后才能发现的错误，比如给一个非数字字符加 1 ； 那么对于这两种不同的异常，redis 中有不同的处理策略。对于第一种错误，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务（这个是2.6.5之后的版本做法，之前的版本做法小伙伴可以参考官方文档）。如下： 12345678910111213141516171819127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set kv1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3 3 3QUEUED127.0.0.1:6379&gt; set k4 v4QUEUED127.0.0.1:6379&gt; EXEC1) OK2) OK3) (error) ERR syntax error4) OK127.0.0.1:6379&gt; keys *1) &quot;k4&quot;2) &quot;k2&quot;3) &quot;kv1&quot; 而对于第二种情况，redis 并没有对它们进行特别处理， 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。如下： 1234567891011127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set k1 vvQUEUED127.0.0.1:6379&gt; INCR k1QUEUED127.0.0.1:6379&gt; EXEC1) OK2) (error) ERR value is not an integer or out of range127.0.0.1:6379&gt; GET k1&quot;vv&quot; 不同于关系型数据库，redis 中的事务出错时没有回滚，对此，官方的解释如下： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 WATCH 命令事务中的 WATCH 命令可以用来监控一个 key，通过这种监控，我们可以为 redis 事务提供(CAS)行为。 如果有至少一个被 WATCH 监视的键在 EXEC 执行之前被修改了，那么整个事务都会被取消，EXEC 返回 nil-reply 来表示事务已经失败。如下： 通过 unwatch 命令，可以取消对一个 key 的监控，如下： OK,发布订阅和事务我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-pub-sub.html"},{"title":"Redis 字符串 STRING 中 BIT 相关命令","text":"上篇文章我们对 STRING 数据类型中一些基本的命令进行了介绍，但是没有涉及到 BIT 相关的命令，本文我们就来看看几个和 BIT 相关的命令。 本文是 Redis 系列的第四篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍 BIT 相关的命令是指 BITCOUNT/BITFIELD/BITOP/BITPOS/SETBIT/GETBIT 几个命令，灵活使用这几个命令，可以给我们的项目带来很多惊喜。 准备知识在学习这几个命令之前，我们得先了解下 redis 中字符串的存储方式，redis 中的字符串都是以二进制的方式进行存储的，比如说我执行如下命令： 12127.0.0.1:6379&gt; SET k1 aOK a 对应的 ASCII 码是 97 ，转换为二进制数据是 01100001 ，我们 BIT 相关命令都是对这个二进制数据进行操作。请继续往下看。 GETBITGETBIT 命令可以返回 key 对应的 value 在 offset 处的 bit 值，以上文提到的 k1 为例， a 对应的二进制数据是 01100001 ，所以当 offset 为 0 时，对应的 bit 值为 0 ； offset 为 1 时，对应的 bit 值为 1 ； offset 为 2 时，对应的 bit 值为 1 ；offset 为 3 时，对应的 bit 值为 0，依此类推….，如下： 12345678910111213141516127.0.0.1:6379&gt; GETBIT k1 0(integer) 0127.0.0.1:6379&gt; GETBIT k1 1(integer) 1127.0.0.1:6379&gt; GETBIT k1 2(integer) 1127.0.0.1:6379&gt; GETBIT k1 3(integer) 0127.0.0.1:6379&gt; GETBIT k1 4(integer) 0127.0.0.1:6379&gt; GETBIT k1 5(integer) 0127.0.0.1:6379&gt; GETBIT k1 6(integer) 0127.0.0.1:6379&gt; GETBIT k1 7(integer) 1 SETBITSETBIT 可以用来修改二进制数据，比如 a 对应的 ASCII 码为 97，c 对应的 ASCII 码为 99，97 转为二进制是 01100001 ，99 转为二进制是 01100011 ，两个的差异在于第六位一个是 0 一个是 1 ，通过 SETBIT 命令，我们可以将 k1 的第六位的 0 改为 1 （第六位是从 0 开始算），如下： 1234127.0.0.1:6379&gt; SETBIT k1 6 1(integer) 0127.0.0.1:6379&gt; GET k1&quot;c&quot; 此时，k1 中存储的字符也就变为了 c。SETBIT 在执行时所返回的数字，表示该位上原本的 bit 值。 BITCOUNTBITCOUNT 可以用来统计这个二进制数据中 1 的个数，如下： 12127.0.0.1:6379&gt; BITCOUNT k1(integer) 4 关于 BITCOUNT，redis 官网上有一个非常有意思的案例：用户上线次数统计。节选部分原文如下： 举个例子，如果今天是网站上线的第 100 天，而用户 peter 在今天阅览过网站，那么执行命令 SETBIT peter 100 1 ；如果明天 peter 也继续阅览网站，那么执行命令 SETBIT peter 101 1 ，以此类推。当要计算 peter 总共以来的上线次数时，就使用 BITCOUNT 命令：执行 BITCOUNT peter ，得出的结果就是 peter 上线的总天数。 这种统计方式最大的好处就是节省空间并且运算速度快。每天占用一个 bit，一年也就 365 个 bit，10 年也就 10*365 个 bit ，也就是 456 个字节，对于这么大的数据，bit 的操作速度非常快。 BITOPBITOP 可以对一个或者多个二进制位串执行并 (AND)、或 (OR)、异或 (XOR) 以及非 (NOT) 运算，如下：a 对应的 ASCII 码转为二进制是 01100001 ，c 对应的二进制位串是 01100011 。对这两个二进制位串分别执行 AND\\OR\\XOR 的结果如下： 12345678910111213141516127.0.0.1:6379&gt; set k1 aOK127.0.0.1:6379&gt; set k2 cOK127.0.0.1:6379&gt; BITOP and k3 k1 k2(integer) 1127.0.0.1:6379&gt; get k3&quot;a&quot;127.0.0.1:6379&gt; BITOP or k3 k1 k2(integer) 1127.0.0.1:6379&gt; get k3&quot;c&quot;127.0.0.1:6379&gt; BITOP xor k3 k1 k2(integer) 1127.0.0.1:6379&gt; get k3&quot;\\x02&quot; 另外，BITOP 也可以执行 NOT 运算，但是注意参数个数，如下： 12127.0.0.1:6379&gt; BITOP not k3 k4(integer) 1 这里会对 k4 的二进制位串取反，将取反结果交给 k3 。 BITPOSBITPOS 用来获取二进制位串中第一个 1 或者 0 的位置，如下： 123456127.0.0.1:6379&gt; set k1 aOK127.0.0.1:6379&gt; BITPOS k1 1(integer) 1127.0.0.1:6379&gt; BITPOS k1 0(integer) 0 也可以在后面设置一个范围，不过后面的范围是字节的范围，而不是二进制位串的范围。 OK,STRING 中 BIT 相关的命令我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-string-bit.html"},{"title":"Redis 快照持久化","text":"redis 的基础知识我们已经准备的差不多了，接下来两篇文章，我想和大家聊聊 redis 持久化这个话题。 本文是 Redis 系列的第八篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务 redis 持久化整体上来说，redis 持久化有两种方式，快照持久化和 AOF ，在项目中我们可以根据实际情况选择合适的持久化方式，也可以不用持久化，这关键看我们的 redis 在项目中扮演了什么样的角色。那么我将分别用两篇文章来介绍这两种不同的持久化方式，本文我们先来看看第一种方式。 快照持久化快照持久化，顾名思义，就是通过拍摄快照的方式实现数据的持久化，redis 可以在某个时间点上对内存中的数据创建一个副本文件，副本文件中的数据在 redis 重启时会被自动加载，我们也可以将副本文件拷贝到其他地方一样可以使用。 如何配置快照持久化redis中的快照持久化默认是开启的，redis.conf中相关配置主要有如下几项： 1234567save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesdbfilename dump.rdbdir ./ 前面三个 save 相关的选项表示备份的频率，分别表示 900 秒内至少一个键被更改则进行快照，300 秒内至少 10 个键被更改则进行快照，60 秒内至少 10000 个键被更改则进行快照， stop-writes-on-bgsave-error 表示在快照创建出错后，是否继续执行写命令， rdbcompression 则表示是否对快照文件进行压缩， dbfilename 表示生成的快照文件的名字，dir 则表示生成的快照文件的位置，在 redis 中，快照持久化默认就是开启的。我们可以通过如下步骤验证快照持久化的效果： 1.进入 redis 安装目录，如果有 dump.rdb 文件，先将之删除。如下： 2.启动 redis ，随便向 redis 中存储几个数据，然后关闭redis并退出，如下： 12345678[root@localhost redis-4.0.8]# redis-server redis.conf[root@localhost redis-4.0.8]# redis-cli127.0.0.1:6379&gt; set k1 v1OK127.0.0.1:6379&gt; set k2 v2OK127.0.0.1:6379&gt; SHUTDOWNnot connected&gt; exit 3.退出来后，我们发现刚刚删掉的 dump.rdb 文件又回来了，这就是生成的备份文件。4.此时再次启动 redis 并进入，发现刚刚存储的数据都还在，这是因为 redis 在启动时加载了 dump.rdb 中的数据。好了，关闭 redis 并退出。5.将 redis 目录下的 dump.rdb 文件删除。6.再次启动 redis 并进入到控制台，所有的数据都不存在了。 快照持久化操作流程通过上面的介绍，小伙伴们对快照持久化都有一个大致的认识了，那么这个东西到底是怎么运行的？持久化的时机是什么？我们来仔细扒一扒。 1.在 redis 运行过程中，我们可以向 redis 发送一条 save 命令来创建一个快照，save 是一个阻塞命令，redis 在接收到 save 命令之后，开始执行备份操作之后，在备份操作执行完毕之前，将不再处理其他请求，其他请求将被挂起，因此这个命令我们用的不多。save 命令执行如下： 12127.0.0.1:6379&gt; SAVEOK 2.在 redis 运行过程中，我们也可以发送一条 bgsave 命令来创建一个快照，不同于 save 命令，bgsave 命令会 fork 一个子进程，然后这个子进程负责执行将快照写入硬盘，而父进程则继续处理客户端发来的请求，这样就不会导致客户端命令阻塞了。如下： 12127.0.0.1:6379&gt; BGSAVEBackground saving started 3.如果我们在 redis.conf 中配置了如下选项： 123save 900 1save 300 10save 60 10000 那么当条件满足时，比如 900 秒内有一个 key 被操作了，那么 redis 就会自动触发 bgsava 命令进行备份。我们可以根据实际需求在 redis.conf 中配置多个这种触发规则。 4.还有一种情况也会触发 save 命令，那就是我们执行 shutdown 命令时，当我们用 shutdown 命令关闭 redis 时，此时也会执行一个 save 命令进行备份操作，并在备份操作完成后将服务器关闭。 5.还有一种特殊情况也会触发 bgsave 命令，就是在主从备份的时候。当从机连接上主机后，会发送一条 sync 命令来开始一次复制操作，此时主机会开始一次 bgsave 操作，并在 bgsave 操作结束后向从机发送快照数据实现数据同步。 快照持久化的缺点快照持久化有一些缺点，比如 save 命令会发生阻塞，bgsave 虽然不会发生阻塞，但是 fork 一个子进程又要耗费资源，在一些极端情况下，fork 子进程的时间甚至超过数据备份的时间。定期的持久化也会让我们存在数据丢失的风险，最坏的情况我们可能丢失掉最近一次备份到当下的数据，具体丢失多久的数据，要看我们项目的承受能力，我们可以根据项目的承受能力配饰 save 参数。 OK,快照持久化我们就介绍这么多，更多资料，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-rdb.html"},{"title":"Redis 散列与有序集合","text":"前面文章我们介绍了列表与集合中的基本命令，本文我们来看看Redis中的散列与有序集合。 本文是 Redis 系列的第六篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合 散列很多时候，散列就像一个微缩版的 redis ，在本文中，小伙伴们对看到的许多散列命令都会有似曾相识的感觉。 HSETHSET 命令可以用来设置 key 指定的哈希集中指定字段的值，如下： 12127.0.0.1:6379&gt; HSET k1 h1 v1(integer) 1 HGETHGET 命令可以用来返回 key 指定的哈希集中该字段所关联的值，如下： 12127.0.0.1:6379&gt; HGET k1 h1&quot;v1&quot; HMSETHMSET 命令可以批量设置 key 指定的哈希集中指定字段的值，如下： 12127.0.0.1:6379&gt; HMSET k2 h1 v1 h2 v2 h3 v3OK HMGETHMGET 可以批量返回 key 指定的哈希集中指定字段的值，如下： 1234127.0.0.1:6379&gt; HMGET k2 h1 h2 h31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot; HDELHDEL 命令可以从 key 指定的哈希集中移除指定的域，在哈希集中不存在的域将被忽略，如下： 12345678910127.0.0.1:6379&gt; HMGET k2 h1 h2 h31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; HDEL k2 h1(integer) 1127.0.0.1:6379&gt; HMGET k2 h1 h2 h31) (nil)2) &quot;v2&quot;3) &quot;v3&quot; HSETNXHSETNX 命令只在 key 指定的哈希集中不存在指定的字段时，设置字段的值，如果字段已存在，该操作无效果。如下： 1234127.0.0.1:6379&gt; HSETNX k2 h3 1(integer) 0127.0.0.1:6379&gt; HSETNX k2 h4 1(integer) 1 HVALSHVALS 命令可以返回 key 指定的哈希集中所有字段的值，如下： 1234127.0.0.1:6379&gt; HVALS k21) &quot;v2&quot;2) &quot;v3&quot;3) &quot;1&quot; HKEYSHKEYS 命令可以返回 key 指定的哈希集中所有字段的名字，如下： 1234127.0.0.1:6379&gt; HKEYS k21) &quot;h2&quot;2) &quot;h3&quot;3) &quot;h4&quot; HGETALLHGETALL 命令可以返回 key 指定的哈希集中所有的字段和值。返回值中，每个字段名的下一个是它的值，所以返回值的长度是哈希集大小的两倍，如下： 1234567127.0.0.1:6379&gt; HGETALL k21) &quot;h2&quot;2) &quot;v2&quot;3) &quot;h3&quot;4) &quot;v3&quot;5) &quot;h4&quot;6) &quot;1&quot; HEXISTSHEXISTS 命令可以返回 hash 里面 field 是否存在，如下： 12127.0.0.1:6379&gt; HEXISTS k2 h3(integer) 1 HINCRBYHINCRBY 可以增加 key 指定的哈希集中指定字段的数值。如果 key 不存在，会创建一个新的哈希集并与 key 关联。如果字段不存在，则字段的值在该操作执行前被设置为 0， HINCRBY 支持的值的范围限定在 64 位有符号整数，如下： 123456789101112131415127.0.0.1:6379&gt; HEXISTS k2 h3(integer) 1127.0.0.1:6379&gt;127.0.0.1:6379&gt; HGET k2 h4&quot;1&quot;127.0.0.1:6379&gt; HINCRBY k2 h4 5(integer) 6127.0.0.1:6379&gt; HGET k2 h4&quot;6&quot;127.0.0.1:6379&gt; HGET k2 h5(nil)127.0.0.1:6379&gt; HINCRBY k2 h5 99(integer) 99127.0.0.1:6379&gt; HGET k2 h5&quot;99&quot; HINCRBYFLOATHINCRBYFLOAT 与 HINCRBY 用法基本一致，只不过这里允许 float 类型的数据，不赘述。 HLENHLEN 返回 key 指定的哈希集包含的字段的数量，如下： 12127.0.0.1:6379&gt; HLEN k2(integer) 4 HSTRLENHSTRLEN 可以返回 hash 指定 field 的 value 的字符串长度，如果 hash 或者 field 不存在，返回 0 ，如下： 12127.0.0.1:6379&gt; HSTRLEN k2 h2(integer) 2 有序集合有序集合类似 Sets ,但是每个字符串元素都关联到一个叫 score 浮动数值。里面的元素总是通过 score 进行着排序，因此它是可以检索的一系列元素。 ZADDZADD 命令可以将所有指定成员添加到键为 key 的有序集合里面。添加时可以指定多个分数/成员（score/member）对。 如果指定添加的成员已经是有序集合里面的成员，则会更新该成员的分数（scrore）并更新到正确的排序位置。如下： 12127.0.0.1:6379&gt; ZADD k1 60 v1(integer) 1 ZSCOREZSCORE 命令可以返回有序集 key 中，成员 member 的score 值。如下： 12127.0.0.1:6379&gt; ZSCORE k1 v1&quot;60&quot; ZRANGEZRANGE 命令可以根据 index 返回 member ，该命令在执行时加上 withscores 参数可以连同 score 一起返回： 1234567891011121314127.0.0.1:6379&gt; ZRANGE k1 0 31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;4) &quot;v4&quot;127.0.0.1:6379&gt; ZRANGE k1 0 3 withscores1) &quot;v1&quot;2) &quot;60&quot;3) &quot;v2&quot;4) &quot;70&quot;5) &quot;v3&quot;6) &quot;80&quot;7) &quot;v4&quot;8) &quot;90&quot; ZREVRANGEZREVRANGE 和 ZRANGE 功能基本一致，不同的是 ZREVRANGE 是反着来的，如下： 1234567891011121314127.0.0.1:6379&gt; ZREVRANGE k1 0 31) &quot;v5&quot;2) &quot;v4&quot;3) &quot;v3&quot;4) &quot;v2&quot;127.0.0.1:6379&gt; ZREVRANGE k1 0 3 withscores1) &quot;v5&quot;2) &quot;100&quot;3) &quot;v4&quot;4) &quot;90&quot;5) &quot;v3&quot;6) &quot;80&quot;7) &quot;v2&quot;8) &quot;70&quot; ZCARDZCARD 命令可以返回 key 的有序集元素个数。如下： 12127.0.0.1:6379&gt; ZCARD k1(integer) 5 ZCOUNTZCOUNT 命令可以返回有序集 key 中，score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员。如下： 12127.0.0.1:6379&gt; ZCOUNT k1 60 90(integer) 4 如果在统计时，不需要包含 60 或者 90 ，则添加一个 ( 即可，如下： 12127.0.0.1:6379&gt; ZCOUNT k1 60 (90(integer) 3 ZRANGEBYSCOREZRANGEBYSCORE 命令可以按照 score 范围范围元素，加上 withscores 可以连 score 一起返回。如下： 12345678910111213141516127.0.0.1:6379&gt; ZRANGEBYSCORE k1 60 801) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE k1 60 80 withscores1) &quot;v1&quot;2) &quot;60&quot;3) &quot;v2&quot;4) &quot;70&quot;5) &quot;v3&quot;6) &quot;80&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE k1 (60 80 withscores1) &quot;v2&quot;2) &quot;70&quot;3) &quot;v3&quot;4) &quot;80&quot; ZRANKZRANK 命令可以返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。排名以 0 为底，即 score 值最小的成员排名为 0 。如下： 1234127.0.0.1:6379&gt; ZRANK k1 v1(integer) 0127.0.0.1:6379&gt; ZRANK k1 v2(integer) 1 ZREVRANKZREVRANK 和 ZRANK 命令功能基本一致，不同的是，ZREVRANK 中的排序是从大到小： 1234127.0.0.1:6379&gt; ZREVRANK k1 v1(integer) 4127.0.0.1:6379&gt; ZREVRANK k1 v2(integer) 3 ZINCRBYZINCRBY 命令可以为有序集 key 的成员 member 的 score 值加上增量 increment 。如果 key 中不存在 member ，就在 key 中添加一个 member ，score 是 increment（就好像它之前的 score 是0.0）。如果 key 不存在，就创建一个只含有指定 member 成员的有序集合： 12345127.0.0.1:6379&gt; ZINCRBY k1 3 v1&quot;63&quot;127.0.0.1:6379&gt; ZRANGE k1 0 0 withscores1) &quot;v1&quot;2) &quot;63&quot; ZINTERSTOREZINTERSTORE 命令可以计算给定的 numkeys 个有序集合的交集，并且把结果放到 destination 中。 在给定要计算的 key 和其它参数之前，必须先给定 key 个数( numberkeys )。该命令也可以在执行的过程中给原 score 乘以 weights 后再求和，如下： 1234567891011121314151617181920212223242526127.0.0.1:6379&gt; ZADD k2 2 v1(integer) 1127.0.0.1:6379&gt; ZADD k2 3 v2(integer) 1127.0.0.1:6379&gt; ZADD k2 4 v3(integer) 1127.0.0.1:6379&gt; ZADD k3 9 v2(integer) 1127.0.0.1:6379&gt; ZADD k3 10 v3(integer) 1127.0.0.1:6379&gt; ZADD k3 11 v4(integer) 1127.0.0.1:6379&gt; ZINTERSTORE k4 2 k2 k3(integer) 2127.0.0.1:6379&gt; ZRANGE k4 0 -1 withscores1) &quot;v2&quot;2) &quot;12&quot;3) &quot;v3&quot;4) &quot;14&quot;127.0.0.1:6379&gt; ZINTERSTORE k5 2 k2 k3 weights 3 1(integer) 2127.0.0.1:6379&gt; ZRANGE k5 0 -1 withscores1) &quot;v2&quot;2) &quot;18&quot;3) &quot;v3&quot;4) &quot;22&quot; ZREMZREM 命令可以从集合中弹出一个元素，如下： 1234567891011121314127.0.0.1:6379&gt; ZRANGE k2 0 -1 withscores1) &quot;v1&quot;2) &quot;2&quot;3) &quot;v2&quot;4) &quot;3&quot;5) &quot;v3&quot;6) &quot;4&quot;127.0.0.1:6379&gt; ZREM k2 v1(integer) 1127.0.0.1:6379&gt; ZRANGE k2 0 -1 withscores1) &quot;v2&quot;2) &quot;3&quot;3) &quot;v3&quot;4) &quot;4&quot; ZLEXCOUNTZLEXCOUNT 命令用于计算有序集合中指定成员之间的成员数量。如下： 1234127.0.0.1:6379&gt; ZLEXCOUNT k2 - +(integer) 2127.0.0.1:6379&gt; ZLEXCOUNT k2 [v2 [v4(integer) 2 注意：可以用 - 和 + 表示得分最小值和最大值，如果使用成员名的话，一定要在成员名之前加上 [ 。 ZRANGEBYLEXZRANGEBYLEX 返回指定成员区间内的成员，按成员字典正序排序, 分数必须相同。如下： 1234567127.0.0.1:6379&gt; ZRANGEBYLEX k2 [v2 [v41) &quot;v2&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; ZRANGEBYLEX k2 - +1) &quot;v2&quot;2) &quot;v3&quot;127.0.0.1:6379&gt; 注意 min 和 max 参数的写法和 ZLEXCOUNT 一致。 OK,散列和有序集合的命令我们就介绍这么多，更多命令小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-hash-zset.html"},{"title":"Redis 字符串 STRING 介绍","text":"上篇文章我们介绍了五种数据类型中一些通用的命令，本文我们来看看 STRING 数据类型独有的操作命令。 本文是 Redis 系列的第三篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介 STRINGAPPEND使用 APPEND 命令时，如果 key 已经存在，则会直接在 value 后追加值，如果 key 不存在，则会先创建一个 value 为空字符串的 key ，然后再追加： 12345678127.0.0.1:6379&gt; APPEND k1 hello(integer) 5127.0.0.1:6379&gt; GET k1&quot;hello&quot;127.0.0.1:6379&gt; APPEND k1 world(integer) 10127.0.0.1:6379&gt; GET k1&quot;helloworld&quot; DECRDECR 命令可以实现对 value 的减 1 操作，如果 key 不存在，则 key 对应的初始值会被置为 0 ，如果 key 的 value 不为数字，则会报错，如下： 12345678910127.0.0.1:6379&gt; SET k3 19OK127.0.0.1:6379&gt; DECR k3(integer) 18127.0.0.1:6379&gt; GET k3&quot;18&quot;127.0.0.1:6379&gt; SET k4 aaOK127.0.0.1:6379&gt; DECR k4(error) ERR value is not an integer or out of range DECRBYDECRBY 和 DECR 类似，不同的是 DECRBY 可以指定步长，如下： 123456127.0.0.1:6379&gt; GET k3&quot;8&quot;127.0.0.1:6379&gt; DECRBY k3 4(integer) 4127.0.0.1:6379&gt; GET k3&quot;4&quot; GETGET 命令用来获取对应 key 的 value，如果 key 不存在则返回 nil ，如下： 12127.0.0.1:6379&gt; GET k5(nil) GETRANGEGETRANGE 用来返回 key 所对应的 value 的子串，子串由 start 和 end 决定，从左往右计算，如果下标是负数，则从右往左计算，其中 -1 表示最后一个字符， -2 是倒数第二个…，如下： 123456127.0.0.1:6379&gt; SET k1 helloworldOK127.0.0.1:6379&gt; GETRANGE k1 0 2&quot;hel&quot;127.0.0.1:6379&gt; GETRANGE k1 -3 -1&quot;rld&quot; GETSETGETSET 命令可以用来获取 key 所对应的 value ，并对 key 进行重置，如下： 12345678127.0.0.1:6379&gt; SET k1 v1OK127.0.0.1:6379&gt; GET k1&quot;v1&quot;127.0.0.1:6379&gt; GETSET k1 vv&quot;v1&quot;127.0.0.1:6379&gt; GET k1&quot;vv&quot; INCRINCR 操作可以对指定 key 的 value 执行加 1 操作，如果指定的 key 不存在，那么在加 1 操作之前，会先将 key 的 value 设置为 0 ，如果 key 的 value 不是数字，则会报错。如下： 12127.0.0.1:6379&gt; INCR k2(integer) 1 INCRBYINCRBY 和 INCR 功能类似，不同的是可以指定增长的步长，如下： 12127.0.0.1:6379&gt; INCRBY k2 99(integer) 100 INCRBYFLOATINCRBYFLOAT 命令可以用来增长浮点数，如下： 1234127.0.0.1:6379&gt; SET k1 0.5OK127.0.0.1:6379&gt; INCRBYFLOAT k1 0.33&quot;0.83&quot; MGET与MSETMGET 与 MSET 分别用来批量设置值和批量获取值，如下： 123456127.0.0.1:6379&gt; MSET k1 v1 k2 v2 k3 v3OK127.0.0.1:6379&gt; MGET k1 k2 k31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot; SETEXSETEX 用来给 key 设置 value ，同时设置过期时间，等效于先给 key 设置 value ，再给 key 设置过期时间，如下： 123456127.0.0.1:6379&gt; SETEX k1 30 v1OK127.0.0.1:6379&gt; TTL k1(integer) 26127.0.0.1:6379&gt; GET k1&quot;v1&quot; PSETEXPSETEX 的作用和 SETEX 类似，不同的是，这里设置过期时间的单位是毫秒，如下： 1234127.0.0.1:6379&gt; PSETEX k1 60000 v1OK127.0.0.1:6379&gt; PTTL k1(integer) 55412 SETNXSETNX 是 SET if Not eXists 的简写，SET 命令在执行时，如果 key 已经存在，则新值会覆盖掉旧值，而对于 SETNX 命令，如果 key 已经存在，则不做任何操作，如果 key 不存在，则效果等同于 SET 命令。如下： 123456127.0.0.1:6379&gt; SETNX k1 v1(integer) 1127.0.0.1:6379&gt; SETNX k1 vv(integer) 0127.0.0.1:6379&gt; GET k1&quot;v1&quot; MSETNXMSETNX 兼具了 SETNX 和 MSET 的特性，但是 MSETNX 在执行时，如果有一个 key 存在，则所有的都不会执行，如下： 12127.0.0.1:6379&gt; MSETNX k1 v1 k2 v2(integer) 0 因为 k1 已经存在，所以 k2 也没执行成功。 SETRANGESETRANGE 用来覆盖一个已经存在的 key 的 value ，如下： 12345678127.0.0.1:6379&gt; set k1 helloworldOK127.0.0.1:6379&gt; get k1&quot;helloworld&quot;127.0.0.1:6379&gt; SETRANGE k1 5 redis(integer) 10127.0.0.1:6379&gt; get k1&quot;helloredis&quot; 但是如果已经存在的 key 的 value 长度小于 offset ，则不足的地方用 0 补齐，如下： 123456127.0.0.1:6379&gt; set k1 helloredisOK127.0.0.1:6379&gt; SETRANGE k1 20 --java(integer) 26127.0.0.1:6379&gt; GET k1&quot;helloredis\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00--java&quot; STRLENSTRLEN 用来计算 key 的 value 的长度，如下： 12127.0.0.1:6379&gt; STRLEN k1(integer) 26 OK,STRING 相关的命令我们就介绍这么多，当然还有很多其他的，小伙伴们可以参考官方文档。小伙伴在看官方文档时，有什么问题欢迎留言讨论。","link":"/2019/0615/redis-string.html"},{"title":"Redis 集群搭建","text":"主从的搭建差不多说完了，本文我们来看看集群如何搭建。 本文是 Redis 系列的第十二篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一)11.Redis 主从复制(二) 集群原理Redis 集群架构如下图： Redis 集群运行原理如下： 所有的 Redis 节点彼此互联( PING-PONG 机制),内部使用二进制协议优化传输速度和带宽 节点的 fail 是通过集群中超过半数的节点检测失效时才生效 客户端与 Redis 节点直连,不需要中间 proxy 层，客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 Redis-cluster 把所有的物理节点映射到 [0-16383] slot 上, cluster (簇)负责维护 node&lt;-&gt;slot&lt;-&gt;value 。Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，Redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，Redis 会根据节点数量大致均等的将哈希槽映射到不同的节点 怎么样投票投票过程是集群中所有 master 参与,如果半数以上 master 节点与 master 节点通信超过 cluster-node-timeout 设置的时间,认为当前 master 节点挂掉。 怎么样判定节点不可用1.如果集群任意 master 挂掉,且当前 master 没有 slave， 集群进入 fail 状态,也可以理解成集群的 slot 映射 [0-16383] 不完整时进入 fail 状态。2.如果集群超过半数以上 master 挂掉，无论是否有 slave ,集群进入 fail 状态，当集群不可用时,所有对集群的操作做都不可用，收到 ((error) CLUSTERDOWN The cluster is down) 错误。 ruby 环境Redis 集群管理工具 redis-trib.rb 依赖 ruby 环境，首先需要安装 ruby 环境： 安装 ruby: 12yum install rubyyum install rubygems 但是这种安装方式装好的 ruby 版本可能不适用，如果安装失败，可以参考这篇文章解决 redis requires Ruby version &gt;= 2.2.2。 集群搭建首先我们对集群做一个简单规划，假设我的集群中一共有三个节点，每个节点一个主机一个从机，这样我一共需要 6 个 Redis 实例。首先创建 redis-cluster 文件夹，在该文件夹下分别创建 7001、7002、7003、7004、7005、7006 文件夹，用来存放我的 Redis 配置文件，如下： 将 Redis 也在 redis-cluster 目录下安装一份，然后将 redis.conf 文件向 7001-7006 这 6 个文件夹中分别拷贝一份，拷贝完成后，分别修改如下参数： 123456port 7001#bind 127.0.0.1cluster-enabled yescluster-config-XX XXX7001.confprotected nodaemonize yes 这是 7001 目录下的配置，其他的文件夹将 7001 改为对应的数字即可。修改完成后，进入到 redis 安装目录中，分别启动各个 redis ，使用刚刚修改过的配置文件，如下： 启动成功后，我们可以查看 redis 进程，如下： 这个表示各个节点都启动成功了。接下来我们就可以进行集群的创建了，首先将 redis/src 目录下的 redis-trib.rb 文件拷贝到 redis-cluster 目录下，然后在 redis-cluster 目录下执行如下命令： 1./redis-trib.rb create --replicas 1 192.168.248.128:7001 192.168.248.128:7002 192.168.248.128:7003 192.168.248.128:7004 192.168.248.128:7005 192.168.248.128:7006 注意，replicas 后面的 1 表示每个主机都带有 1 个从机，执行过程如下： 注意创建过程的日志，每个 redis 都获得了一个编号，同时日志也说明了哪些实例做主机，哪些实例做从机，每个从机的主机是谁，每个主机所分配到的 hash 槽范围等等。 查询集群信息集群创建成功后，我们可以登录到 Redis 控制台查看集群信息，注意登录时要添加 -c 参数，表示以集群方式连接，如下： 添加主节点首先我们准备一个端口为 7007 的主节点并启动，准备方式和前面步骤一样，启动成功后，通过如下命令添加主节点： 1./redis-trib.rb add-node 127.0.0.1:7007 127.0.0.1:7001 主节点添加之后，我们可以通过 cluster nodes 命令查看主节点是否添加成功，此时我们发现新添加的节点没有分配到 slot ，如下： 没有分配到 slot 将不能存储数据，此时我们需要手动分配 slot，分配命令如下： 1./redis-trib.rb reshard 127.0.0.1:7001 后面的地址为任意一个节点地址，在分配的过程中，我们一共要输入如下几个参数： 1.一共要划分多少个 hash 槽出来？就是我们总共要给新添加的节点分多少 hash 槽，这个参数依实际情况而定，如下： 2.这些划分出来的槽要给谁，这里输入 7007 节点的编号，如下： 3.要让谁出血？因为 hash 槽目前已经全部分配完毕，要重新从已经分好的节点中拿出来一部分给 7007 ，必然要让另外三个节点把吃进去的吐出来，这里我们可以输入多个节点的编号，每次输完一个点击回车，输完所有的输入 done 表示输入完成，这样就让这几个节点让出部分 slot，如果要让所有具有 slot 的节点都参与到此次 slot 重新分配的活动中，那么这里直接输入 all 即可，如下： OK，主要就是这几个参数，输完之后进入到 slot 重新分配环节，分配完成后，通过 cluster nodes 命令，我们可以发现 7007 已经具有 slot 了，如下： OK,刚刚我们是添加主节点，我们也可以添加从节点，比如我要把 7008 作为 7007 的从节点，添加方式如下： 1./redis-trib.rb add-node --slave --master-id 79bbb30bba66b4997b9360dd09849c67d2d02bb9 192.168.31.135:7008 192.168.31.135:7007 其中 79bbb30bba66b4997b9360dd09849c67d2d02bb9 是 7007 的编号。 删除节点删除节点也比较简单，如下： 1./redis-trib.rb del-node 127.0.0.1:7005 4b45eb75c8b428fbd77ab979b85080146a9bc017 注意 4b45eb75c8b428fbd77ab979b85080146a9bc017 是要删除节点的编号。 再注意：删除已经占有 hash 槽的结点会失败，报错如下： 1[ERR] Node 127.0.0.1:7005 is not empty! Reshard data away and try again. 需要将该结点占用的 hash 槽分配出去（分配方式与上文一致，不赘述）。 好了，redis 集群搭建我们先说这么多，有问题欢迎留言讨论。","link":"/2019/0615/redis-cluster.html"},{"title":"Spring Boot + Vue 前后端分离开发，权限管理的一点思路","text":"在传统的前后端不分的开发中，权限管理主要通过过滤器或者拦截器来进行（权限管理框架本身也是通过过滤器来实现功能），如果用户不具备某一个角色或者某一个权限，则无法访问某一个页面。 但是在前后端分离中，页面的跳转统统交给前端去做，后端只提供数据，这种时候，权限管理不能再按照之前的思路来。 首先要明确一点，前端是展示给用户看的，所有的菜单显示或者隐藏目的不是为了实现权限管理，而是为了给用户一个良好的体验，不能依靠前端隐藏控件来实现权限管理，即数据安全不能依靠前端。 这点就像普通的表单提交一样，前端做数据校验是为了提高效率，提高用户体验，后端才是真正的确保数据完整性。 所以，真正的数据安全管理是在后端实现的，后端在接口设计的过程中，就要确保每一个接口都是在满足某种权限的基础上才能访问，也就是说，不怕将后端数据接口地址暴露出来，即使暴露出来，只要你没有相应的角色，也是访问不了的。 前端为了良好的用户体验，需要将用户不能访问的接口或者菜单隐藏起来。 有人说，如果用户直接在地址拦输入某一个页面的路径，怎么办？此时，如果没有做任何额外的处理的话，用户确实可以通过直接输入某一个路径进入到系统中的某一个页面中，但是，不用担心数据泄露问题，因为没有相关的角色，就无法访问相关的接口。 但是，如果用户非这样操作，进入到一个空白的页面，用户体验不好，此时，我们可以使用 Vue 中的前置路由导航守卫，来监听页面跳转，如果用户想要去一个未获授权的页面，则直接在前置路由导航守卫中将之拦截下来，重定向到登录页，或者直接就停留在当前页，不让用户跳转，也可以顺手再给用户一点点未获授权的提示信息。 总而言之一句话，前端的所有操作，都是为了提高用户体验，不是为了数据安全，真正的权限校验要在后端来做，后端如果是 SSM 架构，建议使用 Shiro ，如果是 Spring Boot + 微服务，建议使用 Spring Security 。","link":"/2019/0523/springboot-vue-permission.html"},{"title":"Spring Boot + Vue 前后端分离开发，前端网络请求封装与配置","text":"前端网络访问，主流方案就是 Ajax，Vue 也不例外，在 Vue2.0 之前，网络访问较多的采用 vue-resources，Vue2.0 之后，官方不再建议使用 vue-resources ，这个项目本身也停止维护，目前建议使用的方案是 axios。今天松哥就带大家来看看 axios 的使用。 axios 引入axios 使用步骤很简单，首先在前端项目中，引入 axios： 1npm install axios -S 装好之后，按理说可以直接使用了，但是，一般在生产环境中，我们都需要对网络请求进行封装。 因为网络请求可能会出错，这些错误有的是代码错误导致的，也有的是业务错误，不管是哪一种错误，都需要开发者去处理，而我们不可能在每一次发送请求时都去枚举各种错误情况。 因此我们需要对前端请求进行封装，封装完成后，将前端错误统一处理，这样，开发者只需要在每一次发送请求的地方处理请求成功的情况即可。 请求封装在 axios 中，我们可以使用 axios 自带的拦截器来实现对错误的统一处理。 在 axios 中，有请求拦截器，也有响应拦截器。 请求拦截器中可以统一添加公共的请求参数，例如单点登录中前端统一添加 token 参数。 响应拦截器则可以实现对错误的统一处理。 另外一个需要注意的地方则是错误的展示需要使用一种通用的方式，而不可以和页面绑定（例如，登录失败，在用户名/密码输入框后面展示错误信息，不支持这种错误显示方式），这里推荐使用 ElementUI 中的 Massage 来展示错误信息，这是一个页面无关的组件。 封装后的 axios 如下： 12345678910111213141516171819202122232425262728293031import axios from 'axios'import {Message} from 'element-ui'axios.interceptors.request.use(config =&gt; { return config;}, err =&gt; { Message.error({message: '请求超时!'});})axios.interceptors.response.use(data =&gt; { if (data.status &amp;&amp; data.status == 200 &amp;&amp; data.data.status == 500) { Message.error({message: data.data.msg}); return; } if (data.data.msg) { Message.success({message: data.data.msg}); } return data.data;}, err =&gt; { if (err.response.status == 504 || err.response.status == 404) { Message.error({message: '服务器被吃了⊙﹏⊙∥'}); } else if (err.response.status == 403) { Message.error({message: '权限不足,请联系管理员!'}); } else if (err.response.status == 401) { Message.error({message: err.response.data.msg}); } else { if (err.response.data.msg) { Message.error({message: err.response.data.msg}); }else{ Message.error({message: '未知错误!'}); } }}) 代码解释： 首先导入 axios 和 Massage 组件 接下来定义一个请求拦截器 最后定义一个响应拦截器，这个拦截器有两个参数，第一个参数 data 表示服务端处理成功的响应，第二个 err 表示服务端处理失败的响应。对照着 jQuery 中的 Ajax ，第一个相当于 success 回调，第二个相当于 error 回调。 响应的 data 表示服务端返回的数据，数据格式是 {data:{status:200,msg&quot;&quot;,obj:{}},status:200} 其中，data 中的对象就是服务端返回的具体的 JSON ，外面的 status 表示 HTTP 响应码，里边的 status 是自定义的 RespBean 中返回的数据 首先判断 HTTP 响应码为 200 ，并且服务端返回的 status 为 500 ，表示业务逻辑错误，此时直接通过 Message 将错误信息展示出来，然后 return 即可。 如果服务端返回的字段中包含 msg ，则将 msg 显示出来，这个 msg 一般是成功的提示。 最后返回 data.data ，即将服务端返回的数据 return ，这个数据最终会来到请求调用的地方。 当 HTTP 响应码大于等于 400 时，进入 err 中。 方法封装请求封装完成后，还需要对方法进行封装，方便调用： 123456789101112131415161718192021222324252627282930313233let base = '';export const postRequest = (url, params) =&gt; { return axios({ method: 'post', url: `${base}${url}`, data: params, headers: { 'Content-Type': 'application/json' } });}export const putRequest = (url, params) =&gt; { return axios({ method: 'put', url: `${base}${url}`, data: params, headers: { 'Content-Type': 'application/json' } });}export const deleteRequest = (url) =&gt; { return axios({ method: 'delete', url: `${base}${url}` });}export const getRequest = (url) =&gt; { return axios({ method: 'get', url: `${base}${url}` });} 由于在前后端分离项目中，大多数情况下，后端接口都采用 RESTful 风格来设计，所以前端主要封装 GET\\POST\\PUT\\DELETE 方法，然后所有的请求参数都是用 JSON。 这里一开始定义了一个 base 变量，这是请求的前缀，方便后期维护（如果需要统一修改请求前缀）。 制作 Vue 插件封装好的方法已经可以直接使用了，但是比较麻烦，每次使用时，都需要在相关的 vue 文件中引入方法，像下面这样： 1import {postRequest} from \"../utils/api\"; 但是这种操作方式太麻烦，所以我们可以考虑将方法进一步封装成 Vue 的插件，这样在每一个 vue 文件中，不需要引入方法就能够直接调用方法了。 参考 Vue 官方文档 https://cn.vuejs.org/v2/guide/plugins.html，如下： 官方给出了 5 种插件制作方式，我们这里采用第 4 种方案。具体操作就是在 main.js 中引入所有的封装好的方法，然后挂载到 Vue.prototype 上即可，如下： 12345678import {postRequest} from &quot;./utils/api&quot;;import {putRequest} from &quot;./utils/api&quot;;import {deleteRequest} from &quot;./utils/api&quot;;import {getRequest} from &quot;./utils/api&quot;;Vue.prototype.getRequest = getRequest;Vue.prototype.deleteRequest = deleteRequest;Vue.prototype.putRequest = putRequest;Vue.prototype.postRequest = postRequest; 封装完成后，以后在 vue 文件中，直接通过 this 就可以获取到网络请求方法的引用了，如下： 12345this.postRequest(\"/doLogin\", this.user).then(msg=&gt;{ if (msg) { //登录成功，页面跳转 }}) 注意 ，then 中的 msg 就是响应拦截器中返回的 msg ，这个 msg 如果没有值，表示请求失败（失败已经在拦截器中进行处理了），如果有值，表示请求成功！ 配置请求转发在前后端分离中，前端和后端在不同的端口或者地址上运行，如果前端直接向后端发送请求，这个请求是跨域的。 但是在项目部署时，前端打包编译后拷贝到 Java 项目中，和 Java 项目一起运行，此时不存在跨域问题。 所以这里我们的解决思路不是解决跨域问题，而是通过配置 NodeJS 的请求转发，来实现网络请求顺利发送。 请求转发在 vue 项目的 config/index.js 文件中配置： 添加了请求转发配置之后，一定要重启前端项目才会生效。 此时启动前端项目，就可以顺利发送网络请求了。 总结本文主要和大伙分享了在前后端分离的情况下，如何对前端网络请求进行封装，并且如何配置请求转发，这是前后端分离中的基础课，小伙伴们有问题欢迎留言讨论。松哥将自己封装的网络请求库已经放在 GitHub 上，欢迎大家参考 https://github.com/lenve/javaboy-code-samples。","link":"/2019/0521/springboot-vue-axios.html"},{"title":"Spring Boot + Vue 前后端分离，两种文件上传方式总结","text":"在Vue.js 中，如果网络请求使用 axios ，并且使用了 ElementUI 库，那么一般来说，文件上传有两种不同的实现方案： 通过 Ajax 实现文件上传 通过 ElementUI 里边的 Upload 组件实现文件上传 两种方案，各有优缺点，我们分别来看。 准备工作首先我们需要一点点准备工作，就是在后端提供一个文件上传接口，这是一个普通的 Spring Boot 项目，如下： 12345678910111213141516SimpleDateFormat sdf = new SimpleDateFormat(\"/yyyy/MM/dd/\");@PostMapping(\"/import\")public RespBean importData(MultipartFile file, HttpServletRequest req) throws IOException { String format = sdf.format(new Date()); String realPath = req.getServletContext().getRealPath(\"/upload\") + format; File folder = new File(realPath); if (!folder.exists()) { folder.mkdirs(); } String oldName = file.getOriginalFilename(); String newName = UUID.randomUUID().toString() + oldName.substring(oldName.lastIndexOf(\".\")); file.transferTo(new File(folder,newName)); String url = req.getScheme() + \"://\" + req.getServerName() + \":\" + req.getServerPort() + \"/upload\" + format + newName; System.out.println(url); return RespBean.ok(\"上传成功!\");} 这里的文件上传比较简单，上传的文件按照日期进行归类，使用 UUID 给文件重命名。 这里为了简化代码，我省略掉了异常捕获，上传结果直接返回成功，后端代码大伙可根据自己的实际情况自行修改。 Ajax 上传在 Vue 中，通过 Ajax 实现文件上传，方案和传统 Ajax 实现文件上传基本上是一致的，唯一不同的是查找元素的方式。 12&lt;input type=\"file\" ref=\"myfile\"&gt;&lt;el-button @click=\"importData\" type=\"success\" size=\"mini\" icon=\"el-icon-upload2\"&gt;导入数据&lt;/el-button&gt; 在这里，首先提供一个文件导入 input 组件，再来一个导入按钮，在导入按钮的事件中来完成导入的逻辑。 123456789101112importData() { let myfile = this.$refs.myfile; let files = myfile.files; let file = files[0]; var formData = new FormData(); formData.append(\"file\", file); this.uploadFileRequest(\"/system/basic/jl/import\",formData).then(resp=&gt;{ if (resp) { console.log(resp); } })} 关于这段上传核心逻辑，解释如下： 首先利用 Vue 中的 $refs 查找到存放文件的元素。 type 为 file 的 input 元素内部有一个 files 数组，里边存放了所有选择的 file，由于文件上传时，文件可以多选，因此这里拿到的 files 对象是一个数组。 从 files 对象中，获取自己要上传的文件，由于这里是单选，所以其实就是数组中的第一项。 构造一个 FormData ，用来存放上传的数据,FormData 不可以像 Java 中的 StringBuffer 使用链式配置。 构造好 FromData 后，就可以直接上传数据了，FormData 就是要上传的数据。 文件上传注意两点，1. 请求方法为 post，2. 设置 Content-Type 为 multipart/form-data 。 这种文件上传方式，实际上就是传统的 Ajax 上传文件，和大家常见的 jQuery 中写法不同的是，这里元素查找的方式不一样（实际上元素查找也可以按照JavaScript 中原本的写法来实现），其他写法一模一样。这种方式是一个通用的方式，和使用哪一种前端框架无关。最后再和大家来看下封装的上传方法： 12345678910export const uploadFileRequest = (url, params) =&gt; { return axios({ method: 'post', url: `${base}${url}`, data: params, headers: { 'Content-Type': 'multipart/form-data' } });} 经过这几步的配置后，前端就算上传完成了，可以进行文件上传了。 使用 Upload 组件如果使用 Upload ，则需要引入 ElementUI，所以一般建议，如果使用了 ElementUI 做 UI 控件的话，则可以考虑使用 Upload 组件来实现文件上传，如果没有使用 ElementUI 的话，则不建议使用 Upload 组件，至于其他的 UI 控件，各自都有自己的文件上传组件，具体使用可以参考各自文档。 123456789&lt;el-upload style=\"display: inline\" :show-file-list=\"false\" :on-success=\"onSuccess\" :on-error=\"onError\" :before-upload=\"beforeUpload\" action=\"/system/basic/jl/import\"&gt; &lt;el-button size=\"mini\" type=\"success\" :disabled=\"!enabledUploadBtn\" :icon=\"uploadBtnIcon\"&gt;{{btnText}}&lt;/el-button&gt;&lt;/el-upload&gt; show-file-list 表示是否展示上传文件列表，默认为true，这里设置为不展示。 before-upload 表示上传之前的回调，可以在该方法中，做一些准备工作，例如展示一个进度条给用户 。 on-success 和 on-error 分别表示上传成功和失败时候的回调，可以在这两个方法中，给用户一个相应的提示，如果有进度条，还需要在这两个方法中关闭进度条。 action 指文件上传地址。 上传按钮的点击状态和图标都设置为变量 ，在文件上传过程中，修改上传按钮的点击状态为不可点击，同时修改图标为一个正在加载的图标 loading。 上传的文本也设为变量，默认上传 button 的文本是 数据导入 ，当开始上传后，将找个 button 上的文本修改为 正在导入。 相应的回调如下： 123456789101112131415onSuccess(response, file, fileList) { this.enabledUploadBtn = true; this.uploadBtnIcon = 'el-icon-upload2'; this.btnText = '数据导入';},onError(err, file, fileList) { this.enabledUploadBtn = true; this.uploadBtnIcon = 'el-icon-upload2'; this.btnText = '数据导入';},beforeUpload(file) { this.enabledUploadBtn = false; this.uploadBtnIcon = 'el-icon-loading'; this.btnText = '正在导入';} 在文件开始上传时，修改上传按钮为不可点击，同时修改上传按钮的图标和文本。 文件上传成功或者失败时，修改上传按钮的状态为可以点击，同时恢复上传按钮的图标和文本。 上传效果图如下： 总结两种上传方式各有优缺点： 第一种方式最大的优势是通用，一招鲜吃遍天，到哪里都能用，但是对于上传过程的监控，进度条的展示等等逻辑都需要自己来实现。 第二种方式不够通用，因为它是 ElementUI 中的组件，得引入 ElementUI 才能使用，不过这种方式很明显有需多比较方便的回调，可以实现非常方便的处理常见的各种上传问题。 常规的上传需求第二种方式可以满足，但是如果要对上传的方法进行定制，则还是建议使用第一种上传方案。","link":"/2019/0428/springboot-vue-upload.html"},{"title":"Spring Boot 中实现定时任务的两种方式","text":"在 Spring + SpringMVC 环境中，一般来说，要实现定时任务，我们有两中方案，一种是使用 Spring 自带的定时任务处理器 @Scheduled 注解，另一种就是使用第三方框架 Quartz ，Spring Boot 源自 Spring+SpringMVC ，因此天然具备这两个 Spring 中的定时任务实现策略，当然也支持 Quartz，本文我们就来看下 Spring Boot 中两种定时任务的实现方式。 @Scheduled使用 @Scheduled 非常容易，直接创建一个 Spring Boot 项目，并且添加 web 依赖 spring-boot-starter-web，项目创建成功后，添加 @EnableScheduling 注解，开启定时任务： 123456789@SpringBootApplication@EnableSchedulingpublic class ScheduledApplication { public static void main(String[] args) { SpringApplication.run(ScheduledApplication.class, args); }} 接下来配置定时任务： 123456789101112@Scheduled(fixedRate = 2000)public void fixedRate() { System.out.println(\"fixedRate&gt;&gt;&gt;\"+new Date()); }@Scheduled(fixedDelay = 2000)public void fixedDelay() { System.out.println(\"fixedDelay&gt;&gt;&gt;\"+new Date());}@Scheduled(initialDelay = 2000,fixedDelay = 2000)public void initialDelay() { System.out.println(\"initialDelay&gt;&gt;&gt;\"+new Date());} 首先使用 @Scheduled 注解开启一个定时任务。 fixedRate 表示任务执行之间的时间间隔，具体是指两次任务的开始时间间隔，即第二次任务开始时，第一次任务可能还没结束。 fixedDelay 表示任务执行之间的时间间隔，具体是指本次任务结束到下次任务开始之间的时间间隔。 initialDelay 表示首次任务启动的延迟时间。 所有时间的单位都是毫秒。 上面这是一个基本用法，除了这几个基本属性之外，@Scheduled 注解也支持 cron 表达式，使用 cron 表达式，可以非常丰富的描述定时任务的时间。cron 表达式格式如下： [秒] [分] [小时] [日] [月] [周] [年] 具体取值如下： 序号 说明 是否必填 允许填写的值 允许的通配符 1 秒 是 0-59 - * / 2 分 是 0-59 - * / 3 时 是 0-23 - * / 4 日 是 1-31 - * ? / L W 5 月 是 1-12 or JAN-DEC - * / 6 周 是 1-7 or SUN-SAT - * ? / L # 7 年 否 1970-2099 - * / 这一块需要大家注意的是，月份中的日期和星期可能会起冲突，因此在配置时这两个得有一个是 ? 通配符含义： ? 表示不指定值，即不关心某个字段的取值时使用。需要注意的是，月份中的日期和星期可能会起冲突，因此在配置时这两个得有一个是 ? * 表示所有值，例如:在秒的字段上设置 *,表示每一秒都会触发 , 用来分开多个值，例如在周字段上设置 “MON,WED,FRI” 表示周一，周三和周五触发 - 表示区间，例如在秒上设置 “10-12”,表示 10,11,12秒都会触发 / 用于递增触发，如在秒上面设置”5/15” 表示从5秒开始，每增15秒触发(5,20,35,50) # 序号(表示每月的第几个周几)，例如在周字段上设置”6#3”表示在每月的第三个周六，(用 在母亲节和父亲节再合适不过了) 周字段的设置，若使用英文字母是不区分大小写的 ，即 MON 与mon相同 L 表示最后的意思。在日字段设置上，表示当月的最后一天(依据当前月份，如果是二月还会自动判断是否是润年), 在周字段上表示星期六，相当于”7”或”SAT”（注意周日算是第一天）。如果在”L”前加上数字，则表示该数据的最后一个。例如在周字段上设置”6L”这样的格式,则表示”本月最后一个星期五” W 表示离指定日期的最近工作日(周一至周五)，例如在日字段上设置”15W”，表示离每月15号最近的那个工作日触发。如果15号正好是周六，则找最近的周五(14号)触发, 如果15号是周未，则找最近的下周一(16号)触发，如果15号正好在工作日(周一至周五)，则就在该天触发。如果指定格式为 “1W”,它则表示每月1号往后最近的工作日触发。如果1号正是周六，则将在3号下周一触发。(注，”W”前只能设置具体的数字,不允许区间”-“) L 和 W 可以一组合使用。如果在日字段上设置”LW”,则表示在本月的最后一个工作日触发(一般指发工资 ) 例如，在 @Scheduled 注解中来一个简单的 cron 表达式，每隔5秒触发一次，如下： 1234@Scheduled(cron = \"0/5 * * * * *\")public void cron() { System.out.println(new Date());} 上面介绍的是使用 @Scheduled 注解的方式来实现定时任务，接下来我们再来看看如何使用 Quartz 实现定时任务。 Quartz一般在项目中，除非定时任务涉及到的业务实在是太简单，使用 @Scheduled 注解来解决定时任务，否则大部分情况可能都是使用 Quartz 来做定时任务。在 Spring Boot 中使用 Quartz ，只需要在创建项目时，添加 Quartz 依赖即可： 项目创建完成后，也需要添加开启定时任务的注解： 1234567@SpringBootApplication@EnableSchedulingpublic class QuartzApplication { public static void main(String[] args) { SpringApplication.run(QuartzApplication.class, args); }} Quartz 在使用过程中，有两个关键概念，一个是JobDetail（要做的事情），另一个是触发器（什么时候做），要定义 JobDetail，需要先定义 Job，Job 的定义有两种方式： 第一种方式，直接定义一个Bean： 123456@Componentpublic class MyJob1 { public void sayHello() { System.out.println(\"MyJob1&gt;&gt;&gt;\"+new Date()); }} 关于这种定义方式说两点： 首先将这个 Job 注册到 Spring 容器中。 这种定义方式有一个缺陷，就是无法传参。 第二种定义方式，则是继承 QuartzJobBean 并实现默认的方法： 123456789101112131415161718public class MyJob2 extends QuartzJobBean { HelloService helloService; public HelloService getHelloService() { return helloService; } public void setHelloService(HelloService helloService) { this.helloService = helloService; } @Override protected void executeInternal(JobExecutionContext jobExecutionContext) throws JobExecutionException { helloService.sayHello(); }}public class HelloService { public void sayHello() { System.out.println(\"hello service &gt;&gt;&gt;\"+new Date()); }} 和第1种方式相比，这种方式支持传参，任务启动时，executeInternal 方法将会被执行。 Job 有了之后，接下来创建类，配置 JobDetail 和 Trigger 触发器，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Configurationpublic class QuartzConfig { @Bean MethodInvokingJobDetailFactoryBean methodInvokingJobDetailFactoryBean() { MethodInvokingJobDetailFactoryBean bean = new MethodInvokingJobDetailFactoryBean(); bean.setTargetBeanName(\"myJob1\"); bean.setTargetMethod(\"sayHello\"); return bean; } @Bean JobDetailFactoryBean jobDetailFactoryBean() { JobDetailFactoryBean bean = new JobDetailFactoryBean(); bean.setJobClass(MyJob2.class); JobDataMap map = new JobDataMap(); map.put(\"helloService\", helloService()); bean.setJobDataMap(map); return bean; } @Bean SimpleTriggerFactoryBean simpleTriggerFactoryBean() { SimpleTriggerFactoryBean bean = new SimpleTriggerFactoryBean(); bean.setStartTime(new Date()); bean.setRepeatCount(5); bean.setJobDetail(methodInvokingJobDetailFactoryBean().getObject()); bean.setRepeatInterval(3000); return bean; } @Bean CronTriggerFactoryBean cronTrigger() { CronTriggerFactoryBean bean = new CronTriggerFactoryBean(); bean.setCronExpression(\"0/10 * * * * ?\"); bean.setJobDetail(jobDetailFactoryBean().getObject()); return bean; } @Bean SchedulerFactoryBean schedulerFactoryBean() { SchedulerFactoryBean bean = new SchedulerFactoryBean(); bean.setTriggers(cronTrigger().getObject(), simpleTriggerFactoryBean().getObject()); return bean; } @Bean HelloService helloService() { return new HelloService(); }} 关于这个配置说如下几点： JobDetail 的配置有两种方式：MethodInvokingJobDetailFactoryBean 和 JobDetailFactoryBean 。 使用 MethodInvokingJobDetailFactoryBean 可以配置目标 Bean 的名字和目标方法的名字，这种方式不支持传参。 使用 JobDetailFactoryBean 可以配置 JobDetail ，任务类继承自 QuartzJobBean ，这种方式支持传参，将参数封装在 JobDataMap 中进行传递。 Trigger 是指触发器，Quartz 中定义了多个触发器，这里向大家展示其中两种的用法，SimpleTrigger 和 CronTrigger 。 SimpleTrigger 有点类似于前面说的 @Scheduled 的基本用法。 CronTrigger 则有点类似于 @Scheduled 中 cron 表达式的用法。 全部定义完成后，启动 Spring Boot 项目就可以看到定时任务的执行了。 总结这里主要向大家展示了 Spring Boot 中整合两种定时任务的方法，整合成功之后，剩下的用法基本上就和在 SSM 中使用一致了，不再赘述。","link":"/2019/0418/springboot-schedule-task.html"},{"title":"Spring Boot 中关于自定义异常处理的套路！","text":"在 Spring Boot 项目中 ，异常统一处理，可以使用 Spring 中 @ControllerAdvice 来统一处理，也可以自己来定义异常处理方案。Spring Boot 中，对异常的处理有一些默认的策略，我们分别来看。 默认情况下，Spring Boot 中的异常页面 是这样的： 我们从这个异常提示中，也能看出来，之所以用户看到这个页面，是因为开发者没有明确提供一个 /error 路径，如果开发者提供了 /error 路径 ，这个页面就不会展示出来，不过在 Spring Boot 中，提供 /error 路径实际上是下下策，Spring Boot 本身在处理异常时，也是当所有条件都不满足时，才会去找 /error 路径。那么我们就先来看看，在 Spring Boot 中，如何自定义 error 页面，整体上来说，可以分为两种，一种是静态页面，另一种是动态页面。 静态异常页面自定义静态异常页面，又分为两种，第一种 是使用 HTTP 响应码来命名页面，例如 404.html、405.html、500.html ….，另一种就是直接定义一个 4xx.html，表示400-499 的状态都显示这个异常页面，5xx.html 表示 500-599 的状态显示这个异常页面。 默认是在 classpath:/static/error/ 路径下定义相关页面： 此时，启动项目，如果项目抛出 500 请求错误，就会自动展示 500.html 这个页面，发生 404 就会展示 404.html 页面。如果异常展示页面既存在 5xx.html，也存在 500.html ，此时，发生500异常时，优先展示 500.html 页面。 动态异常页面动态的异常页面定义方式和静态的基本 一致，可以采用的页面模板有 jsp、freemarker、thymeleaf。动态异常页面，也支持 404.html 或者 4xx.html ，但是一般来说，由于动态异常页面可以直接展示异常详细信息，所以就没有必要挨个枚举错误了 ，直接定义 4xx.html（这里使用thymeleaf模板）或者 5xx.html 即可。 注意，动态页面模板，不需要开发者自己去定义控制器，直接定义异常页面即可 ，Spring Boot 中自带的异常处理器会自动查找到异常页面。 页面定义如下： 页面内容如下： 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;5xx&lt;/h1&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;path&lt;/td&gt; &lt;td th:text=\"${path}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;error&lt;/td&gt; &lt;td th:text=\"${error}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;message&lt;/td&gt; &lt;td th:text=\"${message}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;timestamp&lt;/td&gt; &lt;td th:text=\"${timestamp}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;status&lt;/td&gt; &lt;td th:text=\"${status}\"&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 默认情况下，完整的异常信息就是这5条，展示 效果如下 ： 如果动态页面和静态页面同时定义了异常处理页面，例如 classpath:/static/error/404.html 和 classpath:/templates/error/404.html 同时存在时，默认使用动态页面。即完整的错误页面查找方式应该是这样： 发生了500错误–&gt;查找动态 500.html 页面–&gt;查找静态 500.html –&gt; 查找动态 5xx.html–&gt;查找静态 5xx.html。 自定义异常数据默认情况下，在Spring Boot 中，所有的异常数据其实就是上文所展示出来的5条数据，这5条数据定义在 org.springframework.boot.web.reactive.error.DefaultErrorAttributes 类中，具体定义在 getErrorAttributes 方法中 ： 1234567891011121314@Overridepublic Map&lt;String, Object&gt; getErrorAttributes(ServerRequest request, boolean includeStackTrace) { Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;&gt;(); errorAttributes.put(\"timestamp\", new Date()); errorAttributes.put(\"path\", request.path()); Throwable error = getError(request); HttpStatus errorStatus = determineHttpStatus(error); errorAttributes.put(\"status\", errorStatus.value()); errorAttributes.put(\"error\", errorStatus.getReasonPhrase()); errorAttributes.put(\"message\", determineMessage(error)); handleException(errorAttributes, determineException(error), includeStackTrace); return errorAttributes;} DefaultErrorAttributes 类本身则是在org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration 异常自动配置类中定义的，如果开发者没有自己提供一个 ErrorAttributes 的实例的话，那么 Spring Boot 将自动提供一个ErrorAttributes 的实例，也就是 DefaultErrorAttributes 。 基于此 ，开发者自定义 ErrorAttributes 有两种方式 ： 直接实现 ErrorAttributes 接口 继承 DefaultErrorAttributes（推荐），因为 DefaultErrorAttributes 中对异常数据的处理已经完成，开发者可以直接使用。 具体定义如下： 1234567891011@Componentpublic class MyErrorAttributes extends DefaultErrorAttributes { @Override public Map&lt;String, Object&gt; getErrorAttributes(WebRequest webRequest, boolean includeStackTrace) { Map&lt;String, Object&gt; map = super.getErrorAttributes(webRequest, includeStackTrace); if ((Integer)map.get(\"status\") == 500) { map.put(\"message\", \"服务器内部错误!\"); } return map; }} 定义好的 ErrorAttributes 一定要注册成一个 Bean ，这样，Spring Boot 就不会使用默认的 DefaultErrorAttributes 了，运行效果如下图： 自定义异常视图异常视图默认就是前面所说的静态或者动态页面，这个也是可以自定义的，首先 ，默认的异常视图加载逻辑在 org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController 类的 errorHtml 方法中，这个方法用来返回异常页面+数据，还有另外一个 error 方法，这个方法用来返回异常数据（如果是 ajax 请求，则该方法会被触发）。 12345678910@RequestMapping(produces = MediaType.TEXT_HTML_VALUE)public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) { HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView != null) ? modelAndView : new ModelAndView(\"error\", model);} 在该方法中 ，首先会通过 getErrorAttributes 方法去获取异常数据（实际上会调用到 ErrorAttributes 的实例 的 getErrorAttributes 方法），然后调用 resolveErrorView 去创建一个 ModelAndView ，如果这里创建失败，那么用户将会看到默认的错误提示页面。 正常情况下， resolveErrorView 方法会来到 DefaultErrorViewResolver 类的 resolveErrorView 方法中： 123456789@Overridepublic ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) { ModelAndView modelAndView = resolve(String.valueOf(status.value()), model); if (modelAndView == null &amp;&amp; SERIES_VIEWS.containsKey(status.series())) { modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); } return modelAndView;} 在这里，首先以异常响应码作为视图名分别去查找动态页面和静态页面，如果没有查找到，则再以 4xx 或者 5xx 作为视图名再去分别查找动态或者静态页面。 要自定义异常视图解析，也很容易 ，由于 DefaultErrorViewResolver 是在 ErrorMvcAutoConfiguration 类中提供的实例，即开发者没有提供相关实例时，会使用默认的 DefaultErrorViewResolver ，开发者提供了自己的 ErrorViewResolver 实例后，默认的配置就会失效，因此，自定义异常视图，只需要提供 一个 ErrorViewResolver 的实例即可： 12345678910@Componentpublic class MyErrorViewResolver extends DefaultErrorViewResolver { public MyErrorViewResolver(ApplicationContext applicationContext, ResourceProperties resourceProperties) { super(applicationContext, resourceProperties); } @Override public ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) { return new ModelAndView(\"/aaa/123\", model); }} 实际上，开发者也可以在这里定义异常数据（直接在 resolveErrorView 方法重新定义一个 model ，将参数中的model 数据拷贝过去并修改，注意参数中的 model 类型为 UnmodifiableMap，即不可以直接修改），而不需要自定义MyErrorAttributes。定义完成后，提供一个名为123的视图，如下图： 如此之后，错误试图就算定义成功了。 总结实际上也可以自定义异常控制器 BasicErrorController ，不过松哥觉得这样太大动干戈了，没必要，前面几种方式已经可以满足我们的大部分开发需求了。","link":"/2019/0417/springboot-exception.html"},{"title":"Spring Boot 中 10 行代码构建 RESTful 风格应用","text":"RESTful ，到现在相信已经没人不知道这个东西了吧！关于 RESTful 的概念，我这里就不做过多介绍了，传统的 Struts 对 RESTful 支持不够友好 ，但是 SpringMVC 对于 RESTful 提供了很好的支持，常见的相关注解有： 1234567@RestController@GetMapping@PutMapping@PostMapping@DeleteMapping@ResponseBody... 这些注解都是和 RESTful 相关的，在移动互联网中，RESTful 得到了非常广泛的使用。RESTful 这个概念提出来很早，但是以前没有移动互联网时，我们做的大部分应用都是前后端不分的，在这种架构的应用中，数据基本上都是在后端渲染好返回给前端展示的，此时 RESTful 在 Web 应用中基本就没用武之地，移动互联网的兴起，让我们一套后台对应多个前端项目，因此前后端分离，RESTful 顺利走上前台。 Spring Boot 继承自 Spring + SpringMVC， SpringMVC 中对于 RESTful 支持的特性在 Spring Boot 中全盘接收，同时，结合 Jpa 和 自动化配置，对于 RESTful 还提供了更多的支持，使得开发者几乎不需要写代码（很少几行），就能快速实现一个 RESTful 风格的增删改查。 接下来，松哥通过一个简单的案例，来向大家展示 Spring Boot 对于 RESTful 的支持。 实战创建工程首先创建一个 Spring Boot 工程，引入 Web 、 Jpa 、 MySQL 、Rest Repositories 依赖： 创建完成后，还需要锁定 MySQL 驱动的版本以及加入 Druid 数据库连接池，完整依赖如下： 12345678910111213141516171819202122232425&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-rest&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;version&gt;5.1.27&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置数据库主要配置两个，一个是数据库，另一个是 Jpa： 12345678910spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.username=rootspring.datasource.password=rootspring.datasource.url=jdbc:mysql:///test01spring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL57Dialectspring.jpa.show-sql=truespring.jpa.hibernate.ddl-auto=updatespring.jpa.database-platform=mysqlspring.jpa.database=mysql 这里的配置，和 Jpa 中的基本一致。 前面五行配置了数据库的基本信息，包括数据库连接池、数据库用户名、数据库密码、数据库连接地址以及数据库驱动名称。 接下来的五行配置了 JPA 的基本信息，分别表示生成 SQL 的方言、打印出生成的 SQL 、每次启动项目时根据实际情况选择是否更新表、数据库平台是 MySQL。 这两段配置是关于 MySQL + JPA 的配置，没用过 JPA 的小伙伴可以参考松哥之前的 JPA 文章：http://www.javaboy.org/2019/0407/springboot-jpa.html 构建实体类123456789101112@Entity(name = \"t_book\")public class Book { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(name = \"book_name\") private String name; private String author; //省略 getter/setter}public interface BookRepository extends JpaRepository&lt;Book,Long&gt; {} 这里一个是配置了一个实体类 Book，另一个则是配置了一个 BookRepository ，项目启动成功后，框架会根据 Book 类的定义，在数据库中自动创建相应的表，BookRepository 接口则是继承自 JpaRepository ，JpaRepository 中自带了一些基本的增删改查方法。 好了，代码写完了。 啥？你好像啥都没写啊？是的，啥都没写，啥都不用写，一个 RESTful 风格的增删改查应用就有了，这就是 Spring Boot 的魅力！ 测试此时，我们就可以启动项目进行测试了，使用 POSTMAN 来测试（大家也可以自行选择趁手的 HTTP 请求工具）。 此时我们的项目已经默认具备了一些接口，我们分别来看： 根据 id 查询接口 http://127.0.0.1:8080/books/{id} 这个接口表示根据 id 查询某一本书： 分页查询 http://127.0.0.1:8080/books 这是一个批量查询接口，默认请求路径是类名首字母小写，并且再加一个 s 后缀。这个接口实际上是一个分页查询接口，没有传参数，表示查询第一页，每页 20 条数据。 查询结果中，除了该有的数据之外，也包含了分页数据： 分页数据中： size 表示每页查询记录数 totalElements 表示总记录数 totalPages 表示总页数 number 表示当前页数，从0开始计 如果要分页或者排序查询，可以使用 _links 中的链接。http://127.0.0.1:8080/books?page=1&amp;size=3&amp;sort=id,desc 。 添加也可以添加数据，添加是 POST 请求，数据通过 JSON 的形式传递，如下： 添加成功之后，默认会返回添加成功的数据。 修改修改接口默认也是存在的，数据修改请求是一个 PUT 请求，修改的参数也是通过 JSON 的形式传递： 默认情况下，修改成功后，会返回修改成功的数据。 删除当然也可以通过 DELETE 请求根据 id 删除数据： 删除成功后，是没有返回值的。 不需要几行代码，一个基本的增删改查就有了。 这些都是默认的配置，这些默认的配置实际上都是在 JpaRepository 的基础上实现的，实际项目中，我们还可以对这些功能进行定制。 查询定制最广泛的定制，就是查询，因为增删改操作的变化不像查询这么丰富。对于查询的定制，非常容易，只需要提供相关的方法即可。例如根据作者查询书籍： 123public interface BookRepository extends JpaRepository&lt;Book,Long&gt; { List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author);} 注意，方法的定义，参数要有 @Param 注解。 定制完成后，重启项目，此时就多了一个查询接口，开发者可以通过 http://localhost:8080/books/search 来查看和 book 相关的自定义接口都有哪些： 查询结果表示，只有一个自定义接口，接口名就是方法名，而且查询结果还给出了接口调用的示例。我们来尝试调用一下自己定义的查询接口： 开发者可以根据实际情况，在 BookRepository 中定义任意多个查询方法，查询方法的定义规则和 Jpa 中一模一样（不懂 Jpa 的小伙伴，可以参考干货|一文读懂 Spring Data Jpa！，或者在松哥个人网站 www.javaboy.org 上搜索 JPA，有相关教程参考）。但是，这样有一个缺陷，就是 Jpa 中方法名太长，因此，如果不想使用方法名作为接口名，则可以自定义接口名： 1234public interface BookRepository extends JpaRepository&lt;Book, Long&gt; { @RestResource(rel = \"byauthor\",path = \"byauthor\") List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author);} @RestResource 注解中，两个参数的含义： rel 表示接口查询中，这个方法的 key path 表示请求路径 这样定义完成后，表示接口名为 byauthor ，重启项目，继续查询接口： 除了 rel 和 path 两个属性之外，@RestResource 中还有一个属性，exported 表示是否暴露接口，默认为 true ，表示暴露接口，即方法可以在前端调用，如果仅仅只是想定义一个方法，不需要在前端调用这个方法，可以设置 exported 属性为 false 。 如果不想暴露官方定义好的方法，例如根据 id 删除数据，只需要在自定义接口中重写该方法，然后在该方法上加 @RestResource 注解并且配置相关属性即可。 1234567public interface BookRepository extends JpaRepository&lt;Book, Long&gt; { @RestResource(rel = \"byauthor\",path = \"byauthor\") List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author); @Override @RestResource(exported = false) void deleteById(Long aLong);} 另外生成的 JSON 字符串中的集合名和单个 item 的名字都是可以自定义的： 12345678@RepositoryRestResource(collectionResourceRel = \"bs\",itemResourceRel = \"b\",path = \"bs\")public interface BookRepository extends JpaRepository&lt;Book, Long&gt; { @RestResource(rel = \"byauthor\",path = \"byauthor\") List&lt;Book&gt; findBookByAuthorContaining(@Param(\"author\") String author); @Override @RestResource(exported = false) void deleteById(Long aLong);} path 属性表示请求路径，请求路径默认是类名首字母小写+s，可以在这里自己重新定义。 其他配置最后，也可以在 application.properties 中配置 REST 基本参数： 12345678spring.data.rest.base-path=/apispring.data.rest.sort-param-name=sortspring.data.rest.page-param-name=pagespring.data.rest.limit-param-name=sizespring.data.rest.max-page-size=20spring.data.rest.default-page-size=0spring.data.rest.return-body-on-update=truespring.data.rest.return-body-on-create=true 配置含义，从上往下，依次是： 给所有的接口添加统一的前缀 配置排序参数的 key ，默认是 sort 配置分页查询时页码的 key，默认是 page 配置分页查询时每页查询页数的 key，默认是size 配置每页最大查询记录数，默认是 20 条 分页查询时默认的页码 更新成功时是否返回更新记录 添加成功时是否返回添加记录 总结本文主要向大家介绍了 Spring Boot 中快速实现一个 RESTful 风格的增删改查应用的方案，整体来说还是比较简单的，并不难。相关案例我已上传到 GitHub 上了，小伙伴可以自行下载：https://github.com/lenve/javaboy-code-samples。 关于本文，有问题欢迎留言讨论。","link":"/2019/0606/springboot-restful.html"},{"title":"Spring Boot 中的静态资源到底要放在哪里？","text":"当我们使用 SpringMVC 框架时，静态资源会被拦截，需要添加额外配置，之前老有小伙伴在微信上问松哥Spring Boot 中的静态资源加载问题：“松哥，我的HTML页面好像没有样式？”，今天我就通过一篇文章，来和大伙仔细聊一聊这个问题。 SSM 中的配置要讲 Spring Boot 中的问题，我们得先回到 SSM 环境搭建中，一般来说，我们可以通过 &lt;mvc:resources /&gt; 节点来配置不拦截静态资源，如下： 123&lt;mvc:resources mapping=\"/js/**\" location=\"/js/\"/&gt;&lt;mvc:resources mapping=\"/css/**\" location=\"/css/\"/&gt;&lt;mvc:resources mapping=\"/html/**\" location=\"/html/\"/&gt; 由于这是一种Ant风格的路径匹配符，/** 表示可以匹配任意层级的路径，因此上面的代码也可以像下面这样简写： 1&lt;mvc:resources mapping=\"/**\" location=\"/\"/&gt; 这种配置是在 XML 中的配置，大家知道，SpringMVC 的配置除了在XML中配置，也可以在 Java 代码中配置，如果在Java代码中配置的话，我们只需要自定义一个类，继承自WebMvcConfigurationSupport即可： 12345678@Configuration@ComponentScan(basePackages = \"org.sang.javassm\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/**\").addResourceLocations(\"/\"); }} 重写 WebMvcConfigurationSupport 类中的addResourceHandlers方法，在该方法中配置静态资源位置即可，这里的含义和上面 xml 配置的含义一致，因此无需多说。这是我们传统的解决方案，在Spring Boot 中，其实配置方式和这个一脉相承，只是有一些自动化的配置了。 Spring Boot 中的配置在 Spring Boot 中，如果我们是从 https://start.spring.io 这个网站上创建的项目，或者使用 IntelliJ IDEA 中的 Spring Boot 初始化工具创建的项目，默认都会存在 resources/static 目录，很多小伙伴也知道静态资源只要放到这个目录下，就可以直接访问，除了这里还有没有其他可以放静态资源的位置呢？为什么放在这里就能直接访问了呢？这就是本文要讨论的问题了。 整体规划首先，在 Spring Boot 中，默认情况下，一共有5个位置可以放静态资源，五个路径分别是如下5个： classpath:/META-INF/resources/ classpath:/resources/ classpath:/static/ classpath:/public/ / 前四个目录好理解，分别对应了resources目录下不同的目录，第5个 / 是啥意思呢？我们知道，在 Spring Boot 项目中，默认是没有 webapp 这个目录的，当然我们也可以自己添加（例如在需要使用JSP的时候），这里第5个 / 其实就是表示 webapp 目录中的静态资源也不被拦截。如果同一个文件分别出现在五个目录下，那么优先级也是按照上面列出的顺序。 不过，虽然有5个存储目录，除了第5个用的比较少之外，其他四个，系统默认创建了 classpath:/static/ ， 正常情况下，我们只需要将我们的静态资源放到这个目录下即可，也不需要额外去创建其他静态资源目录，例如我在 classpath:/static/ 目录下放了一张名为1.png 的图片，那么我的访问路径是： 1http://localhost:8080/1.png 这里大家注意，请求地址中并不需要 static，如果加上了static反而多此一举会报404错误。很多人会觉得奇怪，为什么不需要添加 static呢？资源明明放在 static 目录下。其实这个效果很好实现，例如在SSM配置中，我们的静态资源拦截配置如果是下面这样： 1&lt;mvc:resources mapping=\"/**\" location=\"/static/\"/&gt; 如果我们是这样配置的话，请求地址如果是 http://localhost:8080/1.png 实际上系统会去 /static/1.png 目录下查找相关的文件。 所以我们理所当然的猜测，在 Spring Boot 中可能也是类似的配置。 源码解读胡适之先生说：“大胆猜想，小心求证”，我们这里就通过源码解读来看看 Spring Boot 中的静态资源到底是怎么配置的。 首先我们在 WebMvcAutoConfiguration 类中看到了 SpringMVC 自动化配置的相关的内容，找到了静态资源拦截的配置，如下： 可以看到这里静态资源的定义和我们前面提到的Java配置SSM中的配置非常相似，其中，this.mvcProperties.getStaticPathPattern() 方法对应的值是 “/**”，this.resourceProperties.getStaticLocations()方法返回了四个位置，分别是：”classpath:/META-INF/resources/“, “classpath:/resources/“,”classpath:/static/“, “classpath:/public/“，然后在getResourceLocations方法中，又添加了“/”，因此这里返回值一共有5个。其中，/表示webapp目录，即webapp中的静态文件也可以直接访问。静态资源的匹配路径按照定义路径优先级依次降低。因此这里的配置和我们前面提到的如出一辙。这样大伙就知道了为什么Spring Boot 中支持5个静态资源位置，同时也明白了为什么静态资源请求路径中不需要/static，因为在路径映射中已经自动的添加上了/static了。 自定义配置当然，这个是系统默认配置，如果我们并不想将资源放在系统默认的这五个位置上，也可以自定义静态资源位置和映射，自定义的方式也有两种，可以通过 application.properties 来定义，也可以在 Java 代码中来定义，下面分别来看。 application.properties在配置文件中定义的方式比较简单，如下： 12spring.resources.static-locations=classpath:/spring.mvc.static-path-pattern=/** 第一行配置表示定义资源位置，第二行配置表示定义请求 URL 规则。以上文的配置为例，如果我们这样定义了，表示可以将静态资源放在 resources目录下的任意地方，我们访问的时候当然也需要写完整的路径，例如在resources/static目录下有一张名为1.png 的图片，那么访问路径就是 http://localhost:8080/static/1.png ,注意此时的static不能省略。 Java 代码定义当然，在Spring Boot中我们也可以通过 Java代码来自定义，方式和 Java 配置的 SSM 比较类似，如下： 1234567@Configurationpublic class WebMVCConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/**\").addResourceLocations(\"classpath:/aaa/\"); }} 这里代码基本和前面一致，比较简单，不再赘述。 总结这里需要提醒大家的是，松哥见到有很多人用了 Thymeleaf 之后，会将静态资源也放在 resources/templates 目录下，注意，templates 目录并不是静态资源目录，它是一个放页面模板的位置（你看到的 Thymeleaf 模板虽然后缀为 .html，其实并不是静态资源）。好了，通过上面的讲解，相信大家对 Spring Boot 中静态资源的位置有一个深刻了解了，应该不会再在项目中出错了吧！","link":"/2019/0408/springboot-static-resources.html"},{"title":"Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！","text":"有的人可能会觉得题目有点夸张，其实不夸张，题目没有使用任何修辞手法！认真读完本文，你就知道松哥说的是对的了！ 在传统的单服务架构中，一般来说，只有一个服务器，那么不存在 Session 共享问题，但是在分布式/集群项目中，Session 共享则是一个必须面对的问题，先看一个简单的架构图： 在这样的架构中，会出现一些单服务中不存在的问题，例如客户端发起一个请求，这个请求到达 Nginx 上之后，被 Nginx 转发到 Tomcat A 上，然后在 Tomcat A 上往 session 中保存了一份数据，下次又来一个请求，这个请求被转发到 Tomcat B 上，此时再去 Session 中获取数据，发现没有之前的数据。对于这一类问题的解决，思路很简单，就是将各个服务之间需要共享的数据，保存到一个公共的地方（主流方案就是 Redis）： 当所有 Tomcat 需要往 Session 中写数据时，都往 Redis 中写，当所有 Tomcat 需要读数据时，都从 Redis 中读。这样，不同的服务就可以使用相同的 Session 数据了。 这样的方案，可以由开发者手动实现，即手动往 Redis 中存储数据，手动从 Redis 中读取数据，相当于使用一些 Redis 客户端工具来实现这样的功能，毫无疑问，手动实现工作量还是蛮大的。 一个简化的方案就是使用 Spring Session 来实现这一功能，Spring Session 就是使用 Spring 中的代理过滤器，将所有的 Session 操作拦截下来，自动的将数据 同步到 Redis 中，或者自动的从 Redis 中读取数据。 对于开发者来说，所有关于 Session 同步的操作都是透明的，开发者使用 Spring Session，一旦配置完成后，具体的用法就像使用一个普通的 Session 一样。 1 实战1.1 创建工程首先 创建一个 Spring Boot 工程，引入 Web、Spring Session 以及 Redis: 创建成功之后，pom.xml 文件如下： 1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 注意： 这里我使用的 Spring Boot 版本是 2.1.4 ，如果使用当前最新版 Spring Boot2.1.5 的话，除了上面这些依赖之外，需要额外添加 Spring Security 依赖（其他操作不受影响，仅仅只是多了一个依赖，当然也多了 Spring Security 的一些默认认证流程）。 1.2 配置 Redis1234spring.redis.host=192.168.66.128spring.redis.port=6379spring.redis.password=123spring.redis.database=0 这里的 Redis ，我虽然配置了四行，但是考虑到端口默认就是 6379 ，database 默认就是 0，所以真正要配置的，其实就是两行。 1.3 使用配置完成后 ，就可以使用 Spring Session 了，其实就是使用普通的 HttpSession ，其他的 Session 同步到 Redis 等操作，框架已经自动帮你完成了： 1234567891011121314@RestControllerpublic class HelloController { @Value(\"${server.port}\") Integer port; @GetMapping(\"/set\") public String set(HttpSession session) { session.setAttribute(\"user\", \"javaboy\"); return String.valueOf(port); } @GetMapping(\"/get\") public String get(HttpSession session) { return session.getAttribute(\"user\") + \":\" + port; }} 考虑到一会 Spring Boot 将以集群的方式启动 ，为了获取每一个请求到底是哪一个 Spring Boot 提供的服务，需要在每次请求时返回当前服务的端口号，因此这里我注入了 server.port 。 接下来 ，项目打包： 打包之后，启动项目的两个实例： 12java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8080java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8081 然后先访问 localhost:8080/set 向 8080 这个服务的 Session 中保存一个变量，访问完成后，数据就已经自动同步到 Redis 中 了 ： 然后，再调用 localhost:8081/get 接口，就可以获取到 8080 服务的 session 中的数据： 此时关于 session 共享的配置就已经全部完成了，session 共享的效果我们已经看到了，但是每次访问都是我自己手动切换服务实例，因此，接下来我们来引入 Nginx ，实现服务实例自动切换。 1.4 引入 Nginx很简单，进入 Nginx 的安装目录的 conf 目录下（默认是在 /usr/local/nginx/conf），编辑 nginx.conf 文件: 在这段配置中： upstream 表示配置上游服务器 javaboy.org 表示服务器集群的名字，这个可以随意取名字 upstream 里边配置的是一个个的单独服务 weight 表示服务的权重，意味者将有多少比例的请求从 Nginx 上转发到该服务上 location 中的 proxy_pass 表示请求转发的地址，/ 表示拦截到所有的请求，转发转发到刚刚配置好的服务集群中 proxy_redirect 表示设置当发生重定向请求时，nginx 自动修正响应头数据（默认是 Tomcat 返回重定向，此时重定向的地址是 Tomcat 的地址，我们需要将之修改使之成为 Nginx 的地址）。 配置完成后，将本地的 Spring Boot 打包好的 jar 上传到 Linux ，然后在 Linux 上分别启动两个 Spring Boot 实例： 12nohup java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8080 &amp;nohup java -jar sessionshare-0.0.1-SNAPSHOT.jar --server.port=8081 &amp; 其中 nohup 表示当终端关闭时，Spring Boot 不要停止运行 &amp; 表示让 Spring Boot 在后台启动 配置完成后，重启 Nginx： 1/usr/local/nginx/sbin/nginx -s reload Nginx 启动成功后，我们首先手动清除 Redis 上的数据，然后访问 192.168.66.128/set 表示向 session 中保存数据，这个请求首先会到达 Nginx 上，再由 Nginx 转发给某一个 Spring Boot 实例： 如上，表示端口为 8081 的 Spring Boot 处理了这个 /set 请求，再访问 /get 请求： 可以看到，/get 请求是被端口为 8080 的服务所处理的。 2 总结本文主要向大家介绍了 Spring Session 的使用，另外也涉及到一些 Nginx 的使用 ，虽然本文较长，但是实际上 Spring Session 的配置没啥。 我们写了一些代码，也做了一些配置，但是全都和 Spring Session 无关，配置是配置 Redis，代码就是普通的 HttpSession，和 Spring Session 没有任何关系！ 唯一和 Spring Session 相关的，可能就是我在一开始引入了 Spring Session 的依赖吧！ 如果大家没有在 SSM 架构中用过 Spring Session ，可能不太好理解我们在 Spring Boot 中使用 Spring Session 有多么方便，因为在 SSM 架构中，Spring Session 的使用要配置三个地方 ，一个是 web.xml 配置代理过滤器，然后在 Spring 容器中配置 Redis，最后再配置 Spring Session，步骤还是有些繁琐的，而 Spring Boot 中直接帮我们省去了这些繁琐的步骤！不用再去配置 Spring Session。 好了 ，本文就说到这里，本文相关案例我已经上传到 GitHub ，大家可以自行下载:https://github.com/lenve/javaboy-code-samples","link":"/2019/0604/springboot-springsession.html"},{"title":"Spring Boot 定义系统启动任务，你会几种方式？","text":"在 Servlet/Jsp 项目中，如果涉及到系统任务，例如在项目启动阶段要做一些数据初始化操作，这些操作有一个共同的特点，只在项目启动时进行，以后都不再执行，这里，容易想到web基础中的三大组件（ Servlet、Filter、Listener ）之一 Listener ，这种情况下，一般定义一个 ServletContextListener，然后就可以监听到项目启动和销毁，进而做出相应的数据初始化和销毁操作，例如下面这样： 12345678910public class MyListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent sce) { //在这里做数据初始化操作 } @Override public void contextDestroyed(ServletContextEvent sce) { //在这里做数据备份操作 }} 当然，这是基础 web 项目的解决方案，如果使用了 Spring Boot，那么我们可以使用更为简便的方式。Spring Boot 中针对系统启动任务提供了两种解决方案，分别是 CommandLineRunner 和 ApplicationRunner，分别来看。 CommandLineRunner使用 CommandLineRunner 时，首先自定义 MyCommandLineRunner1 并且实现 CommandLineRunner 接口： 1234567@Component@Order(100)public class MyCommandLineRunner1 implements CommandLineRunner { @Override public void run(String... args) throws Exception { }} 关于这段代码，我做如下解释： 首先通过 @Compoent 注解将 MyCommandLineRunner1 注册为Spring容器中的一个 Bean。 添加 @Order注解，表示这个启动任务的执行优先级，因为在一个项目中，启动任务可能有多个，所以需要有一个排序。@Order 注解中，数字越小，优先级越大，默认情况下，优先级的值为 Integer.MAX_VALUE，表示优先级最低。 在 run 方法中，写启动任务的核心逻辑，当项目启动时，run方法会被自动执行。 run 方法的参数，来自于项目的启动参数，即项目入口类中，main方法的参数会被传到这里。 此时启动项目，run方法就会被执行，至于参数，可以通过两种方式来传递，如果是在 IDEA 中，可以通过如下方式来配置参数： 另一种方式，则是将项目打包，在命令行中启动项目，然后启动时在命令行传入参数，如下： 1java -jar devtools-0.0.1-SNAPSHOT.jar 三国演义 西游记 注意，这里参数传递时没有key，直接写value即可，执行结果如下： ApplicationRunnerApplicationRunner 和 CommandLineRunner 功能一致，用法也基本一致，唯一的区别主要体现在对参数的处理上，ApplicationRunner 可以接收更多类型的参数（ApplicationRunner 除了可以接收 CommandLineRunner 的参数之外，还可以接收 key/value形式的参数）。 使用 ApplicationRunner ，自定义类实现 ApplicationRunner 接口即可，组件注册以及组件优先级的配置都和 CommandLineRunner 一致，如下： 123456789101112131415@Component@Order(98)public class MyApplicationRunner1 implements ApplicationRunner { @Override public void run(ApplicationArguments args) throws Exception { List&lt;String&gt; nonOptionArgs = args.getNonOptionArgs(); System.out.println(\"MyApplicationRunner1&gt;&gt;&gt;\"+nonOptionArgs); Set&lt;String&gt; optionNames = args.getOptionNames(); for (String key : optionNames) { System.out.println(\"MyApplicationRunner1&gt;&gt;&gt;\"+key + \":\" + args.getOptionValues(key)); } String[] sourceArgs = args.getSourceArgs(); System.out.println(\"MyApplicationRunner1&gt;&gt;&gt;\"+Arrays.toString(sourceArgs)); }} 当项目启动时，这里的 run 方法就会被自动执行，关于 run 方法的参数 ApplicationArguments ，我说如下几点： args.getNonOptionArgs();可以用来获取命令行中的无key参数（和CommandLineRunner一样）。 args.getOptionNames();可以用来获取所有key/value形式的参数的key。 args.getOptionValues(key));可以根据key获取key/value 形式的参数的value。 args.getSourceArgs(); 则表示获取命令行中的所有参数。 ApplicationRunner 定义完成后，传启动参数也是两种方式，参数类型也有两种，第一种和 CommandLineRunner 一致，第二种则是 –key=value 的形式，在 IDEA 中定义方式如下： 或者使用 如下启动命令： 1java -jar devtools-0.0.1-SNAPSHOT.jar 三国演义 西游记 --age=99 运行结果如下： 总结整体来说 ，这两种的用法的差异不大 ，主要体现在对参数的处理上，小伙伴可以根据项目中的实际情况选择合适的解决方案。","link":"/2019/0415/springboot-commandlinerunner.html"},{"title":"Spring Boot 中通过 CORS 解决跨域问题","text":"今天和小伙伴们来聊一聊通过 CORS 解决跨域问题。 同源策略很多人对跨域有一种误解，以为这是前端的事，和后端没关系，其实不是这样的，说到跨域，就不得不说说浏览器的同源策略。 同源策略是由 Netscape 提出的一个著名的安全策略，它是浏览器最核心也最基本的安全功能，现在所有支持 JavaScript 的浏览器都会使用这个策略。所谓同源是指协议、域名以及端口要相同。同源策略是基于安全方面的考虑提出来的，这个策略本身没问题，但是我们在实际开发中，由于各种原因又经常有跨域的需求，传统的跨域方案是 JSONP，JSONP 虽然能解决跨域但是有一个很大的局限性，那就是只支持 GET 请求，不支持其他类型的请求，而今天我们说的 CORS（跨域源资源共享）（CORS，Cross-origin resource sharing）是一个 W3C 标准，它是一份浏览器技术的规范，提供了 Web 服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，这是 JSONP 模式的现代版。 在 Spring 框架中，对于 CORS 也提供了相应的解决方案，今天我们就来看看 SpringBoot 中如何实现 CORS 。 实践接下来我们就来看看 Spring Boot 中如何实现这个东西。 首先创建两个普通的 Spring Boot 项目，这个就不用我多说，第一个命名为 provider 提供服务，第二个命名为 consumer 消费服务，第一个配置端口为 8080 ，第二个配置配置为 8081 ，然后在 provider 上提供两个 hello 接口，一个 get ，一个 post ，如下： 1234567891011@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @PostMapping(\"/hello\") public String hello2() { return \"post hello\"; }} 在 consumer 的 resources/static 目录下创建一个 html 文件，发送一个简单的 ajax 请求，如下： 12345678910111213141516&lt;div id=\"app\"&gt;&lt;/div&gt;&lt;input type=\"button\" onclick=\"btnClick()\" value=\"get_button\"&gt;&lt;input type=\"button\" onclick=\"btnClick2()\" value=\"post_button\"&gt;&lt;script&gt; function btnClick() { $.get('http://localhost:8080/hello', function (msg) { $(\"#app\").html(msg); }); } function btnClick2() { $.post('http://localhost:8080/hello', function (msg) { $(\"#app\").html(msg); }); }&lt;/script&gt; 然后分别启动两个项目，发送请求按钮，观察浏览器控制台如下： 1Access to XMLHttpRequest at &apos;http://localhost:8080/hello&apos; from origin &apos;http://localhost:8081&apos; has been blocked by CORS policy: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. 可以看到，由于同源策略的限制，请求无法发送成功。 使用 CORS 可以在前端代码不做任何修改的情况下，实现跨域，那么接下来看看在 provider 中如何配置。首先可以通过 @CrossOrigin 注解配置某一个方法接受某一个域的请求，如下： 1234567891011121314@RestControllerpublic class HelloController { @CrossOrigin(value = \"http://localhost:8081\") @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @CrossOrigin(value = \"http://localhost:8081\") @PostMapping(\"/hello\") public String hello2() { return \"post hello\"; }} 这个注解表示这两个接口接受来自 http://localhost:8081 地址的请求，配置完成后，重启 provider ，再次发送请求，浏览器控制台就不会报错了，consumer 也能拿到数据了。 此时观察浏览器请求网络控制台，可以看到响应头中多了如下信息： 这个表示服务端愿意接收来自 http://localhost:8081 的请求，拿到这个信息后，浏览器就不会再去限制本次请求的跨域了。 provider 上，每一个方法上都去加注解未免太麻烦了，在 Spring Boot 中，还可以通过全局配置一次性解决这个问题，全局配置只需要在配置类中重写 addCorsMappings 方法即可，如下： 12345678910@Configurationpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(\"/**\") .allowedOrigins(\"http://localhost:8081\") .allowedMethods(\"*\") .allowedHeaders(\"*\"); }} /** 表示本应用的所有方法都会去处理跨域请求， allowedMethods 表示允许通过的请求数，allowedHeaders 则表示允许的请求头。经过这样的配置之后，就不必在每个方法上单独配置跨域了。 存在的问题了解了整个 CORS 的工作过程之后，我们通过 Ajax 发送跨域请求，虽然用户体验提高了，但是也有潜在的威胁存在，常见的就是 CSRF（Cross-site request forgery）跨站请求伪造。跨站请求伪造也被称为 one-click attack 或者 session riding，通常缩写为 CSRF 或者 XSRF ，是一种挟制用户在当前已登录的 Web 应用程序上执行非本意的操作的攻击方法，举个例子： 假如一家银行用以运行转账操作的 URL 地址如下： http://icbc.com/aa?bb=cc ，那么，一个恶意攻击者可以在另一个网站上放置如下代码： &lt;img src=&quot;http://icbc.com/aa?bb=cc&quot;&gt; ，如果用户访问了恶意站点，而她之前刚访问过银行不久，登录信息尚未过期，那么她就会遭受损失。 基于此，浏览器在实际操作中，会对请求进行分类，分为简单请求，预先请求，带凭证的请求等，预先请求会首先发送一个 options 探测请求，和浏览器进行协商是否接受请求。默认情况下跨域请求是不需要凭证的，但是服务端可以配置要求客户端提供凭证，这样就可以有效避免 csrf 攻击。 好了，这个问题就说这么多。 本文案例我已上传到 GitHub，欢迎大家 star:https://github.com/lenve/javaboy-code-samples","link":"/2019/0613/springboot-cors.html"},{"title":"Spring Boot 整合 Shiro ，两种方式全总结！","text":"在 Spring Boot 中做权限管理，一般来说，主流的方案是 Spring Security ，但是，仅仅从技术角度来说，也可以使用 Shiro。 今天松哥就来和大家聊聊 Spring Boot 整合 Shiro 的话题！ 一般来说，Spring Security 和 Shiro 的比较如下： Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架 Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单 Spring Security 功能强大；Shiro 功能简单 … 虽然 Shiro 功能简单，但是也能满足大部分的业务场景。所以在传统的 SSM 项目中，一般来说，可以整合 Shiro。 在 Spring Boot 中，由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，当然也提供了 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。 这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。 在 Spring Boot 中整合 Shiro ，有两种不同的方案： 第一种就是原封不动的，将 SSM 整合 Shiro 的配置用 Java 重写一遍。 第二种就是使用 Shiro 官方提供的一个 Starter 来配置，但是，这个 Starter 并没有简化多少配置。 原生的整合 创建项目 创建一个 Spring Boot 项目，只需要添加 Web 依赖即可： 项目创建成功后，加入 Shiro 相关的依赖，完整的 pom.xml 文件中的依赖如下： 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 Realm 接下来我们来自定义核心组件 Realm： 1234567891011121314public class MyRealm extends AuthorizingRealm { @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) { return null; } @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { String username = (String) token.getPrincipal(); if (!\"javaboy\".equals(username)) { throw new UnknownAccountException(\"账户不存在!\"); } return new SimpleAuthenticationInfo(username, \"123\", getName()); }} 在 Realm 中实现简单的认证操作即可，不做授权，授权的具体写法和 SSM 中的 Shiro 一样，不赘述。这里的认证表示用户名必须是 javaboy ，用户密码必须是 123 ，满足这样的条件，就能登录成功！ 配置 Shiro 接下来进行 Shiro 的配置： 12345678910111213141516171819202122232425262728@Configurationpublic class ShiroConfig { @Bean MyRealm myRealm() { return new MyRealm(); } @Bean SecurityManager securityManager() { DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setRealm(myRealm()); return manager; } @Bean ShiroFilterFactoryBean shiroFilterFactoryBean() { ShiroFilterFactoryBean bean = new ShiroFilterFactoryBean(); bean.setSecurityManager(securityManager()); bean.setLoginUrl(\"/login\"); bean.setSuccessUrl(\"/index\"); bean.setUnauthorizedUrl(\"/unauthorizedurl\"); Map&lt;String, String&gt; map = new LinkedHashMap&lt;&gt;(); map.put(\"/doLogin\", \"anon\"); map.put(\"/**\", \"authc\"); bean.setFilterChainDefinitionMap(map); return bean; }} 在这里进行 Shiro 的配置主要配置 3 个 Bean ： 首先需要提供一个 Realm 的实例。 需要配置一个 SecurityManager，在 SecurityManager 中配置 Realm。 配置一个 ShiroFilterFactoryBean ，在 ShiroFilterFactoryBean 中指定路径拦截规则等。 配置登录和测试接口。 其中，ShiroFilterFactoryBean 的配置稍微多一些，配置含义如下： setSecurityManager 表示指定 SecurityManager。 setLoginUrl 表示指定登录页面。 setSuccessUrl 表示指定登录成功页面。 接下来的 Map 中配置了路径拦截规则，注意，要有序。 这些东西都配置完成后，接下来配置登录 Controller: 12345678910111213141516171819202122@RestControllerpublic class LoginController { @PostMapping(\"/doLogin\") public void doLogin(String username, String password) { Subject subject = SecurityUtils.getSubject(); try { subject.login(new UsernamePasswordToken(username, password)); System.out.println(\"登录成功!\"); } catch (AuthenticationException e) { e.printStackTrace(); System.out.println(\"登录失败!\"); } } @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @GetMapping(\"/login\") public String login() { return \"please login!\"; }} 测试时，首先访问 /hello 接口，由于未登录，所以会自动跳转到 /login 接口： 然后调用 /doLogin 接口完成登录： 再次访问 /hello 接口，就可以成功访问了： 使用 Shiro Starter上面这种配置方式实际上相当于把 SSM 中的 XML 配置拿到 Spring Boot 中用 Java 代码重新写了一遍，除了这种方式之外，我们也可以直接使用 Shiro 官方提供的 Starter 。 创建工程，和上面的一样 创建成功后，添加 shiro-spring-boot-web-starter ，这个依赖可以代替之前的 shiro-web 和 shiro-spring 两个依赖，pom.xml 文件如下： 1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring-boot-web-starter&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 Realm 这里的 Realm 和前面的一样，我就不再赘述。 配置 Shiro 基本信息 接下来在 application.properties 中配置 Shiro 的基本信息： 123456shiro.sessionManager.sessionIdCookieEnabled=trueshiro.sessionManager.sessionIdUrlRewritingEnabled=trueshiro.unauthorizedUrl=/unauthorizedurlshiro.web.enabled=trueshiro.successUrl=/indexshiro.loginUrl=/login 配置解释： 第一行表示是否允许将sessionId 放到 cookie 中 第二行表示是否允许将 sessionId 放到 Url 地址拦中 第三行表示访问未获授权的页面时，默认的跳转路径 第四行表示开启 shiro 第五行表示登录成功的跳转页面 第六行表示登录页面 配置 ShiroConfig 1234567891011121314151617181920@Configurationpublic class ShiroConfig { @Bean MyRealm myRealm() { return new MyRealm(); } @Bean DefaultWebSecurityManager securityManager() { DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setRealm(myRealm()); return manager; } @Bean ShiroFilterChainDefinition shiroFilterChainDefinition() { DefaultShiroFilterChainDefinition definition = new DefaultShiroFilterChainDefinition(); definition.addPathDefinition(\"/doLogin\", \"anon\"); definition.addPathDefinition(\"/**\", \"authc\"); return definition; }} 这里的配置和前面的比较像，但是不再需要 ShiroFilterFactoryBean 实例了，替代它的是 ShiroFilterChainDefinition ，在这里定义 Shiro 的路径匹配规则即可。 这里定义完之后，接下来的登录接口定义以及测试方法都和前面的一致，我就不再赘述了。大家可以参考上文。 总结本文主要向大家介绍了 Spring Boot 整合 Shiro 的两种方式，一种是传统方式的 Java 版，另一种则是使用 Shiro 官方提供的 Starter，两种方式，不知道大家有没有学会呢？ 本文案例，我已经上传到 GitHub ，欢迎大家 star：https://github.com/lenve/javaboy-code-samples 关于本文，有问题欢迎留言讨论。","link":"/2019/0611/springboot-shiro.html"},{"title":"Spring Boot中的yaml配置简介","text":"搞Spring Boot的小伙伴都知道，Spring Boot中的配置文件有两种格式，properties或者yaml，一般情况下，两者可以随意使用，选择自己顺手的就行了，那么这两者完全一样吗？肯定不是啦！本文就来和大伙重点介绍下yaml配置，最后再来看看yaml和properties配置有何区别。 狡兔三窟首先application.yaml在Spring Boot中可以写在四个不同的位置，分别是如下位置： 项目根目录下的config目录中 项目根目录下 classpath下的config目录中 classpath目录下 四个位置中的application.yaml文件的优先级按照上面列出的顺序依次降低。即如果有同一个属性在四个文件中都出现了，以优先级高的为准。 那么application.yaml是不是必须叫application.yaml这个名字呢？当然不是必须的。开发者可以自己定义yaml名字，自己定义的话，需要在项目启动时指定配置文件的名字，像下面这样： 当然这是在IntelliJ IDEA中直接配置的，如果项目已经打成jar包了，则在项目启动时加入如下参数： 1java -jar myproject.jar --spring.config.name=app 这样配置之后，在项目启动时，就会按照上面所说的四个位置按顺序去查找一个名为app.yaml的文件。当然这四个位置也不是一成不变的，也可以自己定义，有两种方式，一个是使用spring.config.location属性，另一个则是使用spring.config.additional-location这个属性，在第一个属性中，表示自己重新定义配置文件的位置，项目启动时就按照定义的位置去查找配置文件，这种定义方式会覆盖掉默认的四个位置，也可以使用第二种方式，第二种方式则表示在四个位置的基础上，再添加几个位置，新添加的位置的优先级大于原本的位置。 配置方式如下： 这里要注意，配置文件位置时，值一定要以/结尾。 数组注入yaml也支持数组注入，例如 1234my: servers: - dev.example.com - another.example.com 这段数据可以绑定到一个带Bean的数组中： 12345678910@ConfigurationProperties(prefix=\"my\")@Componentpublic class Config { private List&lt;String&gt; servers = new ArrayList&lt;String&gt;(); public List&lt;String&gt; getServers() { return this.servers; }} 项目启动后，配置中的数组会自动存储到servers集合中。当然，yaml不仅可以存储这种简单数据，也可以在集合中存储对象。例如下面这种： 123456redis: redisConfigs: - host: 192.168.66.128 port: 6379 - host: 192.168.66.129 port: 6380 这个可以被注入到如下类中： 123456@Component@ConfigurationProperties(prefix = \"redis\")public class RedisCluster { private List&lt;SingleRedisConfig&gt; redisConfigs; //省略getter/setter} 优缺点不同于properties文件的无序，yaml配置是有序的，这一点在有些配置中是非常有用的，例如在Spring Cloud Zuul的配置中，当我们配置代理规则时，顺序就显得尤为重要了。当然yaml配置也不是万能的，例如，yaml配置目前不支持@PropertySource注解。","link":"/2019/0416/springboot-yaml.html"},{"title":"Spring Data Redis 使用","text":"上文我们介绍了 Redis，在开发环境中，我们还有另外一个解决方案，那就是 Spring Data Redis 。本文我们就来看看这个东西。 本文是 Redis 系列的第十四篇文章，了解前面的文章有助于更好的理解本文： 1.Linux 上安装 Redis2.Redis 中的五种数据类型简介3.Redis 字符串 (STRING) 介绍4.Redis 字符串 (STRING) 中 BIT 相关命令5.Redis 列表与集合6.Redis 散列与有序集合7.Redis 中的发布订阅和事务8.Redis 快照持久化9.Redis 之 AOF 持久化10.Redis 主从复制(一)11.Redis 主从复制(二)12.Redis 集群搭建13.Jedis 使用 Spring Data Redis 介绍Spring Data Redis 是 Spring 官方推出，可以算是 Spring 框架集成 Redis 操作的一个子框架，封装了 Redis 的很多命令，可以很方便的使用 Spring 操作 Redis 数据库，Spring 对很多工具都提供了类似的集成，如 Spring Data MongDB、Spring Data JPA 等, Spring Data Redis 只是其中一种。 环境搭建要使用 SDR，首先需要搭建 Spring+SpringMVC 环境，由于这个不是本文的重点，因此这一步我直接略过，Spring+SpringMVC 环境搭建成功后，接下来我们要整合 SDR，首先需要添加如下依赖： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt;&lt;/dependency&gt; 然后创建在 resources 目录下创建 redis.properties 文件作为 redis 的配置文件，如下： 123456redis.host=192.168.248.128redis.port=6379redis.maxIdle=300redis.maxTotal=600redis.maxWait=1000redis.testOnBorrow=true 在 spring 的配置文件中，添加如下 bean： 1234567891011121314151617181920212223242526&lt;!--引入redis.properties文件--&gt;&lt;context:property-placeholder location=\"classpath:redis.properties\"/&gt;&lt;!--配置连接池信息--&gt;&lt;bean class=\"redis.clients.jedis.JedisPoolConfig\" id=\"poolConfig\"&gt; &lt;property name=\"maxIdle\" value=\"${redis.maxIdle}\"/&gt; &lt;property name=\"maxTotal\" value=\"${redis.maxTotal}\"/&gt; &lt;property name=\"maxWaitMillis\" value=\"${redis.maxWait}\"/&gt; &lt;property name=\"testOnBorrow\" value=\"${redis.testOnBorrow}\"/&gt;&lt;/bean&gt;&lt;!--配置基本连接信息--&gt;&lt;bean class=\"org.springframework.data.redis.connection.jedis.JedisConnectionFactory\" id=\"connectionFactory\"&gt; &lt;property name=\"hostName\" value=\"${redis.host}\"/&gt; &lt;property name=\"port\" value=\"${redis.port}\"/&gt; &lt;property name=\"poolConfig\" ref=\"poolConfig\"/&gt;&lt;/bean&gt;&lt;!--配置RedisTemplate--&gt;&lt;bean class=\"org.springframework.data.redis.core.RedisTemplate\" id=\"redisTemplate\"&gt; &lt;property name=\"connectionFactory\" ref=\"connectionFactory\"/&gt; &lt;!--key和value要进行序列化，否则存储对象时会出错--&gt; &lt;property name=\"keySerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt; &lt;property name=\"valueSerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.JdkSerializationRedisSerializer\"/&gt; &lt;/property&gt;&lt;/bean&gt; 好了，在 Spring 中配置了 redisTemplate 之后，接下来我们就可以在 Dao 层注入 redisTemplate 进而使用了。 接下来我们首先创建实体类 User ，注意 User 一定要可序列化： 123456public class User implements Serializable{ private String username; private String password; private String id; //get/set省略} 然后在 Dao 层实现数据的添加和获取，如下： 1234567891011121314151617181920212223@Repositorypublic class HelloDao { @Autowired RedisTemplate redisTemplate; public void set(String key, String value) { ValueOperations ops = redisTemplate.opsForValue(); ops.set(key, value); } public String get(String key) { ValueOperations ops = redisTemplate.opsForValue(); return ops.get(key).toString(); } public void setuser(User user) { ValueOperations ops = redisTemplate.opsForValue(); ops.set(user.getId(), user); } public User getuser(String id) { ValueOperations&lt;String, User&gt; ops = redisTemplate.opsForValue(); User user = ops.get(id); System.out.println(user); return user; }} SDR 官方文档中对 Redistemplate 的介绍，通过 Redistemplate 可以调用 ValueOperations 和 ListOperations 等等方法，分别是对 Redis 命令的高级封装。但是 ValueOperations 等等这些命令最终是要转化成为 RedisCallback 来执行的。也就是说通过使用 RedisCallback 可以实现更强的功能。 最后，给大家展示下我的 Service 和 Controller ，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Servicepublic class HelloService { @Autowired HelloDao helloDao; public void set(String key, String value) { helloDao.set(key,value); } public String get(String key) { return helloDao.get(key); } public void setuser(User user) { helloDao.setuser(user); } public String getuser(String id) { String s = helloDao.getuser(id).toString(); return s; }}Controller：@Controllerpublic class HelloController { @Autowired HelloService helloService; @RequestMapping(\"/set\") @ResponseBody public void set(String key, String value) { helloService.set(key, value); } @RequestMapping(\"/get\") @ResponseBody public String get(String key) { return helloService.get(key); } @RequestMapping(\"/setuser\") @ResponseBody public void setUser() { User user = new User(); user.setId(\"1\"); user.setUsername(\"深圳\"); user.setPassword(\"sang\"); helloService.setuser(user); } @RequestMapping(value = \"/getuser\",produces = \"text/html;charset=UTF-8\") @ResponseBody public String getUser() { return helloService.getuser(\"1\"); }} 测试过程就不再展示了，小伙伴们可以用 POSTMAN 等工具自行测试。 好了，Spring Data Redis 我们就说到这里，有问题欢迎留言讨论。","link":"/2019/0615/springdata-redis.html"},{"title":"Spring Boot中通过CORS解决跨域问题","text":"今天和小伙伴们来聊一聊通过CORS解决跨域问题。 同源策略很多人对跨域有一种误解，以为这是前端的事，和后端没关系，其实不是这样的，说到跨域，就不得不说说浏览器的同源策略。同源策略是由Netscape提出的一个著名的安全策略，它是浏览器最核心也最基本的安全功能，现在所有支持JavaScript的浏览器都会使用这个策略。所谓同源是指协议、域名以及端口要相同。同源策略是基于安全方面的考虑提出来的，这个策略本身没问题，但是我们在实际开发中，由于各种原因又经常有跨域的需求，传统的跨域方案是JSONP，JSONP虽然能解决跨域但是有一个很大的局限性，那就是只支持GET请求，不支持其他类型的请求，而今天我们说的CORS（跨域源资源共享）（CORS，Cross-origin resource sharing）是一个W3C标准，它是一份浏览器技术的规范，提供了Web服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，这是JSONP模式的现代版。在Spring框架中，对于CORS也提供了相应的解决方案，今天我们就来看看SpringBoot中如何实现CORS。 实践接下来我们就来看看Spring Boot中如何实现这个东西。 首先创建两个普通的SpringBoot项目，这个就不用我多说，第一个命名为provider提供服务，第二个命名为consumer消费服务，第一个配置端口为8080，第二个配置配置为8081，然后在provider上提供两个hello接口，一个get，一个post，如下： 1234567891011@RestControllerpublic class HelloController { @GetMapping(&quot;/hello&quot;) public String hello() { return &quot;hello&quot;; } @PostMapping(&quot;/hello&quot;) public String hello2() { return &quot;post hello&quot;; }} 在consumer的resources/static目录下创建一个html文件，发送一个简单的ajax请求，如下： 12345678910111213141516&lt;div id=&quot;app&quot;&gt;&lt;/div&gt;&lt;input type=&quot;button&quot; onclick=&quot;btnClick()&quot; value=&quot;get_button&quot;&gt;&lt;input type=&quot;button&quot; onclick=&quot;btnClick2()&quot; value=&quot;post_button&quot;&gt;&lt;script&gt; function btnClick() { $.get(&apos;http://localhost:8080/hello&apos;, function (msg) { $(&quot;#app&quot;).html(msg); }); } function btnClick2() { $.post(&apos;http://localhost:8080/hello&apos;, function (msg) { $(&quot;#app&quot;).html(msg); }); }&lt;/script&gt; 然后分别启动两个项目，发送请求按钮，观察浏览器控制台如下： 1Access to XMLHttpRequest at &apos;http://localhost:8080/hello&apos; from origin &apos;http://localhost:8081&apos; has been blocked by CORS policy: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. 可以看到，由于同源策略的限制，请求无法发送成功。 使用CORS可以在前端代码不做任何修改的情况下，实现跨域，那么接下来看看在provider中如何配置。首先可以通过@CrossOrigin注解配置某一个方法接受某一个域的请求，如下： 1234567891011121314@RestControllerpublic class HelloController { @CrossOrigin(value = &quot;http://localhost:8081&quot;) @GetMapping(&quot;/hello&quot;) public String hello() { return &quot;hello&quot;; } @CrossOrigin(value = &quot;http://localhost:8081&quot;) @PostMapping(&quot;/hello&quot;) public String hello2() { return &quot;post hello&quot;; }} 这个注解表示这两个接口接受来自http://localhost:8081地址的请求，配置完成后，重启provider，再次发送请求，浏览器控制台就不会报错了，consumer也能拿到数据了。 此时观察浏览器请求网络控制台，可以看到响应头中多了如下信息： 这个表示服务端愿意接收来自http://localhost:8081的请求，拿到这个信息后，浏览器就不会再去限制本次请求的跨域了。 provider上，每一个方法上都去加注解未免太麻烦了，在Spring Boot中，还可以通过全局配置一次性解决这个问题，全局配置只需要在配置类中重写addCorsMappings方法即可，如下： 12345678910@Configurationpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;http://localhost:8081&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;); }} /**表示本应用的所有方法都会去处理跨域请求，allowedMethods表示允许通过的请求数，allowedHeaders则表示允许的请求头。经过这样的配置之后，就不必在每个方法上单独配置跨域了。 存在的问题了解了整个CORS的工作过程之后，我们通过Ajax发送跨域请求，虽然用户体验提高了，但是也有潜在的威胁存在，常见的就是CSRF（Cross-site request forgery）跨站请求伪造。跨站请求伪造也被称为one-click attack 或者 session riding，通常缩写为CSRF或者XSRF，是一种挟制用户在当前已登录的Web应用程序上执行非本意的操作的攻击方法，举个例子： 假如一家银行用以运行转账操作的URL地址如下：http://icbc.com/aa?bb=cc，那么，一个恶意攻击者可以在另一个网站上放置如下代码：&lt;img src=&quot;http://icbc.com/aa?bb=cc&quot;&gt;，如果用户访问了恶意站点，而她之前刚访问过银行不久，登录信息尚未过期，那么她就会遭受损失。 基于此，浏览器在实际操作中，会对请求进行分类，分为简单请求，预先请求，带凭证的请求等，预先请求会首先发送一个options探测请求，和浏览器进行协商是否接受请求。默认情况下跨域请求是不需要凭证的，但是服务端可以配置要求客户端提供凭证，这样就可以有效避免csrf攻击。 好了，这个问题就说这么多，关于springboot中cors，还有一个小小的视频教程，加入我的知识星球免费观看。","link":"/2019/0412/springboot-cors.html"},{"title":"Spring Boot 操作 Redis，三种方案全解析！","text":"在 Redis 出现之前，我们的缓存框架各种各样，有了 Redis ，缓存方案基本上都统一了，关于 Redis，松哥之前有一个系列教程，尚不了解 Redis 的小伙伴可以参考这个教程： Redis 教程合集 使用 Java 操作 Redis 的方案很多，Jedis 是目前较为流行的一种方案，除了 Jedis ，还有很多其他解决方案，如下： 除了这些方案之外，还有一个使用也相当多的方案，就是 Spring Data Redis。 在传统的 SSM 中，需要开发者自己来配置 Spring Data Redis ，这个配置比较繁琐，主要配置 3 个东西：连接池、连接器信息以及 key 和 value 的序列化方案。 在 Spring Boot 中，默认集成的 Redis 就是 Spring Data Redis，默认底层的连接池使用了 lettuce ，开发者可以自行修改为自己的熟悉的，例如 Jedis。 Spring Data Redis 针对 Redis 提供了非常方便的操作模板 RedisTemplate 。这是 Spring Data 擅长的事情，那么接下来我们就来看看 Spring Boot 中 Spring Data Redis 的具体用法。 方案一：Spring Data Redis创建工程创建工程，引入 Redis 依赖： 创建成功后，还需要手动引入 commos-pool2 的依赖，因此最终完整的 pom.xml 依赖如下： 1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 这里主要就是引入了 Spring Data Redis + 连接池。 配置 Redis 信息接下来配置 Redis 的信息，信息包含两方面，一方面是 Redis 的基本信息，另一方面则是连接池信息: 123456789spring.redis.database=0spring.redis.password=123spring.redis.port=6379spring.redis.host=192.168.66.128spring.redis.lettuce.pool.min-idle=5spring.redis.lettuce.pool.max-idle=10spring.redis.lettuce.pool.max-active=8spring.redis.lettuce.pool.max-wait=1msspring.redis.lettuce.shutdown-timeout=100ms 自动配置当开发者在项目中引入了 Spring Data Redis ，并且配置了 Redis 的基本信息，此时，自动化配置就会生效。 我们从 Spring Boot 中 Redis 的自动化配置类中就可以看出端倪： 12345678910111213141516171819202122@Configuration@ConditionalOnClass(RedisOperations.class)@EnableConfigurationProperties(RedisProperties.class)@Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class })public class RedisAutoConfiguration { @Bean @ConditionalOnMissingBean(name = \"redisTemplate\") public RedisTemplate&lt;Object, Object&gt; redisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean public StringRedisTemplate stringRedisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; }} 这个自动化配置类很好理解： 首先标记这个是一个配置类，同时该配置在 RedisOperations 存在的情况下才会生效(即项目中引入了 Spring Data Redis) 然后导入在 application.properties 中配置的属性 然后再导入连接池信息（如果存在的话） 最后，提供了两个 Bean ，RedisTemplate 和 StringRedisTemplate ，其中 StringRedisTemplate 是 RedisTemplate 的子类，两个的方法基本一致，不同之处主要体现在操作的数据类型不同，RedisTemplate 中的两个泛型都是 Object ，意味者存储的 key 和 value 都可以是一个对象，而 StringRedisTemplate 的 两个泛型都是 String ，意味者 StringRedisTemplate 的 key 和 value 都只能是字符串。如果开发者没有提供相关的 Bean ，这两个配置就会生效，否则不会生效。 使用接下来，可以直接在 Service 中注入 StringRedisTemplate 或者 RedisTemplate 来使用： 1234567891011@Servicepublic class HelloService { @Autowired RedisTemplate redisTemplate; public void hello() { ValueOperations ops = redisTemplate.opsForValue(); ops.set(\"k1\", \"v1\"); Object k1 = ops.get(\"k1\"); System.out.println(k1); }} Redis 中的数据操作，大体上来说，可以分为两种： 针对 key 的操作，相关的方法就在 RedisTemplate 中 针对具体数据类型的操作，相关的方法需要首先获取对应的数据类型，获取相应数据类型的操作方法是 opsForXXX 调用该方法就可以将数据存储到 Redis 中去了，如下： k1 前面的字符是由于使用了 RedisTemplate 导致的，RedisTemplate 对 key 进行序列化之后的结果。 RedisTemplate 中，key 默认的序列化方案是 JdkSerializationRedisSerializer 。 而在 StringRedisTemplate 中，key 默认的序列化方案是 StringRedisSerializer ，因此，如果使用 StringRedisTemplate ，默认情况下 key 前面不会有前缀。 不过开发者也可以自行修改 RedisTemplate 中的序列化方案，如下: 123456789101112@Servicepublic class HelloService { @Autowired RedisTemplate redisTemplate; public void hello() { redisTemplate.setKeySerializer(new StringRedisSerializer()); ValueOperations ops = redisTemplate.opsForValue(); ops.set(\"k1\", \"v1\"); Object k1 = ops.get(\"k1\"); System.out.println(k1); }} 当然也可以直接使用 StringRedisTemplate： 1234567891011@Servicepublic class HelloService { @Autowired StringRedisTemplate stringRedisTemplate; public void hello2() { ValueOperations ops = stringRedisTemplate.opsForValue(); ops.set(\"k2\", \"v2\"); Object k1 = ops.get(\"k2\"); System.out.println(k1); }} 另外需要注意 ，Spring Boot 的自动化配置，只能配置单机的 Redis ，如果是 Redis 集群，则所有的东西都需要自己手动配置，关于如何操作 Redis 集群，松哥以后再来和大家分享。 方案二：Spring Cache通过 Spring Cache 的形式来操作 Redis，Spring Cache 统一了缓存江湖的门面，这种方案，松哥之前有过一篇专门的文章介绍，小伙伴可以移步这里：Spring Boot中，Redis缓存还能这么用！。 方案三：回归原始时代第三种方案，就是直接使用 Jedis 或者 其他的客户端工具来操作 Redis ，这种方案在 Spring Boot 中也是支持的，虽然操作麻烦，但是支持，这种操作松哥之前也有介绍的文章，因此这里就不再赘述了，可以参考 Jedis 使用。 总结Spring Boot 中，Redis 的操作，这里松哥给大家总结了三种方案，实际上前两个使用广泛一些，直接使用 Jedis 还是比较少，基本上 Spring Boot 中没见过有人直接这么搞。 好了，本文就说到这里，有问题欢迎留言讨论。","link":"/2019/0603/springboot-redis.html"},{"title":"Spring Boot中，Redis缓存还能这么用！","text":"经过Spring Boot的整合封装与自动化配置，在Spring Boot中整合Redis已经变得非常容易了，开发者只需要引入Spring Data Redis依赖，然后简单配下redis的基本信息，系统就会提供一个RedisTemplate供开发者使用，但是今天松哥想和大伙聊的不是这种用法，而是结合Cache的用法。Spring3.1中开始引入了令人激动的Cache，在Spring Boot中，可以非常方便的使用Redis来作为Cache的实现，进而实现数据的缓存。 工程创建首先创建一个Spring Boot工程，注意创建的时候需要引入三个依赖，web、cache以及redis，如下图： 对应的依赖内容如下： 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 基本配置工程创建好之后，首先需要简单配置一下Redis，Redis的基本信息，另外，这里要用到Cache，因此还需要稍微配置一下Cache，如下： 1234spring.redis.port=6380spring.redis.host=192.168.66.128spring.cache.cache-names=c1 简单起见，这里我只是配置了Redis的端口和地址，然后给缓存取了一个名字，这个名字在后文会用到。 另外，还需要在配置类上添加如下代码，表示开启缓存： 123456789@SpringBootApplication@EnableCachingpublic class RediscacheApplication { public static void main(String[] args) { SpringApplication.run(RediscacheApplication.class, args); }} 完成了这些配置之后，Spring Boot就会自动帮我们在后台配置一个RedisCacheManager，相关的配置是在org.springframework.boot.autoconfigure.cache.RedisCacheConfiguration类中完成的。部分源码如下： 123456789101112131415161718192021@Configuration@ConditionalOnClass(RedisConnectionFactory.class)@AutoConfigureAfter(RedisAutoConfiguration.class)@ConditionalOnBean(RedisConnectionFactory.class)@ConditionalOnMissingBean(CacheManager.class)@Conditional(CacheCondition.class)class RedisCacheConfiguration { @Bean public RedisCacheManager cacheManager(RedisConnectionFactory redisConnectionFactory, ResourceLoader resourceLoader) { RedisCacheManagerBuilder builder = RedisCacheManager .builder(redisConnectionFactory) .cacheDefaults(determineConfiguration(resourceLoader.getClassLoader())); List&lt;String&gt; cacheNames = this.cacheProperties.getCacheNames(); if (!cacheNames.isEmpty()) { builder.initialCacheNames(new LinkedHashSet&lt;&gt;(cacheNames)); } return this.customizerInvoker.customize(builder.build()); }} 看类上的注解，发现在万事俱备的情况下，系统会自动提供一个RedisCacheManager的Bean，这个RedisCacheManager间接实现了Spring中的Cache接口，有了这个Bean，我们就可以直接使用Spring中的缓存注解和接口了，而缓存数据则会被自动存储到Redis上。在单机的Redis中，这个Bean系统会自动提供，如果是Redis集群，这个Bean需要开发者来提供（后面的文章会讲到）。 缓存使用这里主要向小伙伴们介绍缓存中几个核心的注解使用。 @CacheConfig这个注解在类上使用，用来描述该类中所有方法使用的缓存名称，当然也可以不使用该注解，直接在具体的缓存注解上配置名称，示例代码如下： 1234@Service@CacheConfig(cacheNames = \"c1\")public class UserService {} @Cacheable这个注解一般加在查询方法上，表示将一个方法的返回值缓存起来，默认情况下，缓存的key就是方法的参数，缓存的value就是方法的返回值。示例代码如下： 12345@Cacheable(key = \"#id\")public User getUserById(Integer id,String username) { System.out.println(\"getUserById\"); return getUserFromDBById(id);} 当有多个参数时，默认就使用多个参数来做key，如果只需要其中某一个参数做key，则可以在@Cacheable注解中，通过key属性来指定key，如上代码就表示只使用id作为缓存的key，如果对key有复杂的要求，可以自定义keyGenerator。当然，Spring Cache中提供了root对象，可以在不定义keyGenerator的情况下实现一些复杂的效果： @CachePut这个注解一般加在更新方法上，当数据库中的数据更新后，缓存中的数据也要跟着更新，使用该注解，可以将方法的返回值自动更新到已经存在的key上，示例代码如下： 1234@CachePut(key = \"#user.id\")public User updateUserById(User user) { return user;} @CacheEvict这个注解一般加在删除方法上，当数据库中的数据删除后，相关的缓存数据也要自动清除，该注解在使用的时候也可以配置按照某种条件删除（condition属性）或者或者配置清除所有缓存（allEntries属性），示例代码如下： 1234@CacheEvict()public void deleteUserById(Integer id) { //在这里执行删除操作， 删除是去数据库中删除} 总结在Spring Boot中，使用Redis缓存，既可以使用RedisTemplate自己来实现，也可以使用使用这种方式，这种方式是Spring Cache提供的统一接口，实现既可以是Redis，也可以是Ehcache或者其他支持这种规范的缓存框架。从这个角度来说，Spring Cache和Redis、Ehcache的关系就像JDBC与各种数据库驱动的关系。 好了，关于这个问题就说到这里，有问题欢迎留言讨论。","link":"/2019/0416/springboot-redis.html"},{"title":"Spring Boot整合Jpa多数据源","text":"本文是Spring Boot整合数据持久化方案的最后一篇，主要和大伙来聊聊Spring Boot整合Jpa多数据源问题。在Spring Boot整合JbdcTemplate多数据源、Spring Boot整合MyBatis多数据源以及Spring Boot整合Jpa多数据源这三个知识点中，整合Jpa多数据源算是最复杂的一种，也是很多人在配置时最容易出错的一种。本文大伙就跟着松哥的教程，一步一步整合Jpa多数据源。 工程创建首先是创建一个Spring Boot工程，创建时添加基本的Web、Jpa以及MySQL依赖，如下： 创建完成后，添加Druid依赖，这里和前文的要求一样，要使用专为Spring Boot打造的Druid，大伙可能发现了，如果整合多数据源一定要使用这个依赖，因为这个依赖中才有DruidDataSourceBuilder，最后还要记得锁定数据库依赖的版本，因为可能大部分人用的还是5.x的MySQL而不是8.x。完整依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 如此之后，工程就创建成功了。 基本配置在基本配置中，我们首先来配置多数据源基本信息以及DataSource，首先在application.properties中添加如下配置信息： 123456789101112131415161718# 数据源一spring.datasource.one.username=rootspring.datasource.one.password=rootspring.datasource.one.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=UTF-8spring.datasource.one.type=com.alibaba.druid.pool.DruidDataSource# 数据源二spring.datasource.two.username=rootspring.datasource.two.password=rootspring.datasource.two.url=jdbc:mysql:///test02?useUnicode=true&amp;characterEncoding=UTF-8spring.datasource.two.type=com.alibaba.druid.pool.DruidDataSource# Jpa配置spring.jpa.properties.database=mysqlspring.jpa.properties.show-sql=truespring.jpa.properties.database-platform=mysqlspring.jpa.properties.hibernate.ddl-auto=updatespring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL57Dialect 这里Jpa的配置和上文相比key中多了properties，多数据源的配置和前文一致，然后接下来配置两个DataSource，如下： 1234567891011121314@Configurationpublic class DataSourceConfig { @Bean @ConfigurationProperties(prefix = \"spring.datasource.one\") @Primary DataSource dsOne() { return DruidDataSourceBuilder.create().build(); } @Bean @ConfigurationProperties(prefix = \"spring.datasource.two\") DataSource dsTwo() { return DruidDataSourceBuilder.create().build(); }} 这里的配置和前文的多数据源配置基本一致，但是注意多了一个在Spring中使用较少的注解@Primary，这个注解一定不能少，否则在项目启动时会出错，@Primary表示当某一个类存在多个实例时，优先使用哪个实例。 好了，这样，DataSource就有了。 多数据源配置接下来配置Jpa的基本信息，这里两个数据源，我分别在两个类中来配置，先来看第一个配置： 123456789101112131415161718192021222324252627@Configuration@EnableJpaRepositories(basePackages = &quot;org.sang.jpa.dao&quot;,entityManagerFactoryRef = &quot;localContainerEntityManagerFactoryBeanOne&quot;,transactionManagerRef = &quot;platformTransactionManagerOne&quot;)public class JpaConfigOne { @Autowired @Qualifier(value = &quot;dsOne&quot;) DataSource dsOne; @Autowired JpaProperties jpaProperties; @Bean @Primary LocalContainerEntityManagerFactoryBean localContainerEntityManagerFactoryBeanOne(EntityManagerFactoryBuilder builder) { return builder.dataSource(dsOne) .packages(&quot;org.sang.jpa.model&quot;) .properties(jpaProperties.getProperties()) .persistenceUnit(&quot;pu1&quot;) .build(); } @Bean PlatformTransactionManager platformTransactionManagerOne(EntityManagerFactoryBuilder builder) { LocalContainerEntityManagerFactoryBean factoryBeanOne = localContainerEntityManagerFactoryBeanOne(builder); return new JpaTransactionManager(factoryBeanOne.getObject()); }} 首先这里注入dsOne，再注入JpaProperties，JpaProperties是系统提供的一个实例，里边的数据就是我们在application.properties中配置的jpa相关的配置。然后我们提供两个Bean，分别是LocalContainerEntityManagerFactoryBean和PlatformTransactionManager事务管理器，不同于MyBatis和JdbcTemplate，在Jpa中，事务一定要配置。在提供LocalContainerEntityManagerFactoryBean的时候，需要指定packages，这里的packages指定的包就是这个数据源对应的实体类所在的位置，另外在这里配置类上通过@EnableJpaRepositories注解指定dao所在的位置，以及LocalContainerEntityManagerFactoryBean和PlatformTransactionManager分别对应的引用的名字。 好了，这样第一个就配置好了，第二个基本和这个类似，主要有几个不同点： dao的位置不同 persistenceUnit不同 相关bean的名称不同 注意实体类可以共用。 代码如下： 1234567891011121314151617181920212223242526@Configuration@EnableJpaRepositories(basePackages = \"org.sang.jpa.dao2\",entityManagerFactoryRef = \"localContainerEntityManagerFactoryBeanTwo\",transactionManagerRef = \"platformTransactionManagerTwo\")public class JpaConfigTwo { @Autowired @Qualifier(value = \"dsTwo\") DataSource dsTwo; @Autowired JpaProperties jpaProperties; @Bean LocalContainerEntityManagerFactoryBean localContainerEntityManagerFactoryBeanTwo(EntityManagerFactoryBuilder builder) { return builder.dataSource(dsTwo) .packages(\"org.sang.jpa.model\") .properties(jpaProperties.getProperties()) .persistenceUnit(\"pu2\") .build(); } @Bean PlatformTransactionManager platformTransactionManagerTwo(EntityManagerFactoryBuilder builder) { LocalContainerEntityManagerFactoryBean factoryBeanTwo = localContainerEntityManagerFactoryBeanTwo(builder); return new JpaTransactionManager(factoryBeanTwo.getObject()); }} 接下来，在对应位置分别提供相关的实体类和dao即可，数据源一的dao如下： 12345678package org.sang.jpa.dao;public interface UserDao extends JpaRepository&lt;User,Integer&gt; { List&lt;User&gt; getUserByAddressEqualsAndIdLessThanEqual(String address, Integer id); @Query(value = &quot;select * from t_user where id=(select max(id) from t_user)&quot;,nativeQuery = true) User maxIdUser();} 数据源二的dao如下： 12345678package org.sang.jpa.dao2;public interface UserDao2 extends JpaRepository&lt;User,Integer&gt; { List&lt;User&gt; getUserByAddressEqualsAndIdLessThanEqual(String address, Integer id); @Query(value = &quot;select * from t_user where id=(select max(id) from t_user)&quot;,nativeQuery = true) User maxIdUser();} 共同的实体类如下： 1234567891011package org.sang.jpa.model;@Entity(name = &quot;t_user&quot;)public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String username; private String address; //省略getter/setter} 到此，所有的配置就算完成了，接下来就可以在Service中注入不同的UserDao，不同的UserDao操作不同的数据源。 其实整合Jpa多数据源也不算难，就是有几个细节问题，这些细节问题解决，其实前面介绍的其他多数据源整个都差不多。 好了，欢迎大家加入松哥的星球，关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0407/springboot-jpa-multi.html"},{"title":"Spring Boot数据持久化之JdbcTemplate","text":"在Java领域，数据持久化有几个常见的方案，有Spring自带的JdbcTemplate、有MyBatis，还有JPA，在这些方案中，最简单的就是Spring自带的JdbcTemplate了，这个东西虽然没有MyBatis那么方便，但是比起最开始的Jdbc已经强了很多了，它没有MyBatis功能那么强大，当然也意味着它的使用比较简单，事实上，JdbcTemplate算是最简单的数据持久化方案了，本文就和大伙来说说这个东西的使用。 基本配置JdbcTemplate基本用法实际上很简单，开发者在创建一个SpringBoot项目时，除了选择基本的Web依赖，再记得选上Jdbc依赖，以及数据库驱动依赖即可，如下： 项目创建成功之后，记得添加Druid数据库连接池依赖（注意这里可以添加专门为Spring Boot打造的druid-spring-boot-starter，而不是我们一般在SSM中添加的Druid），所有添加的依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 项目创建完后，接下来只需要在application.properties中提供数据的基本配置即可，如下： 1234spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.username=rootspring.datasource.password=123spring.datasource.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=UTF-8 如此之后，所有的配置就算完成了，接下来就可以直接使用JdbcTemplate了？咋这么方便呢？其实这就是SpringBoot的自动化配置带来的好处，我们先说用法，一会来说原理。 基本用法首先我们来创建一个User Bean，如下： public class User { private Long id; private String username; private String address; //省略getter/setter } 然后来创建一个UserService类，在UserService类中注入JdbcTemplate，如下： @Service public class UserService { @Autowired JdbcTemplate jdbcTemplate; } 好了，如此之后，准备工作就算完成了。 增JdbcTemplate中，除了查询有几个API之外，增删改统一都使用update来操作，自己来传入SQL即可。例如添加数据，方法如下： public int addUser(User user) { return jdbcTemplate.update(\"insert into user (username,address) values (?,?);\", user.getUsername(), user.getAddress()); } update方法的返回值就是SQL执行受影响的行数。 这里只需要传入SQL即可，如果你的需求比较复杂，例如在数据插入的过程中希望实现主键回填，那么可以使用PreparedStatementCreator，如下： public int addUser2(User user) { KeyHolder keyHolder = new GeneratedKeyHolder(); int update = jdbcTemplate.update(new PreparedStatementCreator() { @Override public PreparedStatement createPreparedStatement(Connection connection) throws SQLException { PreparedStatement ps = connection.prepareStatement(\"insert into user (username,address) values (?,?);\", Statement.RETURN_GENERATED_KEYS); ps.setString(1, user.getUsername()); ps.setString(2, user.getAddress()); return ps; } }, keyHolder); user.setId(keyHolder.getKey().longValue()); System.out.println(user); return update; } 实际上这里就相当于完全使用了JDBC中的解决方案了，首先在构建PreparedStatement时传入Statement.RETURN_GENERATED_KEYS，然后传入KeyHolder，最终从KeyHolder中获取刚刚插入数据的id保存到user对象的id属性中去。 你能想到的JDBC的用法，在这里都能实现，Spring提供的JdbcTemplate虽然不如MyBatis，但是比起Jdbc还是要方便很多的。 删删除也是使用update API，传入你的SQL即可： public int deleteUserById(Long id) { return jdbcTemplate.update(\"delete from user where id=?\", id); } 当然你也可以使用PreparedStatementCreator。 改public int updateUserById(User user) { return jdbcTemplate.update(\"update user set username=?,address=? where id=?\", user.getUsername(), user.getAddress(),user.getId()); } 当然你也可以使用PreparedStatementCreator。 查查询的话，稍微有点变化，这里主要向大伙介绍query方法，例如查询所有用户： public List&lt;User&gt; getAllUsers() { return jdbcTemplate.query(\"select * from user\", new RowMapper&lt;User&gt;() { @Override public User mapRow(ResultSet resultSet, int i) throws SQLException { String username = resultSet.getString(\"username\"); String address = resultSet.getString(\"address\"); long id = resultSet.getLong(\"id\"); User user = new User(); user.setAddress(address); user.setUsername(username); user.setId(id); return user; } }); } 查询的时候需要提供一个RowMapper，就是需要自己手动映射，将数据库中的字段和对象的属性一一对应起来，这样。。。。嗯看起来有点麻烦，实际上，如果数据库中的字段和对象属性的名字一模一样的话，有另外一个简单的方案，如下： public List&lt;User&gt; getAllUsers2() { return jdbcTemplate.query(\"select * from user\", new BeanPropertyRowMapper&lt;&gt;(User.class)); } 至于查询时候传参也是使用占位符，这个和前文的一致，这里不再赘述。 其他除了这些基本用法之外，JdbcTemplate也支持其他用法，例如调用存储过程等，这些都比较容易，而且和Jdbc本身都比较相似，这里也就不做介绍了，有兴趣可以留言讨论。 原理分析那么在SpringBoot中，配置完数据库基本信息之后，就有了一个JdbcTemplate了，这个东西是从哪里来的呢？源码在org.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration类中，该类源码如下： @Configuration @ConditionalOnClass({ DataSource.class, JdbcTemplate.class }) @ConditionalOnSingleCandidate(DataSource.class) @AutoConfigureAfter(DataSourceAutoConfiguration.class) @EnableConfigurationProperties(JdbcProperties.class) public class JdbcTemplateAutoConfiguration { @Configuration static class JdbcTemplateConfiguration { private final DataSource dataSource; private final JdbcProperties properties; JdbcTemplateConfiguration(DataSource dataSource, JdbcProperties properties) { this.dataSource = dataSource; this.properties = properties; } @Bean @Primary @ConditionalOnMissingBean(JdbcOperations.class) public JdbcTemplate jdbcTemplate() { JdbcTemplate jdbcTemplate = new JdbcTemplate(this.dataSource); JdbcProperties.Template template = this.properties.getTemplate(); jdbcTemplate.setFetchSize(template.getFetchSize()); jdbcTemplate.setMaxRows(template.getMaxRows()); if (template.getQueryTimeout() != null) { jdbcTemplate .setQueryTimeout((int) template.getQueryTimeout().getSeconds()); } return jdbcTemplate; } } @Configuration @Import(JdbcTemplateConfiguration.class) static class NamedParameterJdbcTemplateConfiguration { @Bean @Primary @ConditionalOnSingleCandidate(JdbcTemplate.class) @ConditionalOnMissingBean(NamedParameterJdbcOperations.class) public NamedParameterJdbcTemplate namedParameterJdbcTemplate( JdbcTemplate jdbcTemplate) { return new NamedParameterJdbcTemplate(jdbcTemplate); } } } 从这个类中，大致可以看出，当当前类路径下存在DataSource和JdbcTemplate时，该类就会被自动配置，jdbcTemplate方法则表示，如果开发者没有自己提供一个JdbcOperations的实例的话，系统就自动配置一个JdbcTemplate Bean（JdbcTemplate是JdbcOperations接口的一个实现）。好了，不知道大伙有没有收获呢？ 关于JdbcTemplate，我还有一个小小视频，加入我的知识星球，免费观看： 加入我的星球，和众多大牛一起切磋技术推荐一个技术圈子，Java技能提升就靠它了。","link":"/2019/0406/jdbctemplate.html"},{"title":"Spring Security 登录添加验证码","text":"登录添加验证码是一个非常常见的需求，网上也有非常成熟的解决方案，其实，要是自己自定义登录实现这个并不难，但是如果需要在 Spring Security 框架中实现这个功能，还得稍费一点功夫，本文就和小伙伴来分享下在 Spring Security 框架中如何添加验证码。 关于 Spring Security 基本配置，这里就不再多说，小伙伴有不懂的可以参考我的书《SpringBoot+Vue全栈开发实战》，本文主要来看如何加入验证码功能。 准备验证码要有验证码，首先得先准备好验证码，本文采用 Java 自画的验证码，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 生成验证码的工具类 */public class VerifyCode { private int width = 100;// 生成验证码图片的宽度 private int height = 50;// 生成验证码图片的高度 private String[] fontNames = { \"宋体\", \"楷体\", \"隶书\", \"微软雅黑\" }; private Color bgColor = new Color(255, 255, 255);// 定义验证码图片的背景颜色为白色 private Random random = new Random(); private String codes = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"; private String text;// 记录随机字符串 /** * 获取一个随意颜色 * * @return */ private Color randomColor() { int red = random.nextInt(150); int green = random.nextInt(150); int blue = random.nextInt(150); return new Color(red, green, blue); } /** * 获取一个随机字体 * * @return */ private Font randomFont() { String name = fontNames[random.nextInt(fontNames.length)]; int style = random.nextInt(4); int size = random.nextInt(5) + 24; return new Font(name, style, size); } /** * 获取一个随机字符 * * @return */ private char randomChar() { return codes.charAt(random.nextInt(codes.length())); } /** * 创建一个空白的BufferedImage对象 * * @return */ private BufferedImage createImage() { BufferedImage image = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g2 = (Graphics2D) image.getGraphics(); g2.setColor(bgColor);// 设置验证码图片的背景颜色 g2.fillRect(0, 0, width, height); return image; } public BufferedImage getImage() { BufferedImage image = createImage(); Graphics2D g2 = (Graphics2D) image.getGraphics(); StringBuffer sb = new StringBuffer(); for (int i = 0; i &lt; 4; i++) { String s = randomChar() + \"\"; sb.append(s); g2.setColor(randomColor()); g2.setFont(randomFont()); float x = i * width * 1.0f / 4; g2.drawString(s, x, height - 15); } this.text = sb.toString(); drawLine(image); return image; } /** * 绘制干扰线 * * @param image */ private void drawLine(BufferedImage image) { Graphics2D g2 = (Graphics2D) image.getGraphics(); int num = 5; for (int i = 0; i &lt; num; i++) { int x1 = random.nextInt(width); int y1 = random.nextInt(height); int x2 = random.nextInt(width); int y2 = random.nextInt(height); g2.setColor(randomColor()); g2.setStroke(new BasicStroke(1.5f)); g2.drawLine(x1, y1, x2, y2); } } public String getText() { return text; } public static void output(BufferedImage image, OutputStream out) throws IOException { ImageIO.write(image, \"JPEG\", out); }} 这个工具类很常见，网上也有很多，就是画一个简单的验证码，通过流将验证码写到前端页面，提供验证码的 Controller 如下： 123456789101112@RestControllerpublic class VerifyCodeController { @GetMapping(\"/vercode\") public void code(HttpServletRequest req, HttpServletResponse resp) throws IOException { VerifyCode vc = new VerifyCode(); BufferedImage image = vc.getImage(); String text = vc.getText(); HttpSession session = req.getSession(); session.setAttribute(\"index_code\", text); VerifyCode.output(image, resp.getOutputStream()); }} 这里创建了一个 VerifyCode 对象，将生成的验证码字符保存到 session 中，然后通过流将图片写到前端，img标签如下： 1&lt;img src=\"/vercode\" alt=\"\"&gt; 展示效果如下： 自定义过滤器在登陆页展示验证码这个就不需要我多说了，接下来我们来看看如何自定义验证码处理器： 12345678910111213141516171819202122@Componentpublic class VerifyCodeFilter extends GenericFilterBean { private String defaultFilterProcessUrl = \"/doLogin\"; @Override public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; if (\"POST\".equalsIgnoreCase(request.getMethod()) &amp;&amp; defaultFilterProcessUrl.equals(request.getServletPath())) { // 验证码验证 String requestCaptcha = request.getParameter(\"code\"); String genCaptcha = (String) request.getSession().getAttribute(\"index_code\"); if (StringUtils.isEmpty(requestCaptcha)) throw new AuthenticationServiceException(\"验证码不能为空!\"); if (!genCaptcha.toLowerCase().equals(requestCaptcha.toLowerCase())) { throw new AuthenticationServiceException(\"验证码错误!\"); } } chain.doFilter(request, response); }} 自定义过滤器继承自 GenericFilterBean ，并实现其中的 doFilter 方法，在 doFilter 方法中，当请求方法是 POST ，并且请求地址是 /doLogin 时，获取参数中的 code 字段值，该字段保存了用户从前端页面传来的验证码，然后获取 session 中保存的验证码，如果用户没有传来验证码，则抛出验证码不能为空异常，如果用户传入了验证码，则判断验证码是否正确，如果不正确则抛出异常，否则执行 chain.doFilter(request, response); 使请求继续向下走。 配置最后在 Spring Security 的配置中，配置过滤器，如下： 12345678910111213141516171819@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Autowired VerifyCodeFilter verifyCodeFilter; ... ... @Override protected void configure(HttpSecurity http) throws Exception { http.addFilterBefore(verifyCodeFilter, UsernamePasswordAuthenticationFilter.class); http.authorizeRequests() .antMatchers(\"/admin/**\").hasRole(\"admin\") ... ... .permitAll() .and() .csrf().disable(); }} 这里只贴出了部分核心代码，即 http.addFilterBefore(verifyCodeFilter, UsernamePasswordAuthenticationFilter.class); ，如此之后，整个配置就算完成了。接下来在登录中，就需要传入验证码了，如果不传或者传错，都会抛出异常，例如不传的话，抛出如下异常： 好了，本文就先说到这里，有问题欢迎留言讨论。","link":"/2019/0613/springsecurity-verifycode.html"},{"title":"SpringBoot整合Swagger2，再也不用维护接口文档了！","text":"前后端分离后，维护接口文档基本上是必不可少的工作。一个理想的状态是设计好后，接口文档发给前端和后端，大伙按照既定的规则各自开发，开发好了对接上了就可以上线了。当然这是一种非常理想的状态，实际开发中却很少遇到这样的情况，接口总是在不断的变化之中，有变化就要去维护，做过的小伙伴都知道这件事有多么头大！还好，有一些工具可以减轻我们的工作量，Swagger2就是其中之一，至于其他类似功能但是却收费的软件，这里就不做过多介绍了。本文主要和大伙来聊下在Spring Boot中如何整合Swagger2。 工程创建当然，首先是创建一个Spring Boot项目，加入web依赖，创建成功后，加入两个Swagger2相关的依赖，完整的依赖如下： 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; Swagger2配置Swagger2的配置也是比较容易的，在项目创建成功之后，只需要开发者自己提供一个Docket的Bean即可，如下： 1234567891011121314151617181920@Configuration@EnableSwagger2public class SwaggerConfig { @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .pathMapping(\"/\") .select() .apis(RequestHandlerSelectors.basePackage(\"com.nvn.controller\")) .paths(PathSelectors.any()) .build().apiInfo(new ApiInfoBuilder() .title(\"SpringBoot整合Swagger\") .description(\"SpringBoot整合Swagger，详细信息......\") .version(\"9.0\") .contact(new Contact(\"啊啊啊啊\",\"blog.csdn.net\",\"aaa@gmail.com\")) .license(\"The Apache License\") .licenseUrl(\"http://www.baidu.com\") .build()); }} 这里提供一个配置类，首先通过@EnableSwagger2注解启用Swagger2，然后配置一个Docket Bean，这个Bean中，配置映射路径和要扫描的接口的位置，在apiInfo中，主要配置一下Swagger2文档网站的信息，例如网站的title，网站的描述，联系人的信息，使用的协议等等。 如此，Swagger2就算配置成功了，非常方便。 此时启动项目，输入http://localhost:8080/swagger-ui.html，能够看到如下页面，说明已经配置成功了： 创建接口接下来就是创建接口了，Swagger2相关的注解其实并不多，而且很容易懂，下面我来分别向小伙伴们举例说明： 123456789101112131415161718192021222324252627282930@RestController@Api(tags = \"用户管理相关接口\")@RequestMapping(\"/user\")public class UserController { @PostMapping(\"/\") @ApiOperation(\"添加用户的接口\") @ApiImplicitParams({ @ApiImplicitParam(name = \"username\", value = \"用户名\", defaultValue = \"李四\"), @ApiImplicitParam(name = \"address\", value = \"用户地址\", defaultValue = \"深圳\", required = true) } ) public RespBean addUser(String username, @RequestParam(required = true) String address) { return new RespBean(); } @GetMapping(\"/\") @ApiOperation(\"根据id查询用户的接口\") @ApiImplicitParam(name = \"id\", value = \"用户id\", defaultValue = \"99\", required = true) public User getUserById(@PathVariable Integer id) { User user = new User(); user.setId(id); return user; } @PutMapping(\"/{id}\") @ApiOperation(\"根据id更新用户的接口\") public User updateUserById(@RequestBody User user) { return user; }} 这里边涉及到多个API，我来向小伙伴们分别说明： @Api注解可以用来标记当前Controller的功能。 @ApiOperation注解用来标记一个方法的作用。 @ApiImplicitParam注解用来描述一个参数，可以配置参数的中文含义，也可以给参数设置默认值，这样在接口测试的时候可以避免手动输入。 如果有多个参数，则需要使用多个@ApiImplicitParam注解来描述，多个@ApiImplicitParam注解需要放在一个@ApiImplicitParams注解中。 需要注意的是，@ApiImplicitParam注解中虽然可以指定参数是必填的，但是却不能代替@RequestParam(required = true)，前者的必填只是在Swagger2框架内必填，抛弃了Swagger2，这个限制就没用了，所以假如开发者需要指定一个参数必填，@RequestParam(required = true)注解还是不能省略。 如果参数是一个对象（例如上文的更新接口），对于参数的描述也可以放在实体类中。例如下面一段代码： 12345678910@ApiModelpublic class User { @ApiModelProperty(value = \"用户id\") private Integer id; @ApiModelProperty(value = \"用户名\") private String username; @ApiModelProperty(value = \"用户地址\") private String address; //getter/setter} 好了，经过如上配置之后，接下来，刷新刚刚打开的页面，可以看到如下效果： 可以看到，所有的接口这里都列出来了，包括接口请求方式，接口地址以及接口的名字等，点开一个接口，可以看到如下信息： 可以看到，接口的参数，参数要求，参数默认值等等统统都展示出来了，参数类型下的query表示参数以key/value的形式传递，点击右上角的Try it out，就可以进行接口测试： 点击Execute按钮，表示发送请求进行测试。测试结果会展示在下面的Response中。 小伙伴们注意，参数类型下面的query表示参数以key/value的形式传递，这里的值也可能是body，body表示参数以请求体的方式传递，例如上文的更新接口，如下： 当然还有一种可能就是这里的参数为path，表示参数放在路径中传递，例如根据id查询用户的接口： 当然，除了这些之外，还有一些响应值的注解，都比较简单，小伙伴可以自己摸索下。 在Security中的配置如果我们的Spring Boot项目中集成了Spring Security，那么如果不做额外配置，Swagger2文档可能会被拦截，此时只需要在Spring Security的配置类中重写configure方法，添加如下过滤即可： 1234567@Overridepublic void configure(WebSecurity web) throws Exception { web.ignoring() .antMatchers(\"/swagger-ui.html\") .antMatchers(\"/v2/**\") .antMatchers(\"/swagger-resources/**\");} 如此之后，Swagger2文件就不需要认证就能访问了。不知道小伙伴们有没有看懂呢？有问题欢迎留言讨论。","link":"/2019/0416/springboot-swagger.html"},{"title":"Spring Boot多数据源配置之JdbcTemplate","text":"多数据源配置也算是一个常见的开发需求，Spring和SpringBoot中，对此都有相应的解决方案，不过一般来说，如果有多数据源的需求，我还是建议首选分布式数据库中间件MyCat去解决相关问题，之前有小伙伴在我的知识星球上提问，他的数据根据条件的不同，可能保存在四十多个不同的数据库中，怎么办？这种场景下使用多数据源其实就有些费事了，我给的建议是使用MyCat，然后分表策略使用sharding-by-intfile。当然如果一些简单的需求，还是可以使用多数据源的，Spring Boot中，JdbcTemplate、MyBatis以及Jpa都可以配置多数据源，本文就先和大伙聊一聊JdbcTemplate中多数据源的配置（关于JdbcTemplate的用法，如果还有小伙伴不了解，可以参考我的上篇文章）。 创建工程首先是创建工程，和前文一样，创建工程时，也是选择Web、Jdbc以及MySQL驱动，如下图： 创建成功之后，一定接下来手动添加Druid依赖，由于这里一会需要开发者自己配置DataSoruce，所以这里必须要使用druid-spring-boot-starter依赖，而不是传统的那个druid依赖，因为druid-spring-boot-starter依赖提供了DruidDataSourceBuilder类，这个可以用来构建一个DataSource实例，而传统的Druid则没有该类。完整的依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt; 配置数据源.接下来，在application.properties中配置数据源，不同于上文，这里的数据源需要配置两个，如下： spring.datasource.one.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=utf-8 spring.datasource.one.username=root spring.datasource.one.password=root spring.datasource.one.type=com.alibaba.druid.pool.DruidDataSource spring.datasource.two.url=jdbc:mysql:///test02?useUnicode=true&amp;characterEncoding=utf-8 spring.datasource.two.username=root spring.datasource.two.password=root spring.datasource.two.type=com.alibaba.druid.pool.DruidDataSource 这里通过one和two对数据源进行了区分，但是加了one和two之后，这里的配置就没法被SpringBoot自动加载了（因为前面的key变了），需要我们自己去加载DataSource了，此时，需要自己配置一个DataSourceConfig，用来提供两个DataSource Bean，如下： @Configuration public class DataSourceConfig { @Bean @ConfigurationProperties(prefix = \"spring.datasource.one\") DataSource dsOne() { return DruidDataSourceBuilder.create().build(); } @Bean @ConfigurationProperties(prefix = \"spring.datasource.two\") DataSource dsTwo() { return DruidDataSourceBuilder.create().build(); } } 这里提供了两个Bean，其中@ConfigurationProperties是Spring Boot提供的类型安全的属性绑定，以第一个Bean为例，@ConfigurationProperties(prefix = &quot;spring.datasource.one&quot;)表示使用spring.datasource.one前缀的数据库配置去创建一个DataSource，这样配置之后，我们就有了两个不同的DataSource，接下来再用这两个不同的DataSource去创建两个不同的JdbcTemplate。 配置JdbcTemplate实例创建JdbcTemplateConfig类，用来提供两个不同的JdbcTemplate实例，如下： @Configuration public class JdbcTemplateConfig { @Bean JdbcTemplate jdbcTemplateOne(@Qualifier(\"dsOne\") DataSource dsOne) { return new JdbcTemplate(dsOne); } @Bean JdbcTemplate jdbcTemplateTwo(@Qualifier(\"dsTwo\") DataSource dsTwo) { return new JdbcTemplate(dsTwo); } } 每一个JdbcTemplate的创建都需要一个DataSource，由于Spring容器中现在存在两个DataSource，默认使用类型查找，会报错，因此加上@Qualifier注解，表示按照名称查找。这里创建了两个JdbcTemplate实例，分别对应了两个DataSource。 接下来直接去使用这个JdbcTemplate就可以了。 测试使用关于JdbcTemplate的详细用法大伙可以参考我的上篇文章，这里我主要演示数据源的差异，在Controller中注入两个不同的JdbcTemplate，这两个JdbcTemplate分别对应了不同的数据源，如下： @RestController public class HelloController { @Autowired @Qualifier(\"jdbcTemplateOne\") JdbcTemplate jdbcTemplateOne; @Resource(name = \"jdbcTemplateTwo\") JdbcTemplate jdbcTemplateTwo; @GetMapping(\"/user\") public List&lt;User&gt; getAllUser() { List&lt;User&gt; list = jdbcTemplateOne.query(\"select * from t_user\", new BeanPropertyRowMapper&lt;&gt;(User.class)); return list; } @GetMapping(\"/user2\") public List&lt;User&gt; getAllUser2() { List&lt;User&gt; list = jdbcTemplateTwo.query(\"select * from t_user\", new BeanPropertyRowMapper&lt;&gt;(User.class)); return list; } } 和DataSource一样，Spring容器中的JdbcTemplate也是有两个，因此不能通过byType的方式注入进来，这里给大伙提供了两种注入思路，一种是使用@Resource注解，直接通过byName的方式注入进来，另外一种就是@Autowired注解加上@Qualifier注解，两者联合起来，实际上也是byName。将JdbcTemplate注入进来之后，jdbcTemplateOne和jdbcTemplateTwo此时就代表操作不同的数据源，使用不同的JdbcTemplate操作不同的数据源，实现了多数据源配置。 好了，这个问题就先说到这里，关于这个多数据源配置，还有一个小小的视频教程，加入我的星球免费观看： 关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0406/springboot-jdbctemplate.html"},{"title":"Spring Security 中的角色继承问题","text":"今天想和小伙伴们来聊一聊 Spring Security 中的角色继承问题。 角色继承实际上是一个很常见的需求，因为大部分公司治理可能都是金字塔形的，上司可能具备下属的部分甚至所有权限，这一现实场景，反映到我们的代码中，就是角色继承了。 Spring Security 中为开发者提供了相关的角色继承解决方案，但是这一解决方案在最近的 Spring Security 版本变迁中，使用方法有所变化。今天除了和小伙伴们分享角色继承外，也来顺便说说这种变化，避免小伙伴们踩坑，同时购买了我的书的小伙伴也需要留意，书是基于 Spring Boot2.0.4 这个版本写的，这个话题和最新版 Spring Boot 的还是有一点差别。 版本分割线上文说过，SpringSecurity 在角色继承上有两种不同的写法，在 Spring Boot2.0.8（对应 Spring Security 也是 5.0.11）上面是一种写法，从 Spring Boot2.1.0（对应 Spring Security5.1.1）又是另外一种写法，本文将从这两种角度出发，向读者介绍两种不同的角色继承写法。 以前的写法这里说的以前写法，就是指 SpringBoot2.0.8（含）之前的写法，在之前的写法中，角色继承只需要开发者提供一个 RoleHierarchy 接口的实例即可，例如下面这样： 1234567@BeanRoleHierarchy roleHierarchy() { RoleHierarchyImpl roleHierarchy = new RoleHierarchyImpl(); String hierarchy = \"ROLE_dba &gt; ROLE_admin ROLE_admin &gt; ROLE_user\"; roleHierarchy.setHierarchy(hierarchy); return roleHierarchy;} 在这里我们提供了一个 RoleHierarchy 接口的实例，使用字符串来描述了角色之间的继承关系， ROLE_dba 具备 ROLE_admin 的所有权限，而 ROLE_admin 则具备 ROLE_user 的所有权限，继承与继承之间用一个空格隔开。提供了这个 Bean 之后，以后所有具备 ROLE_user 角色才能访问的资源， ROLE_dba 和 ROLE_admin 也都能访问，具备 ROLE_amdin 角色才能访问的资源， ROLE_dba 也能访问。 现在的写法但是上面这种写法仅限于 Spring Boot2.0.8（含）之前的版本，在之后的版本中，这种写法则不被支持，新版的写法是下面这样： 1234567@BeanRoleHierarchy roleHierarchy() { RoleHierarchyImpl roleHierarchy = new RoleHierarchyImpl(); String hierarchy = \"ROLE_dba &gt; ROLE_admin \\n ROLE_admin &gt; ROLE_user\"; roleHierarchy.setHierarchy(hierarchy); return roleHierarchy;} 变化主要就是分隔符，将原来用空格隔开的地方，现在用换行符了。这里表达式的含义依然和上面一样，不再赘述。 上面两种不同写法都是配置角色的继承关系，配置完成后，接下来指定角色和资源的对应关系即可，如下： 123456789101112131415@Overrideprotected void configure(HttpSecurity http) throws Exception { http.authorizeRequests().antMatchers(\"/admin/**\") .hasRole(\"admin\") .antMatchers(\"/db/**\") .hasRole(\"dba\") .antMatchers(\"/user/**\") .hasRole(\"user\") .and() .formLogin() .loginProcessingUrl(\"/doLogin\") .permitAll() .and() .csrf().disable();} 这个表示 /db/** 格式的路径需要具备 dba 角色才能访问， /admin/** 格式的路径则需要具备 admin 角色才能访问， /user/** 格式的路径，则需要具备 user 角色才能访问，此时提供相关接口，会发现，dba 除了访问 /db/** ，也能访问 /admin/** 和 /user/** ，admin 角色除了访问 /admin/** ，也能访问 /user/** ，user 角色则只能访问 /user/** 。 源码分析这样两种不同的写法，其实也对应了两种不同的解析策略，角色继承关系的解析在 RoleHierarchyImpl 类的 buildRolesReachableInOneStepMap 方法中，Spring Boot2.0.8（含）之前该方法的源码如下： 12345678910111213141516171819202122232425private void buildRolesReachableInOneStepMap() { Pattern pattern = Pattern.compile(\"(\\\\s*([^\\\\s&gt;]+)\\\\s*&gt;\\\\s*([^\\\\s&gt;]+))\"); Matcher roleHierarchyMatcher = pattern .matcher(this.roleHierarchyStringRepresentation); this.rolesReachableInOneStepMap = new HashMap&lt;GrantedAuthority, Set&lt;GrantedAuthority&gt;&gt;(); while (roleHierarchyMatcher.find()) { GrantedAuthority higherRole = new SimpleGrantedAuthority( roleHierarchyMatcher.group(2)); GrantedAuthority lowerRole = new SimpleGrantedAuthority( roleHierarchyMatcher.group(3)); Set&lt;GrantedAuthority&gt; rolesReachableInOneStepSet; if (!this.rolesReachableInOneStepMap.containsKey(higherRole)) { rolesReachableInOneStepSet = new HashSet&lt;&gt;(); this.rolesReachableInOneStepMap.put(higherRole, rolesReachableInOneStepSet); } else { rolesReachableInOneStepSet = this.rolesReachableInOneStepMap .get(higherRole); } addReachableRoles(rolesReachableInOneStepSet, lowerRole); logger.debug(\"buildRolesReachableInOneStepMap() - From role \" + higherRole + \" one can reach role \" + lowerRole + \" in one step.\"); }} 从这段源码中我们可以看到，角色的继承关系是通过正则表达式进行解析，通过空格进行切分，然后构建相应的 map 出来。 Spring Boot2.1.0（含）之后该方法的源码如下： 12345678910111213141516171819202122232425262728private void buildRolesReachableInOneStepMap() { this.rolesReachableInOneStepMap = new HashMap&lt;GrantedAuthority, Set&lt;GrantedAuthority&gt;&gt;(); try (BufferedReader bufferedReader = new BufferedReader( new StringReader(this.roleHierarchyStringRepresentation))) { for (String readLine; (readLine = bufferedReader.readLine()) != null;) { String[] roles = readLine.split(\" &gt; \"); for (int i = 1; i &lt; roles.length; i++) { GrantedAuthority higherRole = new SimpleGrantedAuthority( roles[i - 1].replaceAll(\"^\\\\s+|\\\\s+$\", \"\")); GrantedAuthority lowerRole = new SimpleGrantedAuthority(roles[i].replaceAll(\"^\\\\s+|\\\\s+$ Set&lt;GrantedAuthority&gt; rolesReachableInOneStepSet; if (!this.rolesReachableInOneStepMap.containsKey(higherRole)) { rolesReachableInOneStepSet = new HashSet&lt;GrantedAuthority&gt;(); this.rolesReachableInOneStepMap.put(higherRole, rolesReachableInOneStepSet); } else { rolesReachableInOneStepSet = this.rolesReachableInOneStepMap.get(higherRole); } addReachableRoles(rolesReachableInOneStepSet, lowerRole); if (logger.isDebugEnabled()) { logger.debug(\"buildRolesReachableInOneStepMap() - From role \" + higherRole + \" one can reach role \" + lowerRole + \" in one step.\"); } } } } catch (IOException e) { throw new IllegalStateException(e); }} 从这里我们可以看到，这里并没有一上来就是用正则表达式，而是先将角色继承字符串转为一个 BufferedReader ，然后一行一行的读出来，再进行解析，最后再构建相应的 map。从这里我们可以看出为什么前后版本对此有不同的写法。 那么小伙伴在开发过程中，还是需要留意这一个差异。好了，角色继承我们就先说到这里，本文并没有讲 Spring Security 一些基本用法，想了解 Spring Security 更多用法，敬请留意后续文章。","link":"/2019/0613/springsecurity-role.html"},{"title":"Spring Security 登录使用 JSON 格式数据","text":"在使用 SpringSecurity 中，大伙都知道默认的登录数据是通过 key/value 的形式来传递的，默认情况下不支持 JSON格式的登录数据，如果有这种需求，就需要自己来解决，本文主要和小伙伴来聊聊这个话题。 基本登录方案在说如何使用 JSON 登录之前，我们还是先来看看基本的登录吧，本文为了简单，SpringSecurity 在使用中就不连接数据库了，直接在内存中配置用户名和密码，具体操作步骤如下： 创建 Spring Boot 工程 首先创建 SpringBoot 工程，添加 SpringSecurity 依赖，如下： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 添加 Security 配置 创建 SecurityConfig，完成 SpringSecurity 的配置，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Bean PasswordEncoder passwordEncoder() { return new BCryptPasswordEncoder(); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication().withUser(\"zhangsan\").password(\"$2a$10$2O4EwLrrFPEboTfDOtC0F.RpUMk.3q3KvBHRx7XXKUMLBGjOOBs8q\").roles(\"user\"); } @Override public void configure(WebSecurity web) throws Exception { } @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .anyRequest().authenticated() .and() .formLogin() .loginProcessingUrl(\"/doLogin\") .successHandler(new AuthenticationSuccessHandler() { @Override public void onAuthenticationSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { RespBean ok = RespBean.ok(\"登录成功！\",authentication.getPrincipal()); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(ok)); out.flush(); out.close(); } }) .failureHandler(new AuthenticationFailureHandler() { @Override public void onAuthenticationFailure(HttpServletRequest req, HttpServletResponse resp, AuthenticationException e) throws IOException, ServletException { RespBean error = RespBean.error(\"登录失败\"); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(error)); out.flush(); out.close(); } }) .loginPage(\"/login\") .permitAll() .and() .logout() .logoutUrl(\"/logout\") .logoutSuccessHandler(new LogoutSuccessHandler() { @Override public void onLogoutSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { RespBean ok = RespBean.ok(\"注销成功！\"); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(ok)); out.flush(); out.close(); } }) .permitAll() .and() .csrf() .disable() .exceptionHandling() .accessDeniedHandler(new AccessDeniedHandler() { @Override public void handle(HttpServletRequest req, HttpServletResponse resp, AccessDeniedException e) throws IOException, ServletException { RespBean error = RespBean.error(\"权限不足，访问失败\"); resp.setStatus(403); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(error)); out.flush(); out.close(); } }); }} 这里的配置虽然有点长，但是很基础，配置含义也比较清晰，首先提供 BCryptPasswordEncoder 作为 PasswordEncoder ，可以实现对密码的自动加密加盐，非常方便，然后提供了一个名为 zhangsan 的用户，密码是 123 ，角色是 user ，最后配置登录逻辑，所有的请求都需要登录后才能访问，登录接口是 /doLogin ，用户名的 key 是 username ，密码的 key 是 password ，同时配置登录成功、登录失败以及注销成功、权限不足时都给用户返回JSON提示，另外，这里虽然配置了登录页面为 /login ，实际上这不是一个页面，而是一段 JSON ，在 LoginController 中提供该接口，如下： 123456789101112@RestController@ResponseBodypublic class LoginController { @GetMapping(\"/login\") public RespBean login() { return RespBean.error(\"尚未登录，请登录\"); } @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} 这里 /login 只是一个 JSON 提示，而不是页面， /hello 则是一个测试接口。 OK，做完上述步骤就可以开始测试了，运行SpringBoot项目，访问 /hello 接口，结果如下： 此时先调用登录接口进行登录，如下： 登录成功后，再去访问 /hello 接口就可以成功访问了。 使用JSON登录上面演示的是一种原始的登录方案，如果想将用户名密码通过 JSON 的方式进行传递，则需要自定义相关过滤器，通过分析源码我们发现，默认的用户名密码提取在 UsernamePasswordAuthenticationFilter 过滤器中，部分源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class UsernamePasswordAuthenticationFilter extends AbstractAuthenticationProcessingFilter { public static final String SPRING_SECURITY_FORM_USERNAME_KEY = \"username\"; public static final String SPRING_SECURITY_FORM_PASSWORD_KEY = \"password\"; private String usernameParameter = SPRING_SECURITY_FORM_USERNAME_KEY; private String passwordParameter = SPRING_SECURITY_FORM_PASSWORD_KEY; private boolean postOnly = true; public UsernamePasswordAuthenticationFilter() { super(new AntPathRequestMatcher(\"/login\", \"POST\")); } public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException { if (postOnly &amp;&amp; !request.getMethod().equals(\"POST\")) { throw new AuthenticationServiceException( \"Authentication method not supported: \" + request.getMethod()); } String username = obtainUsername(request); String password = obtainPassword(request); if (username == null) { username = \"\"; } if (password == null) { password = \"\"; } username = username.trim(); UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken( username, password); // Allow subclasses to set the \"details\" property setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); } protected String obtainPassword(HttpServletRequest request) { return request.getParameter(passwordParameter); } protected String obtainUsername(HttpServletRequest request) { return request.getParameter(usernameParameter); } //... //...} 从这里可以看到，默认的用户名/密码提取就是通过 request 中的 getParameter 来提取的，如果想使用 JSON 传递用户名密码，只需要将这个过滤器替换掉即可，自定义过滤器如下： 12345678910111213141516171819202122232425public class CustomAuthenticationFilter extends UsernamePasswordAuthenticationFilter { @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException { if (request.getContentType().equals(MediaType.APPLICATION_JSON_UTF8_VALUE) || request.getContentType().equals(MediaType.APPLICATION_JSON_VALUE)) { ObjectMapper mapper = new ObjectMapper(); UsernamePasswordAuthenticationToken authRequest = null; try (InputStream is = request.getInputStream()) { Map&lt;String,String&gt; authenticationBean = mapper.readValue(is, Map.class); authRequest = new UsernamePasswordAuthenticationToken( authenticationBean.get(\"username\"), authenticationBean.get(\"password\")); } catch (IOException e) { e.printStackTrace(); authRequest = new UsernamePasswordAuthenticationToken( \"\", \"\"); } finally { setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); } } else { return super.attemptAuthentication(request, response); } }} 这里只是将用户名/密码的获取方案重新修正下，改为了从 JSON 中获取用户名密码，然后在 SecurityConfig 中作出如下修改： 123456789101112131415161718192021222324252627282930313233343536@Overrideprotected void configure(HttpSecurity http) throws Exception { http.authorizeRequests().anyRequest().authenticated() .and() .formLogin() .and().csrf().disable(); http.addFilterAt(customAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class);}@BeanCustomAuthenticationFilter customAuthenticationFilter() throws Exception { CustomAuthenticationFilter filter = new CustomAuthenticationFilter(); filter.setAuthenticationSuccessHandler(new AuthenticationSuccessHandler() { @Override public void onAuthenticationSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); RespBean respBean = RespBean.ok(\"登录成功!\"); out.write(new ObjectMapper().writeValueAsString(respBean)); out.flush(); out.close(); } }); filter.setAuthenticationFailureHandler(new AuthenticationFailureHandler() { @Override public void onAuthenticationFailure(HttpServletRequest req, HttpServletResponse resp, AuthenticationException e) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); RespBean respBean = RespBean.error(\"登录失败!\"); out.write(new ObjectMapper().writeValueAsString(respBean)); out.flush(); out.close(); } }); filter.setAuthenticationManager(authenticationManagerBean()); return filter;} 将自定义的 CustomAuthenticationFilter 类加入进来即可，接下来就可以使用 JSON 进行登录了，如下： 好了，本文就先介绍到这里，有问题欢迎留言讨论。","link":"/2019/0613/springsecurity-json.html"},{"title":"我来啦","text":"Hello 老铁们，写了几年博客之后，我终于建立起自己的站点了，这个 Hexo 使用起来还是挺方便的，半天时间就弄好了，以后我的文章将在我的公众号上首发，然后也会同步到这里，欢迎老铁们关注！","link":"/2019/0406/helloworld.html"},{"title":"SpringMVC 中 @ControllerAdvice 注解的三种使用场景！","text":"@ControllerAdvice ，很多初学者可能都没有听说过这个注解，实际上，这是一个非常有用的注解，顾名思义，这是一个增强的 Controller。使用这个 Controller ，可以实现三个方面的功能： 全局异常处理 全局数据绑定 全局数据预处理 灵活使用这三个功能，可以帮助我们简化很多工作，需要注意的是，这是 SpringMVC 提供的功能，在 Spring Boot 中可以直接使用，下面分别来看。 全局异常处理使用 @ControllerAdvice 实现全局异常处理，只需要定义类，添加该注解即可定义方式如下： 12345678910@ControllerAdvicepublic class MyGlobalExceptionHandler { @ExceptionHandler(Exception.class) public ModelAndView customException(Exception e) { ModelAndView mv = new ModelAndView(); mv.addObject(\"message\", e.getMessage()); mv.setViewName(\"myerror\"); return mv; }} 在该类中，可以定义多个方法，不同的方法处理不同的异常，例如专门处理空指针的方法、专门处理数组越界的方法…，也可以直接向上面代码一样，在一个方法中处理所有的异常信息。 @ExceptionHandler 注解用来指明异常的处理类型，即如果这里指定为 NullpointerException，则数组越界异常就不会进到这个方法中来。 全局数据绑定全局数据绑定功能可以用来做一些初始化的数据操作，我们可以将一些公共的数据定义在添加了 @ControllerAdvice 注解的类中，这样，在每一个 Controller 的接口中，就都能够访问导致这些数据。 使用步骤，首先定义全局数据，如下： 12345678910@ControllerAdvicepublic class MyGlobalExceptionHandler { @ModelAttribute(name = \"md\") public Map&lt;String,Object&gt; mydata() { HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"age\", 99); map.put(\"gender\", \"男\"); return map; }} 使用 @ModelAttribute 注解标记该方法的返回数据是一个全局数据，默认情况下，这个全局数据的 key 就是返回的变量名，value 就是方法返回值，当然开发者可以通过 @ModelAttribute 注解的 name 属性去重新指定 key。 定义完成后，在任何一个Controller 的接口中，都可以获取到这里定义的数据： 12345678910@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello(Model model) { Map&lt;String, Object&gt; map = model.asMap(); System.out.println(map); int i = 1 / 0; return \"hello controller advice\"; }} 全局数据预处理考虑我有两个实体类，Book 和 Author，分别定义如下： 12345678910public class Book { private String name; private Long price; //getter/setter}public class Author { private String name; private Integer age; //getter/setter} 此时，如果我定义一个数据添加接口，如下： 12345@PostMapping(\"/book\")public void addBook(Book book, Author author) { System.out.println(book); System.out.println(author);} 这个时候，添加操作就会有问题，因为两个实体类都有一个 name 属性，从前端传递时 ，无法区分。此时，通过 @ControllerAdvice 的全局数据预处理可以解决这个问题 解决步骤如下: 1.给接口中的变量取别名 12345@PostMapping(\"/book\")public void addBook(@ModelAttribute(\"b\") Book book, @ModelAttribute(\"a\") Author author) { System.out.println(book); System.out.println(author);} 2.进行请求数据预处理在 @ControllerAdvice 标记的类中添加如下代码: 12345678@InitBinder(\"b\")public void b(WebDataBinder binder) { binder.setFieldDefaultPrefix(\"b.\");}@InitBinder(\"a\")public void a(WebDataBinder binder) { binder.setFieldDefaultPrefix(\"a.\");} @InitBinder(“b”) 注解表示该方法用来处理和Book和相关的参数,在方法中,给参数添加一个 b 前缀,即请求参数要有b前缀. 3.发送请求 请求发送时,通过给不同对象的参数添加不同的前缀,可以实现参数的区分. 总结这就是松哥给大伙介绍的 @ControllerAdvice 的几个简单用法，这些点既可以在传统的 SSM 项目中使用，也可以在 Spring Boot + Spring Cloud 微服务中使用，欢迎大家有问题一起讨论。","link":"/2019/0422/springmvc-controlleradvice.html"},{"title":"一文读懂 Spring Data Jpa！","text":"有很多读者留言希望松哥能好好聊聊 Spring Data Jpa!其实这个话题松哥以前零零散散的介绍过，在我的书里也有介绍过，但是在公众号中还没和大伙聊过，因此本文就和大家来仔细聊聊 Spring Data 和 Jpa! 故事的主角Jpa1. JPA是什么 Java Persistence API：用于对象持久化的 API Java EE 5.0 平台标准的 ORM 规范，使得应用程序以统一的方式访问持久层 2. JPA和Hibernate的关系 JPA 是 Hibernate 的一个抽象（就像JDBC和JDBC驱动的关系）； JPA 是规范：JPA 本质上就是一种 ORM 规范，不是ORM 框架，这是因为 JPA 并未提供 ORM 实现，它只是制订了一些规范，提供了一些编程的 API 接口，但具体实现则由 ORM 厂商提供实现； Hibernate 是实现：Hibernate 除了作为 ORM 框架之外，它也是一种 JPA 实现 从功能上来说， JPA 是 Hibernate 功能的一个子集 3. JPA的供应商JPA 的目标之一是制定一个可以由很多供应商实现的 API，Hibernate 3.2+、TopLink 10.1+ 以及 OpenJPA 都提供了 JPA 的实现，Jpa 供应商有很多，常见的有如下四种： HibernateJPA 的始作俑者就是 Hibernate 的作者，Hibernate 从 3.2 开始兼容 JPA。 OpenJPAOpenJPA 是 Apache 组织提供的开源项目。 TopLinkTopLink 以前需要收费，如今开源了。 EclipseLink 4. JPA的优势 标准化: 提供相同的 API，这保证了基于JPA 开发的企业应用能够经过少量的修改就能够在不同的 JPA 框架下运行。 简单易用，集成方便: JPA 的主要目标之一就是提供更加简单的编程模型，在 JPA 框架下创建实体和创建 Java 类一样简单，只需要使用 javax.persistence.Entity 进行注解；JPA 的框架和接口也都非常简单。 可媲美JDBC的查询能力: JPA的查询语言是面向对象的，JPA定义了独特的JPQL，而且能够支持批量更新和修改、JOIN、GROUP BY、HAVING 等通常只有 SQL 才能够提供的高级查询特性，甚至还能够支持子查询。 支持面向对象的高级特性: JPA 中能够支持面向对象的高级特性，如类之间的继承、多态和类之间的复杂关系，最大限度的使用面向对象的模型 5. JPA包含的技术 ORM 映射元数据：JPA 支持 XML 和 JDK 5.0 注解两种元数据的形式，元数据描述对象和表之间的映射关系，框架据此将实体对象持久化到数据库表中。 JPA 的 API：用来操作实体对象，执行CRUD操作，框架在后台完成所有的事情，开发者从繁琐的 JDBC 和 SQL 代码中解脱出来。 查询语言（JPQL）：这是持久化操作中很重要的一个方面，通过面向对象而非面向数据库的查询语言查询数据，避免程序和具体的 SQL 紧密耦合。 Spring DataSpring Data 是 Spring 的一个子项目。用于简化数据库访问，支持NoSQL 和 关系数据存储。其主要目标是使数据库的访问变得方便快捷。Spring Data 具有如下特点： SpringData 项目支持 NoSQL 存储：MongoDB （文档数据库）Neo4j（图形数据库）Redis（键/值存储）Hbase（列族数据库） SpringData 项目所支持的关系数据存储技术：JDBCJPA Spring Data Jpa 致力于减少数据访问层 (DAO) 的开发量. 开发者唯一要做的，就是声明持久层的接口，其他都交给 Spring Data JPA 来帮你完成！ 框架怎么可能代替开发者实现业务逻辑呢？比如：当有一个 UserDao.findUserById() 这样一个方法声明，大致应该能判断出这是根据给定条件的 ID 查询出满足条件的 User 对象。Spring Data JPA 做的便是规范方法的名字，根据符合规范的名字来确定方法需要实现什么样的逻辑。 主角的故事Jpa 的故事为了让大伙彻底把这两个东西学会，这里我就先来介绍单纯的Jpa使用，然后我们再结合 Spring Data 来看 Jpa如何使用。 整体步骤如下： 1.使用 IntelliJ IDEA 创建项目，创建时选择 JavaEE Persistence ，如下： 2.创建成功后，添加依赖jar，由于 Jpa 只是一个规范，因此我们说用Jpa实际上必然是用Jpa的某一种实现，那么是哪一种实现呢？当然就是Hibernate了，所以添加的jar，实际上来自 Hibernate，如下： 3.添加实体类 接下来在项目中添加实体类，如下： 12345678910111213@Entity(name = \"t_book\")public class Book { private Long id; private String name; private String author; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) public Long getId() { return id; } // 省略其他getter/setter} 首先@Entity注解表示这是一个实体类，那么在项目启动时会自动针对该类生成一张表，默认的表名为类名，@Entity注解的name属性表示自定义生成的表名。@Id注解表示这个字段是一个id，@GeneratedValue注解表示主键的自增长策略，对于类中的其他属性，默认都会根据属性名在表中生成相应的字段，字段名和属性名相同，如果开发者想要对字段进行定制，可以使用@Column注解，去配置字段的名称，长度，是否为空等等。 4.创建 persistence.xml 文件 JPA 规范要求在类路径的 META-INF 目录下放置persistence.xml，文件的名称是固定的 123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;persistence xmlns=\"http://java.sun.com/xml/ns/persistence\" version=\"2.0\"&gt; &lt;persistence-unit name=\"NewPersistenceUnit\" transaction-type=\"RESOURCE_LOCAL\"&gt; &lt;provider&gt;org.hibernate.jpa.HibernatePersistenceProvider&lt;/provider&gt; &lt;class&gt;org.sang.Book&lt;/class&gt; &lt;properties&gt; &lt;property name=\"hibernate.connection.url\" value=\"jdbc:mysql:///jpa01?useUnicode=true&amp;amp;characterEncoding=UTF-8\"/&gt; &lt;property name=\"hibernate.connection.driver_class\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"hibernate.connection.username\" value=\"root\"/&gt; &lt;property name=\"hibernate.connection.password\" value=\"123\"/&gt; &lt;property name=\"hibernate.archive.autodetection\" value=\"class\"/&gt; &lt;property name=\"hibernate.show_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.format_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.hbm2ddl.auto\" value=\"update\"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 注意： persistence-unit 的name 属性用于定义持久化单元的名字, 必填。 transaction-type：指定 JPA 的事务处理策略。RESOURCE_LOCAL：默认值，数据库级别的事务，只能针对一种数据库，不支持分布式事务。如果需要支持分布式事务，使用JTA：transaction-type=”JTA” class节点表示显式的列出实体类 properties中的配置分为两部分：数据库连接信息以及Hibernate信息 执行持久化操作 1234567891011EntityManagerFactory entityManagerFactory = Persistence.createEntityManagerFactory(\"NewPersistenceUnit\");EntityManager manager = entityManagerFactory.createEntityManager();EntityTransaction transaction = manager.getTransaction();transaction.begin();Book book = new Book();book.setAuthor(\"罗贯中\");book.setName(\"三国演义\");manager.persist(book);transaction.commit();manager.close();entityManagerFactory.close(); 这里首先根据配置文件创建出来一个 EntityManagerFactory ，然后再根据 EntityManagerFactory 的实例创建出来一个 EntityManager ，然后再开启事务，调用 EntityManager 中的 persist 方法执行一次持久化操作，最后提交事务，执行完这些操作后，数据库中旧多出来一个 t_book 表，并且表中有一条数据。 关于 JPQL JPQL语言，即 Java Persistence Query Language 的简称。JPQL 是一种和 SQL 非常类似的中间性和对象化查询语言，它最终会被编译成针对不同底层数据库的 SQL 查询，从而屏蔽不同数据库的差异。JPQL语言的语句可以是 select 语句、update 语句或delete语句，它们都通过 Query 接口封装执行。 Query接口封装了执行数据库查询的相关方法。调用 EntityManager 的 createQuery、create NamedQuery 及 createNativeQuery 方法可以获得查询对象，进而可调用 Query 接口的相关方法来执行查询操作。 Query接口的主要方法如下： int executeUpdate(); | 用于执行update或delete语句。 List getResultList(); | 用于执行select语句并返回结果集实体列表。 Object getSingleResult(); | 用于执行只返回单个结果实体的select语句。 Query setFirstResult(int startPosition); | 用于设置从哪个实体记录开始返回查询结果。 Query setMaxResults(int maxResult); | 用于设置返回结果实体的最大数。与setFirstResult结合使用可实现分页查询。 Query setFlushMode(FlushModeType flushMode); | 设置查询对象的Flush模式。参数可以取2个枚举值：FlushModeType.AUTO 为自动更新数据库记录，FlushMode Type.COMMIT 为直到提交事务时才更新数据库记录。 setHint(String hintName, Object value); | 设置与查询对象相关的特定供应商参数或提示信息。参数名及其取值需要参考特定 JPA 实现库提供商的文档。如果第二个参数无效将抛出IllegalArgumentException异常。 setParameter(int position, Object value); | 为查询语句的指定位置参数赋值。Position 指定参数序号，value 为赋给参数的值。 setParameter(int position, Date d, TemporalType type); | 为查询语句的指定位置参数赋 Date 值。Position 指定参数序号，value 为赋给参数的值，temporalType 取 TemporalType 的枚举常量，包括 DATE、TIME 及 TIMESTAMP 三个，，用于将 Java 的 Date 型值临时转换为数据库支持的日期时间类型（java.sql.Date、java.sql.Time及java.sql.Timestamp）。 setParameter(int position, Calendar c, TemporalType type); | 为查询语句的指定位置参数赋 Calenda r值。position 指定参数序号，value 为赋给参数的值，temporalType 的含义及取舍同前。 setParameter(String name, Object value); | 为查询语句的指定名称参数赋值。 setParameter(String name, Date d, TemporalType type); | 为查询语句的指定名称参数赋 Date 值,用法同前。 setParameter(String name, Calendar c, TemporalType type); | 为查询语句的指定名称参数设置Calendar值。name为参数名，其它同前。该方法调用时如果参数位置或参数名不正确，或者所赋的参数值类型不匹配，将抛出 IllegalArgumentException 异常。 JPQL 举例和在 SQL 中一样，JPQL 中的 select 语句用于执行查询。其语法可表示为：select_clause form_clause [where_clause] [groupby_clause] [having_clause] [orderby_clause] 其中： from 子句是查询语句的必选子句。 select 用来指定查询返回的结果实体或实体的某些属性。 from 子句声明查询源实体类，并指定标识符变量（相当于SQL表的别名）。 如果不希望返回重复实体，可使用关键字 distinct 修饰。select、from 都是 JPQL 的关键字，通常全大写或全小写，建议不要大小写混用。 在 JPQL 中，查询所有实体的 JPQL 查询语句很简单，如下：select o from Order o 或 select o from Order as o这里关键字 as 可以省去，标识符变量的命名规范与 Java 标识符相同，且区分大小写,调用 EntityManager 的 createQuery() 方法可创建查询对象，接着调用 Query 接口的 getResultList() 方法就可获得查询结果集，如下： 123456Query query = entityManager.createQuery( \"select o from Order o\"); List orders = query.getResultList();Iterator iterator = orders.iterator();while(iterator.hasNext() ) { // 处理Order} 其他方法的与此类似，这里不再赘述。 Spring Data 的故事在 Spring Boot 中，Spring Data Jpa 官方封装了太多东西了，导致很多人用的时候不知道底层到底是怎么配置的，本文就和大伙来看看在手工的Spring环境下，Spring Data Jpa要怎么配置，配置完成后，用法和 Spring Boot 中的用法是一致的。 基本环境搭建首先创建一个普通的Maven工程，并添加如下依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-oxm&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;5.2.12.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-jpamodelgen&lt;/artifactId&gt; &lt;version&gt;5.2.12.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.29&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;1.11.3.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 这里除了 Jpa 的依赖之外，就是Spring Data Jpa 的依赖了。 接下来创建一个 User 实体类，创建方式参考 Jpa中实体类的创建方式，这里不再赘述。 接下来在resources目录下创建一个applicationContext.xml文件，并配置Spring和Jpa，如下： 123456789101112131415161718192021222324252627282930&lt;context:property-placeholder location=\"classpath:db.properties\"/&gt;&lt;context:component-scan base-package=\"org.sang\"/&gt;&lt;bean class=\"com.alibaba.druid.pool.DruidDataSource\" id=\"dataSource\"&gt; &lt;property name=\"driverClassName\" value=\"${db.driver}\"/&gt; &lt;property name=\"url\" value=\"${db.url}\"/&gt; &lt;property name=\"username\" value=\"${db.username}\"/&gt; &lt;property name=\"password\" value=\"${db.password}\"/&gt;&lt;/bean&gt;&lt;bean class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\" id=\"entityManagerFactory\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"jpaVendorAdapter\"&gt; &lt;bean class=\"org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter\"/&gt; &lt;/property&gt; &lt;property name=\"packagesToScan\" value=\"org.sang.model\"/&gt; &lt;property name=\"jpaProperties\"&gt; &lt;props&gt; &lt;prop key=\"hibernate.show_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQL57Dialect&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean class=\"org.springframework.orm.jpa.JpaTransactionManager\" id=\"transactionManager\"&gt; &lt;property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt;&lt;!-- 配置jpa --&gt;&lt;jpa:repositories base-package=\"org.sang.dao\" entity-manager-factory-ref=\"entityManagerFactory\"/&gt; 这里和 Jpa 相关的配置主要是三个，一个是entityManagerFactory，一个是Jpa的事务，还有一个是配置dao的位置，配置完成后，就可以在 org.sang.dao 包下创建相应的 Repository 了，如下： 123public interface UserDao extends Repository&lt;User, Long&gt; { User getUserById(Long id);} getUserById表示根据id去查询User对象，只要我们的方法名称符合类似的规范，就不需要写SQL，具体的规范一会来说。好了，接下来，创建 Service 和 Controller 来调用这个方法，如下： 12345678910111213141516@Service@Transactionalpublic class UserService { @Resource UserDao userDao; public User getUserById(Long id) { return userDao.getUserById(id); }}public void test1() { ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); UserService userService = ctx.getBean(UserService.class); User user = userService.getUserById(1L); System.out.println(user);} 这样，就可以查询到id为1的用户了。 Repository上文我们自定义的 UserDao 实现了 Repository 接口，这个 Repository 接口是什么来头呢？ 首先来看 Repository 的一个继承关系图： 可以看到，实现类不少。那么到底如何理解 Repository 呢？ Repository 接口是 Spring Data 的一个核心接口，它不提供任何方法，开发者需要在自己定义的接口中声明需要的方法 public interface Repository&lt;T, ID extends Serializable&gt; { } 若我们定义的接口继承了 Repository, 则该接口会被 IOC 容器识别为一个 Repository Bean，进而纳入到 IOC 容器中，进而可以在该接口中定义满足一定规范的方法。 Spring Data可以让我们只定义接口，只要遵循 Spring Data 的规范，就无需写实现类。 与继承 Repository 等价的一种方式，就是在持久层接口上使用 @RepositoryDefinition 注解，并为其指定 domainClass 和 idClass 属性。像下面这样： 123456@RepositoryDefinition(domainClass = User.class, idClass = Long.class)public interface UserDao{ User findById(Long id); List&lt;User&gt; findAll();} 基础的 Repository 提供了最基本的数据访问功能，其几个子接口则扩展了一些功能，它的几个常用的实现类如下： CrudRepository： 继承 Repository，实现了一组 CRUD 相关的方法 PagingAndSortingRepository： 继承 CrudRepository，实现了一组分页排序相关的方法 JpaRepository： 继承 PagingAndSortingRepository，实现一组 JPA 规范相关的方法 自定义的 XxxxRepository 需要继承 JpaRepository，这样的 XxxxRepository 接口就具备了通用的数据访问控制层的能力。 JpaSpecificationExecutor： 不属于Repository体系，实现一组 JPA Criteria 查询相关的方法 方法定义规范1.简单条件查询 按照 Spring Data 的规范，查询方法以 find | read | get 开头 涉及条件查询时，条件的属性用条件关键字连接，要注意的是：条件属性以首字母大写 例如：定义一个 Entity 实体类： 1234class User｛ private String firstName; private String lastName; ｝ 使用And条件连接时，条件的属性名称与个数要与参数的位置与个数一一对应，如下： 1findByLastNameAndFirstName(String lastName,String firstName); 支持属性的级联查询. 若当前类有符合条件的属性, 则优先使用, 而不使用级联属性. 若需要使用级联属性, 则属性之间使用 _ 进行连接. 查询举例：1.按照id查询 12User getUserById(Long id);User getById(Long id); 2.查询所有年龄小于90岁的人 1List&lt;User&gt; findByAgeLessThan(Long age); 3.查询所有姓赵的人 1List&lt;User&gt; findByUsernameStartingWith(String u); 4.查询所有姓赵的、并且id大于50的人 1List&lt;User&gt; findByUsernameStartingWithAndIdGreaterThan(String name, Long id); 5.查询所有姓名中包含”上”字的人 1List&lt;User&gt; findByUsernameContaining(String name); 6.查询所有姓赵的或者年龄大于90岁的 1List&lt;User&gt; findByUsernameStartingWithOrAgeGreaterThan(String name, Long age); 7.查询所有角色为1的用户 1List&lt;User&gt; findByRole_Id(Long id); 2.支持的关键字支持的查询关键字如下图： 3.查询方法流程解析为什么写上方法名，JPA就知道你想干嘛了呢？假如创建如下的查询：findByUserDepUuid()，框架在解析该方法时，首先剔除 findBy，然后对剩下的属性进行解析，假设查询实体为Doc： 先判断 userDepUuid （根据 POJO 规范，首字母变为小写）是否为查询实体的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，继续第二步； 从右往左截取第一个大写字母开头的字符串(此处为Uuid)，然后检查剩下的字符串是否为查询实体的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，则重复第二步，继续从右往左截取；最后假设 user 为查询实体的一个属性； 接着处理剩下部分（DepUuid），先判断 user 所对应的类型是否有depUuid属性，如果有，则表示该方法最终是根据 “ Doc.user.depUuid” 的取值进行查询；否则继续按照步骤 2 的规则从右往左截取，最终表示根据 “Doc.user.dep.uuid” 的值进行查询。 可能会存在一种特殊情况，比如 Doc包含一个 user 的属性，也有一个 userDep 属性，此时会存在混淆。可以明确在属性之间加上 “_” 以显式表达意图，比如 “findByUser_DepUuid()” 或者 “findByUserDep_uuid()” 还有一些特殊的参数：例如分页或排序的参数： 12Page&lt;UserModel&gt; findByName(String name, Pageable pageable); List&lt;UserModel&gt; findByName(String name, Sort sort); @Query注解有的时候，这里提供的查询关键字并不能满足我们的查询需求，这个时候就可以使用 @Query 关键字，来自定义查询 SQL，例如查询Id最大的User： 12@Query(\"select u from t_user u where id=(select max(id) from t_user)\")User getMaxIdUser(); 如果查询有参数的话，参数有两种不同的传递方式: 1.利用下标索引传参，索引参数如下所示，索引值从1开始，查询中 ”?X” 个数需要与方法定义的参数个数相一致，并且顺序也要一致： 12@Query(\"select u from t_user u where id&gt;?1 and username like ?2\")List&lt;User&gt; selectUserByParam(Long id, String name); 2.命名参数（推荐）：这种方式可以定义好参数名，赋值时采用@Param(“参数名”)，而不用管顺序： 12@Query(\"select u from t_user u where id&gt;:id and username like :name\")List&lt;User&gt; selectUserByParam2(@Param(\"name\") String name, @Param(\"id\") Long id); 查询时候，也可以是使用原生的SQL查询，如下： 12@Query(value = \"select * from t_user\",nativeQuery = true)List&lt;User&gt; selectAll(); @Modifying注解涉及到数据修改操作，可以使用 @Modifying 注解，@Query 与 @Modifying 这两个 annotation一起声明，可定义个性化更新操作，例如涉及某些字段更新时最为常用，示例如下： 123@Modifying@Query(\"update t_user set age=:age where id&gt;:id\")int updateUserById(@Param(\"age\") Long age, @Param(\"id\") Long id); 注意： 可以通过自定义的 JPQL 完成 UPDATE 和 DELETE 操作. 注意: JPQL 不支持使用 INSERT 方法的返回值应该是 int，表示更新语句所影响的行数 在调用的地方必须加事务，没有事务不能正常执行 默认情况下, Spring Data 的每个方法上有事务, 但都是一个只读事务. 他们不能完成修改操作 说到这里，再来顺便说说Spring Data 中的事务问题： Spring Data 提供了默认的事务处理方式，即所有的查询均声明为只读事务。 对于自定义的方法，如需改变 Spring Data 提供的事务默认方式，可以在方法上添加 @Transactional 注解。 进行多个 Repository 操作时，也应该使它们在同一个事务中处理，按照分层架构的思想，这部分属于业务逻辑层，因此，需要在Service 层实现对多个 Repository 的调用，并在相应的方法上声明事务。 好了，关于Spring Data Jpa 本文就先说这么多。","link":"/2019/0412/springdata-jpa.html"},{"title":"一个Java程序猿眼中的前后端分离以及Vue.js入门","text":"松哥的书里边，其实有涉及到 Vue，但是并没有详细说过，原因很简单，Vue 的资料都是中文的，把 Vue.js 官网的资料从头到尾浏览一遍该懂的基本就懂了，个人感觉这个是最好的 Vue.js 学习资料 ，因此在我的书里边就没有多说。但是最近总结小伙伴遇到的问题，感觉很多人对前后端分离开发还是两眼一抹黑，所以今天松哥想和大家聊一下前后端分离以及 Vue.js 的一点事，算是一个简单的入门科普吧。 前后端不分 后端模板：Jsp、FreeMarker、Velocity 前端模板：Thymeleaf 前后端不分，Jsp 是一个非常典型写法，Jsp 将 HTML 和 Java 代码结合在一起，刚开始的时候，确实提高了生产力，但是时间久了，大伙就发现 Jsp 存在的问题了，对于后端工程师来说，可能不太精通 css ，所以流程一般是这样前端设计页面–&gt;后端把页面改造成 Jsp –&gt; 后端发现问题 –&gt; 页面给前端 –&gt; 前端不会Jsp。这种方式效率低下。特别是在移动互联网兴起后，公司的业务，一般除了 PC 端，还有手机端、小程序等，通常，一套后台系统需要对应多个前端，此时就不可以继续使用前后端不分的开发方式了。 在前后端不分的开发方式中，一般来说，后端可能返回一个 ModelAndView ，渲染成 HTML 之后，浏览器当然可以展示，但是对于小程序、移动端来说，并不能很好的展示 HTML（实际上移动端也支持HTML，只不过运行效率低下）。这种时候，后端和前端数据交互，主流方案就是通过 JSON 来实现。 前后端分离前后端分离后，后端不再写页面，只提供 JSON 数据接口（XML数据格式现在用的比较少），前端可以移动端、小程序、也可以是 PC 端，前端负责 JSON 的展示，页面跳转等都是通过前端来实现的。前端后分离后，前端目前有三大主流框架： Vue 作者尤雨溪，Vue本身借鉴了 Angular，目前GitHubstar数最多，建议后端工程师使用这个，最大的原因是Vue上手容易，可以快速学会，对于后端工程师来说，能快速搭建页面解决问题即可，但是如果你是专业的前端工程师，我会推荐你三个都去学习 。就目前国内前端框架使用情况来说，Vue 算是使用最多的。而且目前来说，有大量 Vue 相关的周边产品，各种 UI 框架，开源项目，学习资料非常多。 React Facebook 的产品。是一个用于构建用户界面的 js 库，React 性能较好，代码逻辑简单。 Angular AngularJS 是一款由 Google 维护的开源 JavaScript 库，用来协助单一页面应用程序运行。它的目标是透过 MVC 模式（MVC）功能增强基于浏览器的应用，使开发和测试变得更加容易。 Vue简介Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。 只关注视图层 MVVM 框架 大家在使用 jQuery 过程中，掺杂了大量的 DOM 操作，修改视图或者获取 value ，都需要 DOM 操作，MVVM 是一种视图和数据模型双向绑定的框架，即数据发生变化，视图会跟着变化，视图发生变化，数据模型也会跟着变化，开发者再也不需要操作 DOM 节点。 如下一个简单的九九乘法表让大家感受一下 MVVM ： 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vue/dist/vue.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"app\"&gt; &lt;input type=\"text\" v-model=\"num\"&gt; &lt;table border=\"1\"&gt; &lt;tr v-for=\"i in parseInt(num)\"&gt; &lt;td v-for=\"j in i\"&gt;{{j}}*{{i}}={{i*j}}&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;&lt;script&gt; var app = new Vue({ el: \"#app\", data: { num:9 } });&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 用户修改输入框中的数据，引起变量的变化，进而实现九九乘法表的更新。 SPASPA（single page web application），单页面应用，是一种网络应用程序或网站的模型，它通过动态重写当前页面来与用户交互，而非传统的从服务器重新加载整个新页面。这种方法避免了页面之间切换打断用户体验，使应用程序更像一个桌面应用程序。在单页应用中，所有必要的代码（ HTML、JavaScript 和 CSS ）都通过单个页面的加载而检索，或者根据需要（通常是为响应用户操作）动态装载适当的资源并添加到页面。SPA 有一个缺点，因为 SPA 应用部署后只有1个页面，而且这个页面只是一堆 js 、css 引用，没有其他有效价值，因此，SPA 应用不易被搜索引擎收录，所以，一般来说，SPA 适合做大型企业后台管理系统。 Vue 使用方式大致上可以分为两大类： 直接将Vue在页面中引入，不做 SPA 应用 SPA应用 基本环境搭建首先需要安装两个东西： NodeJS npm 直接搜索下载 NodeJS 即可，安装成功之后，npm 也就有了。安装成功之后，可以 在 cmd 命令哈验证是否安装成功： NodeJS 安装成功之后，接下来安装 Vue的工具： 12345npm install -g vue-cli # 只需要第一次安装时执行vue init webpack my-project # 使用webpack模板创建一个vue项目cd my-project #进入到项目目录中npm install # 下载依赖（如果在项目创建的最后一步选择了自动执行npm install，则该步骤可以省略）npm run dev # 启动项目 启动成功后，浏览器输入 http://localhost:8080 就能看到如下页面： 执行 npm install 命令时，默认使用的是国外的下载源 ，可以通过如下代码配置为使用淘宝的镜像： 1npm config set registry https://registry.npm.taobao.org 修改完成后，就能有效提高下载的成功率。 Vue 项目结构介绍Vue 项目创建完成后，使用 Web Storm 打开项目，项目目录如下： build 文件夹，用来存放项目构建脚本 config 中存放项目的一些基本配置信息，最常用的就是端口转发 node_modules 这个目录存放的是项目的所有依赖，即 npm install 命令下载下来的文件 src 这个目录下存放项目的源码，即开发者写的代码放在这里 static 用来存放静态资源 index.html 则是项目的首页，入口页，也是整个项目唯一的HTML页面 package.json 中定义了项目的所有依赖，包括开发时依赖和发布时依赖 对于开发者来说，以后 99.99% 的工作都是在 src 中完成的，src 中的文件目录如下： assets 目录用来存放资产文件 components 目录用来存放组件（一些可复用，非独立的页面），当然开发者也可以在 components 中直接创建完整页面。 推荐在 components 中存放组件，另外单独新建一个 page 文件夹，专门用来放完整页面。 router 目录中，存放了路由的js文件 App.vue 是一个Vue组件，也是项目的第一个Vue组件 main.js相当于Java中的main方法，是整个项目的入口js main.js 内容如下： 1234567891011import Vue from 'vue'import App from './App'import router from './router'Vue.config.productionTip = false/* eslint-disable no-new */new Vue({ el: '#app', router, components: { App }, template: '&lt;App/&gt;'}) 在main.js 中，首先导入 Vue 对象 导入 App.vue ，并且命名为 App 导入router，注意，由于router目录下路由默认文件名为 index.js ，因此可以省略 所有东西都导入成功后，创建一个Vue对象，设置要被Vue处理的节点是 ‘#app’，’#app’ 指提前在index.html 文件中定义的一个div 将 router 设置到 vue 对象中，这里是一个简化的写法，完整的写法是 router:router，如果 key/value 一模一样，则可以简写。 声明一个组件 App，App 这个组件在一开始已经导入到项目中了，但是直接导入的组件无法直接使用，必须要声明。 template 中定义了页面模板，即将 App 组件中的内容渲染到 ‘#app’ 这个div 中。 因此，可以猜测，项目启动成功后，看到的页面效果定义在 App.vue 中 123456789101112131415161718192021&lt;template&gt; &lt;div id=\"app\"&gt; &lt;img src=\"./assets/logo.png\"&gt; &lt;router-view/&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'App'}&lt;/script&gt;&lt;style&gt;#app { font-family: 'Avenir', Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; color: #2c3e50; margin-top: 60px;}&lt;/style&gt; App.vue 是一个vue组件，这个组件中包含三部分内容：1.页面模板（template）；2.页面脚本（script）；3.页面样式（style） 页面模板中，定义了页面的 HTML 元素，这里定义了两个，一个是一张图片，另一个则是一个 router-view 页面脚本主要用来实现当前页面数据初始化、事件处理等等操作 页面样式就是针对 template 中 HTML 元素的页面美化操作 需要额外解释的是，router-view，这个指展示路由页面的位置，可以简单理解为一个占位符，这个占位符展示的内容将根据当前具体的 URL 地址来定。具体展示的内容，要参考路由表，即 router/index.js 文件，该文件如下： 12345678910111213import Vue from 'vue'import Router from 'vue-router'import HelloWorld from '@/components/HelloWorld'Vue.use(Router)export default new Router({ routes: [ { path: '/', name: 'HelloWorld', component: HelloWorld } ]}) 这个文件中，首先导入了Vue对象、Router对象以及 HelloWorld 组件， 创建一个Router对象，并定义路由表 这里定义的路由表，path为 / ，对应的组件为 HelloWorld，即浏览器地址为 / 时，在router-view位置显示 HelloWorld 组件 WebStorm 中启动Vue也可以直接在 webstorm 中配置vue并启动，点击右上角进行配置： 然后配置一下脚本 ： 配置完成后，点击右上角启动按钮，就可以启动一个 Vue 项目，如下： 项目编译这么大一个前端项目，肯定没法直接发布运行，当开发者完成项目开发后，将 cmd 命令行定位到当前项目目录，然后执行如下命令对项目进行打包： 1npm run build 打包成功后，当前项目目录下会生成一个 dist 文件夹，这个文件夹中有两个文件，分别是 index.html 和 static ，index.html 页面就是我们 SPA 项目中唯一的 HTML 页面了，static 中则保存了编译后的 js、css等文件，项目发布时，可以使用 nginx 独立部署 dist 中的静态文件，也可以将静态文件拷贝到 Spring Boot 项目的 static 目录下，然后对 Spring Boot 项目进行编译打包发布。 总结因为松哥的读者以后端程序猿居多，也有少量前端程序猿，因此本文松哥想从一个后端程序猿的角度来带大家理解一下前后端分离以及 Vue 的一些基本用法，也欢迎专业的前端工程师出来拍砖。","link":"/2019/0419/springboot-vue.html"},{"title":"SpringMVC 方法四种类型返回值总结，你用过几种？","text":"SpringMVC 现在算是 Java 领域的一个基础性框架了，很多人天天用，可是对于 SpringMVC 方法的返回值，你又是否完全清楚呢？今天松哥就来和大家聊一聊 SpringMVC 中四种不同类型的返回值，看看有没有 get 到你的知识盲点？ 1. ModelAndView以前前后端不分的情况下，ModelAndView 应该是最最常见的返回值类型了，现在前后端分离后，后端都是以返回 JSON 数据为主了。后端返回 ModelAndView 这个比较容易理解，开发者可以在 ModelAndView 对象中指定视图名称，然后也可以绑定数据，像下面这样： 12345678910111213141516171819@RequestMapping(\"/book\")public ModelAndView getAllBook() { ModelAndView mv = new ModelAndView(); List&lt;Book&gt; books = new ArrayList&lt;&gt;(); Book b1 = new Book(); b1.setId(1); b1.setName(\"三国演义\"); b1.setAuthor(\"罗贯中\"); books.add(b1); Book b2 = new Book(); b2.setId(2); b2.setName(\"红楼梦\"); b2.setAuthor(\"曹雪芹\"); books.add(b2); //指定数据模型 mv.addObject(\"bs\", books); mv.setViewName(\"book\");//指定视图名 return mv;} 返回 ModelAndView ，最常见的两个操作就是指定数据模型+指定视图名 。 2. Void返回值为 void 时，可能是你真的没有值要返回，也可能是你有其他办法，松哥将之归为如下四类，大伙来看下。 2.1 没有值如果确实没有返回值，那就返回 void ，但是一定要注意，此时，方法上需要添加 @ResponseBody 注解，像下面这样： 12345@RequestMapping(\"/test2\")@ResponseBodypublic void test2(){ //你的代码} 2.2 重定向由于 SpringMVC 中的方法默认都具备 HttpServletResponse 参数，因此可以重拾 Servlet/Jsp 中的技能，可以实现重定向，像下面这样手动设置响应头： 123456@RequestMapping(\"/test1\")@ResponseBodypublic void test1(HttpServletResponse resp){ resp.setStatus(302); resp.addHeader(\"Location\",\"/aa/index\");} 也可以像下面这样直接调用重定向的方法： 12345@RequestMapping(\"/test1\")@ResponseBodypublic void test1(HttpServletResponse resp){ resp.sendRedirect(\"/aa/index\");} 当然，重定向无论你怎么写，都是 Servlet/Jsp 中的知识点，上面两种写法都相当于是重回远古时代。 2.3 服务端跳转既然可以重定向，当然也可以服务端跳转，像下面这样： 1234@GetMapping(\"/test5\")public void test5(HttpServletRequest req,HttpServletResponse resp) { req.getRequestDispatcher(\"/WEB-INF/jsp/index.jsp\").forward(req,resp);} 2.4 返回字符串当然也可以利用 HttpServletResponse 返回其他字符串数据，包括但不局限于 JSON，像下面这样： 123456789101112131415161718192021@RequestMapping(\"/test2\")@ResponseBodypublic void test2(HttpServletResponse resp) throws IOException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); List&lt;Book&gt; books = new ArrayList&lt;&gt;(); Book b1 = new Book(); b1.setId(1); b1.setName(\"三国演义\"); b1.setAuthor(\"罗贯中\"); books.add(b1); Book b2 = new Book(); b2.setId(2); b2.setName(\"红楼梦\"); b2.setAuthor(\"曹雪芹\"); books.add(b2); String s = new Gson().toJson(books); out.write(s); out.flush(); out.close();} 这是返回值为 void 时候的情况，方法返回值为 void ，不一定就真的不返回了，可能还有其他的方式给前端数据。 3. String当 SpringMVC 方法的返回值为 String 类型时，也有几种不同情况。 3.1 逻辑视图名返回 String 最常见的是逻辑视图名，这种时候一般利用默认的参数 Model 来传递数据，像下面这样 ： 12345@RequestMapping(\"/hello\")public String aaa(Model model) { model.addAttribute(\"username\", \"张三\"); return \"hello\";} 此时返回的 hello 就是逻辑视图名，需要携带的数据放在 model 中。 3.2 重定向也可以重定向，事实上，如果在 SpringMVC 中有重定向的需求，一般采用这种方式： 1234@RequestMapping(\"/test4\")public String test4() { return \"redirect:/aa/index\";} 3.3 forward 转发也可以 forward 转发，事实上，如果在 SpringMVC 中有 forward 转发的需求，一般采用这种方式： 1234@RequestMapping(\"/test3\")public String test3() { return \"forward:/WEB-INF/jsp/order.jsp\";} 3.4 真的是 String当然，也有一种情况，就是你真的想返回一个 String ，此时，只要在方法上加上 @ResponseBody 注解即可，或者 Controller 上本身添加的是组合注解 @RestController，像下面这样： 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello provider!\"; }} 也可以像下面这样 ： 12345678@Controllerpublic class HelloController { @GetMapping(\"/hello\") @ResponseBody public String hello() { return \"hello provider!\"; }} 这是返回值为 String 的几种情况。 4. JSON返回 JSON 算是最最常见的了，现在前后端分离的趋势下，大部分后端只需要返回 JSON 即可，那么常见的 List 集合、Map，实体类等都可以返回，这些数据由 HttpMessageConverter 自动转为 JSON ，如果大家用了 Jackson 或者 Gson ，不需要额外配置就可以自动返回 JSON 了，因为框架帮我们提供了对应的 HttpMessageConverter ，如果大家使用了 Alibaba 的 Fastjson 的话，则需要自己手动提供一个相应的 HttpMessageConverter 的实例，方法的返回值像下面这样： 123456789101112131415161718192021222324@GetMapping(\"/user\")@ResponseBodypublic User getUser() { User user = new User(); List&lt;String&gt; favorites = new ArrayList&lt;&gt;(); favorites.add(\"足球\"); favorites.add(\"篮球\"); user.setFavorites(favorites); user.setUsername(\"zhagnsan\"); user.setPassword(\"123\"); return user;}@GetMapping(\"/users\")@ResponseBodypublic List&lt;User&gt; getALlUser() { List&lt;User&gt; users = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { User e = new User(); e.setUsername(\"zhangsan:\" + i); e.setPassword(\"pwd:\" + i); users.add(e); } return users;} 总结好了，这是松哥为大伙总结的 SpringMVC 方法四种不同类型的返回值，难倒是不难！有问题欢迎大伙留言讨论。","link":"/2019/0506/springmvc-return.html"},{"title":"你真的理解 Spring Boot 项目中的 parent 吗？","text":"前面和大伙聊了 Spring Boot 项目的三种创建方式，这三种创建方式，无论是哪一种，创建成功后，pom.xml 坐标文件中都有如下一段引用： 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; 对于这个 parent 的作用，你是否完全理解？有小伙伴说，不就是依赖的版本号定义在 parent 里边吗？是的，没错，但是 parent 的作用可不仅仅这么简单哦！本文松哥就来和大伙聊一聊这个 parent 到底有什么作用。 基本功能当我们创建一个 Spring Boot 工程时，可以继承自一个 spring-boot-starter-parent ，也可以不继承自它，我们先来看第一种情况。先来看 parent 的基本功能有哪些？ 定义了 Java 编译版本为 1.8 。 使用 UTF-8 格式编码。 继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。 执行打包操作的配置。 自动化的资源过滤。 自动化的插件配置。 针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。 请注意，由于application.properties和application.yml文件接受Spring样式占位符 $ {...} ，因此 Maven 过滤更改为使用 @ .. @ 占位符，当然开发者可以通过设置名为 resource.delimiter 的Maven 属性来覆盖 @ .. @ 占位符。 源码分析当我们创建一个 Spring Boot 项目后，我们可以在本地 Maven 仓库中看到看到这个具体的 parent 文件，以 2.1.4 这个版本为例，松哥 这里的路径是 C:\\Users\\sang\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-parent\\2.1.4.RELEASE\\spring-boot-starter-parent-2.1.4.RELEASE.pom ,打开这个文件，快速阅读文件源码，基本上就可以证实我们前面说的功能，如下图： 我们可以看到，它继承自 spring-boot-dependencies ，这里保存了基本的依赖信息，另外我们也可以看到项目的编码格式，JDK 的版本等信息，当然也有我们前面提到的数据过滤信息。最后，我们再根据它的 parent 中指定的 spring-boot-dependencies 位置，来看看 spring-boot-dependencies 中的定义： 在这里，我们看到了版本的定义以及 dependencyManagement 节点，明白了为啥 Spring Boot 项目中部分依赖不需要写版本号了。 不用 parent但是并非所有的公司都需要这个 parent ，有的时候，公司里边会有自己定义的 parent ，我们的 Spring Boot 项目要继承自公司内部的 parent ，这个时候该怎么办呢？ 一个简单的办法就是我们自行定义 dependencyManagement 节点，然后在里边定义好版本号，再接下来在引用依赖时也就不用写版本号了，像下面这样： 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 这样写之后，依赖的版本号问题虽然解决了，但是关于打包的插件、编译的 JDK 版本、文件的编码格式等等这些配置，在没有 parent 的时候，这些统统要自己去配置。 总结好了，一篇简单的文章，向大伙展示一下 Spring Boot 项目中 parent 的作用，有问题欢迎留言讨论。","link":"/2019/0413/spring-boot-parent.html"},{"title":"干货最新版 Spring Boot2.1.5 教程+案例合集","text":"最近发了一系列的 Spring Boot 教程，但是发的时候没有顺序，有小伙伴反映不知道该从哪篇文章开始看起，刚好最近工作告一个小小段落，松哥就把这些资料按照学习顺序重新整理了一遍，给大家做一个索引，大家照着索引就可以由浅入深的学习了。 松哥刚开始写这个系列的时候最新版是 Spring Boot2.1.4 ，后来写着写着版本升级了变成 Spring Boot2.1.5 了，于是我又用 Spring Boot2.1.5 接着写，因此索引中的教程主要是这两个版本的教程。 可能有人觉得小版本的变化差异不大，事实上也确实如此，不过变化不大不意味着没有变化，给大家随便举两个例子： 在整合 Redis 时，Spring Boot2.1.4 不用引入 Spring Security，而 Spring Boot2.1.5 则需要引入 Spring Security。 再比如 Spring Security 中的角色继承，在 Spring Boot2.0.8 之前和之后的写法完全不同，这些差异松哥也给大家细细剖析了。 这一系列教程不是终点，而是一个起点，松哥后期还会不断完善这个教程，也会持续更新 Spring Boot 最新版本的教程，希望能帮到大家。教程索引如下： 创建一个 Spring Boot 项目，你会几种方法？ 这一次，我连 web.xml 都不要了，纯 Java 搭建 SSM 环境 你真的理解 Spring Boot 项目中的 parent 吗？ 一文读懂 Spring Boot 配置文件 application.properties ！ Spring Boot中的yaml配置简介 Spring Boot 中的静态资源到底要放在哪里？ 极简 Spring Boot 整合 Thymeleaf 页面模板 Spring Boot 中关于自定义异常处理的套路！ Spring Boot中通过CORS解决跨域问题 SpringMVC 中 @ControllerAdvice 注解的三种使用场景！ Spring Boot中，Redis缓存还能这么用！ Spring Boot 操作 Redis，三种方案全解析！ Spring Boot 一个依赖搞定 session 共享，没有比这更简单的方案了！ 另一种缓存，Spring Boot 整合 Ehcache 徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！ Spring Boot 定义系统启动任务，你会几种方式？ 干货|一文读懂 Spring Data Jpa！ Spring Boot数据持久化之JdbcTemplate Spring Boot多数据源配置之JdbcTemplate 最简单的SpringBoot整合MyBatis教程 极简Spring Boot整合MyBatis多数据源 Spring Boot 中 10 行代码构建 RESTful 风格应用 Spring Boot 整合 Shiro ，两种方式全总结！ 干货|一个案例学会Spring Security 中使用 JWT! Spring Security 中的角色继承问题 Spring Security 登录添加验证码 SpringSecurity登录使用JSON格式数据 Spring Boot 中实现定时任务的两种方式! SpringBoot整合Swagger2，再也不用维护接口文档了！ 整理了八个开源的 Spring Boot 学习资源 另外，还有一件重要的事，就是松哥把微信公众号中文章的案例，都整理到 GitHub 上了，每个案例都对应了一篇解读的文章，方便大家学习。松哥以前写博客没养成好习惯，有的案例丢失了，现在在慢慢整理补上。 GitHub 仓库地址：https://github.com/lenve/javaboy-code-samples，欢迎大家 star。已有的案例如下图： 好了，这就是松哥说的干货，大家撸起袖子加油学吧！","link":"/2019/0614/springboot-resources.html"},{"title":"创建一个 Spring Boot 项目，你会几种方法？","text":"我最早是 2016 年底开始写 Spring Boot 相关的博客，当时使用的版本还是 1.4.x ，文章发表在 CSDN 上，阅读量最大的一篇有 42W+，如下图： 2017 年由于种种原因，就没有再继续更新 Spring Boot 相关的博客了，2018年又去写书了，也没更新，现在 Spring Boot 最新稳定版是 2.1.4 ，松哥想针对此写一个系列教程，专门讲 Spring Boot2 中相关的知识点。这个系列，就从本篇开始吧。 Spring Boot 介绍我们刚开始学习 JavaWeb 的时候，使用 Servlet/JSP 做开发，一个接口搞一个 Servlet ，很头大，后来我们通过隐藏域或者反射等方式，可以减少 Servlet 的创建，但是依然不方便，再后来，我们引入 Struts2/SpringMVC 这一类的框架，来简化我们的开发 ，和 Servlet/JSP 相比，引入框架之后，生产力确实提高了不少，但是用久了，又发现了新的问题，即配置繁琐易出错，要做一个新项目，先搭建环境，环境搭建来搭建去，就是那几行配置，不同的项目，可能就是包不同，其他大部分的配置都是一样的，Java 总是被人诟病配置繁琐代码量巨大，这就是其中一个表现。那么怎么办？Spring Boot 应运而生，Spring Boot 主要提供了如下功能： 为所有基于 Spring 的 Java 开发提供方便快捷的入门体验。 开箱即用，有自己自定义的配置就是用自己的，没有就使用官方提供的默认的。 提供了一系列通用的非功能性的功能，例如嵌入式服务器、安全管理、健康检测等。 绝对没有代码生成，也不需要XML配置。 Spring Boot 的出现让 Java 开发又回归简单，因为确确实实解决了开发中的痛点，因此这个技术得到了非常广泛的使用，松哥很多朋友出去面试 Java 工程师，从2017年年初开始，Spring Boot基本就是必问，现在流行的 Spring Cloud 微服务也是基于 Spring Boot，因此，所有的 Java 工程师都有必要掌握好 Spring Boot。 系统要求截至本文写作（2019.04.11），Spring Boot 目前最新版本是 2.1.4，要求至少 JDK8，集成的 Spring 版本是 5.1.6 ，构建工具版本要求如下： Build Tool Version Maven 3.3+ Gradle 4.4+ 内置的容器版本分别如下： Name Version Tomcat 9.0 4.0 Jetty 9.4 3.1 Undertow 2.0 4.0 三种创建方式初学者看到 Spring Boot 工程创建成功后有那么多文件就会有点懵圈，其实 Spring Boot 工程本质上就是一个 Maven 工程，从这个角度出发，松哥在这里向大家介绍三种项目创建方式。 在线创建这是官方提供的一个创建方式，实际上，如果我们使用开发工具去创建 Spring Boot 项目的话（即第二种方案），也是从这个网站上创建的，只不过这个过程开发工具帮助我们完成了，我们只需要在开发工具中进行简单的配置即可。 首先打开 https://start.spring.io 这个网站，如下： 这里要配置的按顺序分别如下： 项目构建工具是 Maven 还是 Gradle ？松哥见到有人用 Gradle 做 Java 后端项目，但是整体感觉 Gradle 在 Java 后端中使用的还是比较少，Gradle 在 Android 中使用较多，Java 后端，目前来看还是 Maven 为主，因此这里选择第一项。 开发语言，这个当然是选择 Java 了。 Spring Boot 版本，可以看到，目前最新的稳定版是 2.1.4 ，这里我们就是用最新稳定版。 既然是 Maven 工程，当然要有项目坐标，项目描述等信息了，另外这里还让输入了包名，因为创建成功后会自动创建启动类。 Packing 表示项目要打包成 jar 包还是 war 包，Spring Boot 的一大优势就是内嵌了 Servlet 容器，打成 jar 包后可以直接运行，所以这里建议打包成 jar 包，当然，开发者根据实际情况也可以选择 war 包。 然后选选择构建的 JDK 版本。 最后是选择所需要的依赖，输入关键字如 web ，会有相关的提示，这里我就先加入 web 依赖。 所有的事情全部完成后，点击最下面的 Generate Project 按钮，或者点击 Alt+Enter 按键，此时会自动下载项目，将下载下来的项目解压，然后用 IntelliJ IDEA 或者 Eclipse 打开即可进行开发。 使用开发工具创建有人觉得上面的步骤太过于繁琐，那么也可以使用 IDE 来创建，松哥这里以 IntelliJ IDEA 和 STS 为例，需要注意的是，IntelliJ IDEA 只有 ultimate 版才有直接创建 Spring Boot 项目的功能，社区版是没有此项功能的。 IntelliJ IDEA首先在创建项目时选择 Spring Initializr，如下图： 然后点击 Next ，填入 Maven 项目的基本信息，如下： 再接下来选择需要添加的依赖，如下图： 勾选完成后，点击 Next 完成项目的创建。 STS这里我再介绍下 Eclipse 派系的 STS 给大家参考， STS 创建 Spring Boot 项目，实际上也是从上一小节的那个网站上来的，步骤如下： 首先右键单击，选择 New -&gt; Spring Starter Project ，如下图： 然后在打开的页面中填入项目的相关信息，如下图： 这里的信息和前面提到的都一样，不再赘述。最后一路点击 Next ，完成项目的创建。 Maven 创建上面提到的几种方式，实际上都借助了 https://start.spring.io/ 这个网站，松哥记得在 2017 年的时候，这个网站还不是很稳定，经常发生项目创建失败的情况，从2018年开始，项目创建失败就很少遇到了，不过有一些读者偶尔还是会遇到这个问题，他们会在微信上问松哥这个问题腰怎么处理？我一般给的建议就是直接使用 Maven 来创建项目。步骤如下： 首先创建一个普通的 Maven 项目，以 IntelliJ IDEA 为例，创建步骤如下： 注意这里不用选择项目骨架（如果大伙是做练习的话，也可以去尝试选择一下，这里大概有十来个 Spring Boot 相关的项目骨架），直接点击 Next ，下一步中填入一个 Maven 项目的基本信息，如下图： 然后点击 Next 完成项目的创建。 创建完成后，在 pom.xml 文件中，添加如下依赖： 1234567891011&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加成功后，再在 java 目录下创建包，包中创建一个名为 App 的启动类，如下： 1234567891011@EnableAutoConfiguration@RestControllerpublic class App { public static void main(String[] args) { SpringApplication.run(App.class, args); } @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} @EnableAutoConfiguration 注解表示开启自动化配置。 然后执行这里的 main 方法就可以启动一个 Spring Boot 工程了。 项目结构使用工具创建出来的项目结构大致如下图： 对于我们来说，src 是最熟悉的， Java 代码和配置文件写在这里，test 目录用来做测试，pom.xml 是 Maven 的坐标文件，就这几个。 总结本文主要向大家介绍了三种创建 Spring Boot 工程的方式，大家有更6的方法欢迎来讨论。","link":"/2019/0412/springboot-init.html"},{"title":"另一种缓存，Spring Boot 整合 Ehcache","text":"用惯了 Redis ，很多人已经忘记了还有另一个缓存方案 Ehcache ，是的，在 Redis 一统江湖的时代，Ehcache 渐渐有点没落了，不过，我们还是有必要了解下 Ehcache ，在有的场景下，我们还是会用到 Ehcache。 今天松哥就来和大家聊聊 Spring Boot 中使用 Ehcache 的情况。相信看完本文，大家对于[Spring Boot 操作 Redis，三种方案全解析！]一文中的第二种方案会有更加深刻的理解。 Ehcache 也是 Java 领域比较优秀的缓存方案之一，Ehcache 这个缓存的名字很有意思，正着念反着念，都是 Ehcache，Spring Boot 中对此也提供了很好的支持，这个支持主要是通过 Spring Cache 来实现的。 Spring Cache 可以整合 Redis，当然也可以整合 Ehcache，两种缓存方案的整合还是比较相似，主要是配置的差异，具体的用法是一模一样的，就类似于 JDBC 和 数据库驱动的关系一样。前面配置完成后，后面具体使用的 API 都是一样的。 和 Spring Cache + Redis 相比，Spring Cache + Ehcache 主要是配置有所差异，具体的用法是一模一样的。我们来看下使用步骤。 项目创建首先，来创建一个 Spring Boot 项目，引入 Cache 依赖： 工程创建完成后，引入 Ehcache 的依赖，Ehcache 目前有两个版本： 这里采用第二个，在 pom.xml 文件中，引入 Ehcache 依赖： 123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt; &lt;version&gt;2.10.6&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加 Ehcache 配置在 resources 目录下，添加 ehcache 的配置文件 ehcache.xml ，文件内容如下： 123456789101112131415161718&lt;ehcache&gt; &lt;diskStore path=\"java.io.tmpdir/shiro-spring-sample\"/&gt; &lt;defaultCache maxElementsInMemory=\"10000\" eternal=\"false\" timeToIdleSeconds=\"120\" timeToLiveSeconds=\"120\" overflowToDisk=\"false\" diskPersistent=\"false\" diskExpiryThreadIntervalSeconds=\"120\" /&gt; &lt;cache name=\"user\" maxElementsInMemory=\"10000\" eternal=\"true\" overflowToDisk=\"true\" diskPersistent=\"true\" diskExpiryThreadIntervalSeconds=\"600\"/&gt;&lt;/ehcache&gt; 配置含义： name:缓存名称。 maxElementsInMemory：缓存最大个数。 eternal:对象是否永久有效，一但设置了，timeout将不起作用。 timeToIdleSeconds：设置对象在失效前的允许闲置时间（单位：秒）。仅当eternal=false对象不是永久有效时使用，可选属性，默认值是0，也就是可闲置时间无穷大。 timeToLiveSeconds：设置对象在失效前允许存活时间（单位：秒）。最大时间介于创建时间和失效时间之间。仅当eternal=false对象不是永久有效时使用，默认是0.，也就是对象存活时间无穷大。 overflowToDisk：当内存中对象数量达到maxElementsInMemory时，Ehcache将会对象写到磁盘中。 diskSpoolBufferSizeMB：这个参数设置DiskStore（磁盘缓存）的缓存区大小。默认是30MB。每个Cache都应该有自己的一个缓冲区。 maxElementsOnDisk：硬盘最大缓存个数。 diskPersistent：是否缓存虚拟机重启期数据。 diskExpiryThreadIntervalSeconds：磁盘失效线程运行时间间隔，默认是120秒。 memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。默认策略是LRU（最近最少使用）。你可以设置为FIFO（先进先出）或是LFU（较少使用）。 clearOnFlush：内存数量最大时是否清除。 diskStore 则表示临时缓存的硬盘目录。 注意 默认情况下，这个文件名是固定的，必须叫 ehcache.xml ，如果一定要换一个名字，那么需要在 application.properties 中明确指定配置文件名，配置方式如下： 1spring.cache.ehcache.config=classpath:aaa.xml 开启缓存开启缓存的方式，也和 Redis 中一样，如下添加 @EnableCaching 依赖即可： 1234567@SpringBootApplication@EnableCachingpublic class EhcacheApplication { public static void main(String[] args) { SpringApplication.run(EhcacheApplication.class, args); }} 其实到这一步，Ehcache 就算配置完成了，接下来的用法，和松哥之前讲 Redis 的文章一模一样。不过这里松哥还是带大家使用下。 使用缓存这里主要向小伙伴们介绍缓存中几个核心的注解使用。 @CacheConfig这个注解在类上使用，用来描述该类中所有方法使用的缓存名称，当然也可以不使用该注解，直接在具体的缓存注解上配置名称，示例代码如下： 1234@Service@CacheConfig(cacheNames = \"user\")public class UserService {} @Cacheable这个注解一般加在查询方法上，表示将一个方法的返回值缓存起来，默认情况下，缓存的 key 就是方法的参数，缓存的 value 就是方法的返回值。示例代码如下： 12345@Cacheable(key = \"#id\")public User getUserById(Integer id,String username) { System.out.println(\"getUserById\"); return getUserFromDBById(id);} 当有多个参数时，默认就使用多个参数来做 key ，如果只需要其中某一个参数做 key ，则可以在 @Cacheable 注解中，通过 key 属性来指定 key ，如上代码就表示只使用 id 作为缓存的 key ，如果对 key 有复杂的要求，可以自定义 keyGenerator 。当然，Spring Cache 中提供了root对象，可以在不定义 keyGenerator 的情况下实现一些复杂的效果，root 对象有如下属性： 也可以通过 keyGenerator 自定义 key ，方式如下： 1234567@Componentpublic class MyKeyGenerator implements KeyGenerator { @Override public Object generate(Object target, Method method, Object... params) { return method.getName()+Arrays.toString(params); }} 然后在方法上使用该 keyGenerator ： 12345678@Cacheable(keyGenerator = \"myKeyGenerator\")public User getUserById(Long id) { User user = new User(); user.setId(id); user.setUsername(\"lisi\"); System.out.println(user); return user;} @CachePut这个注解一般加在更新方法上，当数据库中的数据更新后，缓存中的数据也要跟着更新，使用该注解，可以将方法的返回值自动更新到已经存在的 key 上，示例代码如下： 1234@CachePut(key = \"#user.id\")public User updateUserById(User user) { return user;} @CacheEvict这个注解一般加在删除方法上，当数据库中的数据删除后，相关的缓存数据也要自动清除，该注解在使用的时候也可以配置按照某种条件删除（ condition 属性）或者或者配置清除所有缓存（ allEntries 属性），示例代码如下： 1234@CacheEvict()public void deleteUserById(Integer id) { //在这里执行删除操作， 删除是去数据库中删除} 总结本文主要向大家了 Spring Boot 整合 Ehcache 的用法，其实说白了还是 Spring Cache 的用法。相信读完本文，大家对于 Redis + Spring Cache 的用法会有更深的认识。 本文案例我已上传到 GitHub ，欢迎大家 star：https://github.com/lenve/javaboy-code-samples 关于本文，有问题欢迎留言讨论。","link":"/2019/0612/springboot-ehcache.html"},{"title":"做IT这几年，我整理了这些干货想要送给你!","text":"没有一条路是容易的，特别是转行计算机这条路。 松哥接触过很多转行做开发的小伙伴，我了解到很多转行人的不容易，记得松哥大二时刚刚决定转行计算机，完全不知道这些东西到底应该怎么学，每天就是抱着书啃，书倒是读懂了，但是实在想不出来那些排序算法、水仙花数和做应用有啥关系！ 后来无意中从同学那里看到了一套某培训机构的视频教程，在那个百度云盘还没有流行开的年代，视频都是从优酷上下载，那个时候优酷还是在线视频领域的一哥，不过那个时候网上视频教程的分辨率简直惨不忍睹，不过我还是耐着性子看完了从同学那里拷贝的视频教程，从此打开了一扇新大门，总算把书上的理论和实际应用联系在一起了，以至于后来有人问松哥如何快速掌握一门新技术，我给的建议一般都是先看视频快速上手，再看书深入学习（关于松哥入行这块，有很多好玩的事，以后和各位小伙伴分享）。 工作后，一方面时间更加紧张了，另一方面，随着经验的加深，很多东西看看文档也都能快速掌握，所以就很少看视频了，但是想着各位小伙伴中有不少是初学者，他们经常来找松哥要资料，索性，我把压箱底的东西就都拿出来吧。 精品资源松哥整理了手上一些视频资源，这些资源很多都是最新的视频，但是由于资源数据流巨大，超过 500G ，涉及到 Java 基础、Web 基础、数据库、Java 高级、Java 分布式、微服务、大数据、人工智能、大前端、Python 等，非常丰富，这么多资料如果我分享成一个文件夹，那个链接很容易失效，反而不利于大家以后查看，因此我将资源细分了下，确保每一个分享链接不会轻易失效，这些资源都是长期有效的，大家需要时可以随时来松哥公众号中按需索取，如果有某一个链接失效了，大家也可以在本文末尾留言，我会重新分享。 资源获取方式：根据下面的索引，大家可以选择自己需要的资源，然后在松哥公众号【牧码小子】后台回复对应的口令，就可以获取到资源的百度云盘下载地址。公众号二维码如下： 另外本文会定期更新，松哥有新资源的时候会及时分享给大家，欢迎各位小伙伴保持关注。 Java 基础 资源名称 口令 Java 基础语法 javaboy4096 Java 面向对象 javaboy6148 JavaSE 飞机大战项目 javaboy2053 深入面向对象和数组 javaboy8200 Java 常用类详解 javaboy4105 Java 异常机制解析 javaboy6157 Java 集合与数据结构 javaboy2062 JavaIO 流全解析 javaboy8209 深入理解 Java 多线程 javaboy4114 Java 网络编程 javaboy6166 手动开发一个 Web 服务器 javaboy2071 深入理解 Java 注解+反射 javaboy8218 Java23 种设计模式 javaboy4123 学会 Java 正则表达式 javaboy6175 JDBC 详解 javaboy2080 独立开发 SORM 框架 javaboy8227 快人一步，Java10 新特性全解析 javaboy4132 Java 数据结构和算法 javaboy6184 深入理解 Java 虚拟机 javaboy2089 Java 解析XML文件 javaboy8236 数据库 资源名称 口令 Oracle 数据库安装及简单 SQL javaboy4141 Oracle 账户管理及查询语句 javaboy6193 Oracle 中的函数 javaboy2098 Oracle 中的子查询 javaboy8245 Oracle 中常见的表操作 javaboy4150 Oracle 中的数据备份 javaboy6202 MySQL 基础 javaboy2107 PowerDesigner 教程 javaboy8254 JDBC 操作数据库 javaboy4159 MySQL 优化 javaboy6211 Oracle 高级课程 javaboy2116 数据库与 SQL 优化 javaboy6283 数据库集群与高并发 javaboy2188 Web 基础 资源名称 口令 HTML 入门教程 javaboy8263 CSS 教程 javaboy4168 JavaScript 视频教程 javaboy6220 jQuery 视频教程 javaboy2125 EasyUI 视频教程 javaboy8272 Servlet 基础 javaboy4177 Servlet 中的 Request 和 Response javaboy6229 Servlet 请求转发与重定向 javaboy2134 Session 和 Cookie javaboy8281 JSP 详解 javaboy4186 用户管理系统实战 javaboy6238 Ajax 详解 javaboy2143 EL 和 JSTL javaboy8290 过滤器详解 javaboy4195 监听器详解 javaboy6247 KnockoutJS 实战视频 javaboy2152 Java 高级 资源名称 口令 IntelliJIDEA 视频教程 javaboy4285 Java 高并发秒杀方案 javaboy8299 Activiti 工作流实战解析 javaboy4204 Java 并发编程与高并发实战 javaboy6256 Linux 快速入门 javaboy2161 Maven 详解 javaboy8308 Git 应用详解 javaboy4213 Svn 入门教程 javaboy6265 高并发编程与线程池 javaboy2170 系统优化与 JVM 调优 javaboy8317 Java 编程规范 javaboy4222 AIO、BIO、NIO 详解 javaboy6274 Netty 高级视频教程 javaboy2179 ActiveMQ 消息中间详解 javaboy8326 单点登录视频教程 javaboy4231 Dubbo 详解 javaboy8335 Redis 全解析 javaboy4240 VSFTPD+NGINX 视频教程 javaboy6292 MyBatis 视频教程 javaboy2197 Spring4 视频教程 javaboy8344 SpringMVC 视频教程 javaboy4249 SSM 框架整合视频教程 javaboy6301 RBAC 权限控制视频教程 javaboy2206 Hibernate4 视频教程 javaboy8353 Jfinal 视频教程 javaboy4258 Shiro 视频教程 javaboy6310 Solr 视频教程 javaboy2215 Struts2 视频教程 javaboy8362 Nginx 视频教程 javaboy4267 Redis 缓存详解 javaboy6319 JVM 虚拟机优化 javaboy2224 Zookeeper 详解视频 javaboy8371 Linux 基本操作 javaboy6328 架构师面试攻略（文档） javaboy2233 架构师面试攻略（视频） javaboy8380 JUC 视频教程 javaboy6400 MySQL 高级教程 javaboy2305 Java 邮件开发教程 javaboy8452 Maven 实战视频 javaboy8443 自己 DIY 一个 Tomcat javaboy4339 大前端 资源名称 口令 HTML5 新特性 javaboy4276 AngularJS 视频教程 javaboy6337 Grunt 视频教程 javaboy2242 Gulp 视频教程 javaboy8389 Webpack 视频教程 javaboy4294 Bootstrap 视频教程 javaboy6346 CSS3 视频教程 javaboy2251 ES6 视频教程 javaboy8398 HTML5 核心技术 javaboy4303 HTML5 实战 javaboy6355 HTML5 项目实战 javaboy2260 JS 模块化视频教程 javaboy8407 less 视频教程 javaboy4312 NodeJS 视频教程 javaboy6364 React 视频教程 javaboy2269 Zepto 视频教程 javaboy8416 HTML+CSS 实战视频 javaboy4321 JavaScript140 集 javaboy6373 jQuery 视频教程 javaboy2278 JavaScript 高级语法视频教程 javaboy8425 Vue 项目实战视频 javaboy4330 CSS3 特效实战 javaboy6382 HTML5 特效实战 javaboy2287 HTML5+Canvas 实现刮刮卡 javaboy8434 Gradle 从入门到精通 javaboy6391 mpvue 项目实战 javaboy2296 Vue 最新最全视频教程 javaboy4348 大数据 资源名称 口令 Linux 操作系统 javaboy4357 Linux 基本命令 javaboy6409 Linux 文件安装 javaboy2314 Shell 编程 javaboy8461 网络基础知识 javaboy4366 LVS 集群与高并发 javaboy6418 Nginx 和高并发 javaboy2323 keepalive 和单点故障 javaboy8470 HDFS 分布式文件系统 javaboy4375 mapreduce 分布式计算 javaboy6427 YARN 资源管理与任务调度 javaboy2332 mapreduce 计算案例 javaboy8479 HIVE 视频教程 javaboy4384 Hbase 数据库详解 javaboy6436 zookeeper 协同处理 javaboy2341 CDH 使用 javaboy8488 HUE 使用 javaboy4393 IMPALA 详解 javaboy6445 oozie 详解 javaboy2350 elasticsearch 详解 javaboy8497 Redis 内存数据 javaboy4402 Scala 入门 javaboy6454 Spark 详解 javaboy2359 Spark 高级 javaboy8506 Spark-Stream 流式计算 javaboy4411 Kafka 分布式消息队列 javaboy6463 STORM 流式计算框架 javaboy2368 Python 语言基础 javaboy8515 回归算法 javaboy4420 分类算法、决策树 javaboy6472 聚类算法、微博案例 javaboy2377 推荐算法 javaboy8524 大型电商日志分析（项目实战） javaboy4429 智慧交通（项目实战） javaboy6481 智能 App（项目实战） javaboy2386 人工智能 资源名称 口令 人工智能入门 javaboy8533 线性回归深入与代码实现 javaboy4438 梯度下降算发实现 javaboy6490 逻辑回归详解和应用 javaboy2395 分类项目案例与神经网络算法 javaboy8542 多分类、决策树分类与随机森林分类 javaboy4447 分类评估与聚类 javaboy6499 密度聚类与谱聚类 javaboy2404 Tensorflow 安装并实现线性回归 javaboy8551 TensorFlow 深入、TensorFlow可视化 javaboy4456 DNN 深度神经网络手写图片识别 javaboy6508 TensorBoard 可视化 javaboy2413 卷积神经网络、CNN 识别图片 javaboy8560 卷积神经网络深入，AlexNet 模型实现 javaboy4465 Keras 深度学习框架 javaboy6517 总结资源还是不错的，松哥也是费了很大功夫才整理好的，希望对大家的技能提升有所帮助。","link":"/2019/0527/study-resources.html"},{"title":"徒手撸一个 Spring Boot 中的 Starter ，解密自动化配置黑魔法！","text":"我们使用 Spring Boot，基本上都是沉醉在它 Stater 的方便之中。Starter 为我们带来了众多的自动化配置，有了这些自动化配置，我们可以不费吹灰之力就能搭建一个生产级开发环境，有的小伙伴会觉得这个 Starter 好神奇呀！其实 Starter 也都是 Spring + SpringMVC 中的基础知识点实现的，今天松哥就来带大家自己来撸一个 Starter ，慢慢揭开 Starter 的神秘面纱！ 核心知识其实 Starter 的核心就是条件注解 @Conditional ，当 classpath 下存在某一个 Class 时，某个配置才会生效，前面松哥已经带大家学习过不少 Spring Boot 中的知识点，有的也涉及到源码解读，大伙可能也发现了源码解读时总是会出现条件注解，其实这就是 Starter 配置的核心之一，大伙有兴趣可以翻翻历史记录，看看松哥之前写的关于 Spring Boot 的文章，这里我就不再重复介绍了。 定义自己的 Starter定义所谓的 Starter ，其实就是一个普通的 Maven 项目，因此我们自定义 Starter ，需要首先创建一个普通的 Maven 项目，创建完成后，添加 Starter 的自动化配置类即可，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 配置完成后，我们首先创建一个 HelloProperties 类，用来接受 application.properties 中注入的值，如下： 12345678910111213141516171819@ConfigurationProperties(prefix = \"javaboy\")public class HelloProperties { private static final String DEFAULT_NAME = \"江南一点雨\"; private static final String DEFAULT_MSG = \"牧码小子\"; private String name = DEFAULT_NAME; private String msg = DEFAULT_MSG; public String getName() { return name; } public void setName(String name) { this.name = name; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; }} 这个配置类很好理解，将 application.properties 中配置的属性值直接注入到这个实例中， @ConfigurationProperties 类型安全的属性注入，即将 application.properties 文件中前缀为 javaboy 的属性注入到这个类对应的属性上， 最后使用时候，application.properties 中的配置文件，大概如下： 12javaboy.name=zhangsanjavaboy.msg=java 关注类型安全的属性注入，读者可以参考松哥之前的这篇文章：Spring Boot中的yaml配置简介，这篇文章虽然是讲 yaml 配置，但是关于类型安全的属性注入和 properties 是一样的。 配置完成 HelloProperties 后，接下来我们来定义一个 HelloService ，然后定义一个简单的 say 方法， HelloService 的定义如下： 12345678910111213141516171819public class HelloService { private String msg; private String name; public String sayHello() { return name + \" say \" + msg + \" !\"; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public String getName() { return name; } public void setName(String name) { this.name = name; }} 这个很简单，没啥好说的。 接下来就是我们的重轴戏，自动配置类的定义，用了很多别人定义的自定义类之后，我们也来自己定义一个自定义类。先来看代码吧，一会松哥再慢慢解释： 123456789101112131415@Configuration@EnableConfigurationProperties(HelloProperties.class)@ConditionalOnClass(HelloService.class)public class HelloServiceAutoConfiguration { @Autowired HelloProperties helloProperties; @Bean HelloService helloService() { HelloService helloService = new HelloService(); helloService.setName(helloProperties.getName()); helloService.setMsg(helloProperties.getMsg()); return helloService; }} 关于这一段自动配置，解释如下： 首先 @Configuration 注解表明这是一个配置类。 @EnableConfigurationProperties 注解是使我们之前配置的 @ConfigurationProperties 生效，让配置的属性成功的进入 Bean 中。 @ConditionalOnClass 表示当项目当前 classpath 下存在 HelloService 时，后面的配置才生效。 自动配置类中首先注入 HelloProperties ，这个实例中含有我们在 application.properties 中配置的相关数据。 提供一个 HelloService 的实例，将 HelloProperties 中的值注入进去。 做完这一步之后，我们的自动化配置类就算是完成了，接下来还需要一个 spring.factories 文件，那么这个文件是干嘛的呢？大家知道我们的 Spring Boot 项目的启动类都有一个 @SpringBootApplication 注解，这个注解的定义如下： 12345678@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })public @interface SpringBootApplication {} 大家看到这是一个组合注解，其中的一个组合项就是 @EnableAutoConfiguration ，这个注解是干嘛的呢？ @EnableAutoConfiguration 表示启用 Spring 应用程序上下文的自动配置，该注解会自动导入一个名为 AutoConfigurationImportSelector 的类,而这个类会去读取一个名为 spring.factories 的文件, spring.factories 中则定义需要加载的自动化配置类，我们打开任意一个框架的 Starter ，都能看到它有一个 spring.factories 文件，例如 MyBatis 的 Starter 如下： 那么我们自定义 Starter 当然也需要这样一个文件，我们首先在 Maven 项目的 resources 目录下创建一个名为 META-INF 的文件夹，然后在文件夹中创建一个名为 spring.factories 的文件，文件内容如下： 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=org.javaboy.mystarter.HelloServiceAutoConfiguration 在这里指定我们的自动化配置类的路径即可。 如此之后我们的自动化配置类就算完成了。 本地安装如果在公司里，大伙可能需要将刚刚写好的自动化配置类打包，然后上传到 Maven 私服上，供其他同事下载使用，我这里就简单一些，我就不上传私服了，我将这个自动化配置类安装到本地仓库，然后在其他项目中使用即可。安装方式很简单，在 IntelliJ IDEA 中，点击右边的 Maven Project ，然后选择 Lifecycle 中的 install ，双击即可，如下： 双击完成后，这个 Starter 就安装到我们本地仓库了，当然小伙伴也可以使用 Maven 命令去安装。 使用 Starter接下来，我们来新建一个普通的 Spring Boot 工程，这个 Spring Boot 创建成功之后，加入我们自定义 Starter 的依赖，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.javaboy&lt;/groupId&gt; &lt;artifactId&gt;mystarter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 此时我们引入了上面自定义的 Starter ，也即我们项目中现在有一个默认的 HelloService 实例可以使用，而且关于这个实例的数据，我们还可以在 application.properties 中进行配置，如下： 12javaboy.name=牧码小子javaboy.msg=java 配置完成后，方便起见，我这里直接在单元测试方法中注入 HelloSerivce 实例来使用，代码如下： 1234567891011@RunWith(SpringRunner.class)@SpringBootTestpublic class UsemystarterApplicationTests { @Autowired HelloService helloService; @Test public void contextLoads() { System.out.println(helloService.sayHello()); }} 执行单元测试方法，打印日志如下： 好了，一个简单的自动化配置类我们就算完成了，是不是很简单！ 总结本文主要带领小伙伴自己徒手撸一个 Starter ，使用这种方式帮助大家揭开 Starter 的神秘面纱！大伙有问题可以留言讨论。 本文的案例，松哥已经上传到 GitHub上了，地址：https://github.com/lenve/javaboy-code-samples 。","link":"/2019/0520/springboot-starter.html"},{"title":"干货|一个案例学会Spring Security 中使用 JWT","text":"在前后端分离的项目中，登录策略也有不少，不过 JWT 算是目前比较流行的一种解决方案了，本文就和大家来分享一下如何将 Spring Security 和 JWT 结合在一起使用，进而实现前后端分离时的登录解决方案。 1 无状态登录1.1 什么是有状态？有状态服务，即服务端需要记录每次会话的客户端信息，从而识别客户端身份，根据用户身份进行请求的处理，典型的设计如Tomcat中的Session。例如登录：用户登录后，我们把用户的信息保存在服务端session中，并且给用户一个cookie值，记录对应的session，然后下次请求，用户携带cookie值来（这一步有浏览器自动完成），我们就能识别到对应session，从而找到用户的信息。这种方式目前来看最方便，但是也有一些缺陷，如下： 服务端保存大量数据，增加服务端压力 服务端保存用户状态，不支持集群化部署 1.2 什么是无状态微服务集群中的每个服务，对外提供的都使用RESTful风格的接口。而RESTful风格的一个最重要的规范就是：服务的无状态性，即： 服务端不保存任何客户端请求者信息 客户端的每次请求必须具备自描述信息，通过这些信息识别客户端身份 那么这种无状态性有哪些好处呢？ 客户端请求不依赖服务端的信息，多次请求不需要必须访问到同一台服务器 服务端的集群和状态对客户端透明 服务端可以任意的迁移和伸缩（可以方便的进行集群化部署） 减小服务端存储压力 1.3.如何实现无状态无状态登录的流程： 首先客户端发送账户名/密码到服务端进行认证 认证通过后，服务端将用户信息加密并且编码成一个token，返回给客户端 以后客户端每次发送请求，都需要携带认证的token 服务端对客户端发送来的token进行解密，判断是否有效，并且获取用户登录信息 1.4 JWT1.4.1 简介JWT，全称是Json Web Token， 是一种JSON风格的轻量级的授权和身份认证规范，可实现无状态、分布式的Web应用授权： JWT 作为一种规范，并没有和某一种语言绑定在一起，常用的Java 实现是GitHub 上的开源项目 jjwt，地址如下：https://github.com/jwtk/jjwt 1.4.2 JWT数据格式JWT包含三部分数据： Header：头部，通常头部有两部分信息： 声明类型，这里是JWT 加密算法，自定义 我们会对头部进行Base64Url编码（可解码），得到第一部分数据。 Payload：载荷，就是有效数据，在官方文档中(RFC7519)，这里给了7个示例信息： iss (issuer)：表示签发人 exp (expiration time)：表示token过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 这部分也会采用Base64Url编码，得到第二部分数据。 Signature：签名，是整个数据的认证信息。一般根据前两步的数据，再加上服务的的密钥secret（密钥保存在服务端，不能泄露给客户端），通过Header中配置的加密算法生成。用于验证整个数据完整和可靠性。 生成的数据格式如下图： 注意，这里的数据通过 . 隔开成了三部分，分别对应前面提到的三部分，另外，这里数据是不换行的，图片换行只是为了展示方便而已。 1.4.3 JWT交互流程流程图： 步骤翻译： 应用程序或客户端向授权服务器请求授权 获取到授权后，授权服务器会向应用程序返回访问令牌 应用程序使用访问令牌来访问受保护资源（如API） 因为JWT签发的token中已经包含了用户的身份信息，并且每次请求都会携带，这样服务的就无需保存用户信息，甚至无需去数据库查询，这样就完全符合了RESTful的无状态规范。 1.5 JWT 存在的问题说了这么多，JWT 也不是天衣无缝，由客户端维护登录状态带来的一些问题在这里依然存在，举例如下： 续签问题，这是被很多人诟病的问题之一，传统的cookie+session的方案天然的支持续签，但是jwt由于服务端不保存用户状态，因此很难完美解决续签问题，如果引入redis，虽然可以解决问题，但是jwt也变得不伦不类了。 注销问题，由于服务端不再保存用户信息，所以一般可以通过修改secret来实现注销，服务端secret修改后，已经颁发的未过期的token就会认证失败，进而实现注销，不过毕竟没有传统的注销方便。 密码重置，密码重置后，原本的token依然可以访问系统，这时候也需要强制修改secret。 基于第2点和第3点，一般建议不同用户取不同secret。 2 实战说了这么久，接下来我们就来看看这个东西到底要怎么用？ 2.1 环境搭建首先我们来创建一个Spring Boot项目，创建时需要添加Spring Security依赖，创建完成后，添加 jjwt 依赖，完整的pom.xml文件如下： 12345678910111213&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.1&lt;/version&gt;&lt;/dependency&gt; 然后在项目中创建一个简单的 User 对象实现 UserDetails 接口，如下： 12345678910111213141516171819202122232425public class User implements UserDetails { private String username; private String password; private List&lt;GrantedAuthority&gt; authorities; public String getUsername() { return username; } @Override public boolean isAccountNonExpired() { return true; } @Override public boolean isAccountNonLocked() { return true; } @Override public boolean isCredentialsNonExpired() { return true; } @Override public boolean isEnabled() { return true; } //省略getter/setter} 这个就是我们的用户对象，先放着备用，再创建一个HelloController，内容如下：1234567891011@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello jwt !\"; } @GetMapping(\"/admin\") public String admin() { return \"hello admin !\"; }} HelloController 很简单，这里有两个接口，设计是 /hello 接口可以被具有 user 角色的用户访问，而 /admin 接口则可以被具有 admin 角色的用户访问。 2.2 JWT 过滤器配置接下来提供两个和 JWT 相关的过滤器配置： 一个是用户登录的过滤器，在用户的登录的过滤器中校验用户是否登录成功，如果登录成功，则生成一个token返回给客户端，登录失败则给前端一个登录失败的提示。 第二个过滤器则是当其他请求发送来，校验token的过滤器，如果校验成功，就让请求继续执行。 这两个过滤器，我们分别来看，先看第一个： 1234567891011121314151617181920212223242526272829303132333435363738public class JwtLoginFilter extends AbstractAuthenticationProcessingFilter { protected JwtLoginFilter(String defaultFilterProcessesUrl, AuthenticationManager authenticationManager) { super(new AntPathRequestMatcher(defaultFilterProcessesUrl)); setAuthenticationManager(authenticationManager); } @Override public Authentication attemptAuthentication(HttpServletRequest req, HttpServletResponse resp) throws AuthenticationException, IOException, ServletException { User user = new ObjectMapper().readValue(req.getInputStream(), User.class); return getAuthenticationManager().authenticate(new UsernamePasswordAuthenticationToken(user.getUsername(), user.getPassword())); } @Override protected void successfulAuthentication(HttpServletRequest req, HttpServletResponse resp, FilterChain chain, Authentication authResult) throws IOException, ServletException { Collection&lt;? extends GrantedAuthority&gt; authorities = authResult.getAuthorities(); StringBuffer as = new StringBuffer(); for (GrantedAuthority authority : authorities) { as.append(authority.getAuthority()) .append(\",\"); } String jwt = Jwts.builder() .claim(\"authorities\", as)//配置用户角色 .setSubject(authResult.getName()) .setExpiration(new Date(System.currentTimeMillis() + 10 * 60 * 1000)) .signWith(SignatureAlgorithm.HS512,\"sang@123\") .compact(); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(jwt)); out.flush(); out.close(); } protected void unsuccessfulAuthentication(HttpServletRequest req, HttpServletResponse resp, AuthenticationException failed) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"登录失败!\"); out.flush(); out.close(); }} 关于这个类，我说如下几点： 自定义 JwtLoginFilter 继承自 AbstractAuthenticationProcessingFilter，并实现其中的三个默认方法。 attemptAuthentication方法中，我们从登录参数中提取出用户名密码，然后调用AuthenticationManager.authenticate()方法去进行自动校验。 第二步如果校验成功，就会来到successfulAuthentication回调中，在successfulAuthentication方法中，将用户角色遍历然后用一个 , 连接起来，然后再利用Jwts去生成token，按照代码的顺序，生成过程一共配置了四个参数，分别是用户角色、主题、过期时间以及加密算法和密钥，然后将生成的token写出到客户端。 第二步如果校验失败就会来到unsuccessfulAuthentication方法中，在这个方法中返回一个错误提示给客户端即可。 再来看第二个token校验的过滤器： 123456789101112131415public class JwtFilter extends GenericFilterBean { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest req = (HttpServletRequest) servletRequest; String jwtToken = req.getHeader(\"authorization\"); System.out.println(jwtToken); Claims claims = Jwts.parser().setSigningKey(\"sang@123\").parseClaimsJws(jwtToken.replace(\"Bearer\",\"\")) .getBody(); String username = claims.getSubject();//获取当前登录用户名 List&lt;GrantedAuthority&gt; authorities = AuthorityUtils.commaSeparatedStringToAuthorityList((String) claims.get(\"authorities\")); UsernamePasswordAuthenticationToken token = new UsernamePasswordAuthenticationToken(username, null, authorities); SecurityContextHolder.getContext().setAuthentication(token); filterChain.doFilter(req,servletResponse); }} 关于这个过滤器，我说如下几点： 首先从请求头中提取出 authorization 字段，这个字段对应的value就是用户的token。 将提取出来的token字符串转换为一个Claims对象，再从Claims对象中提取出当前用户名和用户角色，创建一个UsernamePasswordAuthenticationToken放到当前的Context中，然后执行过滤链使请求继续执行下去。 如此之后，两个和JWT相关的过滤器就算配置好了。 2.3 Spring Security 配置接下来我们来配置 Spring Security,如下： 1234567891011121314151617181920212223242526272829@Configurationpublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Bean PasswordEncoder passwordEncoder() { return NoOpPasswordEncoder.getInstance(); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication().withUser(\"admin\") .password(\"123\").roles(\"admin\") .and() .withUser(\"sang\") .password(\"456\") .roles(\"user\"); } @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .antMatchers(\"/hello\").hasRole(\"user\") .antMatchers(\"/admin\").hasRole(\"admin\") .antMatchers(HttpMethod.POST, \"/login\").permitAll() .anyRequest().authenticated() .and() .addFilterBefore(new JwtLoginFilter(\"/login\",authenticationManager()),UsernamePasswordAuthenticationFilter.class) .addFilterBefore(new JwtFilter(),UsernamePasswordAuthenticationFilter.class) .csrf().disable(); }} 简单起见，这里我并未对密码进行加密，因此配置了NoOpPasswordEncoder的实例。 简单起见，这里并未连接数据库，我直接在内存中配置了两个用户，两个用户具备不同的角色。 配置路径规则时， /hello 接口必须要具备 user 角色才能访问， /admin 接口必须要具备 admin 角色才能访问，POST 请求并且是 /login 接口则可以直接通过，其他接口必须认证后才能访问。 最后配置上两个自定义的过滤器并且关闭掉csrf保护。 2.4 测试做完这些之后，我们的环境就算完全搭建起来了，接下来启动项目然后在 POSTMAN 中进行测试，如下： 登录成功后返回的字符串就是经过 base64url 转码的token，一共有三部分，通过一个 . 隔开，我们可以对第一个 . 之前的字符串进行解码，即Header，如下： 再对两个 . 之间的字符解码，即 payload： 可以看到，我们设置信息，由于base64并不是加密方案，只是一种编码方案，因此，不建议将敏感的用户信息放到token中。 接下来再去访问 /hello 接口，注意认证方式选择 Bearer Token，Token值为刚刚获取到的值，如下： 可以看到，访问成功。 总结这就是 JWT 结合 Spring Security 的一个简单用法，讲真，如果实例允许，类似的需求我还是推荐使用 OAuth2 中的 password 模式。 不知道大伙有没有看懂呢？如果没看懂，松哥还有一个关于这个知识点的视频教程，如下： 如何获取这个视频教程呢？很简单，将本文转发到一个超过100人的微信群中(QQ群不算，松哥是群主的微信群也不算，群要为Java方向)，或者多个微信群中，只要累计人数达到100人即可，然后加松哥微信，截图发给松哥即可获取资料。","link":"/2019/0408/springboot-jwt.html"},{"title":"提高性能，MySQL  读写分离环境搭建(一)","text":"这是松哥之前一个零散的笔记，整理出来分享给大伙！ MySQL 读写分离在互联网项目中应该算是一个非常常见的需求了。受困于 Linux 和 MySQL 版本问题，很多人经常会搭建失败，今天松哥就给大伙举一个成功的例子，后面有时间再和大家分享下使用 Docker 搭建环境，那样就 100% 成功了。 CentOS 安装 MySQL自己玩 Linux 松哥一般首选 Ubuntu，不过公司里边使用一般还是 CentOS 为主，因此这里松哥就以 CentOS 为例来向大家演示整个过程，今天这篇文章主要来看看 MySQL 的安装。 环境： CentOS7 MySQL5.7 具体的安装步骤如下： 检查是否安装了 mariadb，如果已经安装了则卸载： 1yum list installed | grep mariadb 如果执行结果如下，表示已经安装了 mariadb，将之卸载： 1mariadb-libs.x86_64 1:5.5.52-1.el7 @anaconda 卸载命令如下： 1yum -y remove mariadb* 接下来下载官方提供的 rpm 包 如果 CentOS 上没有 wget 命令，首先通过如下命令安装 wget： 1yum install wget 然后执行如下操作下载 rpm 包： 1wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm 下载完成后，安装rpm包： 1rpm -ivh mysql57-community-release-el7-11.noarch.rpm 检查 MySQL 的 yum 源是否安装成功： 1yum repolist enabled | grep &quot;mysql.*-community.*&quot; 执行结果如下表示安装成功： 安装 MySQL 1yum install mysql-server 安装完成后，启动MySQL： 1systemctl start mysqld.service 停止MySQL： 1systemctl stop mysqld.service 登录 MySQL： 1mysql -u root -p 默认无密码。有的版本有默认密码，查看默认密码，首先去 /etc/my.cnf 目录下查看 MySQL 的日志位置，然后打开日志文件，可以看到日志中有一个提示，生成了一个临时的默认密码，使用这个密码登录，登录成功后修改密码即可。 改密码 首先修改密码策略(这一步不是必须的，如果不修改密码策略，需要取一个比较复杂的密码，松哥这里简单起见，就修改下密码策略)： 1set global validate_password_policy=0; 然后重置密码： 12set password=password(&quot;123&quot;); flush privileges; 授权远程登录同方式一： 12grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123&apos; with grant option;flush privileges; 授权远程登录同方式二： 修改 mysql 库中的 user 表，将 root 用户的 Host 字段的值改为 % ，然后重启 MySQL 即可。 关闭防火墙MySQL 要能远程访问，还需要关闭防火墙： 1systemctl stop firewalld.service 禁止firewall开机启动: 1systemctl disable firewalld.service 总结装了这么多 MySQL ，还是 Ubuntu 下 MySQL 最好弄，其他系统多多少少总有点麻烦，本文主要和大家分享了 CentOS7 中 MySQL 的安装步骤，大伙有问题欢迎留言讨论。下篇文章和大伙分享 MySQL 读写分离环境搭建。","link":"/2019/0509/mysql-read-write.html"},{"title":"提高性能，MySQL  读写分离环境搭建(二)","text":"上篇文章和大家聊了 CentOS7 安装 MySQL5.7 ，这个大家一般装在虚拟机里边，装好了，把虚拟拷贝一份，这样我们就有两个 MySQL ，就可以开始今天的主从搭建了。 准备工作我这里有一张简单的图向大伙展示 MySQL 主从的工作方式： 这里，我们准备两台机器： 主机：192.168.248.128 从机：192.168.248.139 主机配置主机的配置就三个步骤，比较容易： 1.授权给从机服务器 12GRANT REPLICATION SLAVE ON *.* to &apos;rep1&apos;@&apos;192.168.248.139&apos; identified by &apos;123&apos;;FLUSH PRIVILEGES; 这里表示配置从机登录用户名为 rep1，密码为 123，并且必须从 192.168.248.139这个地址登录，登录成功之后可以操作任意库中的任意表。其中，如果不需要限制登录地址，可以将 IP 地址更换为一个 %。 2.修改主库配置文件，开启 binlog ，并设置 server-id ，每次修改配置文件后都要重启 MySQL 服务才会生效 1vi /etc/my.cnf 修改的文件内容如下： 1234[mysqld]log-bin=/var/lib/mysql/binlogserver-id=128binlog-do-db = cmdb 如下图： log-bin：同步的日志路径及文件名，一定注意这个目录要是 MySQL 有权限写入的（我这里是偷懒了，直接放在了下面那个datadir下面）。 binlog-do-db：要同步的数据库名，当从机连上主机后，只有这里配置的数据库才会被同步，其他的不会被同步。 server-id: MySQL 在主从环境下的唯一标志符，给个任意数字，注意不能和从机重复。 配置完成后重启 MySQL 服务端： 1systemctl restart mysqld 3.查看主服务器当前二进制日志名和偏移量，这个操作的目的是为了在从数据库启动后，从这个点开始进行数据的恢复： 1show master status; 至此，主机配置完成。 从机配置从机的配置也比较简单，我们一步一步来看： 1.在/etc/my.cnf 添加下面配置： 注意从机这里只需要配置一下 server-id 即可。 注意：如果从机是从主机复制来的，即我们通过复制 CentOS 虚拟机获取了 MySQL 实例 ，此时两个 MySQL 的 uuid 一样（正常安装是不会相同的），这时需要手动修改，修改位置在 /var/lib/mysql/auto.cnf ，注意随便修改这里几个字符即可，但也不可太过于随意，例如修改了 uuid 的长度。 2.使用命令来配置从机： 1change master to master_host=&apos;192.168.248.128&apos;,master_port=3306,master_user=&apos;rep1&apos;,master_password=&apos;123&apos;,master_log_file=&apos;binlog.000001&apos;,master_log_pos=120; 这里配置了主机地址、端口以及从机登录主机的用户名和密码，注意最后两个参数要和 master 中的保持一致。 3.启动 slave 进程 1start slave; 启动之后查看从机状态： 1show slave status\\G; 4.查看 slave 的状态 主要是下面两项值都要为为 YES，则表示配置正确： 12Slave_IO_Running: YesSlave_SQL_Running: Yes 至此，配置完成，主机创建库，添加数据，从机会自动同步。 如果这两个有一个不为 YES ，表示主从环境搭建失败，此时可以阅读日志，查看出错的原因，再具体问题具体解决。 总结本文主要和大伙说了 MySQL 主从环境搭建，这几个步骤松哥反反复复操作过很多遍，小伙伴只要按照松哥的步骤一般来说都能成功，有问题欢迎留言讨论。","link":"/2019/0513/mysql-read-write.html"},{"title":"整理了八个开源的 Spring Boot 学习资源","text":"Spring Boot 算是目前 Java 领域最火的技术栈了，松哥年初出版的 《Spring Boot + Vue 全栈开发实战》迄今为止已经加印了 3 次，Spring Boot 的受欢迎程度可见一斑。经常有人问松哥有没有推荐的 Spring Boot 学习资料？当然有！买松哥书就对了，哈哈。除了书呢？当然就是开源项目了，今天松哥整理了几个优质 Spring Boot 开源项目给大家参考，希望能够帮助到正在学习 Spring Boot 的小伙伴！ spring-boot-examples star 数 14821 项目地址：https://github.com/ityouknow/spring-boot-examples 这个项目中整合了 Spring Boot 使用的各种示例，以最简单、最实用为标准，此开源项目中的每个示例都以最小依赖，最简单为标准，帮助初学者快速掌握 Spring Boot 各组件的使用。基本上涉及到了 Spring Boot 使用的方方面面。 项目部分 demo 截图： 微人事 star 数 3333 项目地址：https://github.com/lenve/vhr 微人事是一个前后端分离的人力资源管理系统，项目采用 SpringBoot + Vue 开发。项目打通了前后端，并且提供了非常详尽的文档，从 Spring Boot 接口设计到前端 Vue 的开发思路，作者全部都记录在项目的 wiki 中，是不可多得的 Java 全栈学习资料。 项目效果图: 项目部分文档截图： mall star 数 12668 项目地址：https://github.com/macrozheng/mall mall 项目是一套电商系统，包括前台商城系统及后台管理系统，基于 Spring Boot + MyBatis 实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 项目效果图： spring-boot-pay star 数 2931 项目地址：https://gitee.com/52itstyle/spring-boot-pay 这是一个支付案例，提供了包括支付宝、微信、银联在内的详细支付代码案例，对于有支付需求的小伙伴来说，这个项目再合适不过了。 项目效果图： V 部落 star 数 1060 项目地址：https://github.com/lenve/VBlog V部落是一个多用户博客管理平台，采用 Vue + SpringBoot + ElementUI 开发。这个项目最大的优势是简单，属于功能完整但是又非常简单的那种，非常非常适合初学者。 项目效果图： springboot-plus star 数 2546 项目地址：https://gitee.com/xiandafu/springboot-plus 一个基于SpringBoot 2 的管理后台系统,包含了用户管理，组织机构管理，角色管理，功能点管理，菜单管理，权限分配，数据权限分配，代码生成等功能 相比其他开源的后台系统，SpringBoot-Plus 具有一定的复杂度。系统基于Spring Boot2.1技术，前端采用了Layui2.4。数据库以MySQL/Oracle/Postgres/SQLServer为实例，理论上是跨数据库平台。 项目效果图： litemall star 数 6436 项目地址：https://github.com/linlinjava/litemall 一个商城项目，包括Spring Boot后端 + Vue管理员前端 + 微信小程序用户前端 + Vue用户移动端，功能包括、分类列表、分类详情、品牌列表、品牌详情、新品首发、人气推荐、优惠券列表、优惠券选择、团购（团购业务有待完善）、搜索、商品详情、商品评价、商品分享、购物车、下单、订单列表、订单详情、地址、收藏、足迹、意见反馈以及客服；管理平台功能包括会员管理、商城管理、商品管理、推广管理、系统管理、配置管理、统计报表等。 项目效果图： 其他另外再向大家推荐两个优质的 Spring Boot 和 Spring Cloud 学习网站，如下： http://www.springboot.wiki http://www.springcloud.wiki 总结好了，一点点整理的资源，希望能够帮助到大家。","link":"/2019/0517/springboot-samples.html"},{"title":"是什么驱动着你不断前进?","text":"工作这些年，从来没有一点安全感，担心自己会落后，会被这个不断更新的技术世界抛弃，因此不停的写博客，写博客有人看，有人点赞有人评论，我就知道我的技术还没过时，换言之，写博客让我有一点点的安全感！ 这是输出，另一方面在不停的输入。不停的学习才让我有不断输出的资本，下面都是松哥自己购买的极客时间专栏，极客时间的课程质量都还是非常 OK 的，我自己就经常学习，大家有需要的话，可以扫描我分享的二维码购买，购买后添加我的微信（微信：ws584991843），我将极客时间给我的返现都返给大家。","link":"/2019/0528/geek-time.html"},{"title":"是时候了解下Spring Boot整合 Jpa啦","text":"Spring Boot中的数据持久化方案前面给大伙介绍了两种了，一个是JdbcTemplate，还有一个MyBatis，JdbcTemplate配置简单，使用也简单，但是功能也非常有限，MyBatis则比较灵活，功能也很强大，据我所知，公司采用MyBatis做数据持久化的相当多，但是MyBatis并不是唯一的解决方案，除了MyBatis之外，还有另外一个东西，那就是Jpa，松哥也有一些朋友在公司里使用Jpa来做数据持久化，本文就和大伙来说说Jpa如何实现数据持久化。 Jpa介绍首先需要向大伙介绍一下Jpa，Jpa（Java Persistence API）Java持久化API，它是一套ORM规范，而不是具体的实现，Jpa的江湖地位类似于JDBC，只提供规范，所有的数据库厂商提供实现（即具体的数据库驱动），Java领域，小伙伴们熟知的ORM框架可能主要是Hibernate，实际上，除了Hibernate之外，还有很多其他的ORM框架，例如： Batoo JPA DataNucleus (formerly JPOX) EclipseLink (formerly Oracle TopLink) IBM, for WebSphere Application Server JBoss with Hibernate Kundera ObjectDB OpenJPA OrientDB from Orient Technologies Versant Corporation JPA (not relational, object database) Hibernate只是ORM框架的一种，上面列出来的ORM框架都是支持JPA2.0规范的ORM框架。既然它是一个规范，不是具体的实现，那么必然就不能直接使用（类似于JDBC不能直接使用，必须要加了驱动才能用），我们使用的是具体的实现，在这里我们采用的实现实际上还是Hibernate。 Spring Boot中使用的Jpa实际上是Spring Data Jpa，Spring Data是Spring家族的一个子项目，用于简化SQL和NoSQL的访问，在Spring Data中，只要你的方法名称符合规范，它就知道你想干嘛，不需要自己再去写SQL。 工程创建创建Spring Boot工程，添加Web、Jpa以及MySQL驱动依赖，如下： 工程创建好之后，添加Druid依赖，完整的依赖如下： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 如此，工程就算创建成功了。 基本配置工程创建完成后，只需要在application.properties中进行数据库基本信息配置以及Jpa基本配置，如下： 123456789101112131415# 数据库的基本配置spring.datasource.username=rootspring.datasource.password=rootspring.datasource.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=UTF-8spring.datasource.type=com.alibaba.druid.pool.DruidDataSource# JPA配置spring.jpa.database=mysql# 在控制台打印SQLspring.jpa.show-sql=true# 数据库平台spring.jpa.database-platform=mysql# 每次启动项目时，数据库初始化策略spring.jpa.hibernate.ddl-auto=update# 指定默认的存储引擎为InnoDBspring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL57Dialect 注意这里和JdbcTemplate以及MyBatis比起来，多了Jpa配置，Jpa配置含义我都注释在代码中了，这里不再赘述，需要强调的是，最后一行配置，默认情况下，自动创建表的时候会使用MyISAM做表的引擎，如果配置了数据库方言为MySQL57Dialect，则使用InnoDB做表的引擎。 好了，配置完成后，我们的Jpa差不多就可以开始用了。 基本用法ORM(Object Relational Mapping)框架表示对象关系映射，使用ORM框架我们不必再去创建表，框架会自动根据当前项目中的实体类创建相应的数据表。因此，我这里首先创建一个User对象，如下： 12345678910@Entity(name = \"t_user\")public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; @Column(name = \"name\") private String username; private String address; //省略getter/setter} 首先@Entity注解表示这是一个实体类，那么在项目启动时会自动针对该类生成一张表，默认的表名为类名，@Entity注解的name属性表示自定义生成的表名。@Id注解表示这个字段是一个id，@GeneratedValue注解表示主键的自增长策略，对于类中的其他属性，默认都会根据属性名在表中生成相应的字段，字段名和属性名相同，如果开发者想要对字段进行定制，可以使用@Column注解，去配置字段的名称，长度，是否为空等等。 做完这一切之后，启动Spring Boot项目，就会发现数据库中多了一个名为t_user的表了。 针对该表的操作，则需要我们提供一个Repository，如下： 123456public interface UserDao extends JpaRepository&lt;User,Integer&gt; { List&lt;User&gt; getUserByAddressEqualsAndIdLessThanEqual(String address, Integer id); @Query(value = \"select * from t_user where id=(select max(id) from t_user)\",nativeQuery = true) User maxIdUser();} 这里，自定义UserDao接口继承自JpaRepository，JpaRepository提供了一些基本的数据操作方法，例如保存，更新，删除，分页查询等，开发者也可以在接口中自己声明相关的方法，只需要方法名称符合规范即可，在Spring Data中，只要按照既定的规范命名方法，Spring Data Jpa就知道你想干嘛，这样就不用写SQL了，那么规范是什么呢？参考下图： 当然，这种方法命名主要是针对查询，但是一些特殊需求，可能并不能通过这种方式解决，例如想要查询id最大的用户，这时就需要开发者自定义查询SQL了，如上代码所示，自定义查询SQL，使用@Query注解，在注解中写自己的SQL，默认使用的查询语言不是SQL，而是JPQL，这是一种数据库平台无关的面向对象的查询语言，有点定位类似于Hibernate中的HQL，在@Query注解中设置nativeQuery属性为true则表示使用原生查询，即大伙所熟悉的SQL。上面代码中的只是一个很简单的例子，还有其他一些点，例如如果这个方法中的SQL涉及到数据操作，则需要使用@Modifying注解。 好了，定义完Dao之后，接下来就可以将UserDao注入到Controller中进行测试了(这里为了省事，就没有提供Service了，直接将UserDao注入到Controller中)。 1234567891011121314151617181920212223242526272829303132333435363738394041@RestControllerpublic class UserController { @Autowired UserDao userDao; @PostMapping(\"/\") public void addUser() { User user = new User(); user.setId(1); user.setUsername(\"张三\"); user.setAddress(\"深圳\"); userDao.save(user); } @DeleteMapping(\"/\") public void deleteById() { userDao.deleteById(1); } @PutMapping(\"/\") public void updateUser() { User user = userDao.getOne(1); user.setUsername(\"李四\"); userDao.flush(); } @GetMapping(\"/test1\") public void test1() { List&lt;User&gt; all = userDao.findAll(); System.out.println(all); } @GetMapping(\"/test2\") public void test2() { List&lt;User&gt; list = userDao.getUserByAddressEqualsAndIdLessThanEqual(\"广州\", 2); System.out.println(list); } @GetMapping(\"/test3\") public void test3() { User user = userDao.maxIdUser(); System.out.println(user); }} 如此之后，即可查询到需要的数据。 好了，本文的重点是Spring Boot和Jpa的整合，这个话题就先说到这里。 多说两句在和Spring框架整合时，如果用到ORM框架，大部分人可能都是首选Hibernate，实际上，在和Spring+SpringMVC整合时，也可以选择Spring Data Jpa做数据持久化方案，用法和本文所述基本是一样的，Spring Boot只是将Spring Data Jpa的配置简化了，因此，很多初学者对Spring Data Jpa觉得很神奇，但是又觉得无从下手，其实，此时可以回到Spring框架，先去学习Jpa，再去学习Spring Data Jpa，这是给初学者的一点建议。","link":"/2019/0407/springboot-jpa.html"},{"title":"最简单的SpringBoot整合MyBatis教程","text":"前面两篇文章和读者聊了Spring Boot中最简单的数据持久化方案JdbcTemplate，JdbcTemplate虽然简单，但是用的并不多，因为它没有MyBatis方便，在Spring+SpringMVC中整合MyBatis步骤还是有点复杂的，要配置多个Bean，Spring Boot中对此做了进一步的简化，使MyBatis基本上可以做到开箱即用，本文就来看看在Spring Boot中MyBatis要如何使用。 工程创建首先创建一个基本的Spring Boot工程，添加Web依赖，MyBatis依赖以及MySQL驱动依赖，如下： 创建成功后，添加Druid依赖，并且锁定MySQL驱动版本，完整的依赖如下： 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 如此，工程就算是创建成功了。读者注意，MyBatis和Druid依赖的命名和其他库的命名不太一样，是属于xxx-spring-boot-stater模式的，这表示该starter是由第三方提供的。 基本用法MyBatis的使用和JdbcTemplate基本一致，首先也是在application.properties中配置数据库的基本信息： 1234spring.datasource.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=utf-8spring.datasource.username=rootspring.datasource.password=rootspring.datasource.type=com.alibaba.druid.pool.DruidDataSource 配置完成后，MyBatis就可以创建Mapper来使用了，例如我这里直接创建一个UserMapper2，如下： 12345678910111213141516171819202122232425public interface UserMapper2 { @Select(\"select * from user\") List&lt;User&gt; getAllUsers(); @Results({ @Result(property = \"id\", column = \"id\"), @Result(property = \"username\", column = \"u\"), @Result(property = \"address\", column = \"a\") }) @Select(\"select username as u,address as a,id as id from user where id=#{id}\") User getUserById(Long id); @Select(\"select * from user where username like concat('%',#{name},'%')\") List&lt;User&gt; getUsersByName(String name); @Insert({\"insert into user(username,address) values(#{username},#{address})\"}) @SelectKey(statement = \"select last_insert_id()\", keyProperty = \"id\", before = false, resultType = Integer.class) Integer addUser(User user); @Update(\"update user set username=#{username},address=#{address} where id=#{id}\") Integer updateUserById(User user); @Delete(\"delete from user where id=#{id}\") Integer deleteUserById(Integer id);} 这里是通过全注解的方式来写SQL，不写XML文件，@Select、@Insert、@Update以及@Delete四个注解分别对应XML中的select、insert、update以及delete标签，@Results注解类似于XML中的ResultMap映射文件（getUserById方法给查询结果的字段取别名主要是向小伙伴们演示下@Results注解的用法），另外使用@SelectKey注解可以实现主键回填的功能，即当数据插入成功后，插入成功的数据id会赋值到user对象的id属性上。 UserMapper2创建好之后，还要配置mapper扫描，有两种方式，一种是直接在UserMapper2上面添加@Mapper注解，这种方式有一个弊端就是所有的Mapper都要手动添加，要是落下一个就会报错，还有一个一劳永逸的办法就是直接在启动类上添加Mapper扫描，如下： 1234567@SpringBootApplication@MapperScan(basePackages = \"org.sang.mybatis.mapper\")public class MybatisApplication { public static void main(String[] args) { SpringApplication.run(MybatisApplication.class, args); }} 好了，做完这些工作就可以去测试Mapper的使用了。 mapper映射当然，开发者也可以在XML中写SQL，例如创建一个UserMapper，如下： 123456789public interface UserMapper { List&lt;User&gt; getAllUser(); Integer addUser(User user); Integer updateUserById(User user); Integer deleteUserById(Integer id);} 然后创建UserMapper.xml文件，如下： 123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"org.sang.mybatis.mapper.UserMapper\"&gt; &lt;select id=\"getAllUser\" resultType=\"org.sang.mybatis.model.User\"&gt; select * from t_user; &lt;/select&gt; &lt;insert id=\"addUser\" parameterType=\"org.sang.mybatis.model.User\"&gt; insert into user (username,address) values (#{username},#{address}); &lt;/insert&gt; &lt;update id=\"updateUserById\" parameterType=\"org.sang.mybatis.model.User\"&gt; update user set username=#{username},address=#{address} where id=#{id} &lt;/update&gt; &lt;delete id=\"deleteUserById\"&gt; delete from user where id=#{id} &lt;/delete&gt;&lt;/mapper&gt; 将接口中方法对应的SQL直接写在XML文件中。 那么这个UserMapper.xml到底放在哪里呢？有两个位置可以放，第一个是直接放在UserMapper所在的包下面： 放在这里的UserMapper.xml会被自动扫描到，但是有另外一个Maven带来的问题，就是java目录下的xml资源在项目打包时会被忽略掉，所以，如果UserMapper.xml放在包下，需要在pom.xml文件中再添加如下配置，避免打包时java目录下的XML文件被自动忽略掉： 12345678910111213&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt; 当然，UserMapper.xml也可以直接放在resources目录下，这样就不用担心打包时被忽略了，但是放在resources目录下，又不能自动被扫描到，需要添加额外配置。例如我在resources目录下创建mapper目录用来放mapper文件，如下： 此时在application.properties中告诉mybatis去哪里扫描mapper： 1mybatis.mapper-locations=classpath:mapper/*.xml 如此配置之后，mapper就可以正常使用了。注意第二种方式不需要在pom.xml文件中配置文件过滤。 原理分析在SSM整合中，开发者需要自己提供两个Bean，一个SqlSessionFactoryBean，还有一个是MapperScannerConfigurer，在Spring Boot中，这两个东西虽然不用开发者自己提供了，但是并不意味着这两个Bean不需要了，在org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration类中，我们可以看到Spring Boot提供了这两个Bean，部分源码如下： 1234567891011121314151617181920212223242526272829303132333435@org.springframework.context.annotation.Configuration@ConditionalOnClass({ SqlSessionFactory.class, SqlSessionFactoryBean.class })@ConditionalOnSingleCandidate(DataSource.class)@EnableConfigurationProperties(MybatisProperties.class)@AutoConfigureAfter(DataSourceAutoConfiguration.class)public class MybatisAutoConfiguration implements InitializingBean { @Bean @ConditionalOnMissingBean public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception { SqlSessionFactoryBean factory = new SqlSessionFactoryBean(); factory.setDataSource(dataSource); return factory.getObject(); } @Bean @ConditionalOnMissingBean public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) { ExecutorType executorType = this.properties.getExecutorType(); if (executorType != null) { return new SqlSessionTemplate(sqlSessionFactory, executorType); } else { return new SqlSessionTemplate(sqlSessionFactory); } } @org.springframework.context.annotation.Configuration @Import({ AutoConfiguredMapperScannerRegistrar.class }) @ConditionalOnMissingBean(MapperFactoryBean.class) public static class MapperScannerRegistrarNotFoundConfiguration implements InitializingBean { @Override public void afterPropertiesSet() { logger.debug(\"No {} found.\", MapperFactoryBean.class.getName()); } }} 从类上的注解可以看出，当当前类路径下存在SqlSessionFactory、 SqlSessionFactoryBean以及DataSource时，这里的配置才会生效，SqlSessionFactory和SqlTemplate都被提供了。为什么要看这段代码呢？下篇文章，松哥和大伙分享Spring Boot中MyBatis多数据源的配置时，这里将是一个重要的参考。 好了，欢迎加入我的星球，关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0407/springboot-mybatis.html"},{"title":"是时候彻底搞清楚 Spring Boot 的配置文件 application.properties 了！","text":"在 Spring Boot 中，配置文件有两种不同的格式，一个是 properties ，另一个是 yaml 。 虽然 properties 文件比较常见，但是相对于 properties 而言，yaml 更加简洁明了，而且使用的场景也更多，很多开源项目都是使用 yaml 进行配置（例如 Hexo）。除了简洁，yaml 还有另外一个特点，就是 yaml 中的数据是有序的，properties 中的数据是无序的，在一些需要路径匹配的配置中，顺序就显得尤为重要（例如我们在 Spring Cloud Zuul 中的配置），此时我们一般采用 yaml。关于 yaml ，松哥之前写过一篇文章：Spring Boot 中的 yaml 配置简介。 本文主要来看看 properties 的问题。 位置问题首先，当我们创建一个 Spring Boot 工程时，默认 resources 目录下就有一个 application.properties 文件，可以在 application.properties 文件中进行项目配置，但是这个文件并非唯一的配置文件，在 Spring Boot 中，一共有 4 个地方可以存放 application.properties 文件。 当前项目根目录下的 config 目录下 当前项目的根目录下 resources 目录下的 config 目录下 resources 目录下 按如上顺序，四个配置文件的优先级依次降低。如下： 这四个位置是默认位置，即 Spring Boot 启动，默认会从这四个位置按顺序去查找相关属性并加载。但是，这也不是绝对的，我们也可以在项目启动时自定义配置文件位置。 例如，现在在 resources 目录下创建一个 javaboy 目录，目录中存放一个 application.properties 文件，那么正常情况下，当我们启动 Spring Boot 项目时，这个配置文件是不会被自动加载的。我们可以通过 spring.config.location 属性来手动的指定配置文件位置，指定完成后，系统就会自动去指定目录下查找 application.properties 文件。 此时启动项目，就会发现，项目以 classpath:/javaboy/application.propertie 配置文件启动。 这是在开发工具中配置了启动位置，如果项目已经打包成 jar ，在启动命令中加入位置参数即可： 1java -jar properties-0.0.1-SNAPSHOT.jar --spring.config.location=classpath:/javaboy/ 文件名问题对于 application.properties 而言，它不一定非要叫 application ，但是项目默认是去加载名为 application 的配置文件，如果我们的配置文件不叫 application ，也是可以的，但是，需要明确指定配置文件的文件名。 方式和指定路径一致，只不过此时的 key 是 spring.config.name 。 首先我们在 resources 目录下创建一个 app.properties 文件，然后在 IDEA 中指定配置文件的文件名： 指定完配置文件名之后，再次启动项目，此时系统会自动去默认的四个位置下面分别查找名为 app.properties 的配置文件。当然，允许自定义文件名的配置文件不放在四个默认位置，而是放在自定义目录下，此时就需要明确指定 spring.config.location 。 配置文件位置和文件名称可以同时自定义。 普通的属性注入由于 Spring Boot 源自 Spring ，所以 Spring 中存在的属性注入，在 Spring Boot 中一样也存在。由于 Spring Boot 中，默认会自动加载 application.properties 文件，所以简单的属性注入可以直接在这个配置文件中写。 例如，现在定义一个 Book 类： 123456public class Book { private Long id; private String name; private String author; //省略 getter/setter} 然后，在 application.properties 文件中定义属性： 123book.name=三国演义book.author=罗贯中book.id=1 按照传统的方式（Spring中的方式），可以直接通过 @Value 注解将这些属性注入到 Book 对象中： 12345678910@Componentpublic class Book { @Value(\"${book.id}\") private Long id; @Value(\"${book.name}\") private String name; @Value(\"${book.author}\") private String author; //省略getter/setter} 注意 Book 对象本身也要交给 Spring 容器去管理，如果 Book 没有交给 Spring 容器，那么 Book 中的属性也无法从 Spring 容器中获取到值。 配置完成后，在 Controller 或者单元测试中注入 Book 对象，启动项目，就可以看到属性已经注入到对象中了。 一般来说，我们在 application.properties 文件中主要存放系统配置，这种自定义配置不建议放在该文件中，可以自定义 properties 文件来存在自定义配置。 例如在 resources 目录下，自定义 book.properties 文件，内容如下： 123book.name=三国演义book.author=罗贯中book.id=1 此时，项目启动并不会自动的加载该配置文件，如果是在 XML 配置中，可以通过如下方式引用该 properties 文件： 1&lt;context:property-placeholder location=\"classpath:book.properties\"/&gt; 如果是在 Java 配置中，可以通过 @PropertySource 来引入配置： 1234567891011@Component@PropertySource(\"classpath:book.properties\")public class Book { @Value(\"${book.id}\") private Long id; @Value(\"${book.name}\") private String name; @Value(\"${book.author}\") private String author; //getter/setter} 这样，当项目启动时，就会自动加载 book.properties 文件。 这只是 Spring 中属性注入的一个简单用法，和 Spring Boot 没有任何关系。 类型安全的属性注入Spring Boot 引入了类型安全的属性注入，如果采用 Spring 中的配置方式，当配置的属性非常多的时候，工作量就很大了，而且容易出错。 使用类型安全的属性注入，可以有效的解决这个问题。 123456789@Component@PropertySource(\"classpath:book.properties\")@ConfigurationProperties(prefix = \"book\")public class Book { private Long id; private String name; private String author; //省略getter/setter} 这里，主要是引入 @ConfigurationProperties(prefix = “book”) 注解，并且配置了属性的前缀，此时会自动将 Spring 容器中对应的数据注入到对象对应的属性中，就不用通过 @Value 注解挨个注入了，减少工作量并且避免出错。 总结application.properties 是 Spring Boot 中配置的一个重要载体，很多组件的属性都可以在这里定制。它的用法和 yaml 比较类似，关于 yaml 配置，大家可以参考 Spring Boot 中的 yaml 配置简介 。 本文案例我已上传到 GitHub：https://github.com/lenve/javaboy-code-samples 好了，有问题欢迎留言讨论。","link":"/2019/0530/application.properties.html"},{"title":"极简Spring Boot整合MyBatis多数据源","text":"关于多数据源的配置，前面和大伙介绍过JdbcTemplate多数据源配置，那个比较简单，本文来和大伙说说MyBatis多数据源的配置。其实关于多数据源，我的态度还是和之前一样，复杂的就直接上分布式数据库中间件，简单的再考虑多数据源。这是项目中的建议，技术上的话，当然还是各种技术都要掌握的。 工程创建首先需要创建MyBatis项目，项目创建和前文的一样，添加MyBatis、MySQL以及Web依赖： 项目创建完成后，添加Druid依赖，和JdbcTemplate一样，这里添加Druid依赖也必须是专为Spring boot打造的Druid，不能使用传统的Druid。完整的依赖如下： 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.28&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 多数据源配置接下来配置多数据源，这里基本上还是和JdbcTemplate多数据源的配置方式一致，首先在application.properties中配置数据库基本信息，然后提供两个DataSource即可，这里我再把代码贴出来，里边的道理条条框框的，大伙可以参考前面的文章，这里不再赘述。 application.properties中的配置： 123456789spring.datasource.one.url=jdbc:mysql:///test01?useUnicode=true&amp;characterEncoding=utf-8spring.datasource.one.username=rootspring.datasource.one.password=rootspring.datasource.one.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.two.url=jdbc:mysql:///test02?useUnicode=true&amp;characterEncoding=utf-8spring.datasource.two.username=rootspring.datasource.two.password=rootspring.datasource.two.type=com.alibaba.druid.pool.DruidDataSource 然后再提供两个DataSource，如下： 12345678910111213@Configurationpublic class DataSourceConfig { @Bean @ConfigurationProperties(prefix = \"spring.datasource.one\") DataSource dsOne() { return DruidDataSourceBuilder.create().build(); } @Bean @ConfigurationProperties(prefix = \"spring.datasource.two\") DataSource dsTwo() { return DruidDataSourceBuilder.create().build(); }} MyBatis配置接下来则是MyBatis的配置，不同于JdbcTemplate，MyBatis的配置要稍微麻烦一些，因为要提供两个Bean，因此这里两个数据源我将在两个类中分开来配置，首先来看第一个数据源的配置： 1234567891011121314151617181920212223@Configuration@MapperScan(basePackages = \"org.sang.mybatis.mapper1\",sqlSessionFactoryRef = \"sqlSessionFactory1\",sqlSessionTemplateRef = \"sqlSessionTemplate1\")public class MyBatisConfigOne { @Resource(name = \"dsOne\") DataSource dsOne; @Bean SqlSessionFactory sqlSessionFactory1() { SqlSessionFactory sessionFactory = null; try { SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dsOne); sessionFactory = bean.getObject(); } catch (Exception e) { e.printStackTrace(); } return sessionFactory; } @Bean SqlSessionTemplate sqlSessionTemplate1() { return new SqlSessionTemplate(sqlSessionFactory1()); }} 创建MyBatisConfigOne类，首先指明该类是一个配置类，配置类中要扫描的包是org.sang.mybatis.mapper1，即该包下的Mapper接口将操作dsOne中的数据，对应的SqlSessionFactory和SqlSessionTemplate分别是sqlSessionFactory1和sqlSessionTemplate1，在MyBatisConfigOne内部，分别提供SqlSessionFactory和SqlSessionTemplate即可，SqlSessionFactory根据dsOne创建，然后再根据创建好的SqlSessionFactory创建一个SqlSessionTemplate。 这里配置完成后，依据这个配置，再来配置第二个数据源即可： 1234567891011121314151617181920212223@Configuration@MapperScan(basePackages = \"org.sang.mybatis.mapper2\",sqlSessionFactoryRef = \"sqlSessionFactory2\",sqlSessionTemplateRef = \"sqlSessionTemplate2\")public class MyBatisConfigTwo { @Resource(name = \"dsTwo\") DataSource dsTwo; @Bean SqlSessionFactory sqlSessionFactory2() { SqlSessionFactory sessionFactory = null; try { SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dsTwo); sessionFactory = bean.getObject(); } catch (Exception e) { e.printStackTrace(); } return sessionFactory; } @Bean SqlSessionTemplate sqlSessionTemplate2() { return new SqlSessionTemplate(sqlSessionFactory2()); }} 好了，这样MyBatis多数据源基本上就配置好了，接下来只需要在org.sang.mybatis.mapper1和org.sang.mybatis.mapper2包中提供不同的Mapper，Service中注入不同的Mapper就可以操作不同的数据源。 mapper创建org.sang.mybatis.mapper1中的mapper： 123public interface UserMapperOne { List&lt;User&gt; getAllUser();} 对应的XML文件： 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"org.sang.mybatis.mapper1.UserMapperOne\"&gt; &lt;select id=\"getAllUser\" resultType=\"org.sang.mybatis.model.User\"&gt; select * from t_user; &lt;/select&gt;&lt;/mapper&gt; org.sang.mybatis.mapper2中的mapper： 123public interface UserMapper { List&lt;User&gt; getAllUser();} 对应的XML文件： 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"org.sang.mybatis.mapper2.UserMapper\"&gt; &lt;select id=\"getAllUser\" resultType=\"org.sang.mybatis.model.User\"&gt; select * from t_user; &lt;/select&gt;&lt;/mapper&gt; 接下来，在Service中注入两个不同的Mapper，不同的Mapper将操作不同的数据源。 好了，欢迎加入我的星球，关于我的星球【Java达摩院】，大伙可以参考这篇文章推荐一个技术圈子，Java技能提升就靠它了.","link":"/2019/0407/mybatis-multi.html"},{"title":"起早贪黑几个月，我写完了人生第一本书！","text":"今天有小伙伴在网上问了我一个问题：写书的整个过程是什么感受？想想我好像还没各位小伙伴聊过我写书的故事，只是在书出版后做过一次送书活动，其他的好像就没分享啥了，今天我想借这个机会和大伙聊聊我写书的故事，也希望我的经验能帮助到各位小伙伴。 1.缘起故事得从我大学毕业时候说起啦。大四第一学期忙着准备考研，错过了秋招，然而研究生也没考上，过完年研究生考试成绩出来后，一看不行就赶紧出来找工作，西北农村娃，不敢耗也耗不起。大学所在的城市高校不多，所以春招的时候我回到了老家西安参加春招，花了一个多礼拜，拿了三个offer，感觉差不多了又急匆匆返回学校，返校后，回忆找工作的过程，有得意也有失落，得意的是没想到找工作这么顺利，失落的是想去的公司没去成，我的学校还被一些中等公司鄙视了。 我本科学位是管理学学士，计算机是我从大二开始自学的，自学了JavaEE和Android，当时找工作时候，招Android的，招Java的我都去面，3个offer有两个是Java，一个Android。虽然找工作整体上感觉不错，不过还是有一些不如意的地方，有一个超级大的大厂，过了笔试，也过了两轮技术面，止步于最后一轮人事面，这算是找工作期间最遗憾的一件事了。也有一些不怎么大的厂，却歧视我的学校（某末流211），这让我忿忿不平，但是学校也没法改变，思来想去，决定写博客，提高自己的技术影响力，弥补专业和学校的不足，就这样，在CSDN上注册了博客账号，当年4月15号发表了第一篇博客，从此打开了一扇新大门。 博客写了一段时间，CSDN的运营梦鸽美女邀请我申请博客专家，有了title写的就更有劲了。写博客的过程中，感觉自身的技术也在不断的提高，因为刚开始学一个新技术点的时候，很多东西没太关注，只会用，没细究，写博客则是一个整理的过程，是自己思维一个锻炼的过程，博客写完了，感觉对相关知识点的认知又上升了。 刚开始写的时候，博客的访问量并不高，好在我当时也是刚毕业，不着急，就慢慢的写着，就这样，第二年刚过完年就开始有人找我写书，被我婉拒了，我的理由是刚刚毕业半年，实在没啥好写的，也不知道该写啥。不过我却发现写书好像没那么难，好像很容易，因为竟然有人找我写书。再之后，隔三差五就会有出版社的编辑找来了，电子工业出版社、人民邮电出版社、清华大学出版社等等，不过我自己从来没下定决心，虽然心里也有想法，但是总觉得还差点火候。 2018年刚过完年，那时候我搞Spring Boot+Vue也有一段时间了，自我感觉积累了一点点料，有种想和大伙分享的欲望，另一方面也觉得该为自己的职业生涯留下一点东西，不能就这么默默无闻的搬一辈子砖，在认真考虑后，决定写一本Spring Boot相关的书，刚好清华社的夏老师没过几天就加了我微信，于是一拍即合。 这是写书的第一步，先有技术积累，有博客或者公众号，圈子里有一点点名气，就会有出版社的编辑找来，因为出版社编辑比较喜欢那种在某一领域深耕多年，对相关技术有自己的看法和认识，有原创的博客，并且博客写作思路清晰，文章脉络清楚的作者。在这个阶段我觉得最难的还是坚持，写博客积累技术和名气并非一朝一夕的事，有一些超级大牛，抓住了技术的风口一下就积累了很多的关注，刚入行的小辈看到这些大牛的博客，感觉达到这样的高度太难了，所以放弃了。其实很多时候，你不用成为执牛耳的大牛，成为一个小小的小牛，就够了。 这一阶段，总结两个字：坚持。 2.写作在答应了出版社的邀请之后，就着手开始准备了。在刚开始答应的时候，需要提交一个图书选题单给出版社的老师，选题单中会列出书名，章节，作者等信息。 选题定下来之后，先和出版社签订出版合同，合同中会约定图书字数、作者、稿费计算方式等，签好合同后，和出版社的事情暂时就先告一段落了。 接下来就开始写了，细化每个章节的目录，每章大概写多少，准备写哪些内容，提纲细化之后，后面基本就不动了，主要是填内容进去。写书和写博客不一样，博客，我只需要介绍某一个知识点，解决某一个问题就行了，写书，不仅要介绍知识点解决问题，还要讲究知识点的全面，不能有遗漏，很多东西，我们可能经常用某一种方式实现，但实际上换一种方式也能实现，但是你可能就不知道，关键是你并不知道他还有另一种实现方式，这就很累了，为了不遗漏知识点，只能把官方文档反复看。有的时候卡在某一个技术点上，上班时候脑子里都是相关问题，一有解决思路就赶紧先记下来，回家后赶紧尝试。在写书之前，我在公众号上已经陆续发了Redis系列教程、MongoDB系列教程、Spring Cloud系列教程以及Git系列教程等，因此在写Spring Boot时，遇到这几方面的问题基本上都能得心应手，也算是前期准备比较充分吧（其实写这些教程的时候压根就没想到写书的事，但是掌握了，写出来的技术，总会在某一天发挥作用的）。 写书期间最大的挑战还不是来自技术，而是自信，有的时候写着写着甚至会怀疑自己，这书写出来有人看吗？但是合同签了，没人看也得硬着头皮写下去，而且得认真写。有时候一些出版的问题要和编辑老师沟通，沟通完后，又会信心满满，这一点，还是要感谢出版社编辑老师的鼓励。我自己因为不爱交流，很多问题喜欢自己瞎琢磨，其实很多出版方面的问题都可以和编辑及时沟通，避免给自己徒增烦恼（这个建议送给想要写书的小伙伴）。 那一段时间，我每天早上7点起床，写到8点半然后去上班，晚上6点下班后，差不多7点开始写，写到11点半，周末写两天，拒掉了大部分的社交活动，差不多就这样连续了几个月，交稿的时候有种高考考完的感觉，有的小伙伴可能觉得我是个假程序员，竟然不加班，老实说，敝司确实不怎么加班。稿子交到出版社之后，还要经过排版-&gt;编辑-&gt;改错-&gt;初审-&gt;复审-&gt;终审-&gt;发稿-&gt;申请书号、CIP-&gt;封面设计-&gt;出片-&gt;下厂印制-&gt;发样书-&gt;入库-&gt;上市销售，整个过程大约持续了三个多月。封面设计时候，出版社给了两个参考的封面，纠结了半天，后来选择了绿色的（可能有小伙伴要吐槽我的审美了）： 关于封面这里，也可以自己提一些设计思路给出版社去做，不过我最终还是选择了出版社的方案，想想民国时那些自己给自己设计图书封面的大佬，真是佩服的五体投地。关于书的定价，也是出版社给一个参考范围，作者自己选，现在技术图书的定价基本都是按照印张来的，作者选择的范围不大，除非是超级超级大牛，可能会额外照顾（我瞎猜的）。 到了2019年1月份的时候，有一天午休醒来，有个人加我微信，备注说是读者，我才发现书已经上市销售了，至此，2018年的工程，总算告一段落了，几个月起早贪黑，甚至打了退堂鼓，还好最终没有放弃，总算有了收货。 这一阶段的总结：不要怂，就是干。 3.收获图书出版后，感觉收获还是蛮大的。从以下三个方面来跟大伙聊聊： 技术首先就是技术了，写书是一个非常非常系统化的工程，虽然我以前也写过多个成体系的教程，但是感觉和写书还是有很大的不同，写书的过程，也是重新梳理自己知识体系的过程，对于以前不求甚解的东西都去认真研究了，还要想办法将一些复杂的东西写的浅显易懂，让读者容易上手。在不断的锤炼中，自己的技术也得到了极大的提高。 信心由于我并非科班出身，有幸在这个行业混口饭吃其实已经很满足了，计算机理论捉襟见肘，虽然我一直在努力弥补，但总是不够自信。这本书一定程度上让我更有信心在这个圈子里混下去。 圈子我自己平时不怎么出去玩，比较宅，线下的圈子不多，线上的圈子倒不少，但是很多人都是听其名，不知其人。书出版之后，加入的第一个圈子就是华为云享专家，在华为云组织的openday中，认识了很多大佬，很多人名字和人终于对上了，自己也收获了很多。还有一些由于时间原因被我推掉的活动，但总体感觉就是活动多了。其实这就是我自己一向所说的，提高自己才是最重要的，与其削尖了脑袋挤进某一个圈子，不如修炼内功，时间到了，该有的就有了。 4.一点建议其实经常会有一些读者在后台联系我，有刚毕业的大学生，也有在读的研究生，他们想知道在技术的道路上要如何选择，C\\C++\\Java\\前端，都会，但是却不精通，这里我给的建议就是苍蝇模式，因为我一开始也是自学的，我相信我曾经遇到的困惑也有后来者会遇到，那么什么是苍蝇模式呢？ 美国密歇根大学教授卡尔·韦克做过这样一个实验：把一群蜜蜂和一群苍蝇同时装进一个玻璃瓶里，将瓶子横着放平，让瓶底朝着光，小蜜蜂们会一刻不停地在瓶底附近飞舞，因为蜜蜂的复眼有更强的向光性，对阳光的敏感和偏执决定它们不肯接近黑暗的地方，哪怕是出口，蜜蜂一次次撞到瓶底，直到力竭而死，而苍蝇则在瓶子里乱撞，不一会儿，就能从瓶口逃之夭夭。 刚入行可以多了解、多打听、多去尝试慢慢找到适合自己的，自己喜欢的，选定了方向之后，就可以开始做技术积累了，积累可以从写博客开始，初期建议选个大平台，例如博客园、CSDN或者慕课网之类的，有了名气之后，可以考虑独立建站或者写公众号，慢慢打造个人品牌，个人品牌建立了，写书就是愿不愿意的事了。其实，事儿不难，难在坚持！","link":"/2019/0407/javaboy-book.html"},{"title":"跟着平台混了四年，现在要单飞了！","text":"我记得是2015年4月15在CSDN上发表了我的第一篇博客，是一个学习笔记，从那之后开启了我博客写作之路，到今天为止即将4年，这4年时间我在CSDN上发表的博客最多，共有372篇原创，CSDN是我的大本营，不过在这期间也有断断续续在其他公共平台上发过博客，例如 sf、博客园、掘金、慕课网等，但是都是非常零散，2016年的时候，利用我的 GitHub 也搭建了一个个人站点，但是只是试验了几个页面，并没有好好去维护，前两天清明节，一时心血来潮，花了半天时间搞了一个自己的独立博客 http://www.javaboy.org ，以后将在这个站点上和大伙分享技术。 实际上搭建一个个人站点并不费什么事，唯一的资金投入就是域名，一年也就几十块钱，其他的套用现成的技术即可，接下来我就来和大伙分享下独立博客搭建过程，给小伙伴一个参考。 准备工作博客搭建实际上现在搭建一个个人独立博客，可选方案很多，我这里用了久闻大名的 Hexo 来搭建，用 Hexo 搭建，要是有一点点前端 Node 的使用经验更佳，没有当然也没关系，因为与之相关的命令并不多。使用 Hexo 需要提前在电脑上安装好 Node 和 Git ，安装成功后，就可以开始 Hexo 的安装了。步骤如下： 安装 Hexo 1npm install -g hexo-cli 在本地创建一个博客目录 1hexo init blog 上面这个命令执行完后，会在本地创建一个 blog 目录，这里边就是独立博客所必须的一些文件，然后进入到这个目录中，执行 npm install 命令，安装相关的依赖。 安装完成后，会生成如下目录： 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 这里几个文件/文件夹，我们先来关注其中两个 _config.yml 和 themes 目录， _config.yml 文件中，我们可以做网站的一些基本配置，例如 网站的 title，描述，关键字、图标等，这些配置大都见名知意。如下： 配置完成后，定位到 blog 目录，执行 hexo s 就可以在本地启动项目了，启动成功后，浏览器中输入 http://localhost:4000 就可以看到网站了。 说到 hexo s 命令，这里有几个常用命令需要给大家介绍下，分别是： 命令 简写 中文含义 hexo server hexo s 本地启动 hexo generate hexo g 生成静态文件 hexo deploy hexo d 部署网站 hexo clean 清除缓存和已经生成的静态文件 这四个算是松哥这两天使用最多的命令，其他的命令，大伙可以参考这里。 修改主题一般来说，主题都会自己配置一个，个人感觉 Hexo 的生态还是比较丰富的，有很多可选的主题，Hexo 默认使用的主题是 landscape ，我这里使用了 hexo-theme-next 主题。博客在本地跑起来之后，接下来就是修改主题，主题修改的第一步就是先选一个自己认为好看的主题，选好之后，首先将之克隆到 ./themes 目录下，这个目录下原本有有一个 landscape 文件夹，里边放的默认的样式，当然开发者也可以直接将主题文件下载好拷贝进来，但是我还是建议使用 clone ，使用 clone ，假如有一天这个主题更新了，只需要 pull 一下就可以获取到最新样式了。 以 hexo-theme-next 主题为例， clone 命令如下： 12cd your-hexo-sitegit clone https://github.com/iissnan/hexo-theme-next themes/next 克隆成功后，修改 hexo 的 _config.yml 文件，将主题修改为 next，如下： 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 主题创建好之后，接下来就是对主题的配置了，这个比较容易，直接参考官方文档即可。配置完成后，执行如下命令，即可看到新的主题效果： 123hexo cleanhexo ghexo s 命令含义可以参考上面的表格，这里不再赘述。 绑定到 GitHub大家可能已经迫不及待想要把博客上传到 GitHub 了，绑定到 Github 步骤也很简单，首先以 自己的GitHub ID.github.io 为名创建一个 public 仓库，例如我的 ID 为 lenve，创建的仓库如下： 创建成功之后，修改 hexo 的 _config.yml 文件，配置 GitHub 地址，如下： 1234deploy: type: git repo: git@github.com:lenve/lenve.github.io.git branch: master 这里根据自己的地址来配置即可，配置完成后，执行如下命令： 12hexo ghexo d 执行完成后，就可以将数据上传到 GitHub 了（当然这里需要大家提前配置一下 GitHub 的公钥，具体可以参考Git关联远程仓库）。 上传成功后，访问 https://lenve.github.io 就可以看到自己的个人站点了。 如果你对 GitHub 提供的域名不满意，也可以自己申请一个域名，分分钟就配置好了。 域名申请域名申请建议使用国外的域名提供商，不用备案（一个字，快！不用等），松哥使用了 godaddy ，主要是因为这个服务商支持支付宝付款，域名申请就比较容易了，无需多说。 域名和 GitHub 绑定域名申请成功之后，接下来的配置，也分为两部分。 GitHub 配置首先在博客所在目录下的 source 目录中，创建一个 CNAME 文件，文件内容就是你的域名，如下： 然后执行 hexo d 命令将这个文件上传到 GitHub就可以了。 在网上看到有人直接在 GitHub 上配置这个，如下图： 这种方式也可以，这种方式会自动生成一个CNAME文件到当前仓库中，但是松哥在这里不推荐大家使用这种方式，因为如果你在本地执行了 hexo clean ，然后再去上传，就会丢失掉 CNAME 文件，然后又得重新配置。 域名解析配置域名解析这块，当时遇到了一些问题，因为是在清明节假期，也没法联系客服，后来松哥使用了 DNSPod 去做域名解析了，没有使用 godaddy 提供的域名解析。所以首先要做的，就是修改 godaddy 提供的域名解析服务，登录自己的 godaddy 账号，找到域名管理，修改域名解析服务为 DNSPod ，如下： 然后登录到 DNSPod（没有账号注册一个），然后添加自己的域名解析，如下图： 添加两条 A 记录，指向 GitHub 的 IP 地址，再添加一条 CNAME ，指向你的 GitHub 域名就可以了。 如此之后，大功告成！ 总结因为是第一次做，比较顺利，也很简单，不用花很多钱，就是一个域名的费用而已，不需要额外买服务器，hexo 的使用也很简单，有兴趣小伙伴赶快实践下吧！","link":"/2019/0411/hexo-install.html"},{"title":"这一次，我连 web.xml 都不要了，纯 Java 搭建 SSM 环境","text":"在 Spring Boot 项目中，正常来说是不存在 XML 配置，这是因为 Spring Boot 不推荐使用 XML ，注意，并非不支持，Spring Boot 推荐开发者使用 Java 配置来搭建框架，Spring Boot 中，大量的自动化配置都是通过 Java 配置来实现的，这一套实现方案，我们也可以自己做，即自己也可以使用纯 Java 来搭建一个 SSM 环境，即在项目中，不存在任何 XML 配置，包括 web.xml 。 环境要求： 使用纯 Java 来搭建 SSM 环境，要求 Tomcat 的版本必须在 7 以上。 快速体验1 创建工程创建一个普通的 Maven 工程（注意，这里可以不必创建 Web 工程），并添加 SpringMVC 的依赖，同时，这里环境的搭建需要用到 Servlet ，所以我们还需要引入 Servlet 的依赖（一定不能使用低版本的 Servlet），最终的 pom.xml 文件如下： 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 2 添加 Spring 配置工程创建成功之后，首先添加 Spring 的配置文件，如下： 1234@Configuration@ComponentScan(basePackages = \"org.javaboy\", useDefaultFilters = true, excludeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION, classes = Controller.class)})public class SpringConfig {} 关于这个配置，我说如下几点： @Configuration 注解表示这是一个配置类，在我们这里，这个配置的作用类似于 applicationContext.xml @ComponentScan 注解表示配置包扫描，里边的属性和 xml 配置中的属性都是一一对应的，useDefaultFilters 表示使用默认的过滤器，然后又除去 Controller 注解，即在 Spring 容器中扫描除了 Controller 之外的其他所有 Bean 。 3 添加 SpringMVC 配置接下来再来创建 springmvc 的配置文件： 1234@Configuration@ComponentScan(basePackages = \"org.javaboy\",useDefaultFilters = false,includeFilters = {@ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)})public class SpringMVCConfig {} 注意，如果不需要在 SpringMVC 中添加其他的额外配置，这样就可以了。即 视图解析器、JSON 解析、文件上传……等等，如果都不需要配置的话，这样就可以了。 4 配置 web.xml此时，我们并没有 web.xml 文件，这时，我们可以使用 Java 代码去代替 web.xml 文件，这里会用到 WebApplicationInitializer ，具体定义如下： 12345678910111213public class WebInit implements WebApplicationInitializer { public void onStartup(ServletContext servletContext) throws ServletException { //首先来加载 SpringMVC 的配置文件 AnnotationConfigWebApplicationContext ctx = new AnnotationConfigWebApplicationContext(); ctx.register(SpringMVCConfig.class); // 添加 DispatcherServlet ServletRegistration.Dynamic springmvc = servletContext.addServlet(\"springmvc\", new DispatcherServlet(ctx)); // 给 DispatcherServlet 添加路径映射 springmvc.addMapping(\"/\"); // 给 DispatcherServlet 添加启动时机 springmvc.setLoadOnStartup(1); }} WebInit 的作用类似于 web.xml，这个类需要实现 WebApplicationInitializer 接口，并实现接口中的方法，当项目启动时，onStartup 方法会被自动执行，我们可以在这个方法中做一些项目初始化操作，例如加载 SpringMVC 容器，添加过滤器，添加 Listener、添加 Servlet 等。 注意： 由于我们在 WebInit 中只是添加了 SpringMVC 的配置，这样项目在启动时只会去加载 SpringMVC 容器，而不会去加载 Spring 容器，如果一定要加载 Spring 容器，需要我们修改 SpringMVC 的配置，在 SpringMVC 配置的包扫描中也去扫描 @Configuration 注解，进而加载 Spring 容器，还有一种方案可以解决这个问题，就是直接在项目中舍弃 Spring 配置，直接将所有配置放到 SpringMVC 的配置中来完成，这个在 SSM 整合时是没有问题的，在实际开发中，较多采用第二种方案，第二种方案，SpringMVC 的配置如下： 1234@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig {} 这种方案中，所有的注解都在 SpringMVC 中扫描，采用这种方案的话，则 Spring 的配置文件就可以删除了。 5 测试最后，添加一个 HelloController ，然后启动项目进行测试： 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} 启动项目，访问接口，结果如下： 6 其他配置6.1 静态资源过滤静态资源过滤在 SpringMVC 的 XML 中的配置如下： 1&lt;mvc:resources mapping=\"/**\" location=\"/\"/&gt; 在 Java 配置的 SSM 环境中，如果要配置静态资源过滤，需要让 SpringMVC 的配置继承 WebMvcConfigurationSupport ，进而重写 WebMvcConfigurationSupport 中的方法，如下： 12345678@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/js/**\").addResourceLocations(\"classpath:/\"); }} 重写 addResourceHandlers 方法，在这个方法中配置静态资源过滤，这里我将静态资源放在 resources 目录下，所以资源位置是 classpath:/ ，当然，资源也可以放在 webapp 目录下，此时只需要修改配置中的资源位置即可。如果采用 Java 来配置 SSM 环境，一般来说，可以不必使用 webapp 目录，除非要使用 JSP 做页面模板，否则可以忽略 webapp 目录。 6.2 视图解析器在 XML 文件中，通过如下方式配置视图解析器： 1234&lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt;&lt;/bean&gt; 如果通过 Java 类，一样也可以实现类似功能。 首先为我们的项目添加 webapp 目录，webapp 目录中添加一个 jsp 目录，jsp 目录中添加 jsp 文件： 然后引入 JSP 的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet.jsp-api&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt;&lt;/dependency&gt; 然后，在配置类中，继续重写方法： 12345678@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void configureViewResolvers(ViewResolverRegistry registry) { registry.jsp(\"/jsp/\", \".jsp\"); }} 接下来，在 Controller 中添加控制器即可访问 JSP 页面： 1234567@Controllerpublic class HelloController2 { @GetMapping(\"/hello2\") public String hello() { return \"hello\"; }} 6.3 路径映射有的时候，我们的控制器的作用仅仅只是一个跳转，就像上面小节中的控制器，里边没有任何业务逻辑，像这种情况，可以不用定义方法，可以直接通过路径映射来实现页面访问。如果在 XML 中配置路径映射，如下： 1&lt;mvc:view-controller path=\"/hello\" view-name=\"hello\" status-code=\"200\"/&gt; 这行配置，表示如果用户访问 /hello 这个路径，则直接将名为 hello 的视图返回给用户，并且响应码为 200，这个配置就可以替代 Controller 中的方法。 相同的需求，如果在 Java 代码中，写法如下： 12345678@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(\"/hello3\").setViewName(\"hello\"); }} 此时，用户访问 /hello3 接口，就能看到名为 hello 的视图文件。 6.4 JSON 配置SpringMVC 可以接收JSON 参数，也可以返回 JSON 参数，这一切依赖于 HttpMessageConverter。 HttpMessageConverter 可以将一个 JSON 字符串转为 对象，也可以将一个对象转为 JSON 字符串，实际上它的底层还是依赖于具体的 JSON 库。 所有的 JSON 库要在 SpringMVC 中自动返回或者接收 JSON，都必须提供和自己相关的 HttpMessageConverter 。 SpringMVC 中，默认提供了 Jackson 和 gson 的 HttpMessageConverter ，分别是：MappingJackson2HttpMessageConverter 和 GsonHttpMessageConverter 。 正因为如此，我们在 SpringMVC 中，如果要使用 JSON ，对于 jackson 和 gson 我们只需要添加依赖，加完依赖就可以直接使用了。具体的配置是在 AllEncompassingFormHttpMessageConverter 类中完成的。 如果开发者使用了 fastjson，那么默认情况下，SpringMVC 并没有提供 fastjson 的 HttpMessageConverter ，这个需要我们自己提供，如果是在 XML 配置中，fastjson 除了加依赖，还要显式配置 HttpMessageConverter，如下： 123456&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=\"com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter\"&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; 在 Java 配置的 SSM 中，我们一样也可以添加这样的配置： 12345678910111213@Configuration@ComponentScan(basePackages = \"org.javaboy\")public class SpringMVCConfig extends WebMvcConfigurationSupport { @Override protected void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); converter.setDefaultCharset(Charset.forName(\"UTF-8\")); FastJsonConfig fastJsonConfig = new FastJsonConfig(); fastJsonConfig.setCharset(Charset.forName(\"UTF-8\")); converter.setFastJsonConfig(fastJsonConfig); converters.add(converter); }} 然后，就可以在接口中直接返回 JSON 了，此时的 JSON 数据将通过 fastjson 生成。 总结好了，本文通过一个简单的例子向读者展示了使用 Java 来配置 Spring+SpringMVC 环境，事实上，只要这两个配置 OK ，再加入 MyBatis 就是非常容易的事了，本文相关的案例松哥已经上传到 GitHub 上了：https://github.com/lenve/javaboy-code-samples。 关于本文，有问题欢迎留言讨论。","link":"/2019/0528/javassm.html"},{"title":"极简 Spring Boot 整合 Thymeleaf 页面模板","text":"虽然现在慢慢在流行前后端分离开发，但是据松哥所了解到的，还是有一些公司在做前后端不分的开发，而在前后端不分的开发中，我们就会需要后端页面模板（实际上，即使前后端分离，也会在一些场景下需要使用页面模板，例如邮件发送模板）。 早期的 Spring Boot 中还支持使用 Velocity 作为页面模板，现在的 Spring Boot 中已经不支持 Velocity 了，页面模板主要支持 Thymeleaf 和 Freemarker ，当然，作为 Java 最最基本的页面模板 Jsp ，Spring Boot 也是支持的，只是使用比较麻烦。 松哥打算用三篇文章分别向大家介绍一下这三种页面模板技术。 今天我们主要来看看 Thymeleaf 在 Spring Boot 中的整合！ Thymeleaf 简介Thymeleaf 是新一代 Java 模板引擎，它类似于 Velocity、FreeMarker 等传统 Java 模板引擎，但是与传统 Java 模板引擎不同的是，Thymeleaf 支持 HTML 原型。 它既可以让前端工程师在浏览器中直接打开查看样式，也可以让后端工程师结合真实数据查看显示效果，同时，SpringBoot 提供了 Thymeleaf 自动化配置解决方案，因此在 SpringBoot 中使用 Thymeleaf 非常方便。 事实上， Thymeleaf 除了展示基本的 HTML ，进行页面渲染之外，也可以作为一个 HTML 片段进行渲染，例如我们在做邮件发送时，可以使用 Thymeleaf 作为邮件发送模板。 另外，由于 Thymeleaf 模板后缀为 .html，可以直接被浏览器打开，因此，预览时非常方便。 整合 创建项目 Spring Boot 中整合 Thymeleaf 非常容易，只需要创建项目时添加 Thymeleaf 即可： 创建完成后，pom.xml 依赖如下： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 当然，Thymeleaf 不仅仅能在 Spring Boot 中使用，也可以使用在其他地方，只不过 Spring Boot 针对 Thymeleaf 提供了一整套的自动化配置方案，这一套配置类的属性在 org.springframework.boot.autoconfigure.thymeleaf.ThymeleafProperties 中，部分源码如下： 1234567891011121314@ConfigurationProperties(prefix = \"spring.thymeleaf\")public class ThymeleafProperties { private static final Charset DEFAULT_ENCODING = StandardCharsets.UTF_8; public static final String DEFAULT_PREFIX = \"classpath:/templates/\"; public static final String DEFAULT_SUFFIX = \".html\"; private boolean checkTemplate = true; private boolean checkTemplateLocation = true; private String prefix = DEFAULT_PREFIX; private String suffix = DEFAULT_SUFFIX; private String mode = \"HTML\"; private Charset encoding = DEFAULT_ENCODING; private boolean cache = true; //...} 首先通过 @ConfigurationProperties 注解，将 application.properties 前缀为 spring.thymeleaf 的配置和这个类中的属性绑定。 前三个 static 变量定义了默认的编码格式、视图解析器的前缀、后缀等。 从前三行配置中，可以看出来，Thymeleaf 模板的默认位置在 resources/templates 目录下，默认的后缀是 html 。 这些配置，如果开发者不自己提供，则使用 默认的，如果自己提供，则在 application.properties 中以 spring.thymeleaf 开始相关的配置。 而我们刚刚提到的，Spring Boot 为 Thymeleaf 提供的自动化配置类，则是 org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration ，部分源码如下： 123456@Configuration@EnableConfigurationProperties(ThymeleafProperties.class)@ConditionalOnClass({ TemplateMode.class, SpringTemplateEngine.class })@AutoConfigureAfter({ WebMvcAutoConfiguration.class, WebFluxAutoConfiguration.class })public class ThymeleafAutoConfiguration {} 可以看到，在这个自动化配置类中，首先导入 ThymeleafProperties ，然后 @ConditionalOnClass 注解表示当当前系统中存在 TemplateMode 和 SpringTemplateEngine 类时，当前的自动化配置类才会生效，即只要项目中引入了 Thymeleaf 相关的依赖，这个配置就会生效。 这些默认的配置我们几乎不需要做任何更改就可以直接使用了。如果开发者有特殊需求，则可以在 application.properties 中配置以 spring.thymeleaf 开头的属性即可。 创建 Controller 接下来我们就可以创建 Controller 了，实际上引入 Thymeleaf 依赖之后，我们可以不做任何配置。新建的 IndexController 如下： 12345678910111213141516171819202122@Controllerpublic class IndexController { @GetMapping(\"/index\") public String index(Model model) { List&lt;User&gt; users = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { User u = new User(); u.setId((long) i); u.setName(\"javaboy:\" + i); u.setAddress(\"深圳:\" + i); users.add(u); } model.addAttribute(\"users\", users); return \"index\"; }}public class User { private Long id; private String name; private String address; //省略 getter/setter} 在 IndexController 中返回逻辑视图名+数据，逻辑视图名为 index ，意思我们需要在 resources/templates 目录下提供一个名为 index.html 的 Thymeleaf 模板文件。 创建 Thymeleaf 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;编号&lt;/td&gt; &lt;td&gt;用户名&lt;/td&gt; &lt;td&gt;地址&lt;/td&gt; &lt;/tr&gt; &lt;tr th:each=\"user : ${users}\"&gt; &lt;td th:text=\"${user.id}\"&gt;&lt;/td&gt; &lt;td th:text=\"${user.name}\"&gt;&lt;/td&gt; &lt;td th:text=\"${user.address}\"&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 在 Thymeleaf 中，通过 th:each 指令来遍历一个集合，数据的展示通过 th:text 指令来实现， 注意 index.html 最上面要引入 thymeleaf 名称空间。 配置完成后，就可以启动项目了，访问 /index 接口，就能看到集合中的数据了： 另外，Thymeleaf 支持在 js 中直接获取 Model 中的变量。例如，在 IndexController 中有一个变量 username ： 12345678@Controllerpublic class IndexController { @GetMapping(\"/index\") public String index(Model model) { model.addAttribute(\"username\", \"李四\"); return \"index\"; }} 在页面模板中，可以直接在 js 中获取到这个变量： 1234&lt;script th:inline=\"javascript\"&gt; var username = [[${username}]]; console.log(username)&lt;/script&gt; 这个功能算是 Thymeleaf 的特色之一吧。 手动渲染前面我们说的是返回一个 Thymeleaf 模板，我们也可以手动渲染 Thymeleaf 模板，这个一般在邮件发送时候有用，例如我在 resources/templates 目录下新建一个邮件模板，如下： 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;hello 欢迎 &lt;span th:text=\"${username}\"&gt;&lt;/span&gt;加入 XXX 集团，您的入职信息如下：&lt;/p&gt;&lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;职位&lt;/td&gt; &lt;td th:text=\"${position}\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;薪水&lt;/td&gt; &lt;td th:text=\"${salary}\"&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;img src=\"http://www.javaboy.org/images/sb/javaboy.jpg\" alt=\"\"&gt;&lt;/body&gt;&lt;/html&gt; 这一个 HTML 模板中，有几个变量，我们要将这个 HTML 模板渲染成一个 String 字符串，再把这个字符串通过邮件发送出去，那么如何手动渲染呢？ 1234567891011@AutowiredTemplateEngine templateEngine;@Testpublic void test1() throws MessagingException { Context context = new Context(); context.setVariable(\"username\", \"javaboy\"); context.setVariable(\"position\", \"Java工程师\"); context.setVariable(\"salary\", 99999); String mail = templateEngine.process(\"mail\", context); //省略邮件发送} 渲染时，我们需要首先注入一个 TemplateEngine 对象，这个对象就是在 Thymeleaf 的自动化配置类中配置的（即当我们引入 Thymeleaf 的依赖之后，这个实例就有了）。 然后构造一个 Context 对象用来存放变量。 调用 process 方法进行渲染，该方法的返回值就是渲染后的 HTML 字符串，然后我们将这个字符串发送出去。 这是 Spring Boot 整合 Thymeleaf 的几个关键点，关于 Thymeleaf 这个页面模板本身更多的用法，大家可以参考 Thymeleaf 的文档：https://www.thymeleaf.org。 总结本文主要向大家简单介绍了 Spring Boot 和 Thymeleaf 整合时的几个问题，还是比较简单的，大家可以阅读 Thymeleaf 官方文档学习 Thymeleaf 的更多用法。本文案例我已上传到 GitHub ，欢迎大家 star :https://github.com/lenve/javaboy-code-samples 关于本文，有问题欢迎留言讨论。","link":"/2019/0613/springboot-thymeleaf.html"}],"tags":[{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"JavaWeb","slug":"JavaWeb","link":"/tags/JavaWeb/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"MyBatis","slug":"MyBatis","link":"/tags/MyBatis/"},{"name":"Jedis","slug":"Jedis","link":"/tags/Jedis/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/tags/Spring-Boot/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"Shiro","slug":"Shiro","link":"/tags/Shiro/"},{"name":"yaml","slug":"yaml","link":"/tags/yaml/"},{"name":"Spring Data","slug":"Spring-Data","link":"/tags/Spring-Data/"},{"name":"CORS","slug":"CORS","link":"/tags/CORS/"},{"name":"Jpa","slug":"Jpa","link":"/tags/Jpa/"},{"name":"JdbcTemplate","slug":"JdbcTemplate","link":"/tags/JdbcTemplate/"},{"name":"Spring Security","slug":"Spring-Security","link":"/tags/Spring-Security/"},{"name":"Swagger2","slug":"Swagger2","link":"/tags/Swagger2/"},{"name":"杂谈","slug":"杂谈","link":"/tags/杂谈/"},{"name":"SpringMVC","slug":"SpringMVC","link":"/tags/SpringMVC/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"Ehcache","slug":"Ehcache","link":"/tags/Ehcache/"},{"name":"学习资源","slug":"学习资源","link":"/tags/学习资源/"},{"name":"JWT","slug":"JWT","link":"/tags/JWT/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"Thymeleaf","slug":"Thymeleaf","link":"/tags/Thymeleaf/"}],"categories":[{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"JavaWeb","slug":"JavaWeb","link":"/categories/JavaWeb/"},{"name":"Redis","slug":"Redis","link":"/categories/Redis/"},{"name":"MyBatis","slug":"MyBatis","link":"/categories/MyBatis/"},{"name":"Nginx","slug":"Nginx","link":"/categories/Nginx/"},{"name":"前后端分离","slug":"前后端分离","link":"/categories/前后端分离/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/categories/Spring-Boot/"},{"name":"杂谈","slug":"杂谈","link":"/categories/杂谈/"},{"name":"SpringMVC","slug":"SpringMVC","link":"/categories/SpringMVC/"},{"name":"学习资源","slug":"学习资源","link":"/categories/学习资源/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"hexo","slug":"hexo","link":"/categories/hexo/"}]}